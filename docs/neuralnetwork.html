<!DOCTYPE html>
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>Chương 17 Mô hình mạng nơ-ron | Khoa Học Dữ Liệu trong Kinh tế và Kinh doanh</title>
<meta name="author" content="Nguyễn Quang Huy">
<meta name="description" content="Chương sách này thảo luận một chủ đề quan trọng có ứng dụng rộng rãi nhất trong lĩnh vực trí tuệ nhân tạo là mô hình mạng học sâu (deep learning). Tại thời điểm nhóm tác giả viết cuốn sách (2023),...">
<meta name="generator" content="bookdown 0.37 with bs4_book()">
<meta property="og:title" content="Chương 17 Mô hình mạng nơ-ron | Khoa Học Dữ Liệu trong Kinh tế và Kinh doanh">
<meta property="og:type" content="book">
<meta property="og:description" content="Chương sách này thảo luận một chủ đề quan trọng có ứng dụng rộng rãi nhất trong lĩnh vực trí tuệ nhân tạo là mô hình mạng học sâu (deep learning). Tại thời điểm nhóm tác giả viết cuốn sách (2023),...">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Chương 17 Mô hình mạng nơ-ron | Khoa Học Dữ Liệu trong Kinh tế và Kinh doanh">
<meta name="twitter:description" content="Chương sách này thảo luận một chủ đề quan trọng có ứng dụng rộng rãi nhất trong lĩnh vực trí tuệ nhân tạo là mô hình mạng học sâu (deep learning). Tại thời điểm nhóm tác giả viết cuốn sách (2023),...">
<!-- JS --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script><script src="https://kit.fontawesome.com/6ecbd6c532.js" crossorigin="anonymous"></script><script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="libs/bootstrap-4.6.0/bootstrap.min.css" rel="stylesheet">
<script src="libs/bootstrap-4.6.0/bootstrap.bundle.min.js"></script><link href="libs/Lora-0.4.8/font.css" rel="stylesheet">
<link href="https://fonts.googleapis.com/css2?family=Source%20Code%20Pro:wght@400;450&amp;display=swap" rel="stylesheet">
<script src="libs/bs3compat-0.6.1/transition.js"></script><script src="libs/bs3compat-0.6.1/tabs.js"></script><script src="libs/bs3compat-0.6.1/bs3compat.js"></script><link href="libs/bs4_book-1.0.0/bs4_book.css" rel="stylesheet">
<script src="libs/bs4_book-1.0.0/bs4_book.js"></script><script src="libs/kePrint-0.0.1/kePrint.js"></script><link href="libs/lightable-0.0.1/lightable.css" rel="stylesheet">
<script src="libs/htmlwidgets-1.6.4/htmlwidgets.js"></script><script src="libs/d3-bundle-5.16.0/d3-bundle.min.js"></script><script src="libs/d3-lasso-0.0.5/d3-lasso.min.js"></script><script src="libs/save-svg-as-png-1.4.17/save-svg-as-png.min.js"></script><script src="libs/flatbush-4.0.0/flatbush.min.js"></script><link href="libs/ggiraphjs-0.4.6/ggiraphjs.min.css" rel="stylesheet">
<script src="libs/ggiraphjs-0.4.6/ggiraphjs.min.js"></script><script src="libs/girafe-binding-0.8.8/girafe.js"></script><script src="libs/plotly-binding-4.10.3/plotly.js"></script><script src="libs/typedarray-0.1/typedarray.min.js"></script><link href="libs/crosstalk-1.2.1/css/crosstalk.min.css" rel="stylesheet">
<script src="libs/crosstalk-1.2.1/js/crosstalk.min.js"></script><link href="libs/plotly-htmlwidgets-css-2.11.1/plotly-htmlwidgets.css" rel="stylesheet">
<script src="libs/plotly-main-2.11.1/plotly-latest.min.js"></script><script>
        $(function() {
            $("#toc h2").html("TRONG CHƯƠNG NÀY"); // Change text for "View source"
        });
    </script><script>
        document.addEventListener('DOMContentLoaded', function() {
            var viewSourceElement = document.getElementById('book-source');
            var editPageElement = document.getElementById('book-edit');
            var viewBookSourceElement = document.getElementById('book-repo');

            if (viewSourceElement) {
                viewSourceElement.innerText = 'Xem nguồn trang'; // Change text for "View source"
            }

            if (editPageElement) {
                editPageElement.innerText = 'Chỉnh sửa trang'; // Change text for "Edit this page"
            }

            if (viewBookSourceElement) {
                viewBookSourceElement.innerText = 'Xem nguồn sách'; // Change text for "View book source"
            }
        });
    </script><script>
        document.addEventListener('DOMContentLoaded', (event) => {
          var tocElement = document.querySelector('nav[aria-label="Table of contents"] h2');
          if (tocElement) {
            tocElement.textContent = 'Mục lục'; // Replace 'Mục lục' (Table of contents) with your desired text
          }
        });
    </script><script>
      document.addEventListener('DOMContentLoaded', (event) => {
        var tocElement = document.querySelector('nav[aria-label="Table of contents"] h2');
        if (tocElement) {
          tocElement.textContent = 'NỘI DUNG CUỐN SÁCH'; // Ensure "NỘI DUNG CUỐN SÁCH" text remains
        }
      });
    </script><script>
        document.addEventListener('DOMContentLoaded', (event) => {
          // Find the paragraph containing the original text and date
          const paragraph = document.querySelector('footer.bg-primary.text-light div.container div.row div.col-12.col-md-6.mt-3 p');

          if (paragraph) {
            // Extract the date using a regular expression
            const dateRegex = /(\d{4}-\d{2}-\d{2})/;
            const matches = paragraph.innerHTML.match(dateRegex);

            if (matches && matches.length > 1) {
              // Reformat the date from yyyy-mm-dd to dd/mm/yyyy
              const originalDate = matches[1];
              const [year, month, day] = originalDate.split('-');
              const newDate = `${day}/${month}/${year}`;

              // Replace the entire paragraph content with the new Vietnamese format
              paragraph.innerHTML = `Cuốn sách "${paragraph.querySelector('strong').innerText}" được viết bởi TS. ${paragraph.innerHTML.split('was written by ')[1].split('. ')[0]} - Trường Công nghệ - Đại học Kinh tế Quốc dân`;
            }
          }
        });
    </script><script>
        document.addEventListener('DOMContentLoaded', (event) => {
          // Find the paragraph containing the specific text
          const paragraphs = document.querySelectorAll('footer.bg-primary.text-light div.container div.row div.col-12.col-md-6.mt-3 p');
          paragraphs.forEach(paragraph => {
            if(paragraph.innerHTML.includes('This book was built by the <a class="text-light" href="https://bookdown.org">bookdown</a> R package.')) {
              // Replace the paragraph content
              paragraph.innerHTML = 'Cuốn sách này được viết bằng hoàn toàn bằng ngôn ngữ R. Phiên bản hiện tại được cập nhật vào ngày ${newDate}.';
            }
          });
        });
    </script><script src="local_edit.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script><!-- CSS --><style type="text/css">
    
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  </style>
<link rel="stylesheet" href="style.css">
</head>
<body data-spy="scroll" data-target="#toc">

<div class="container-fluid">
<div class="row">
  <header class="col-sm-12 col-lg-3 sidebar sidebar-book"><a class="sr-only sr-only-focusable" href="#content">Skip to main content</a>

    <div class="d-flex align-items-start justify-content-between">
      <h1>
        <a href="index.html" title="">Khoa Học Dữ Liệu trong Kinh tế và Kinh doanh</a>
      </h1>
      <button class="btn btn-outline-primary d-lg-none ml-2 mt-1" type="button" data-toggle="collapse" data-target="#main-nav" aria-expanded="true" aria-controls="main-nav"><i class="fas fa-bars"></i><span class="sr-only">Show table of contents</span></button>
    </div>

    <div id="main-nav" class="collapse-lg">
      <form role="search">
        <input id="search" class="form-control" type="search" placeholder="Search" aria-label="Search">
</form>

      <nav aria-label="Table of contents"><h2>Table of contents</h2>
        <ul class="book-toc list-unstyled">
<li><a class="" href="index.html">Lời nói đầu</a></li>
<li class="book-part">PHẦN I: GIỚI THIỆU CHUNG</li>
<li><a class="" href="gi%E1%BB%9Bi-thi%E1%BB%87u-v%E1%BB%81-cu%E1%BB%91n-s%C3%A1ch.html"><span class="header-section-number">1</span> Giới thiệu về cuốn sách</a></li>
<li><a class="" href="khdl-v%C3%A0-c%C3%A1c-kh%C3%A1i-ni%E1%BB%87m-c%C6%A1-b%E1%BA%A3n.html"><span class="header-section-number">2</span> KHDL và các khái niệm cơ bản</a></li>
<li class="book-part">PHẦN II: GIỚI THIỆU VỀ R</li>
<li><a class="" href="ki%E1%BA%BFn-th%E1%BB%A9c-r-c%C6%A1-b%E1%BA%A3n.html"><span class="header-section-number">3</span> Kiến thức R cơ bản</a></li>
<li><a class="" href="ki%E1%BA%BFn-th%E1%BB%A9c-r-c%C6%A1-b%E1%BA%A3n-1.html"><span class="header-section-number">4</span> Kiến thức R cơ bản</a></li>
<li><a class="" href="ki%E1%BA%BFn-th%E1%BB%A9c-r-n%C3%A2ng-cao.html"><span class="header-section-number">5</span> Kiến thức R nâng cao</a></li>
<li class="book-part">PHẦN III: PHÂN TÍCH DỮ LIỆU</li>
<li><a class="" href="nh%E1%BA%ADp-d%E1%BB%AF-li%E1%BB%87u-v%C3%A0o-r.html"><span class="header-section-number">6</span> Nhập dữ liệu vào R</a></li>
<li><a class="" href="ti%E1%BB%81n-x%E1%BB%AD-l%C3%BD-d%E1%BB%AF-li%E1%BB%87u.html"><span class="header-section-number">7</span> Tiền xử lý dữ liệu</a></li>
<li><a class="" href="bi%E1%BA%BFn-%C4%91%E1%BB%95i-v%C3%A0-s%E1%BA%AFp-x%E1%BA%BFp-d%E1%BB%AF-li%E1%BB%87u.html"><span class="header-section-number">8</span> Biến đổi và sắp xếp dữ liệu</a></li>
<li><a class="" href="tr%E1%BB%B1c-quan-h%C3%B3a-d%E1%BB%AF-li%E1%BB%87u.html"><span class="header-section-number">9</span> Trực quan hóa dữ liệu</a></li>
<li class="book-part">PHẦN IV: MÔ HÌNH TUYẾN TÍNH</li>
<li><a class="" href="m%C3%B4-h%C3%ACnh-h%E1%BB%93i-quy-tuy%E1%BA%BFn-t%C3%ADnh.html"><span class="header-section-number">10</span> Mô hình hồi quy tuyến tính</a></li>
<li><a class="" href="c%C3%A1c-m%C3%B4-h%C3%ACnh-c%E1%BB%99ng-t%C3%ADnh-t%E1%BB%95ng-qu%C3%A1t.html"><span class="header-section-number">11</span> Các mô hình cộng tính tổng quát</a></li>
<li><a class="" href="m%C3%B4-h%C3%ACnh-tuy%E1%BA%BFn-t%C3%ADnh-t%E1%BB%95ng-qu%C3%A1t..html"><span class="header-section-number">12</span> Mô hình tuyến tính tổng quát.</a></li>
<li><a class="" href="m%C3%B4-h%C3%ACnh-t%E1%BA%A7n-su%E1%BA%A5t---m%E1%BB%A9c-%C4%91%E1%BB%99-nghi%C3%AAm-tr%E1%BB%8Dng..html"><span class="header-section-number">13</span> Mô hình tần suất - mức độ nghiêm trọng.</a></li>
<li><a class="" href="t%C3%ADnh-to%C3%A1n-ph%C3%AD-b%E1%BA%A3o-hi%E1%BB%83m-thu%E1%BA%A7n-b%E1%BA%B1ng-m%C3%B4-h%C3%ACnh-t%E1%BA%A7n-su%E1%BA%A5t---m%E1%BB%A9c-%C4%91%E1%BB%99-nghi%C3%AAm-tr%E1%BB%8Dng..html"><span class="header-section-number">14</span> Tính toán phí bảo hiểm thuần bằng mô hình tần suất - mức độ nghiêm trọng.</a></li>
<li class="book-part">PHẦN V: MÔ HÌNH CÂY QUYẾT ĐỊNH</li>
<li><a class="" href="m%C3%B4-h%C3%ACnh-c%C3%A2y-quy%E1%BA%BFt-%C4%91%E1%BB%8Bnh.html"><span class="header-section-number">15</span> Mô hình cây quyết định</a></li>
<li><a class="" href="boosting-v%C3%A0-c%C3%A2y-quy%E1%BA%BFt-%C4%91%E1%BB%8Bnh..html"><span class="header-section-number">16</span> Boosting và cây quyết định.</a></li>
<li><a class="active" href="neuralnetwork.html"><span class="header-section-number">17</span> Mô hình mạng nơ-ron</a></li>
<li><a class="" href="neuralnetwork1.html"><span class="header-section-number">18</span> Các mạng học sâu điển hình</a></li>
<li><a class="" href="h%E1%BB%8Dc-m%C3%A1y-kh%C3%B4ng-c%C3%B3-gi%C3%A1m-s%C3%A1t.html"><span class="header-section-number">19</span> Học máy không có giám sát</a></li>
</ul>

        <div class="book-extra">
          <p><a id="book-repo" href="https://github.com/NEUKhoaToanKT/Khoa_hoc_du_lieu_trong_KTKD">View book source <i class="fab fa-github"></i></a></p>
        </div>
      </nav>
</div>
  </header><main class="col-sm-12 col-md-9 col-lg-7" id="content"><div id="neuralnetwork" class="section level1" number="17">
<h1>
<span class="header-section-number">Chương 17</span> Mô hình mạng nơ-ron<a class="anchor" aria-label="anchor" href="#neuralnetwork"><i class="fas fa-link"></i></a>
</h1>
<p>Chương sách này thảo luận một chủ đề quan trọng có ứng dụng rộng rãi nhất trong lĩnh vực trí tuệ nhân tạo là mô hình mạng học sâu (deep learning). Tại thời điểm nhóm tác giả viết cuốn sách (2023), học sâu là một lĩnh vực nghiên cứu tích cực nhất không chỉ trong khoa học máy tính, công nghệ thông tin mà còn cả trong các lĩnh vực khác như kinh tế, tài chính, y tế, xây dựng,… Nền tảng của mô hình mạng học sâu là mô hình mạng nơ-ron (hay neural network). Mô hình mạng nơ-ron đã được biết đến đến rộng rãi vào cuối những năm 1980 bởi cách vận hành của mô hình mô tả lại cách thức mà hệ thống thần kinh của con người xử lý thông tin. Mặc dù các đặc tính của mô hình mạng nơ-ron được phân tích bởi những nhà toán học và nhà thống kê nhiều thuật toán liên quan đến mô hình này đã được cải thiện với sự ra đời của các phương pháp học máy khác như SVM, rừng ngẫu nhiên, học tăng cường,…, mà mô hình mạng nơ-ron phần nào không được ưa chuộng.</p>
<p>Từ những năm 2010, với nhu cầu xử lý các dữ liệu ngày càng phức tạp và sự ra đời của các kiến trúc máy tính lớn, mô hình mạng nơ-ron đã quay trở lại với tên mới là mạng học sâu (deep learning). Mạng học sâu vượt trội hoàn toàn các mô hình học máy thông thường trong phân loại hình ảnh/video và mô hình hóa ngôn ngữ tự nhiên bao gồm dữ liệu kiểu văn bản và giọng nói (natural langugue processing hay NLP). Các nhà khoa học trong lĩnh vực này tin rằng lý do chính cho những thành công của mô hình mạng nơ-ron là càng ngày những người xây dựng mô hình càng chú trọng vào xây dựng các bộ dữ liệu khổng lồ để huấn luyện môn hình và cấu trúc của mô hình cho phép nó đáp ứng được với bất kỳ tập kích thước dữ liệu nào.</p>
<div id="nnonelayer" class="section level2" number="17.1">
<h2>
<span class="header-section-number">17.1</span> Mạng nơ-rơn có một lớp ẩn<a class="anchor" aria-label="anchor" href="#nnonelayer"><i class="fas fa-link"></i></a>
</h2>
<p>Mô hình mạng nơ-ron lấy một véc-tơ đầu vào gồm <span class="math inline">\(p\)</span> biến <span class="math inline">\(\textbf{X} = (X_1, X_2, \cdots , X_p)\)</span> và xây dựng một hàm phi tuyến <span class="math inline">\(\hat{f}\)</span> để dự đoán biến mục tiêu <span class="math inline">\(Y\)</span> . Chúng ta đã xây dựng các mô hình dự đoán phi tuyến trong các chương trước, ví dụ như mô hình cộng tính tổng quát, mô hình cây quyết định, mô hình rừng ngẫu nhiên, mô hình tăng cường. Điều làm nên sự khác biệt của mô hình mạng nơ-ron là cấu trúc xây dựng của mô hình. Hình <a href="neuralnetwork.html#fig:fgnn001">17.1</a> mô tả một mạng nơ-ron chuyển tiếp để mô hình biến mục tiêu <span class="math inline">\(Y\)</span> định lượng từ <span class="math inline">\(p = 3\)</span> biến giải thích là <span class="math inline">\(X_1\)</span>, <span class="math inline">\(X_2\)</span>, và <span class="math inline">\(X_3\)</span>.</p>
<div class="figure">
<span style="display:block;" id="fig:fgnn001"></span>
<img src="12-mo-hinh-mang-noron_files/figure-html/fgnn001-1.png" alt="Mô hình mạng nơ-ron có p = 3 đơn vị trong lớp đầu vào, một lớp ẩn có năm đơn vị, và một đơn vị đầu ra." width="672"><p class="caption">
Hình 17.1: Mô hình mạng nơ-ron có p = 3 đơn vị trong lớp đầu vào, một lớp ẩn có năm đơn vị, và một đơn vị đầu ra.
</p>
</div>
<p>Theo thuật ngữ chuyên môn, ba biến giải thích <span class="math inline">\(X_1\)</span>, <span class="math inline">\(X_2\)</span>, và <span class="math inline">\(X_3\)</span> là các đơn vị (unit) của lớp đầu vào (input layer). Các mũi tên được dùng để mô tả rằng mỗi đơn vị đầu vào sẽ chuyển tiếp thông tin vào <span class="math inline">\(k = 5\)</span> đơn vị của lớp ẩn (hidden layer) được ký hiệu là <span class="math inline">\(H_i\)</span> với <span class="math inline">\(i = 1, 2, \cdots, k\)</span>. Dạng của hàm <span class="math inline">\(f\)</span> trong mô hình mạng nơ-ron nhân tạo sẽ được viết như sau:
<span class="math display" id="eq:nn001">\[\begin{align}
f(\textbf{X}) &amp; = \beta_0 + \sum\limits_{i = 1}^k \beta_i \cdot h_i\left(\textbf{X}\right) \\
&amp; = \beta_0 + \sum\limits_{i = 1}^k \beta_i \cdot g\left(w_{i,0} + \sum\limits_{j=1}^p w_{i,j} \cdot X_j\right)
\tag{17.1}
\end{align}\]</span>
trong đó <span class="math inline">\(g\)</span> là một hàm số phi tuyến được xác định trước, được gọi theo thuật ngữ chuyên môn là các hàm kích hoạt (activation function). Các <span class="math inline">\(\beta_i\)</span> và <span class="math inline">\(w_{i,j}\)</span> là các hằng số và cũng là các tham số cần được ước lượng của mô hình. Hàm <span class="math inline">\(f\)</span> trong phương trình <a href="#eq:nn01">(<strong>??</strong>)</a> được xây dựng theo hai bước:</p>
<ul>
<li>Bước thứ nhất, các đơn vị <span class="math inline">\(H_i\)</span> trong lớp ẩn được tính bằng hàm kích hoạt tính trên tổ hợp tuyến tính của các biến đầu vào:</li>
</ul>
<p><span class="math display" id="eq:nn002">\[\begin{align}
H_i = g\left(w_{i,0} + \sum\limits_{j=1}^p w_{i,j} \cdot X_j\right)
\tag{17.2}
\end{align}\]</span></p>
<ul>
<li>Bước thứ hai, <span class="math inline">\(k\)</span> đơn vị của lớp ẩn là yếu tố đầu vào để tính toán giá trị biến đầu ra định lượng:
<span class="math display" id="eq:nn003">\[\begin{align}
Y = \beta_0 + \sum\limits_{i = 1}^k \beta_i \cdot H_i
\tag{17.3}
\end{align}\]</span>
</li>
</ul>
<p>Quá trình tính toán đi từ lớp đầu vào qua các lớp ẩn và kết thúc ở mạng đầu ra được gọi là quá trình chuyển tiếp, và hàm <span class="math inline">\(f\)</span> trong phương trình <a href="neuralnetwork.html#eq:nn001">(17.1)</a> được gọi là một mạng nơ-ron chuyển tiếp. Tất cả các tham số <span class="math inline">\(\beta_0\)</span>, <span class="math inline">\(\beta_1\)</span>, <span class="math inline">\(\cdots\)</span>, <span class="math inline">\(\cdots\)</span> , <span class="math inline">\(\beta_k\)</span> và <span class="math inline">\(w_{1,0}\)</span> , <span class="math inline">\(\cdots\)</span> , <span class="math inline">\(w_{k,p}\)</span> được ước lượng từ dữ liệu. Các hàm kích hoạt thường được sử dụng là hàm Sigmoid và hàm ReLU. Hàm sigmoid là hàm được thường xuyên sử dụng trong hồi quy logistic để chuyển hàm tuyến tính thành xác suất giữa 0 và 1 trong khi hàm ReLU là hàm phi tuyến được xây dựng một cách đơn giản nhất nhằm mục đích dễ dàng tuyến tính trong các mạng phức tạp.
<span class="math display">\[\begin{align}
\text{Sigmoid: } &amp; g(x) = \cfrac{e^x}{1 + e^{x}} = \cfrac{1}{1 + e^{-x}} \\
\text{ReLU: } &amp; g(x) = max(x , 0) = (x)^+
\end{align}\]</span></p>
<div class="figure">
<span style="display:block;" id="fig:fgnn002"></span>
<img src="12-mo-hinh-mang-noron_files/figure-html/fgnn002-1.png" alt="Hàm kích hoạt sigmoid và hàm kích hoạt ReLU. Hàm Sigmoid có đạo hàm tại mọi điểm trong khi hàm ReLU không có đạo hàm tại 0" width="672"><p class="caption">
Hình 17.2: Hàm kích hoạt sigmoid và hàm kích hoạt ReLU. Hàm Sigmoid có đạo hàm tại mọi điểm trong khi hàm ReLU không có đạo hàm tại 0
</p>
</div>
<p>Hình <a href="neuralnetwork.html#fig:fgnn002">17.2</a> mô tả giá trị của hàm Sigmoid và hàm ReLU trên đoạn từ -4 đến 4. Trong thời kỳ đầu của mô hình mạng nơ-ron, hàm Sigmoid được ưa chuộng vì hàm số này có đạo hàm liên tục tại mọi giát trị của <span class="math inline">\(x\)</span>. Sau đó, hàm ReLU lại là lựa chọn ưa thích trong các mô hình mạng nơ-ron hiện đại vì hàm số này đơn giản, dễ tính toán và cho hiệu quả tốt hơn so với hàm Sigmoid.</p>
<p>Có thể tóm tắt lại mô hình được mô tả trong Hình <a href="neuralnetwork.html#fig:fgnn001">17.1</a> như sau: từ 3 biến giải thích ban đầu là <span class="math inline">\(X_1\)</span>, <span class="math inline">\(X_2\)</span>, và <span class="math inline">\(X_3\)</span> chúng ta tạo ra năm biến giải thích mới là <span class="math inline">\(H_1\)</span>, <span class="math inline">\(H_2\)</span>, <span class="math inline">\(H_3\)</span>, <span class="math inline">\(H_4\)</span> và <span class="math inline">\(H_5\)</span> được tính toán bằng giá trị của hàm kích hoạt <span class="math inline">\(g(.)\)</span> trên các tổ hợp tuyến tính của các biến giải thích ban đầu. Sau đó chúng ta sử dụng năm biến giải thích <span class="math inline">\(H_i\)</span>, <span class="math inline">\(i = 1, 2, 3, 4, 5\)</span> để xây dựng một mô hình hồi quy tuyến tính mà trong đó biến phụ thuộc là <span class="math inline">\(Y\)</span>. Tham số của các mô hình bao gồm các hệ số <span class="math inline">\(w_{i,j}\)</span> để tính các biến <span class="math inline">\(H_i\)</span>, và các hệ số <span class="math inline">\(\beta_j\)</span> trong mô hình hồi quy tuyến tính <span class="math inline">\(Y\)</span> theo các biến <span class="math inline">\(H\)</span>.</p>
<p>Mô hình có tên là mạng nơ-ron bởi vì cấu trúc của mô hình bao gồm các đơn vị <span class="math inline">\(H_i\)</span> hoạt động giống như các tế bào thần kinh trong não bộ của con người. Các đơn vị <span class="math inline">\(H_i\)</span> xấp xỉ hoặc bằng 0 giống như các tế bào thần kinh im lặng (slient neuron), những tế bào ít bị kích hoạt trong quá trình lan truyền thông tin, trong khi các đơn vị <span class="math inline">\(H_i\)</span> lớn (khi sử dụng hàm ReLU), hoặc xấp xỉ 1 (khi sử dụng hàm Sigmoid) giống như những tế bào bị kích hoạt mạng trong quá trình lan truyền thông tin.</p>
<p>Sử dụng các hàm kích hoạt <span class="math inline">\(g(.)\)</span> phi tuyến là đặc biệt quan trọng tong vì nếu không hàm <span class="math inline">\(f\)</span> sẽ suy biến thành mô hình tuyến tính thông thường với <span class="math inline">\(p = 3\)</span> biến giải thích trong <span class="math inline">\(X_1\)</span>, <span class="math inline">\(X_2\)</span>, và <span class="math inline">\(X_3\)</span>. Ngoài ra, hàm kích hoạt phi tuyến cho phép mô hình mạng nơ-ron mô tả được những mối liên hệ phi tuyến và phức tạp giữa các biến giải thích <span class="math inline">\(\textbf{X}\)</span> và biến mục tiêu <span class="math inline">\(Y\)</span>.</p>
<p>Giả sử trong mô hình mạng nơ-ron được mô tả trong Hình <a href="neuralnetwork.html#fig:fgnn001">17.1</a>, chúng ta có hàm kích hoạt <span class="math inline">\(g(x) = x^2\)</span> và giá trị của các hệ số <span class="math inline">\(\boldsymbol{\beta} = (\beta_0, \beta_1, \beta_2, \beta_3, \beta_4, \beta_5)\)</span> và <span class="math inline">\(w_{i,j}\)</span>, với <span class="math inline">\(1 \leq i \leq 5\)</span> và <span class="math inline">\(0 \leq j \leq 3\)</span>, được cho như sau
<span class="math display" id="eq:nn004">\[\begin{align}
&amp; \text{hệ số chặn: } \beta_0 = w_{1,0} = w_{2,0} = w_{3,0} = w_{4,0} = w_{5,0} 0 \\
&amp; \\
&amp; \begin{pmatrix}
\beta_1 \\
\beta_2 \\
\beta_3 \\
\beta_4 \\
\beta_5
\end{pmatrix} = \begin{pmatrix}
0.5 \\
0.5 \\
0.5 \\
1 \\
2
\end{pmatrix} \ \text{ và } \ \begin{pmatrix}
w_{1,1} &amp; w_{1,2} &amp; w_{1,3} \\
w_{2,1} &amp; w_{2,2} &amp; w_{2,3} \\
w_{3,1} &amp; w_{3,2} &amp; w_{3,3} \\
w_{4,1} &amp; w_{4,2} &amp; w_{4,3} \\
w_{5,1} &amp; w_{5,2} &amp; w_{5,3}
\end{pmatrix} = \begin{pmatrix}
1 &amp; -1 &amp; 1 \\
1 &amp; 2 &amp; -1 \\
0 &amp; 0 &amp; 0 \\
0 &amp; 0 &amp; 0 \\
0 &amp; 0 &amp; 0
\end{pmatrix}
\tag{17.4}
\end{align}\]</span></p>
<p>Trước hết, để tránh sự phức tạp chúng tôi cho giá trị các hàng 3, 4, và 5 của ma trận <span class="math inline">\(\boldsymbol{w}\)</span> đều bằng 0, điều này dẫn đến giá trị tại các đơn vị <span class="math inline">\(H_3\)</span>, <span class="math inline">\(H_4\)</span> và <span class="math inline">\(H_5\)</span> của lớp ẩn sẽ bằng 0. Các đơn vị này hoạt động như các tế bào im lặng trong mạng nơ-rơn và không có ảnh hưởng đến biến mục tiêu <span class="math inline">\(Y\)</span>. Chúng ta có giá trị tại <span class="math inline">\(H_1\)</span> và <span class="math inline">\(H_2\)</span> được tính theo các biến giải thích và hàm kích hoạt:
- Giá trị tại <span class="math inline">\(H_1\)</span>
<span class="math display" id="eq:nn005">\[\begin{align}
H_1\left(X_1, X_2, X_3\right) &amp; = g\left(w_{1,1} \cdot X_1 + w_{1,2} \cdot X_2 + w_{1,3} \cdot X_3\right) \\
&amp; = \left(w_{1,1} \cdot X_1 + w_{1,2} \cdot X_2 + w_{1,3} \cdot X_3\right)^2 \\
&amp; = \left(X_1 - X_2 + X_3\right)^2
\tag{17.5}
\end{align}\]</span></p>
<ul>
<li>Giá trị tại <span class="math inline">\(H_2\)</span>
<span class="math display" id="eq:nn006">\[\begin{align}
H_2 \left(X_1, X_2, X_3\right) &amp; = g\left(w_{2,1} \cdot X_1 + w_{2,2} \cdot X_2 + w_{2,3} \cdot X_3\right) \\
&amp; = \left(w_{2,1} \cdot X_1 + w_{2,2} \cdot X_2 + w_{2,3} \cdot X_3\right)^2 \\
&amp; = \left(X_1 + 2 \cdot X_2 - X_3\right)^2
\tag{17.6}
\end{align}\]</span>
</li>
</ul>
<p>Giá trị của hàm số <span class="math inline">\(f(\text{X})\)</span> là đầu ra của mạng nơ-ron được xác định như sau:
<span class="math display" id="eq:nn007">\[\begin{align}
f(X_1, X_2, X_3) &amp; = 0.5 \cdot H_1\left(X_1, X_2, X_3\right) + 0.5 \cdot H_2 \left(X_1, X_2, X_3\right) \\
&amp; = X_1^2 + 2.5 \cdot X_2^2 + X_3^2 + X_1 \cdot X_2 - 3 \cdot X_2 \cdot X_3
\tag{17.7}
\end{align}\]</span></p>
<p>Bạn đọc có thể thấy rằng, việc sử dụng hàm kích hoạt phi tuyến cho phép chúng ta có hàm đầu ra bao gồm hàm phi tuyến trên các giá trị biến đầu vào, mà còn tính đến cả biến tương tác giữa các biến ban đầu. Trong ví dụ ở phương trình <a href="neuralnetwork.html#eq:nn007">(17.7)</a> là các giá trị <span class="math inline">\(X_1 \cdot X_2\)</span> và <span class="math inline">\(3 \cdot X_2 \cdot X_3\)</span>. Trong thực tế, chúng ta sẽ không sử dụng hàm bậc hai hay hàm đa thức cho hàm kích hoạt <span class="math inline">\(g(.)\)</span> do hàm kích hoạt đa thức sẽ dẫn tới kết quả cũng chỉ là dạng hàm đa thức. Các hàm kích hoạt Sigmoid hoặc ReLU không bị giới hạn như vậy.</p>
<p>Trong ví dụ trên chúng ta đã cho trước các tham số bao gồm các hệ số <span class="math inline">\(\boldsymbol{\beta}\)</span> và ma trận <span class="math inline">\(\boldsymbol{w}\)</span>. Tuy nhiên trong thực tế, các tham số này được lựa chọn để giảm thiểu sai số giữa giá trị dự đoán và giá trị quan sát của biến mục tiêu:
<span class="math display" id="eq:nn008">\[\begin{align}
(\hat{\boldsymbol{\beta}},\hat{\boldsymbol{w}}) = \underset{\boldsymbol{\beta},\boldsymbol{w}}{\operatorname{argmin}} \sum\limits_{i=1}^n \left( y_i - f(\textbf{x}_i) \right)^2
\tag{17.8}
\end{align}\]</span></p>
<p>Chúng ta sẽ thảo luận về ước lượng tham số cho mô hình mạng nơ-ron trong phần <a href="neuralnetwork.html#nnestimation">17.3</a>.</p>
</div>
<div id="nnmultilayer" class="section level2" number="17.2">
<h2>
<span class="header-section-number">17.2</span> Mạng nơ-ron có nhiều lớp ẩn<a class="anchor" aria-label="anchor" href="#nnmultilayer"><i class="fas fa-link"></i></a>
</h2>
<p>Mô hình mạng nơ-ron được sử dụng hiện tại thường có nhiều hơn một lớp ẩn và có nhiều đơn vị trên mỗi lớp. Về lý thuyết, một lớp ẩn duy nhất với số lượng lớn các đơn vị có khả năng xấp xỉ hầu hết các dạng hàm <span class="math inline">\(f\)</span>. Tuy nhiên, thực tế chỉ ra rằng xây dựng những cấu trúc nhiều lớp và mỗi lớp có kích thước hợp lý là giải pháp tốt hơn so với cấu trúc chỉ có một lớp ẩn và có rất nhiều đơn vị trên cùng một lớp.</p>
<p>Cấu trúc có thể mở rộng của mô hình mạng nơ-ron nhân tạo cho phép nó có khả năng mô hình hóa tốt những bộ dữ liệu phức tạp, mà điển hình là dữ liệu dạng ảnh, dạng văn bản, hay dạng tín hiệu. Để minh họa cho khả năng phù hợp của mô hình với những dữ liệu phức tạp, chúng ta sẽ xây dựng một cấu trúc mạng nơ-ron có nhiều lớp ẩn để dự đoán dữ liệu là ảnh chứa các chữ số viết tay. Dữ liệu được sử dụng để huấn luyện mô hình là tập dữ liệu chữ số viết tay nổi tiếng <span class="math inline">\(\textbf{MNIST}\)</span>. Hình <a href="neuralnetwork.html#fig:fgnn003">17.3</a> minh họa một số quan sát trong dữ liệu về các chữ số viết tay được lưu trữ trong dữ liệu <span class="math inline">\(\textbf{MNIST}\)</span>. Mỗi quan sát của dữ liệu sử dụng để xây dựng mô hình là một hình ảnh có kích thước <span class="math inline">\(p = 28 \times 28 = 784\)</span> pixel và biến mục tiêu là giá trị số của hình ảnh xuất hiện trong biến giải thích. Có tất cả 10 giá trị khác nhau cho biến mục tiêu là các chữ số viết tay tương ứng từ 0 đến 9.</p>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:fgnn003"></span>
<img src="12-mo-hinh-mang-noron_files/figure-html/fgnn003-1.png" alt="Năm mươi giá trị đầu tiên trong dữ liệu số viết tay MNIST. Mỗi số viết tay là một quan sát trong dữ liệu. Một bức ảnh được lưu dưới dạng một véc-tơ có độ dài p = 784. Mỗi giá trị trong véc-tơ là một số tự nhiên nhận giá trị từ 0 đến 255 cho biết độ tối của điểm ảnh." width="672"><p class="caption">
Hình 17.3: Năm mươi giá trị đầu tiên trong dữ liệu số viết tay MNIST. Mỗi số viết tay là một quan sát trong dữ liệu. Một bức ảnh được lưu dưới dạng một véc-tơ có độ dài p = 784. Mỗi giá trị trong véc-tơ là một số tự nhiên nhận giá trị từ 0 đến 255 cho biết độ tối của điểm ảnh.
</p>
</div>
<p>Ý tưởng là xây dựng một mô hình để phân loại các hình ảnh thành chữ số từ 0 đến 9. Chúng ta sẽ sử dụng cấu trúc mạng nơ-ron với hai lớp ẩn được minh họa trong Hình <a href="neuralnetwork.html#fig:fgnn004">17.4</a> để xây dựng mô hình phân loại hình ảnh số viết tay.</p>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:fgnn004"></span>
<img src="12-mo-hinh-mang-noron_files/figure-html/fgnn004-1.png" alt="Mạng nơ-ron xây dựng trên dữ liệu MNIST có hai lớp ẩn, mỗi lớp ẩn có nhiều đơn vị và lớp đầu ra có 10 đơn vị tương ứng với m = 10 giá trị có thể của các số viết tay từ 0 đến 9. Lớp đầu vào có p = 784 đơn vị tương ứng với 784 điểm ảnh." width="672"><p class="caption">
Hình 17.4: Mạng nơ-ron xây dựng trên dữ liệu MNIST có hai lớp ẩn, mỗi lớp ẩn có nhiều đơn vị và lớp đầu ra có 10 đơn vị tương ứng với m = 10 giá trị có thể của các số viết tay từ 0 đến 9. Lớp đầu vào có p = 784 đơn vị tương ứng với 784 điểm ảnh.
</p>
</div>
<p>Giá trị đầu ra trong cấu trúc mạng nơ-ron trong Hình <a href="neuralnetwork.html#fig:fgnn004">17.4</a> là biến kiểu factor, được biểu thị bằng véc-tơ <span class="math inline">\(\textbf{Y} = (Y_1, Y_2, \cdots , Y_{m})\)</span> với <span class="math inline">\(m = 10\)</span>. Dữ liệu có 60 nghìn bức ảnh được sử dụng để huấn luyện mô hình và 10 nghìn bức ảnh được sử dụng để kiểm tra mô hình. Cấu trúc mạng trong Hình <a href="neuralnetwork.html#fig:fgnn004">17.4</a> có hai lớp ẩn thay vì một lớp ẩn giống như trước.</p>
<ul>
<li><p>Lớp ẩn thứ nhất có <span class="math inline">\(k_1\)</span> đơn vị được tính toán từ <span class="math inline">\(p\)</span> đầu vào ban đầu với hàm kích hoạt <span class="math inline">\(g_1(.)\)</span>. Giả sử các nút trong lớp ẩn đầu tiên lần lượt là <span class="math inline">\(H^{(1)}_1\)</span>, <span class="math inline">\(H^{(1)}_2\)</span>, <span class="math inline">\(\cdots\)</span>, <span class="math inline">\(H^{(1)}_{k_1}\)</span>. Ta có <span class="math inline">\(H^{(1)}_j\)</span> được tính toán từ các đầu vào với <span class="math inline">\((p+1)\)</span> tham số <span class="math inline">\(w^{(1)}_{j,i}\)</span> với <span class="math inline">\(0 \leq i \leq p\)</span> như sau:
<span class="math display" id="eq:nn009">\[\begin{align}
H^{(1)}_j = g_1\left( w^{(1)}_{j,0} + w^{(1)}_{j,1} X_1 + w^{(1)}_{j,2} X_2 + \cdots + w^{(1)}_{j,p} X_p \right)
\tag{17.9}
\end{align}\]</span>
Có thể thấy rằng, để tính toán tất cả <span class="math inline">\(k_1\)</span> đơn vị trong lớp ẩn thứ nhất, chúng ta cần sử dụng <span class="math inline">\(k_1 \times (p+1)\)</span> tham số.</p></li>
<li><p>Lớp ẩn thứ hai có <span class="math inline">\(k_2\)</span> đơn vị được tính toán từ <span class="math inline">\(k_1\)</span> đơn vị của lớp ẩn thứ nhất và hàm kích hoạt <span class="math inline">\(g_2(.)\)</span>. Gọi các nút trong lớp ẩn thứ hai lần lượt là <span class="math inline">\(H^{(2)}_1\)</span>, <span class="math inline">\(H^{(2)}_2\)</span>, <span class="math inline">\(\cdots\)</span>, <span class="math inline">\(H^{(2)}_{k_2}\)</span>. Ta có <span class="math inline">\(H^{(2)}_j\)</span> được tính toán từ lớp ẩn thứ nhất vào với <span class="math inline">\((k_1+1)\)</span> tham số <span class="math inline">\(w^{(2)}_{j,i}\)</span> với <span class="math inline">\(0 \leq i \leq k_1\)</span> như sau:
<span class="math display" id="eq:nn010">\[\begin{align}
H^{(2)}_j = g_2\left( w^{(2)}_{j,0} + w^{(2)}_{j,1} H^{(2)}_1 + w^{(2)}_{j,2} H^{(2)}_1 + \cdots + w^{(2)}_{j,k_1} H^{(2)}_{k_1} \right)
\tag{17.10}
\end{align}\]</span>
Để tính toán tất cả <span class="math inline">\(k_2\)</span> đơn vị trong lớp ẩn thứ hai, chúng ta cần sử dụng <span class="math inline">\(k_2 \times (k_1+1)\)</span> tham số.</p></li>
<li><p>Trong lớp đầu ra, do đây là bài toán phân loại, nên chúng ta sử dụng <span class="math inline">\(m = 10\)</span> đơn vị tương ứng với 10 chữ số viết tay từ 0 đến 9. Trong bài toán phân loại, hàm kích hoạt để tính toán các đơn vị trong lớp đầu ra thường là hàm softmax. Giá trị tại đơn vị <span class="math inline">\(Y_j\)</span> với <span class="math inline">\(1 \leq m\)</span> trong lớp đầu ra được xác định như sau:
<span class="math display" id="eq:nn011">\[\begin{align}
Z_j &amp;= \beta_{j,0} + \beta_{j,1} H^{(2)}_1 + \beta_{j,2} H^{(2)}_2 + \cdots +\beta_{j,k_2} H^{(2)}_{k_2} \\
\textbf{Y} &amp;= softmax(\textbf{Z}) \rightarrow Y_j = \cfrac{exp(Z_j)}{exp(Z_1) + exp(Z_2) + \cdots + exp(Z_m)}
\tag{17.11}
\end{align}\]</span>
Để tính toán <span class="math inline">\(m\)</span> đơn vị trong lớp đầu ra, chúng ta cần <span class="math inline">\(m \times (k_2 + 1)\)</span> tham số. Lưu ý rằng khi sử dụng hàm softmax thì tổng giá trị của các đơn vị trong lớp đầu ra luôn bằng 1.</p></li>
</ul>
<p>Như vậy, để tính toán được <span class="math inline">\(m = 10\)</span> giá trị đầu ra cho cấu trúc mạng nơ-ron được trình bày trong Hình <a href="neuralnetwork.html#fig:fgnn004">17.4</a>, số lượng tham số cần sử dụng là
<span class="math display">\[\begin{align}
k_1 \cdot (p + 1) + k_2 \cdot (k_1 + 1) + m \cdot (k_2+1)
\end{align}\]</span></p>
<p>Ví dụ, nếu chúng ta sử dụng 512 đơn vị trong lớp ẩn thứ nhất và 256 đơn vị trong lớp ẩn thứ hai, số lượng tham số của mạng nơ-ron với 784 đơn vị đầu vào và 10 đơn vị đầu ra là
<span class="math display">\[\begin{align}
512 \cdot (784 + 1) + 256 \cdot (512 + 1) + 10 \cdot (256 + 1) = 535.818
\end{align}\]</span>
Nói cách khác, mô hình sử dụng hơn 500 nghìn tham số để tính toán 10 đơn vị đầu ra từ 784 đơn vị đầu vào. Các tham số này được tính toán sao cho sai số giữa véc-tơ đầu ra tính toán từ mô hình và giá trị đầu ra quan sát được trên dữ liệu là nhỏ nhất. Trong bài toán phân loại, sai số thường được sử dụng là hàm cross-entropy. Với quan sát thứ <span class="math inline">\(i\)</span> của biến giải thích <span class="math inline">\(\textbf{x}_i\)</span> thì quan sát <span class="math inline">\(y_i\)</span> tương ứng của biến mục tiêu (nhận một trong các giá trị từ <span class="math inline">\(1\)</span> đến <span class="math inline">\(m\)</span>) sẽ được viết dưới dạng véc-tơ đầu ra <span class="math inline">\(\textbf{y}_i = (y^{i}_{1},y^i_2,\cdots,y^i_m)\)</span> sao cho
<span class="math display" id="eq:nn012">\[\begin{align}
y^i_j &amp;= 1 \text{ nếu } y_i = j \\
y^i_j &amp;= 0 \text{ nếu } y_i \neq j
\tag{17.12}
\end{align}\]</span>
Cách biến đổi biến này thường được gọi là one-hot encoding. Với véc-tơ đầu ra được tính toán từ véc-tơ đầu vào <span class="math inline">\(\textbf{x}_i\)</span> theo cấu trúc mạng nơ-ron bằng các phương trình <a href="neuralnetwork.html#eq:nn009">(17.9)</a>, <a href="neuralnetwork.html#eq:nn010">(17.10)</a>, và <a href="neuralnetwork.html#eq:nn011">(17.11)</a> là <span class="math inline">\(\hat{\textbf{y}}_i = (\hat{y}^{i}_{1},\hat{y}^i_2,\cdots,\hat{y}^i_m)\)</span> thì sai số tính bằng cross-entropy tại quan sát thứ <span class="math inline">\(i\)</span> là
<span class="math display" id="eq:nn013">\[\begin{align}
\sum\limits_{j=1}^m \ y^{i}_{j} \cdot \log(\hat{y}^{i}_{j}) = y^{i}_{1} \cdot \log(\hat{y}^{i}_{1}) + y^{i}_{2} \cdot \log(\hat{y}^{i}_{2}) + \cdots + y^{i}_{m} \cdot \log(\hat{y}^{i}_{m})
\tag{17.13}
\end{align}\]</span>
và sai số tính bằng cross-entropy trên <span class="math inline">\(n\)</span> dữ liệu huấn luyện mô hình là
<span class="math display" id="eq:nn014">\[\begin{align}
CE\_loss = \sum\limits_{i=1}^n \sum\limits_{j=1}^m \ y^{i}_{j} \cdot \log(\hat{y}^{i}_{j})
\tag{17.14}
\end{align}\]</span></p>
<p>Trong mô hình mạng nơ-ron, để đơn giản hóa ký hiệu, chúng ta sẽ sử dụng ký hiệu dạng ma trận. Cấu trúc mạng nơ-ron có hai lớp ẩn được mô tả trong hình <a href="neuralnetwork.html#fig:fgnn004">17.4</a> có ba ma trận tham số <span class="math inline">\(\boldsymbol{w}_1\)</span>, <span class="math inline">\(\boldsymbol{w}_2\)</span>, và <span class="math inline">\(\boldsymbol{\beta}\)</span> được định nghĩa như sau
<span class="math display" id="eq:nn015">\[\begin{align}
\boldsymbol{\beta} &amp;= \begin{pmatrix}
\beta_{1,0} &amp; \beta_{1,1} &amp; \cdots &amp; \beta_{1,k_2} \\
\beta_{2,0} &amp; \beta_{2,1} &amp; \cdots &amp; \beta_{2,k_2} \\
\cdots &amp; \cdots &amp; \cdots &amp; \cdots \\
\beta_{m,0} &amp; \beta_{m,1} &amp; \cdots &amp; \beta_{m,k_2}
\end{pmatrix} \\
&amp; \\
\boldsymbol{w}_1 &amp;= \begin{pmatrix}
w^{(1)}_{1,0} &amp; w^{(1)}_{1,1} &amp; \cdots &amp; w^{(1)}_{1,p} \\
w^{(1)}_{2,0} &amp; w^{(1)}_{2,1} &amp; \cdots &amp; w^{(1)}_{2,p} \\
\cdots &amp; \cdots &amp; \cdots &amp; \cdots \\
w^{(1)}_{k_1,0} &amp; w^{(1)}_{k_1,1} &amp; \cdots &amp; w^{(1)}_{k_1,p}
\end{pmatrix};
\boldsymbol{w}_2 = \begin{pmatrix}
w^{(2)}_{1,0} &amp; w^{(2)}_{1,1} &amp; \cdots &amp; w^{(2)}_{1,k_1} \\
w^{(2)}_{2,0} &amp; w^{(2)}_{2,1} &amp; \cdots &amp; w^{(2)}_{2,p} \\
\cdots &amp; \cdots &amp; \cdots &amp; \cdots \\
w^{(2)}_{k_2,0} &amp; w^{(2)}_{k_2,1} &amp; \cdots &amp; w^{(2)}_{k_2,k_1}
\end{pmatrix}
\tag{17.15}
\end{align}\]</span></p>
<p>Quá trình ước lượng tham số cho mạng nơ-ron là quá trình tìm các ma trận tham số <span class="math inline">\(\boldsymbol{w}_1\)</span>, <span class="math inline">\(\boldsymbol{w}_2\)</span>, và <span class="math inline">\(\boldsymbol{\beta}\)</span> để tối thiểu hóa tổn thất tính bằng cross-entropy</p>
<p><span class="math display">\[\begin{align}
(\hat{\boldsymbol{w}}_1, \hat{\boldsymbol{w}}_2, \hat{\boldsymbol{\beta}}) &amp;= \underset{\boldsymbol{w}_1, \boldsymbol{w}_2, \boldsymbol{\beta}}{\operatorname{argmin}} \sum_{i=1}^n \sum_{j=1}^m y^{i}_{j} \cdot \log(\hat{y}^{i}_{j})
\label{#eq:nn016}
\end{align}\]</span></p>
<p>Như chúng tôi đã đề cập ở phía trước, các mô hình mạng nơ-ron có nhiều lớp ẩn với số lượng đơn vị trong các lớp ẩn không quá nhiều thường cho kết quả tốt hơn so với các mô hình có ít lớp ẩn và sử dụng nhiều đơn vị trong một lớp. Tuy nhiên khi tăng số lớp ẩn lên sẽ làm cho số lượng tham số cần được ước lượng tăng lên rất nhanh, khiến cho mô hình mạng nơ-ron rất dễ rơi vào tình trạng overfitting, nghĩa là sai số trên tập dữ liệu huấn luyện mô hình nhỏ nhưng sai số trên dữ liệu kiểm tra mô hình lại rất lớn. Chính vì thế, trong quá trình ước lượng tham số, người xây dựng mô hình thường sử dụng thêm các ràng buộc tham số, chẳng hạn như ràng buộc tham số kiểu hồi quy ridge. Nghĩa là tổn thất của mô hình tính bằng cross-entropy sẽ được điều chỉnh để cân bằng giữa phương sai và độ lệch của mô hình. Chúng ta sẽ thảo luận về vấn đề này trong phần ước lượng tham số cho mạng nơ-ron.</p>
</div>
<div id="nnestimation" class="section level2" number="17.3">
<h2>
<span class="header-section-number">17.3</span> Ước lượng tham số của mạng nơ-ron<a class="anchor" aria-label="anchor" href="#nnestimation"><i class="fas fa-link"></i></a>
</h2>
<p>Tham số của mạng nơ-ron được chia thành hai nhóm:</p>
<ul>
<li><p>Nhóm thứ nhất bao gồm các siêu tham số như số lượng lớp ẩn trong cấu trúc mạng và trong mỗi lớp ẩn có bao nhiêu đơn vị. Chẳng hạn như cấu trúc được mô tả trong hình <a href="neuralnetwork.html#fig:fgnn004">17.4</a> có hai lớp ẩn, lớp ẩn thứ nhất có 512 đơn vị, lớp ẩn thứ hai có 256 đơn vị. Trong trường hợp chúng ta có sử dụng ràng buộc tham số, chúng ta có thêm một tham số điều chỉnh sự đánh đổi giữa sai lệch và phương sai giống như tham số <span class="math inline">\(\lambda\)</span> trong hồi quy ridge.</p></li>
<li><p>Với mỗi lựa chọn cho các tham số trong nhóm thứ nhất, chúng ta có các tham số để tính toán cấu trúc mạng bao gồm các ma trận <span class="math inline">\(\boldsymbol{w}\)</span> và ma trận <span class="math inline">\(\boldsymbol{\beta}\)</span> được định nghĩa trong phương trình <a href="neuralnetwork.html#eq:nn015">(17.15)</a>. Quá trình ước lượng các tham số này là quá trình giải bài toán tối thiểu hóa hàm tổn thất dạng tổng sai số bình phương trong bài toán hồi quy hoặc hàm cross-entropy trong bài toán phân loại. Nhìn chung, không thể tính toán được lời giải chính xác cho bài toán tối ưu mà chúng ta sẽ phải ước lượng tham số bằng các phương pháp giải số, mà cụ thể là phương pháp stochastic gradient descent. Do đó, chúng ta thường phải xác định định thêm các tham số như tốc độ học, số lượng dữ liệu được sử dụng trong mỗi bước tính toán, hay số vòng lặp của thuật toán gradient descent.</p></li>
</ul>
<p>Lựa chọn tham số trong nhóm thứ nhất có thể ảnh hưởng lớn đến kết quả của mô hình mạng nơ-ron, nhưng lại không có phương pháp chính xác nào để xác định các tham số này. Nếu nguồn lực tính toán cho phép, các tham số này sẽ được xác định bằng cách thử nghiệm và lựa chọn. Nếu nguồn lực tính toán không cho phép, người xây dựng mô hình thường lựa chọn các tham số này dựa trên kinh nghiệm và cấu trúc mạng đã có sẵn trên các bộ dữ liệu tương tự.</p>
<p>Trong phần này, chúng tôi sẽ tập trung vào các tham số cần được ước lượng trong nhóm thứ hai, nghĩa là tập trung vào ước lượng các ma trận <span class="math inline">\(\boldsymbol{w}\)</span> và ma trận <span class="math inline">\(\boldsymbol{\beta}\)</span> khi chúng ta đã có một cấu trúc mạng cụ thể.</p>
<div id="ước-lượng-tham-số-cho-mạng-nơ-ron-hồi-quy-có-một-lớp-ẩn" class="section level3" number="17.3.1">
<h3>
<span class="header-section-number">17.3.1</span> Ước lượng tham số cho mạng nơ-ron hồi quy có một lớp ẩn<a class="anchor" aria-label="anchor" href="#%C6%B0%E1%BB%9Bc-l%C6%B0%E1%BB%A3ng-tham-s%E1%BB%91-cho-m%E1%BA%A1ng-n%C6%A1-ron-h%E1%BB%93i-quy-c%C3%B3-m%E1%BB%99t-l%E1%BB%9Bp-%E1%BA%A9n"><i class="fas fa-link"></i></a>
</h3>
<p>Quá trình ước lượng mạng nơ-ron đòi hỏi kiến thức và các kỹ thuật toán học khá phức tạp và chúng tôi sẽ cố gắng chỉ trình bày tổng quan và ngắn gọn. Bạn đọc cảm thấy khó khăn về phần này này có thể yên tâm bỏ qua và chuyển sang phần tiếp theo bởi chúng ta có thể sử dụng các thư viện như hay để ước lượng mô hình mà không cần hiểu quá sâu về các chi tiết kỹ thuật trong quy trình xây dựng mô hình.</p>
<p>Chúng ta sẽ bắt đầu bằng mô hình mạng nơ-ron hồi quy có một lớp ẩn duy nhất được trình bày trong hình <a href="neuralnetwork.html#fig:fgnn001">17.1</a> và phương trình <a href="neuralnetwork.html#eq:nn001">(17.1)</a>. Để mô hình giữ nguyên tính tổng quát, chúng tôi sử dụng <span class="math inline">\(p\)</span> là số tham số đầu vào, <span class="math inline">\(k\)</span> là số lượng đơn vị trong lớp ẩn. Chúng ta cần tìm các tham số là véc-tơ <span class="math inline">\(\boldsymbol{\beta}\)</span> và ma trận <span class="math inline">\(\boldsymbol{w}\)</span> để tối thiểu hóa tổng sai số bình phương:</p>
<p><span class="math display" id="eq:nn017">\[\begin{align}
RSS\left( \boldsymbol{\beta}, \boldsymbol{w} \right) &amp; = \cfrac{1}{2} \ \sum\limits_{i=1}^n \ \left(y_i - f_{\boldsymbol{\beta}, \boldsymbol{w}}\left(\textbf{x}_i\right) \right)^2 \\
&amp; = \cfrac{1}{2} \sum\limits_{i=1}^n \ \left(y_i - \beta_0 - \sum\limits_{t = 1}^k \beta_t \cdot g\left(z_{t,i}\right) \right)^2
\tag{17.16}
\end{align}\]</span>
với
<span class="math display" id="eq:nn018">\[\begin{align}
z_{i,t} = w_{t,0} + \sum\limits_{j=1}^p w_{t,j} \cdot x_{i,j}
\tag{17.17}
\end{align}\]</span>
Mặc dù hàm <span class="math inline">\(RSS\left( \boldsymbol{\beta}, \boldsymbol{w} \right)\)</span> trong phương trình <a href="neuralnetwork.html#eq:nn017">(17.16)</a> không quá phức tạp, nhưng để giải bài toán tối thiểu hóa hàm số này trên dữ liệu <span class="math inline">\((\textbf{x}_i,y_i)\)</span> với <span class="math inline">\(i = 1, 2, \cdots, n\)</span> và hàm số <span class="math inline">\(g\)</span> cho trước không phải là một nhiệm vụ dễ dàng. Trước hết, mặc dù chúng ta có dạng hàm tường minh cho các tham số, nhưng đây không phải là hàm số lồi theo các tham số, do đó quá trình giải bài toán tối ưu thường chỉ cho đáp số là một điểm cực tiểu địa phương chứ không chắc chắn là điểm cực tiểu toàn cục. Thứ hai, không thể có lời giải chính xác cho bài toán tối ưu nên chúng ta sẽ cần tìm lời giải số, trong trường hợp này là phương pháp gradient descent. Việc lựa chọn các tham số cho thuật toán này cũng sẽ là câu hỏi cần được giải đáp cho quá trình xây dựng mô hình. Thứ ba, mô hình mạng nơ-ron có rất nhiều tham số, do đó rất dễ dẫn đến hiện tượng mô hình quá khớp với dữ liệu huấn luyện mô hình. Hàm mục tiêu thường sẽ bằng RSS cộng thêm một hàm phạt, khiến cho quá trình giải số trở nên khó khăn hơn.</p>
<p>Trước tiên, giả sử bài toán tối ưu trong <a href="neuralnetwork.html#eq:nn017">(17.16)</a> không có ràng buộc tham số, chúng ta cần xác định gradient của RSS theo <span class="math inline">\(\boldsymbol{\beta}\)</span> và <span class="math inline">\(\boldsymbol{w}\)</span>. Để đơn giản hóa, chúng ta giả sử bình phương của sai số thứ <span class="math inline">\(i\)</span> là <span class="math inline">\(RSS_i\)</span>
<span class="math display">\[\begin{align}
RSS_i = \cfrac{1}{2} \ \left[y_i - \beta_0 - \sum\limits_{t = 1}^k \beta_t \cdot g\left(z_{i,t}\right) \right]^2
\end{align}\]</span></p>
<ul>
<li><p>Đạo hàm của bình phương sai số thứ <span class="math inline">\(i\)</span> theo <span class="math inline">\(\beta_l\)</span>, với <span class="math inline">\(0 \leq l \leq k\)</span>, được xác định như sau
<span class="math display" id="eq:nn019">\[\begin{align}
\cfrac{\partial RSS_i}{\partial \beta_l} &amp; = \begin{cases}
-\left(y_i - f\left(\textbf{x}_i\right) \right) &amp; \text{ nếu } l = 0 \\
-g\left(z_{i,l}\right)  \left(y_i - f\left(\textbf{x}_i\right) \right) &amp; \text{ nếu } l &gt; 0
\end{cases}
\tag{17.18}
\end{align}\]</span></p></li>
<li><p>Đạo hàm của bình phương sai số thứ <span class="math inline">\(i\)</span> theo <span class="math inline">\(w_{l,j}\)</span>, với <span class="math inline">\(1 \leq l \leq k\)</span> và <span class="math inline">\(0 \leq j \leq p\)</span>, được xác định như sau</p></li>
</ul>
<p><span class="math display" id="eq:nn020">\[\begin{align}
\cfrac{\partial RSS_i}{\partial w_{l,j}} &amp; = - \cfrac{\partial  \beta_l \cdot g\left(z_{i,l}\right) }{\partial w_{l,j}} \left(y_i - f\left(\textbf{x}_i\right) \right) \\
&amp; =
\begin{cases}
- \beta_l \cdot g^{'}\left(z_{i,l}\right) \cdot \left(y_i - f\left(\textbf{x}_i\right) \right) &amp; \text{ nếu } j = 0 \\
- \beta_l \cdot x_{i,j} \cdot g^{'}\left(z_{i,l}\right) \cdot \left(y_i - f\left(\textbf{x}_i\right) \right) &amp; \text{ nếu } j &gt; 0
\end{cases}
\tag{17.19}
\end{align}\]</span></p>
<p>Trước tiên, có thể thấy rằng cả hai biểu thức đạo hàm của sai số bình phương này đều chứa phần dư <span class="math inline">\(\left(y_i − f(\textbf{x}i)\right)\)</span>. Trong công thức <a href="neuralnetwork.html#eq:nn019">(17.18)</a> chúng ta thấy rằng giá trị tuyệt đối của gradient theo các <span class="math inline">\(\beta_l\)</span> bằng phần dư nhân với giá trị <span class="math inline">\(g\left(z_{i,l}\right)\)</span>, chính là giá trị tại nút <span class="math inline">\(H_l\)</span> tính theo đầu vào <span class="math inline">\(\textbf{x}_i\)</span>. Tiếp theo, trong công thức <a href="neuralnetwork.html#eq:nn020">(17.19)</a> chúng ta thấy sự thay đổi của RSS theo tham số <span class="math inline">\(w_{l,j}\)</span>, tương ứng với hệ số của đầu vào <span class="math inline">\(X_j\)</span> khi tính toán đơn vị <span class="math inline">\(H_l\)</span> của mạng nơ-ron một lớp ẩn, cũng phụ thuộc vào phần dư. Sự ảnh hưởng của phần dư lên đạo hàm theo từng tham số của mô hình được gọi là quá trình lan truyền ngược trong mô hình mạng nơ-ron.</p>
<p>Các công thức đạo hàm ở trên luôn yêu cầu tính toán giá trị của hàm kích hoạt và đạo hàm tại các điểm <span class="math inline">\(z_{i,l}\)</span>. Về lý thuyết, mọi hàm đơn điệu tăng, có đạo hàm, và không tuyến tính đều có thể được sử dụng làm hàm kích hoạt. Tuy nhiên, nếu dạng hàm quá phức tạp, việc tính toán sẽ trở nên phưc tạp, nhất là khi sử dụng nhiều lớp ẩn và trong mỗi lớp ẩn có nhiều đơn vị. Điều này giải thích tại sao hàm ReLU thường xuyên được sử dụng làm hàm kích hoạt để tính toán gradient là đơn giản nhất có thể. Giả sử hàm <span class="math inline">\(g\)</span> trong các công thức <a href="neuralnetwork.html#eq:nn019">(17.18)</a> và <a href="neuralnetwork.html#eq:nn020">(17.19)</a> là hàm ReLU, chúng ta có thể đơn giản hóa các đạo hàm như sau:</p>
<ul>
<li>Đạo hàm theo hệ số <span class="math inline">\(\beta_l\)</span>:</li>
</ul>
<p><span class="math display" id="eq:nn021">\[\begin{align}
\cfrac{\partial RSS_i}{\partial \beta_l} &amp; = \begin{cases}
-\left(y_i - f\left(\textbf{x}_i\right) \right) &amp; \text{ nếu } l = 0 \\
-\mathbb{I}(z_{i,l} &gt; 0) \cdot z_{i,l} \cdot \left(y_i - f\left(\textbf{x}_i\right) \right) &amp; \text{ nếu } l &gt; 0
\end{cases}
\tag{17.20}
\end{align}\]</span></p>
<ul>
<li>Đạo hàm theo <span class="math inline">\(w_{l,0}\)</span>,</li>
</ul>
<p><span class="math display" id="eq:nn022">\[\begin{align}
  \cfrac{\partial RSS_i}{\partial w_{l,0}} &amp; = - \beta_l \cdot \mathbb{I}(z_{i,l} &gt; 0) \cdot \left(y_i - f\left(\textbf{x}_i\right) \right)
  \tag{17.21}
\end{align}\]</span></p>
<ul>
<li>Đạo hàm theo <span class="math inline">\(w_{l,j}\)</span>, với <span class="math inline">\(j &gt; 0\)</span> thì</li>
</ul>
<p><span class="math display" id="eq:nn023">\[\begin{align}
\cfrac{\partial RSS_i}{\partial w_{l,j}} &amp; =
- \beta_l \cdot \mathbb{I}(z_{i,l} &gt; 0) \cdot x_{i,j} \cdot \left(y_i - f\left(\textbf{x}_i\right) \right)
\tag{17.22}
\end{align}\]</span></p>
<p>Gradient của tổng các <span class="math inline">\(RSS_i\)</span> được xác định như sau:</p>
<ul>
<li>Theo <span class="math inline">\(\beta_0\)</span>
</li>
</ul>
<p><span class="math display" id="eq:nn024">\[\begin{align}
\cfrac{\partial RSS}{\partial \beta_0} &amp;= \sum\limits_{i=1}^n \cfrac{\partial RSS_i}{\partial \beta_0} \\
&amp; = - \sum\limits_{i=1}^n \left(y_i - f\left(\textbf{x}_i\right) \right)
\tag{17.23}
\end{align}\]</span></p>
<ul>
<li>Theo <span class="math inline">\(\beta_l\)</span> với <span class="math inline">\(l &gt; 0\)</span>
</li>
</ul>
<p><span class="math display" id="eq:nn025">\[\begin{align}
\cfrac{\partial RSS}{\partial \beta_l} &amp;= \sum\limits_{i=1}^n \cfrac{\partial RSS_i}{\partial \beta_l} \\
&amp; = - \sum\limits_{i=1}^n \mathbb{I}(z_{i,l} &gt; 0) \cdot z_{i,l} \cdot \left(y_i - f\left(\textbf{x}_i\right) \right)
\tag{17.24}
\end{align}\]</span></p>
<ul>
<li>Theo <span class="math inline">\(w_{l,0}\)</span>,</li>
</ul>
<p><span class="math display" id="eq:nn026">\[\begin{align}
\cfrac{\partial RSS_i}{\partial w_{l,0}} &amp; =  - \beta_l \sum\limits_{i=1}^n \mathbb{I}(z_{i,l} &gt; 0) \cdot \left(y_i - f\left(\textbf{x}_i\right) \right)
\tag{17.25}
\end{align}\]</span></p>
<ul>
<li>Theo <span class="math inline">\(w_{l,j}\)</span> với j &gt; 0
<span class="math display" id="eq:nn027">\[\begin{align}
\cfrac{\partial RSS_i}{\partial w_{l,j}} &amp; = - \beta_l \sum\limits_{i=1}^n \mathbb{I}(z_{i,l} &gt; 0) \cdot x_{i,j} \cdot \left(y_i - f\left(\textbf{x}_i\right) \right)
\tag{17.26}
\end{align}\]</span>
</li>
</ul>
<p>Sau khi tính toán gradient của RSS theo các tham số, quá trình ước lượng sẽ được thực hiện thông qua thuật toán Stochastic gradient descent. Bạn đọc tham khảo thuật toán này trong Phụ lục <a href="#sec:sgd"><strong>??</strong></a> của chương Kiến thức R nâng cao. Kết quả của thuật toán Stochastic gradient descent phụ thuộc rất lớn vào giá trị khởi tạo ban đầu của các tham số, và nhất là khi số lượng tham số là rất lớn.</p>
<p>Để mô tả quá trình ước lượng tham số của mạng nơ-ron, chúng ta sẽ sử dụng dữ liệu mô phỏng. Ma trận biến giải thích <span class="math inline">\(\textbf{X}\)</span> có kích thước <span class="math inline">\(10^4 \times 3\)</span> là các số ngẫu nhiên độc lập có phân phối chuẩn <span class="math inline">\(\mathcal{N}(0,1)\)</span>. Hàm <span class="math inline">\(f\)</span> được tạo bởi mô hình mạng nơ-ron với 3 đơn vị của lớp đầu vào, một lớp ẩn với 5 đơn vị, và một đơn vị trong lớp đầu đầu ra. Hàm kích hoạt được sử dụng là hàm ReLU. Biến giải thích <span class="math inline">\(Y\)</span> được tính toán từ hàm <span class="math inline">\(f(\textbf{X})\)</span> công thêm một sai số độc lập với <span class="math inline">\(\textbf{X}\)</span> là một biến ngẫu nhiên phân phối chuẩn với trung bình bằng 0 và độ lệch chuẩn là 0.5. Các tham số dùng để tính toán <span class="math inline">\(f\)</span> được đơn giản hóa: <span class="math inline">\(\beta_l = 2 \forall l = 0, 1, \cdots, 5\)</span> và $w_{l,j} = 1 l = 1, 2, , $ và $j = , $. Quá trình tối thiểu hóa tổng sai số bình phương được mô tả thông qua hình <a href="neuralnetwork.html#fig:fgnn005">17.5</a></p>
<div class="figure">
<span style="display:block;" id="fig:fgnn005"></span>
<img src="12-mo-hinh-mang-noron_files/figure-html/fgnn005-1.png" alt="Sai số của mô hình mạng nơ-ron giảm dần trong quá trình ước lượng tham số sử dụng thuật toán stochastic gradient descent. Hình bên trái: điểm bắt đầu của các tham số là biến ngẫu nhiên phân phối chuẩn độc lập có trung bình bằng 0 và phương sai bằng 3. Hình bên phải: điểm bắt đầu của các tham số expression{beta} là biến ngẫu nhiên có trung bình bằng 2 và phương sai bằng 1. Điểm bắt đầu của các tham số w là biến ngẫu nhiên có trung bình bằng 1 và phương sai bằng 1" width="672"><p class="caption">
Hình 17.5: Sai số của mô hình mạng nơ-ron giảm dần trong quá trình ước lượng tham số sử dụng thuật toán stochastic gradient descent. Hình bên trái: điểm bắt đầu của các tham số là biến ngẫu nhiên phân phối chuẩn độc lập có trung bình bằng 0 và phương sai bằng 3. Hình bên phải: điểm bắt đầu của các tham số expression{beta} là biến ngẫu nhiên có trung bình bằng 2 và phương sai bằng 1. Điểm bắt đầu của các tham số w là biến ngẫu nhiên có trung bình bằng 1 và phương sai bằng 1
</p>
</div>
<p>Hình <a href="neuralnetwork.html#fig:fgnn005">17.5</a> mô tả quá trình tối thiểu hóa sai số tính bằng RSS trên dữ liệu mô phỏng bằng phương pháp stochastic gradient descent. Tổng số tham số cần được ước lượng của mô hình là 26, bao gồm 6 giá trị của véc-tơ <span class="math inline">\(\boldsymbol{\beta}\)</span> và 20 giá trị của ma trận <span class="math inline">\(\boldsymbol{w}\)</span>. Chúng tôi sử dụng 5 nghìn lần lặp và trong mỗi lần lặp sử dụng 5% dữ liệu để tính các gradient.</p>
<p>Hình bên trái mô tả 10 quá trình ước lượng tham số mà các điểm bắt đầu của <span class="math inline">\(\boldsymbol{\beta}\)</span> và <span class="math inline">\(\boldsymbol{w}\)</span> là hoàn toàn ngẫu nhiên. Chúng tôi cho các giá trị ban đầu là các biến ngẫu nhiên phân phối chuẩn với trung bình bằng 0 và phương sai bằng 3. Hình bên phải mô tả 10 quá trình ước lượng tham số với các điểm bắt đầu của <span class="math inline">\(\boldsymbol{\beta}\)</span> có giá trị trung bình là 2 bằng với giá trị dùng để mô phỏng dữ liệu và <span class="math inline">\(\boldsymbol{w}\)</span> có giá trị trung bình là 1 cũng bằng với giá trị dùng để mô phỏng dữ liệu. Phương sai của 26 tham số khởi đầu đều bằng 1. Có thể thấy rằng khi các tham số khởi đầu là hoàn toàn ngẫu nhiên thì về trung bình các quá trình hội tụ về giá ngưỡng nho nhất của RSS chậm hơn khi chúng ta có các giá trị khởi đầu tốt hơn. Có hai trên tám quá trình sau 5000 bước lặp vẫn chưa cho sai số tiệm cận đến giá trị RSS nhỏ nhất. Trong hình bên phải thì các quá trình đều cho kết quả gần với giá trị RSS nhỏ nhất.</p>
</div>
<div id="ước-lượng-tham-số-cho-mạng-nơ-ron-phân-loại-có-hai-lớp-ẩn" class="section level3" number="17.3.2">
<h3>
<span class="header-section-number">17.3.2</span> Ước lượng tham số cho mạng nơ-ron phân loại có hai lớp ẩn<a class="anchor" aria-label="anchor" href="#%C6%B0%E1%BB%9Bc-l%C6%B0%E1%BB%A3ng-tham-s%E1%BB%91-cho-m%E1%BA%A1ng-n%C6%A1-ron-ph%C3%A2n-lo%E1%BA%A1i-c%C3%B3-hai-l%E1%BB%9Bp-%E1%BA%A9n"><i class="fas fa-link"></i></a>
</h3>
<p>Giả sử cấu trúc mạng nơ-ron có hai lớp ẩn như hình <a href="neuralnetwork.html#fig:fgnn004">17.4</a>. Các tham số cần ước lượng của mô hình bao gồm các ma trận <span class="math inline">\(\boldsymbol{w}_1\)</span>, <span class="math inline">\(\boldsymbol{w}_2\)</span>, và <span class="math inline">\(\boldsymbol{\beta}\)</span> được cho bởi phương trình <a href="neuralnetwork.html#eq:nn015">(17.15)</a>. Do đây là bài toán phân loại nên hàm mục tiêu được sử dụng là hàm cross-entropy
<span class="math display" id="eq:nn028">\[\begin{align}
(\hat{\boldsymbol{w}}_1, \hat{\boldsymbol{w}}_2, \hat{\boldsymbol{\beta}}) &amp;= \underset{\boldsymbol{w}_1, \boldsymbol{w}_2, \boldsymbol{\beta}}{\operatorname{argmin}} \sum\limits_{i=1}^n \sum\limits_{j=1}^m \ y^{i}_{j} \cdot \log(\hat{y}^{i}_{j}) \\
\tag{17.27}
\end{align}\]</span></p>
<p>Giá trị của hàm cross-entropy tính trên quan sát thứ <span class="math inline">\(i\)</span> có thể được rút gọn như sau
<span class="math display" id="eq:nn029">\[\begin{align}
\sum\limits_{j=1}^m \ y^{i}_{j} \cdot \log(\hat{y}^{i}_{j}) &amp;= \sum\limits_{j=1}^m \ y^{i}_{j} \cdot \log\left(\cfrac{exp\left(z_{i,j}\right)}{exp\left(z_{i,1}\right)+exp\left(z_{i,2}\right)+\cdots+exp\left(z_{i,m}\right)}\right) \\
&amp; = \sum\limits_{j=1}^m \ y^{i}_{j} \cdot \left(z_{i,j} -  \log\left(exp\left(z_{i,1}\right)+exp\left(z_{i,2}\right)+\cdots+exp\left(z_{i,m}\right)\right) \right)\\
&amp; =  \sum\limits_{j=1}^m \ y^{i}_{j} \cdot z_{i,j} - \log\left(\sum\limits_{j=1}^m exp\left(z_{i,j}\right)\right)
\tag{17.28}
\end{align}\]</span>
với <span class="math inline">\(z_{i,j}\)</span> là tổ hợp tuyến tính của giá trị các nút ẩn thứ hai tính theo đầu vào <span class="math inline">\(\textbf{x}_i\)</span>
<span class="math display" id="eq:nn030">\[\begin{align}
z_{i,j} = \beta_{j,0} + \beta_{j,1} h^{(2)}_{i,1} + \beta_{j,2} h^{(2)}_{i,2} + \cdots + \beta_{j,k_2} h^{(2)}_{i,k_2}
\tag{17.29}
\end{align}\]</span></p>
<p>Với mọi tham số <span class="math inline">\(\theta\)</span> được sử dụng để tính toán giá trị tại các nút đầu ra, đạo hàm của hàm cross-entropy sẽ được tính toán thông qua các <span class="math inline">\(z_{i,j}\)</span>
<span class="math display" id="eq:nn031">\[\begin{align}
\cfrac{\partial CE\_Loss_i}{\partial \theta} &amp;= \sum\limits_{j=1}^m \ y^{i}_{j} \ \cfrac{\partial z_{i,j}}{\partial \theta} - \sum\limits_{j=1}^m \cfrac{\partial z_{i,j}}{\partial \theta} \cfrac{exp(z_{i,j})}{\sum exp\left(z_{i,j}\right)} \\
&amp; = \sum\limits_{j=1}^m  \cfrac{\partial z_{i,j}}{\partial \theta} \left(y^{i}_{j} - p_{i,j}\right)
\tag{17.30}
\end{align}\]</span>
trong đó
<span class="math display" id="eq:nn032">\[\begin{align}
p_{i,j} = \cfrac{exp(z_{i,j})}{\sum\limits_{j=1}^m exp\left(z_{i,j}\right)}
\tag{17.31}
\end{align}\]</span></p>
<p>Từ công thức <a href="neuralnetwork.html#eq:nn030">(17.29)</a> có thể thấy rằng: véc-tơ dữ liệu đầu ra <span class="math inline">\(\textbf{y}_i = (y^{i}_{1}, y^{i}_{2}, \cdots, y^{i}_{m})\)</span> nhận giá trị bằng 1 tại một vị trí và nhận giá trị bằng 0 tại <span class="math inline">\(m-1\)</span> vị trí còn lại, trong khi đó <span class="math inline">\(p_{i,j}\)</span> có thể được hiểu là xác suất mà đầu ra thứ <span class="math inline">\(i\)</span> nhận giá trị bằng <span class="math inline">\(j\)</span> được xác định bởi mô hình mạng nơ-ron. Đạo hàm của hàm tổn thất tính bằng cross-entropy theo tham số <span class="math inline">\(\theta\)</span> bất kỳ, là một phần tử của ma trận <span class="math inline">\(\boldsymbol{\beta}\)</span> hoặc các <span class="math inline">\(\boldsymbol{w}\)</span>, phụ thuộc vào sai số giữa véc-tơ xác suất từ dữ liệu quan sát được <span class="math inline">\(\textbf{y}^i\)</span> và véc-tơ xác suất được xác định bởi mô hình mạng nơ-ron <span class="math inline">\(\textbf{p}_{i} = (p_{i,1}, p_{i,2}, \cdot, p_{i,m})\)</span>. Như vậy, gradient của hàm tổn thất theo từng tham số phụ thuộc vào sai số của mô hình hiện tại. Nói một cách khác, sai số của mô hình trong bước hiện tại sẽ tác động đến việc cập nhật tham số trong bước tiếp theo khi chúng ta sử dụng phương pháp gradient descent. Đây là quá trình lan truyền ngược trong mô hình mạng nơ-ron mà chúng tôi đã đề cập đến trong phần mô hình mạng nơ-ron hồi quy có một lớp ẩn.</p>
<p>Quá trình tính toán đạo hàm của <span class="math inline">\(z_{i,j}\)</span> theo các tham số của ma trận <span class="math inline">\(\boldsymbol{\beta}\)</span> là khá hiển nhiên do <span class="math inline">\(z_{i,j}\)</span> là hàm tuyến tính theo các tham số này. Để tính toán đạo hàm của <span class="math inline">\(z_{i,j}\)</span> theo các tham số của các ma trận <span class="math inline">\(\boldsymbol{w}\)</span>, chúng ta sử dụng nguyên tắc chain-rule đã trình bày trong phụ lục của phần Kiến thức R nâng cao. Do chỉ số của các tham số trong ma trận là quá phức tạp nên chúng tôi sẽ chỉ trình bày cách tính đạo hàm mang tính tổng quát. Ta có mỗi <span class="math inline">\(z\)</span> được tính từ phương trình <a href="neuralnetwork.html#eq:nn030">(17.29)</a> được xác định từ dữ liệu đầu vào <span class="math inline">\(x\)</span> thông qua các hàm kích hoạt <span class="math inline">\(g_1\)</span> và <span class="math inline">\(g_2\)</span> như sau
<span class="math display" id="eq:nn033">\[\begin{align}
z &amp; = \beta h^{(2)} \\
h^{(2)} &amp;= g_2(w^{(2)}\cdot h^{(1)}) \\
h^{(1)} &amp;= g_1(w^{(1)}\cdot x)
\tag{17.32}
\end{align}\]</span></p>
<p>Trong đó <span class="math inline">\(\beta\)</span>, <span class="math inline">\(w^{(2)}\)</span>, và <span class="math inline">\(w^{(1)}\)</span> là các tham số của mô hình. Giá trị đạo hàm của <span class="math inline">\(z\)</span> theo các tham số được xác định như sau</p>
<ul>
<li><p>Theo <span class="math inline">\(\beta\)</span>
<span class="math display" id="eq:nn034">\[\begin{align}
\cfrac{\partial z}{\partial \beta} = h^{(2)}
\tag{17.33}
\end{align}\]</span></p></li>
<li><p>Theo <span class="math inline">\(w^{(2)}\)</span>, chúng ta áp dụng nguyên tắc chain-rule
<span class="math display" id="eq:nn035">\[\begin{align}
\cfrac{\partial z}{\partial w^{(2)}} &amp; = \cfrac{\partial z}{\partial h^{(2)}} \times \cfrac{\partial h^{(2)}}{\partial w^{(2)}} \\
&amp; =  \beta \cdot h^{(1)} \cdot  g^{'}_2(w^{(2)}\cdot h^{(1)})
\tag{17.34}
\end{align}\]</span></p></li>
<li><p>Theo <span class="math inline">\(w^{(1)}\)</span>, áp dụng nguyên tắc chain-rule
<span class="math display" id="eq:nn036">\[\begin{align}
\cfrac{\partial z}{\partial w^{(1)}} &amp; = \cfrac{\partial z}{\partial h^{(2)}} \times \cfrac{\partial h^{(2)}}{\partial h^{(1)}} \times \cfrac{\partial h^{(1)}}{\partial w^{(1)}} \\
&amp; =  \beta \cdot w^{(2)} \cdot g^{'}_2(w^{(2)}\cdot h^{(1)}) \cdot x \cdot g^{'}_1(w^{(1)}\cdot x)
\tag{17.35}
\end{align}\]</span></p></li>
</ul>
<p>Số lượng tham số cần được tính toán trong mô hình mạng nơ-ron phân loại với <span class="math inline">\(p\)</span> đầu vào, <span class="math inline">\(m\)</span> đầu ra, hai lớp ẩn với số lượng đơn vị lần lượt là <span class="math inline">\(k_1\)</span> và <span class="math inline">\(k_2\)</span> là</p>
<ul>
<li><p><span class="math inline">\(m \cdot (k_2+1)\)</span> tham số trong ma trận <span class="math inline">\(\boldsymbol{\beta}\)</span>,</p></li>
<li><p><span class="math inline">\(k_2 \cdot (k_1 + 1)\)</span> tham số trong ma trận <span class="math inline">\(\boldsymbol{w}_2\)</span></p></li>
<li><p><span class="math inline">\(k_1 \cdot (p + 1)\)</span> tham số trong ma trận <span class="math inline">\(\boldsymbol{w}_1\)</span></p></li>
</ul>
<p>Số lượng tham số quá lớn sẽ dẫn đến các vấn đề bao gồm sự phức tạp khi tiến hành tính toán, khối lượng tính toán lớn, và rất dễ dẫn đến overfitting. Để khắc phục vấn đề này, mô hình mạng nơ-ron thường phải sử dụng thêm các kỹ thuật để thêm ràng buộc tham số hoặc loại bỏ đơn vị trong các lớp. Chúng ta sẽ thảo luận các kỹ thuật này trong phần tiếp theo.</p>
</div>
<div id="khắc-phục-hiện-tượng-khớp-quá-mức-của-mạng-nơ-ron" class="section level3" number="17.3.3">
<h3>
<span class="header-section-number">17.3.3</span> Khắc phục hiện tượng khớp quá mức của mạng nơ-ron<a class="anchor" aria-label="anchor" href="#kh%E1%BA%AFc-ph%E1%BB%A5c-hi%E1%BB%87n-t%C6%B0%E1%BB%A3ng-kh%E1%BB%9Bp-qu%C3%A1-m%E1%BB%A9c-c%E1%BB%A7a-m%E1%BA%A1ng-n%C6%A1-ron"><i class="fas fa-link"></i></a>
</h3>
<p>Phương pháp trước hết để hạn chế mô hình khớp quá mức là sử dụng ràng buộc tham số. Nguyên tắc ràng buộc tham số trong mô hình mạng nơ-ron cũng tương tự trong hồi quy ridge. Ví dụ, với mô hình mạng nơ-ron hồi quy được trình bày trong phần <a href="neuralnetwork.html#nnonelayer">17.1</a> hàm mục tiêu cần được tối thiểu hóa là tổng bình phương sai số cộng thêm một hàm phạt có dạng tổng bình phương các tham số sử dụng trong mô hình.</p>
<p><span class="math display" id="eq:nn037">\[\begin{align}
(\hat{\boldsymbol{\beta}},\hat{\boldsymbol{w}}) &amp;= \underset{\boldsymbol{\beta},\boldsymbol{w}}{\operatorname{argmin}} &amp;= \sum\limits_{i=1}^n \ \left(y_i - f_{\boldsymbol{\beta}, \boldsymbol{w}}\left(\textbf{x}_i\right) \right)^2 + \lambda_1 \cdot \sum\limits_{j} \beta_j^2 + \lambda_2 \sum\limits_{l,j} w_{l,j}^2
\tag{17.36}
\end{align}\]</span></p>
<p>Trong mô hình mạng nơ-ron phân loại có hai lớp ẩn trong phần <a href="neuralnetwork.html#nnmultilayer">17.2</a>, hàm mục tiêu tính bằng cross-entropy cũng được biến đổi bằng cách thêm vào một hàm phạt dạng tổng bình phương của các tham số:</p>
<p>Người xây dựng mô hình có thể sử dụng các hàm phạt khác như hàm tổng giá trị tuyệt đối trong hồi quy Lasso, hoặc kết hợp giữa Lasso và ridge. Một lưu ý khác khi sử dụng ràng buộc tham số đó là người xây dựng mô hình thường không sử dụng ràng buộc tham số trực tiếp tính lớp đầu ra, nghĩa là thường cho <span class="math inline">\(\lambda_1\)</span> trong các phương trình <a href="neuralnetwork.html#eq:nn037">(17.36)</a> và <a href="neuralnetwork1.html#eq:nn038">(18.2)</a> bằng 0. Với mỗi giá trị của các tham số <span class="math inline">\(\lambda\)</span>, chúng ta tiến hành ước lượng tham số của mạng nơ-ron giống như đã trình bày trong phần <a href="neuralnetwork.html#nnestimation">17.3</a>.</p>
<p>Một phương pháp khác để giảm bớt hiện tượng khớp quá mức của mạng nơ-ron là phương pháp loại bỏ đơn vị (unit dropout). Các đơn vị của lớp đầu vào và các lớp ẩn có thể tham gia vào mô hình mạng nơ-ron với xác suất là <span class="math inline">\(p\)</span> và không tham gia vào mô hình với xác suất <span class="math inline">\((1-p)\)</span> trong mỗi bước của quá trình huấn luyện mô hình, trong đó <span class="math inline">\(p\)</span> là tham số của kỹ thuật dropout. Ý tưởng của phương pháp này hoàn toàn tương tự như ý tưởng của thuật toán rừng ngẫu nhiên áp dụng trên cây quyết định. Quá trình ước lượng tham số của mô hình mạng nơ-ron bao gồm hai quá trình: quá trình chuyển tiếp từ đầu vào, qua các lớp ẩn, và kết thúc tại lớp đầu ra, và quá trình lan truyền ngược, khi sai số tính tại lớp đầu ra được sử dụng để tính toán sự thay đổi cho tất cả các tham số của mô hình hiện tại. Nếu trong tất cả các lần chuyển tiếp và lan truyền ngược, chúng ta giữ nguyên cấu trúc của mạng, trong mỗi lớp đầu vào hoặc lớp ẩn có thể có một hoặc một số đơn vị chiếm ưu thế so với các đơn vị khác, làm cho kết quả tại đầu ra phụ thuộc rất lớn vào các đơn vị này. Cũng giống như ý tưởng của thuật toán rừng ngẫu nhiên, trong mỗi lần chuyển tiếp và lan truyền ngược tương ứng, tại lớp đầu vào và các lớp ẩn, người xây dựng mô hình chỉ lựa chọn một số ngẫu nhiên các đơn vị vào trong quá trình tính toán tham số. Nói cách khác, mỗi đơn vị được lựa chọn vào quá trình tính toán với xác suất là <span class="math inline">\(p\)</span>. Chúng tôi sẽ mô tả cách thực hiện kỹ thuật dropout trong phần thực hành.</p>
<p>Hàng thứ hai trong Bảng 10.1 được dán nhãn dropout. Đây là một dạng chính quy hóa tương đối mới bị loại bỏ và hiệu quả, tương tự ở một số khía cạnh với chính quy hóa đường gờ. Lấy cảm hứng từ các khu rừng ngẫu nhiên (Phần 8.2), ý tưởng là loại bỏ ngẫu nhiên một phần φ của các đơn vị trong một lớp khi tạo mô hình. Hình 10.19 minh họa điều này. Việc này được thực hiện riêng biệt mỗi khi xử lý quan sát đào tạo. Các đơn vị còn sống thay thế cho những đơn vị bị thiếu và trọng số của chúng được tăng lên theo hệ số 1/(1 − φ) để bù đắp. Điều này ngăn các nút trở nên chuyên biệt hóa quá mức và có thể được coi là một hình thức chính quy hóa. Trong thực tế, việc bỏ học đạt được bằng cách đặt ngẫu nhiên các kích hoạt cho các đơn vị “bị bỏ” về 0, trong khi vẫn giữ nguyên kiến trúc.</p>
</div>
</div>
<div id="pickands-dependent-function" class="section level2" number="17.4">
<h2>
<span class="header-section-number">17.4</span> Pickands dependent function<a class="anchor" aria-label="anchor" href="#pickands-dependent-function"><i class="fas fa-link"></i></a>
</h2>
<p><span class="math display">\[\begin{align}
C(u,v) = exp\left( log(uv) \cdot A\left(\cfrac{log(v)}{log(u)+log(v)} \right) \right)
\end{align}\]</span></p>
<p>Đạo hàm của <span class="math inline">\(C(u,v)\)</span> tính theo <span class="math inline">\(A\)</span>
<span class="math display">\[\begin{align}
\cfrac{\partial C(u,v)}{\partial u} &amp;= C(u,v) \times \cfrac{1}{u} \cdot \left(A(t) - A^{'}(t) \cdot t \right) \\
\cfrac{\partial C(u,v)}{\partial v} &amp;= C(u,v) \times \cfrac{1}{v} \cdot \left(A(t) + A^{'}(t) \cdot (1-t) \right) \\
\end{align}\]</span>
with <span class="math inline">\(t = \cfrac{log(v)}{log(u)+log(v)}\)</span></p>
<p>Density của <span class="math inline">\(C(u,v)\)</span>
<span class="math display">\[\begin{align}
\cfrac{\partial^2 C(u,v)}{\partial u \partial v}  &amp;= C(u,v) \times \cfrac{1}{uv} \cdot \left[ \left(A(t) - A^{'}(t) \cdot t \right) \cdot \left(A(t) + A^{'}(t) \cdot (1-t) \right) - \cfrac{A^{''}(t) t(1-t)}{log(uv)} \right]\\
\end{align}\]</span></p>
</div>
<div id="nhóm-các-hàm-at-để-cuv-là-một-copula" class="section level2" number="17.5">
<h2>
<span class="header-section-number">17.5</span> Nhóm các hàm A(t) để C(u,v) là một copula<a class="anchor" aria-label="anchor" href="#nh%C3%B3m-c%C3%A1c-h%C3%A0m-at-%C4%91%E1%BB%83-cuv-l%C3%A0-m%E1%BB%99t-copula"><i class="fas fa-link"></i></a>
</h2>
<ul>
<li>
<p>Các điều kiện biên: <span class="math inline">\(C(0,v) = 0\)</span>, <span class="math inline">\(C(u,v) = 0\)</span>, <span class="math inline">\(C(1,v) = v\)</span>, và <span class="math inline">\(C(u,1) = u\)</span></p>
<ul>
<li>Khi <span class="math inline">\(u \rightarrow 0\)</span> thì <span class="math inline">\(t \rightarrow 0\)</span> và khi <span class="math inline">\(v \rightarrow 0\)</span> thì <span class="math inline">\(t \rightarrow 1\)</span>
</li>
</ul>
</li>
</ul>
<p><span class="math display">\[\begin{align}
C(0,v) = exp\left( log(0 \cdot v) \cdot A\left(0 \right) \right) = 0^1 = 0
\end{align}\]</span></p>
<p><span class="math display">\[\begin{align}
C(u,0) = exp\left( log(u \cdot 0) \cdot A\left(0 \right) \right) = 0^1 = 0
\end{align}\]</span></p>
<ul>
<li>Khi <span class="math inline">\(u \rightarrow 1\)</span> thì <span class="math inline">\(t \rightarrow 1\)</span> và khi <span class="math inline">\(v \rightarrow 1\)</span> thì <span class="math inline">\(t \rightarrow 0\)</span>
</li>
</ul>
<p><span class="math display">\[\begin{align}
C(1,v) = exp\left( log(1 \cdot v) \cdot A\left(1 \right) \right) = v^1 = v
\end{align}\]</span></p>
<p><span class="math display">\[\begin{align}
C(u,1) = exp\left( log(u \cdot 1) \cdot A\left(0 \right) \right) = u^1 = u
\end{align}\]</span></p>
<ul>
<li>Các hàm <span class="math inline">\(C_u(u,v)\)</span> và <span class="math inline">\(C_v(u,v)\)</span> là hàm phân phối xác suất: <span class="math inline">\(C_u(u,v)\)</span> là hàm phân phối xác suất của biến <span class="math inline">\(V\)</span> với mọi <span class="math inline">\(u\)</span>: Khi <span class="math inline">\(v \rightarrow 0\)</span>,</li>
</ul>
<p><span class="math display">\[\begin{align}
C(u,v) \times \cfrac{1}{u} \cdot \left(A(t) - A^{'}(t) \cdot t \right) &amp;\rightarrow C(u,0) \times \cfrac{1}{u} \cdot \left(A(1) - A^{'}(1) \cdot 1 \right)\\
&amp; = 0 \times \cfrac{1}{u} \cdot \left(A(1) - A^{'}(1) \right) \\
&amp; = 0
\end{align}\]</span></p>
<p>Khi <span class="math inline">\(v \rightarrow 1\)</span>,</p>
<p><span class="math display">\[\begin{align}
C(u,v) \times \cfrac{1}{u} \cdot \left(A(t) - A^{'}(t) \cdot t \right) &amp;\rightarrow C(u,1) \times \cfrac{1}{u} \cdot \left(A(0) - A^{'}(0) \cdot 0 \right)\\
&amp; = 1 \times 1 \\
&amp; = 1
\end{align}\]</span></p>
<p>Đạo hàm của hàm <span class="math inline">\(C_u(u,v)\)</span> theo <span class="math inline">\(v\)</span> là hàm tăng theo <span class="math inline">\(v\)</span>: Nếu <span class="math inline">\(A^{''}(t) \geq 0\)</span> <span class="math inline">\(\forall t\)</span> thì<br><span class="math display">\[\begin{align}
C(u,v) \times \cfrac{1}{uv} \cdot \left[ \left(A(t) - A^{'}(t) \cdot t \right) \cdot \left(A(t) + A^{'}(t) \cdot (1-t) \right) - \cfrac{A^{''}(t) t(1-t)}{log(uv)} \right] \geq 0
\end{align}\]</span>
với mọi <span class="math inline">\(u,v\)</span></p>
<div id="cách-thứ-nhất-để-tham-số-hóa-đa-thức-từng-phần" class="section level3" number="17.5.1">
<h3>
<span class="header-section-number">17.5.1</span> Cách thứ nhất để tham số hóa đa thức từng phần<a class="anchor" aria-label="anchor" href="#c%C3%A1ch-th%E1%BB%A9-nh%E1%BA%A5t-%C4%91%E1%BB%83-tham-s%E1%BB%91-h%C3%B3a-%C4%91a-th%E1%BB%A9c-t%E1%BB%ABng-ph%E1%BA%A7n"><i class="fas fa-link"></i></a>
</h3>
<p>Hàm <span class="math inline">\(f\)</span> là đa thức từng phần thỏa mãn điều kiện thành pickand dependent function
<span class="math display">\[\begin{align}
\theta \in [0.5,1] \\
\lambda_1 \geq \theta \\
\lambda_2 = \cfrac{ \left(\cfrac{\theta^3}{6}+\lambda_1 \cdot \cfrac{\theta^2}{2}\right) \cdot \theta - \left(\theta+\lambda_1\right) \cdot \left(\cfrac{\theta^3}{6}-\cfrac{\theta}{2}+\cfrac{1}{3}\right)  }{\cfrac{(1-\theta)^2\cdot(\theta+\lambda_1)}{2} - \left(\cfrac{\theta^3}{6}+\lambda_1 \cdot \cfrac{\theta^2}{2}\right)}
\end{align}\]</span></p>
</div>
<div id="cách-thứ-hai" class="section level3" number="17.5.2">
<h3>
<span class="header-section-number">17.5.2</span> Cách thứ hai<a class="anchor" aria-label="anchor" href="#c%C3%A1ch-th%E1%BB%A9-hai"><i class="fas fa-link"></i></a>
</h3>
<p>Chọn điểm cắt <span class="math inline">\(\theta\)</span>, lựa chọn hàm <span class="math inline">\(A(t)\)</span> như sau
<span class="math display">\[\begin{align}
A(t) = \mathbb{I}_{(t \leq \theta)} \left(a_0 + a_1 \cdot x + a_2 \cdot x^2 + a_3 \cdot x^3\right) +
\mathbb{I}_{(t &gt; \theta)} \left(b_0 + b_1 \cdot (1-x) + b_2 \cdot (1-x)^2 + b_3 \cdot (1-x)^3\right)
\end{align}\]</span>
với các ràng buộc:</p>
<ul>
<li>Hàm <span class="math inline">\(A(t)\)</span> có đạo hàm cấp 0,1,2 liên tục tại <span class="math inline">\(\theta\)</span>
</li>
</ul>
<p><span class="math display">\[\begin{align}
&amp; \begin{pmatrix}
(1-\theta) &amp; (1-\theta)^2 &amp; (1-\theta)^3 \\
- 1 &amp; - 2\cdot(1 - \theta) &amp; -3(1-\theta)^2 \\
0 &amp; 1 &amp; 3(1-\theta)\\
\end{pmatrix} \times
\begin{pmatrix}
b_1\\
b_2\\
b_3\\
\end{pmatrix} =
\begin{pmatrix}
\theta &amp; \theta^2 &amp; \theta^3 \\
1 &amp; 2\theta &amp; 3\theta^2 \\
0 &amp; 1 &amp; 3\theta\\
\end{pmatrix} \times
\begin{pmatrix}
a_1\\
a_2\\
a_3\\
\end{pmatrix} \\
&amp; \\
\rightarrow &amp;
\begin{pmatrix}
(1-\theta) &amp; (1-\theta)^2 &amp; (1-\theta)^3 \\
- 1 &amp; - 2\cdot(1 - \theta) &amp; -3(1-\theta)^2 \\
0 &amp; 1 &amp; 3(1-\theta)\\
\end{pmatrix}^{-1}
\times \begin{pmatrix}
\theta &amp; \theta^2 &amp; \theta^3 \\
1 &amp; 2\theta &amp; 3\theta^2 \\
0 &amp; 1 &amp; 3\theta\\
\end{pmatrix} \times
\begin{pmatrix}
a_1\\
a_2\\
a_3\\
\end{pmatrix} = \begin{pmatrix}
b_1\\
b_2\\
b_3\\
\end{pmatrix}
&amp; \\
\rightarrow &amp;
\begin{pmatrix}
\cfrac{3}{1-\theta} &amp; 2 &amp; 1 - \theta \\
- \cfrac{3}{(1-\theta)^2} &amp; - \cfrac{3}{1-\theta} &amp; -2 \\
\cfrac{1}{(1-\theta)^3} &amp; \cfrac{1}{(1-\theta)^2} &amp; \cfrac{1}{(1-\theta)}\\
\end{pmatrix}
\times \begin{pmatrix}
\theta &amp; \theta^2 &amp; \theta^3 \\
1 &amp; 2\theta &amp; 3\theta^2 \\
0 &amp; 1 &amp; 3\theta\\
\end{pmatrix} \times
\begin{pmatrix}
a_1\\
a_2\\
a_3\\
\end{pmatrix} = \begin{pmatrix}
b_1\\
b_2\\
b_3\\
\end{pmatrix}
&amp; \\
\rightarrow &amp;
\begin{pmatrix}
\cfrac{2+\theta}{1-\theta} &amp; \cfrac{2\theta+1}{1-\theta} &amp; \cfrac{3\theta}{1 - \theta} \\
- \cfrac{3}{(1-\theta)^2} &amp; \cfrac{\theta^2-2\theta-2}{(1-\theta)^2} &amp; \cfrac{3\theta^2-6\theta}{(1-\theta)^2} \\
\cfrac{1}{(1-\theta)^3} &amp; \cfrac{1}{(1-\theta)^3} &amp; \cfrac{1 - (1-\theta)^3}{(1-\theta)^3}\\
\end{pmatrix}
\times
\begin{pmatrix}
a_1\\
a_2\\
a_3\\
\end{pmatrix} = \begin{pmatrix}
b_1\\
b_2\\
b_3\\
\end{pmatrix}


\end{align}\]</span></p>
<ul>
<li>
</ul>
</div>
</div>
<div id="thực-hành-1" class="section level2" number="17.6">
<h2>
<span class="header-section-number">17.6</span> Thực hành:<a class="anchor" aria-label="anchor" href="#th%E1%BB%B1c-h%C3%A0nh-1"><i class="fas fa-link"></i></a>
</h2>
<div id="mô-hình-mạng-nơ-ron-trên-dữ-liệu-boston" class="section level3" number="17.6.1">
<h3>
<span class="header-section-number">17.6.1</span> Mô hình mạng nơ-ron trên dữ liệu Boston<a class="anchor" aria-label="anchor" href="#m%C3%B4-h%C3%ACnh-m%E1%BA%A1ng-n%C6%A1-ron-tr%C3%AAn-d%E1%BB%AF-li%E1%BB%87u-boston"><i class="fas fa-link"></i></a>
</h3>
</div>
<div id="mô-hình-mạng-nơ-ron-để-phân-loại-khách-hàng" class="section level3" number="17.6.2">
<h3>
<span class="header-section-number">17.6.2</span> Mô hình mạng nơ-ron để phân loại khách hàng<a class="anchor" aria-label="anchor" href="#m%C3%B4-h%C3%ACnh-m%E1%BA%A1ng-n%C6%A1-ron-%C4%91%E1%BB%83-ph%C3%A2n-lo%E1%BA%A1i-kh%C3%A1ch-h%C3%A0ng"><i class="fas fa-link"></i></a>
</h3>
</div>
</div>
<div id="phụ-lục-6" class="section level2" number="17.7">
<h2>
<span class="header-section-number">17.7</span> Phụ lục<a class="anchor" aria-label="anchor" href="#ph%E1%BB%A5-l%E1%BB%A5c-6"><i class="fas fa-link"></i></a>
</h2>
</div>
<div id="bài-tập-4" class="section level2" number="17.8">
<h2>
<span class="header-section-number">17.8</span> Bài tập<a class="anchor" aria-label="anchor" href="#b%C3%A0i-t%E1%BA%ADp-4"><i class="fas fa-link"></i></a>
</h2>
<!-- # REFERENCE -->
<!-- ### Source from thesis -->
<!-- **1.** Chen, Chun-houh, Wolfgang Karl Härdle, and Antony Unwin, eds (2007). *Handbook of data visualization.* \ -->
<!-- **2.** Aparicio, Manuela, and Carlos J. Costa. (2015). *Data visualization - Communication design quarterly review.* \ -->
<!-- **3.** Hadley Wickham. (2010). *A Layered Grammar of Graphics.* \ -->
<!-- ### Souce from website -->
<!-- **4.** [https://www.tableau.com/learn/articles/data-visualization](https://www.tableau.com/learn/articles/data-visualization) \ -->
<!-- **5.** [https://www.r-graph-gallery.com/ggplot2-package.html](https://www.r-graph-gallery.com/ggplot2-package.html) \ -->
<!-- **6.** [http://r-statistics.co/ggplot2-Tutorial-With-R.html](http://r-statistics.co/ggplot2-Tutorial-With-R.html) \ -->
<!-- **7.** [https://www.maths.usyd.edu.au/u/UG/SM/STAT3022/r/current/Misc/data-visualization-2.1.pdf](https://www.maths.usyd.edu.au/u/UG/SM/STAT3022/r/current/Misc/data-visualization-2.1.pdf) \ -->
<!-- **8.** [https://www.kaggle.com/](https://www.kaggle.com/) \ -->
<!-- ### Hồi quy logistic là một mạng nơ-ron không có layer ẩn -->
<!-- Trước khi giới thiệu về cấu trúc của một mô hình mạng nơ-ron, chúng ta sẽ xem xét lại cách hồi quy logistic hoạt động. Sau đó, khi nhìn nhận cách vận hành của hổi quy logistic như một mạng nơ-ron đơn giản, bạn đọc sẽ có hình dung cụ thể hơn về cách xây dựng mô hình mạng nơ-ron.  -->
<!-- Dữ liệu dùng để xây dựng mô hình hồi quy logistic được xây dựng dựa trên dữ liệu bao gồm 5 quan sát được cho trong bảng dưới đây -->
<!-- ```{r table70 input, include=FALSE} -->
<!-- Col0 <- c("A","B","C","D","E","F") -->
<!-- Col1 <- c(1,2,4,3,1.5,3) -->
<!-- Col2 <- c(2,1,1,3,3.0,2) -->
<!-- Col3 <- c("blue", "red", "red", "blue", "blue", "?") -->
<!-- ``` -->
<!-- ```{r, echo=FALSE} -->
<!-- #lik50.tab -->
<!-- df1 <- data.frame(x0 = Col0, x1 = Col1, x2 = Col2, y = Col3) -->
<!-- kable(df1, booktabs = T, -->
<!--       col.names = c("Dữ liệu","x1", -->
<!--         "x2",  -->
<!--         "Màu sắc"), -->
<!--       caption = "Dữ liệu cho hồi quy Logistic",  -->
<!--       escape=F) %>% -->
<!--   column_spec(c(2,3)) %>%  -->
<!--   kable_styling(latex_options = "scale_down") -->
<!-- ``` -->
<!-- ```{r plot1, echo =FALSE} -->
<!-- ggplot(df1, aes(x1,x2,fill=y))+ -->
<!--   geom_point(size=15,alpha=0.8,color="black", shape = 23)+ -->
<!--   geom_text(aes(label=x0),color="black")+ -->
<!--   scale_fill_manual(values=c("grey","red","blue"))+ -->
<!--   xlim(c(0,5))+ylim(c(0,4))+ -->
<!--   theme_classic()+ -->
<!--   xlab("")+ylab("")+ -->
<!--   theme(legend.position = "none") -->
<!-- ``` -->
<!-- Với mỗi điểm dữ liệu bất kỳ, giả sử là điểm A có với các thuộc tính $x_1(A) = $ và $x_2(A) = $, chúng ta sẽ thực hiện hai phép biến đổi kế tiếp nhau: -->
<!-- - Phép biến đổi tuyển tính: với bộ 3 số thực bất kỳ $(b_0, b_1, b_2)$, chúng ta luôn có thể thực hiện phép biến đổi tuyến tính: -->
<!-- $$  -->
<!-- A(1, x_1, x_2) \rightarrow b_0 \times 1 + b_1 \times x_1 + b_2 \times x_2 -->
<!-- $$ -->
<!-- - Phép biến đổi phi tuyến tính: chúng ta sử dụng hàm số $f(x) = sigmoid(x) = \cfrac{1}{1+e^{x}}$ để thực hiện phép biến đổi thứ hai -->
<!-- $$  -->
<!-- b_0 + b_1 x_1 + b_2 x_2 \rightarrow \cfrac{1}{1+e^{b_0 + b_1 x_1 + b_2 x_2}} -->
<!-- $$ -->
<!-- Sau khi thực hiện các phép biến đổi với từng điểm dữ liệu, chúng ta sẽ thu được với mỗi điểm dữ liệu một số nằm trong khoảng $(0,1)$.  -->
<!-- ```{r table701 input, include=FALSE} -->
<!-- Col0 <- c("A","B","C","D","E") -->
<!-- Col1 <- c(1,2,4,3,1.5) -->
<!-- Col2 <- c(2,1,1,3,3.0) -->
<!-- Col3 <- c("blue", "red", "red", "blue", "blue") -->
<!-- Col4 <- c(1,0,0,1,1) -->
<!-- Col5 <- c("$(1+exp(b_0+ b_1+2 b_2))^{-1}$", -->
<!--           "$(1+exp(b_0+2 b_1+ b_2))^{-1}$", -->
<!--           "$(1+exp(b_0+4 b_1+ b_2))^{-1}$", -->
<!--           "$(1+exp(b_0+3 b_1+3 b_2))^{-1}$", -->
<!--           "$(1+exp(b_0+1.5 b_1+3 b_2))^{-1}$") -->
<!-- ``` -->
<!-- ```{r, echo=FALSE} -->
<!-- #lik50.tab -->
<!-- df1 <- data.frame(x0 = Col0, x1=Col1,x2= Col2,y= Col3, yi=Col4, transformation=Col5) -->
<!-- kable(df1, booktabs = T, -->
<!--       col.names = c("Dữ liệu","$x_1$", -->
<!--         "$x_2$",  -->
<!--         "Màu sắc", "Biến mục tiêu ($y_i$)" , "Dữ liệu sau chuyển đổi ($p_i$)"), -->
<!--       caption = "Dữ liệu cho hồi quy Logistic",  -->
<!--       escape=F) %>% -->
<!--   column_spec(c(2,6)) %>%  -->
<!--   kable_styling(latex_options = "scale_down") -->
<!-- ``` -->
<!-- Hàm tổn thất, tính bằng Cross Entropy qua năm điểm dữ liệu A, B, C, D, và E, sẽ là một hàm số của ba biến $(b_0, b_1, b_2)$ như sau -->
<!-- $$ -->
<!-- Loss(b_0, b_1, b_2) = - \sum\limits_{Data = A}^E [y_i \times log(p_i) + (1-y_i) \times log(1-p_i)] -->
<!-- $$ -->
<!-- với giá trị của $y_i$ và $p_i$ được cho bởi bảng ở trên. Bằng thuật toán gradient descent, chúng ta có thể tính toán được giá trị của $(b_0, b_1, b_2)$ sao cho hàm $Loss$ đạt giá trị nhỏ nhất bằng $(25,15,-30)$. Vậy có thể tính toán khả năng điểm $F$ có màu xanh là -->
<!-- $$ -->
<!-- \mathbb{P}(F = blue) = (1+exp(25 + 3 \times 15 + 3 \times -30))^{-1} = 1 -->
<!-- $$ -->
<!-- Như vậy, trong hổi quy logistic, chúng ta đã sử dụng 2 phép biến đổi dữ liệu bao gồm một phép biến đổi tuyến tính thông qua một véc-tơ $b$ và sau đó là một phép biến đổi phi tuyến (hàm sigmoid) để thu được một giá trị duy nhất cho mỗi điểm dữ liệu. Quá trình ước lượng mô hình logistic là quá trình tìm kiếm các tham số $b$ sao cho giá trị đầu ra sau các phép biến đổi của mỗi điểm dữ liệu gần với kết quả mong muốn nhất có thể. -->

<pre><code>## 
## Attaching package: 'dplyr'</code></pre>
<pre><code>## The following objects are masked from 'package:stats':
## 
##     filter, lag</code></pre>
<pre><code>## The following objects are masked from 'package:base':
## 
##     intersect, setdiff, setequal, union</code></pre>
<pre><code>## 
## Attaching package: 'kableExtra'</code></pre>
<pre><code>## The following object is masked from 'package:dplyr':
## 
##     group_rows</code></pre>
<pre><code>## 
## Attaching package: 'gridExtra'</code></pre>
<pre><code>## The following object is masked from 'package:dplyr':
## 
##     combine</code></pre>
<pre><code>## 
## Attaching package: 'pryr'</code></pre>
<pre><code>## The following object is masked from 'package:dplyr':
## 
##     where</code></pre>
<pre><code>## Loading required package: shape</code></pre>
</div>
</div>
  <div class="chapter-nav">
<div class="prev"><a href="boosting-v%C3%A0-c%C3%A2y-quy%E1%BA%BFt-%C4%91%E1%BB%8Bnh..html"><span class="header-section-number">16</span> Boosting và cây quyết định.</a></div>
<div class="next"><a href="neuralnetwork1.html"><span class="header-section-number">18</span> Các mạng học sâu điển hình</a></div>
</div></main><div class="col-md-3 col-lg-2 d-none d-md-block sidebar sidebar-chapter">
    <nav id="toc" data-toggle="toc" aria-label="On this page"><h2>On this page</h2>
      <ul class="nav navbar-nav">
<li><a class="nav-link" href="#neuralnetwork"><span class="header-section-number">17</span> Mô hình mạng nơ-ron</a></li>
<li><a class="nav-link" href="#nnonelayer"><span class="header-section-number">17.1</span> Mạng nơ-rơn có một lớp ẩn</a></li>
<li><a class="nav-link" href="#nnmultilayer"><span class="header-section-number">17.2</span> Mạng nơ-ron có nhiều lớp ẩn</a></li>
<li>
<a class="nav-link" href="#nnestimation"><span class="header-section-number">17.3</span> Ước lượng tham số của mạng nơ-ron</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#%C6%B0%E1%BB%9Bc-l%C6%B0%E1%BB%A3ng-tham-s%E1%BB%91-cho-m%E1%BA%A1ng-n%C6%A1-ron-h%E1%BB%93i-quy-c%C3%B3-m%E1%BB%99t-l%E1%BB%9Bp-%E1%BA%A9n"><span class="header-section-number">17.3.1</span> Ước lượng tham số cho mạng nơ-ron hồi quy có một lớp ẩn</a></li>
<li><a class="nav-link" href="#%C6%B0%E1%BB%9Bc-l%C6%B0%E1%BB%A3ng-tham-s%E1%BB%91-cho-m%E1%BA%A1ng-n%C6%A1-ron-ph%C3%A2n-lo%E1%BA%A1i-c%C3%B3-hai-l%E1%BB%9Bp-%E1%BA%A9n"><span class="header-section-number">17.3.2</span> Ước lượng tham số cho mạng nơ-ron phân loại có hai lớp ẩn</a></li>
<li><a class="nav-link" href="#kh%E1%BA%AFc-ph%E1%BB%A5c-hi%E1%BB%87n-t%C6%B0%E1%BB%A3ng-kh%E1%BB%9Bp-qu%C3%A1-m%E1%BB%A9c-c%E1%BB%A7a-m%E1%BA%A1ng-n%C6%A1-ron"><span class="header-section-number">17.3.3</span> Khắc phục hiện tượng khớp quá mức của mạng nơ-ron</a></li>
</ul>
</li>
<li><a class="nav-link" href="#pickands-dependent-function"><span class="header-section-number">17.4</span> Pickands dependent function</a></li>
<li>
<a class="nav-link" href="#nh%C3%B3m-c%C3%A1c-h%C3%A0m-at-%C4%91%E1%BB%83-cuv-l%C3%A0-m%E1%BB%99t-copula"><span class="header-section-number">17.5</span> Nhóm các hàm A(t) để C(u,v) là một copula</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#c%C3%A1ch-th%E1%BB%A9-nh%E1%BA%A5t-%C4%91%E1%BB%83-tham-s%E1%BB%91-h%C3%B3a-%C4%91a-th%E1%BB%A9c-t%E1%BB%ABng-ph%E1%BA%A7n"><span class="header-section-number">17.5.1</span> Cách thứ nhất để tham số hóa đa thức từng phần</a></li>
<li><a class="nav-link" href="#c%C3%A1ch-th%E1%BB%A9-hai"><span class="header-section-number">17.5.2</span> Cách thứ hai</a></li>
</ul>
</li>
<li>
<a class="nav-link" href="#th%E1%BB%B1c-h%C3%A0nh-1"><span class="header-section-number">17.6</span> Thực hành:</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#m%C3%B4-h%C3%ACnh-m%E1%BA%A1ng-n%C6%A1-ron-tr%C3%AAn-d%E1%BB%AF-li%E1%BB%87u-boston"><span class="header-section-number">17.6.1</span> Mô hình mạng nơ-ron trên dữ liệu Boston</a></li>
<li><a class="nav-link" href="#m%C3%B4-h%C3%ACnh-m%E1%BA%A1ng-n%C6%A1-ron-%C4%91%E1%BB%83-ph%C3%A2n-lo%E1%BA%A1i-kh%C3%A1ch-h%C3%A0ng"><span class="header-section-number">17.6.2</span> Mô hình mạng nơ-ron để phân loại khách hàng</a></li>
</ul>
</li>
<li><a class="nav-link" href="#ph%E1%BB%A5-l%E1%BB%A5c-6"><span class="header-section-number">17.7</span> Phụ lục</a></li>
<li><a class="nav-link" href="#b%C3%A0i-t%E1%BA%ADp-4"><span class="header-section-number">17.8</span> Bài tập</a></li>
</ul>

      <div class="book-extra">
        <ul class="list-unstyled">
<li><a id="book-source" href="https://github.com/NEUKhoaToanKT/Khoa_hoc_du_lieu_trong_KTKD/blob/master/12-mo-hinh-mang-noron.Rmd">View source <i class="fab fa-github"></i></a></li>
          <li><a id="book-edit" href="https://github.com/NEUKhoaToanKT/Khoa_hoc_du_lieu_trong_KTKD/edit/master/12-mo-hinh-mang-noron.Rmd">Edit this page <i class="fab fa-github"></i></a></li>
        </ul>
</div>
    </nav>
</div>

</div>
</div> <!-- .container -->

<footer class="bg-primary text-light mt-5"><div class="container"><div class="row">

  <div class="col-12 col-md-6 mt-3">
    <p>"<strong>Khoa Học Dữ Liệu trong Kinh tế và Kinh doanh</strong>" was written by Nguyễn Quang Huy. It was last built on 2024-08-05.</p>
  </div>

  <div class="col-12 col-md-6 mt-3">
    <p>This book was built by the <a class="text-light" href="https://bookdown.org">bookdown</a> R package.</p>
  </div>

</div></div>
</footer><!-- dynamically load mathjax for compatibility with self-contained --><script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script><script type="text/x-mathjax-config">const popovers = document.querySelectorAll('a.footnote-ref[data-toggle="popover"]');
for (let popover of popovers) {
  const div = document.createElement('div');
  div.setAttribute('style', 'position: absolute; top: 0, left:0; width:0, height:0, overflow: hidden; visibility: hidden;');
  div.innerHTML = popover.getAttribute('data-content');

  var has_math = div.querySelector("span.math");
  if (has_math) {
    document.body.appendChild(div);
    MathJax.Hub.Queue(["Typeset", MathJax.Hub, div]);
    MathJax.Hub.Queue(function() {
      popover.setAttribute('data-content', div.innerHTML);
      document.body.removeChild(div);
    })
  }
}
</script>
</body>
</html>
