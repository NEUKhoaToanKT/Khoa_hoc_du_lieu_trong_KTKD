[{"path":"index.html","id":"prerequisites","chapter":"Chương 1 Prerequisites","heading":"Chương 1 Prerequisites","text":"sample book written Markdown. can use anything Pandoc’s Markdown supports, e.g., math equation \\(^2 + b^2 = c^2\\).bookdown package can installed CRAN Github:Remember Rmd file contains one one chapter, chapter defined first-level heading #.compile example PDF, need XeLaTeX. recommended install TinyTeX (includes XeLaTeX): https://yihui.name/tinytex/.","code":"\n colorize <- function(x, color) {\n if (knitr::is_latex_output()) {\n sprintf(\"\\\\textcolor{%s}{%s}\", color, x)\n } else if (knitr::is_html_output()) {\n sprintf(\"<span style='color: %s;'>%s<\/span>\", \n color,\n x)\n } else x\n }\ninstall.packages(\"bookdown\")\n# or the development version\n# devtools::install_github(\"rstudio/bookdown\")"},{"path":"giới-thiệu-chung.html","id":"giới-thiệu-chung","chapter":"Chương 2 Giới thiệu chung","heading":"Chương 2 Giới thiệu chung","text":"","code":""},{"path":"giới-thiệu-chung.html","id":"giới-thiệu-về-khoa-học-dữ-liệu-và-các-ứng-dụng","chapter":"Chương 2 Giới thiệu chung","heading":"2.1 Giới thiệu về Khoa học dữ liệu và các ứng dụng","text":"Khoa học dữ liệu kết hợp giữa toán học với khoa học máy tính và một kiến thức chuyên ngành để khám phá những thông tin hữu ích và có giá trị trong dữ liệu để hỗ trợ việc ra quyết định và lập kế hoạch hành động. Một quá trình ứng dụng KHDL vào giải quyết một vấn đề cụ thể thường bao gồm các bước được mô tả như Hình @ref().\nFigure 2.1: Quy trình áp dụng Khoa học dữ liệu để giải quyết một vấn đề thực tế\n\\(\\textbf{Nhập liệu}\\) là quá trình tìm kiếm và thu thập dữ liệu từ các nguồn khác nhau để phục vụ cho mục đích ra quyết định. Có những dự án làm việc trực tiếp trên dữ liệu có sẵn và đã được thiết kế sẵn sàng cho mục tiêu phân tích nhưng cũng có những dự án mà quá trình nhập liệu lại chiếm phần lớn thời gian và quyết định sự thành công hay thất bại của dự án.\\(\\textbf{Nhập liệu}\\) là quá trình tìm kiếm và thu thập dữ liệu từ các nguồn khác nhau để phục vụ cho mục đích ra quyết định. Có những dự án làm việc trực tiếp trên dữ liệu có sẵn và đã được thiết kế sẵn sàng cho mục tiêu phân tích nhưng cũng có những dự án mà quá trình nhập liệu lại chiếm phần lớn thời gian và quyết định sự thành công hay thất bại của dự án.\\(\\textbf{Sắp xếp}\\) dữ liệu hay còn được gọi là tiền xử lý dữ liệu là các bước biến dữ liệu từ dạng thô thành dữ liệu theo đúng như định dạng mong muốn.\\(\\textbf{Sắp xếp}\\) dữ liệu hay còn được gọi là tiền xử lý dữ liệu là các bước biến dữ liệu từ dạng thô thành dữ liệu theo đúng như định dạng mong muốn.\\(\\textbf{Biến đổi}\\) dữ liệu là quá trình tính toán trên các biến hoặc các quan sát của dữ liệu để dữ liệu có thể đưa vào các công cụ trực quan hoặc đưa vào trong các mô hình phân tích.\\(\\textbf{Biến đổi}\\) dữ liệu là quá trình tính toán trên các biến hoặc các quan sát của dữ liệu để dữ liệu có thể đưa vào các công cụ trực quan hoặc đưa vào trong các mô hình phân tích.\\(\\textbf{Xây dựng mô hình}\\) là quá trình tính toán và đánh giá mối liên hệ giữa các biến trong dữ liệu đến các biến mục tiêu là đầu ra của toàn bộ quá trình. Mô hình thường được xây dựng với một trong hai mục đích là xem xét sự tác động của một biến đến các biến mục tiêu, hoặc nhằm mục đích dự đoán giá trị của các biến mục tiêu.\\(\\textbf{Xây dựng mô hình}\\) là quá trình tính toán và đánh giá mối liên hệ giữa các biến trong dữ liệu đến các biến mục tiêu là đầu ra của toàn bộ quá trình. Mô hình thường được xây dựng với một trong hai mục đích là xem xét sự tác động của một biến đến các biến mục tiêu, hoặc nhằm mục đích dự đoán giá trị của các biến mục tiêu.Mô hình trên dữ liệu được xây dựng dựa trên những nguyên lý của toán học và xác suất thống kê. Dữ liệu được sử dụng để xây dựng mô hình có thể là các dữ liệu nhỏ với một vài cột và vài chục quan sát, nhưng cũng có thể là các dữ liệu lớn với hàng nghìn cột dữ liệu và hàng triệu quan sát. Dữ liệu thậm chí không có dạng bảng biểu như chúng ta gặp hàng ngày mà có thể là các hình ảnh, các văn bản, giọng nói, dạng đồ thị, … Để xử lý các bộ dữ liệu khổng lồ, hay các dữ liệu không có cấu trúc bảng thông thường, người xử lý dữ liệu và cần có các kiến thức về khoa học máy tính và lập trình để thực hiện các tính toán trên máy tính điện tử. Những ứng dụng của KHDL có thể thuộc về bất kỳ lĩnh vực nào như kinh doanh, y học, vật lý, thiên văn, quản lý nhà nước, chính sách công, … nên đòi hỏi người xây dựng mô hình cũng cần có kiến thức chuyên môn trong lĩnh vực tương ứng để không bị sai định hướng trong quá trình làm việc với dữ liệu.Để minh họa ứng dụng của KHDL trong lĩnh vực Kinh tế và Kinh doanh, chúng tôi thảo luận ngắn gọn về ba dữ liệu được thu nhập trong thế giới thực được xem xét trong cuốn sách này.","code":""},{"path":"giới-thiệu-chung.html","id":"dữ-liệu-chi-phí-quảng-cáo","chapter":"Chương 2 Giới thiệu chung","heading":"2.1.1 Dữ liệu chi phí quảng cáo","text":"","code":""},{"path":"giới-thiệu-chung.html","id":"dữ-liệu-bảo-hiểm-xe-ô-tô","chapter":"Chương 2 Giới thiệu chung","heading":"2.1.2 Dữ liệu bảo hiểm xe ô tô","text":"","code":""},{"path":"giới-thiệu-chung.html","id":"dữ-liệu-về-giá-nhà","chapter":"Chương 2 Giới thiệu chung","heading":"2.1.3 Dữ liệu về giá nhà","text":"","code":""},{"path":"giới-thiệu-chung.html","id":"sơ-lược-quá-trình-phát-triển-của-xây-dựng-mô-hình-dữ-liệu","chapter":"Chương 2 Giới thiệu chung","heading":"2.2 Sơ lược quá trình phát triển của xây dựng mô hình dữ liệu","text":"Mặc dù thuật ngữ xây dựng mô hình trên dữ liệu, hay được gọi một cách kỹ thuật hơn là học máy, còn khá mới mẻ nhưng những khái niệm nền tảng cho lĩnh vực này đã được phát triển từ lâu. Vào đầu thế kỷ 19, phương pháp bình phương nhỏ nhất đã được phát triển và áp dụng để ước lượng các mô hình hồi quy tuyến tính. Mô hình này lần đầu tiên được áp dụng và cho kết quả thành công trong các vấn đề liên quan đến thiên văn học. Vào đầu thế kỷ thứ 20, mô hình hồi quy tuyến tính được sử dụng để dự đoán các giá trị định lượng, chẳng hạn như mức lương của một cá nhân hoặc để dự đoán các giá trị định tính, chẳng hạn như bệnh nhân sống hay chết, hay thị trường chứng khoán tăng hay giảm. Vào những năm 1940, nhiều tác giả đã đưa ra một cách tiếp cận khác, đó là hồi quy logistic. Vào đầu những năm 1970, thuật ngữ mô hình tuyến tính tổng quát đã được phát triển để mô tả toàn bộ lớp phương pháp học thống kê bao gồm cả hồi quy tuyến tính và hồi quy logistic như các trường hợp đặc biệt. Vào cuối những năm 1970, nhiều kỹ thuật xây dựng mô hình trên dữ liệu đã xuất hiện. Tuy nhiên, các mô hình này chỉ xoay quanh là các phương pháp tuyến tính vì việc tạo ra các mối quan hệ phi tuyến tính rất khó khăn về mặt tính toán.Đến những năm 1980, sự phát triển của máy tính điện tử đã hỗ trợ tích cực về mặt tính toán cho các các phương pháp phi tuyến tính. Các mô hình phi tuyến được giới thiệu vào đầu những năm của thập niên 80 bao gồm có mô hình cây quyết định và mô hình cộng tính tổng quát. Những năm cuối thập niên 80 và đầu thập niên 90, mô hình mạng nơ-ron được giới thiệu đến cộng đồng nghiên cứu nhưng chưa có được nhiều sự quan tâm vì dữ liệu chưa đủ phong phú và sự phổ biến của các mô hình học máy khác.Giai đoạn cuối thế kỷ XX và đầu thế kỷ XXI là giai đoạn chiếm ưu thế hoàn toàn của các mô hình học máy rừng ngẫu nhiên và thuật toán học tăng cường. Thuật toán học tăng cường với các biến thể như XGBoost hay LightGBM chiến thắng trong hầu hết các cuộc thi về Khoa học dữ liệu.Từ năm 2010, với sự bùng nổ của các thiết bị thông minh và kết nối internet, dữ liệu trở nên phong phú và đa dạng hơn cũng là thời điểm quay trở lại của mô hình mạng nơ-ron, hay còn được gọi với tên gọi khác là mô hình mạng học sâu (deep learning). Mô hình mạng học sâu vượt trội hoàn toàn các mô hình học máy thông thường khi làm việc với dữ liệu kiểu hình ảnh, video, ngôn ngữ tự nhiên bao gồm cả văn bản và giọng nói. Sự kiện đánh dấu sự phát triển vượt bậc của các mô hình mạng học sâu là sự ra đời của ChatGPT vào cuối những năm 2022 là một mô hình ngôn ngữ lớn cho phép người dùng tương tác, hỏi đáp và trò chuyện một cách hoàn toàn tự nhiên theo định hướng của người sử dụng như phong cách, mức độ chi tiết, hình thức ngôn ngữ. ChatGPT nhanh chóng đạt đến 100 triệu người sau hơn hai tháng phát hành và giúp cho công ty phát hành OpenAI được định giá khoảng 30 tỷ USD. Cho đến thời điểm cuối năm 2023 khi nhóm tác giả bắt đầu viết cuốn sách này, ChatGPT đã được cập nhật đến phiên bản 3.5.","code":""},{"path":"giới-thiệu-chung.html","id":"tại-sao-lại-sử-dụng-phần-mềm-r","chapter":"Chương 2 Giới thiệu chung","heading":"2.3 Tại sao lại sử dụng phần mềm R","text":"Trong thế giới ngày nay, hầu hết các cơ quan, tổ chức, tập đoàn, doanh nghiệp từ lớn đến nhỏ đều sử dụng một số lượng dữ liệu nhất định để phân tích các sự kiện trong quá khứ và cố gắng dự đoán xu hướng trong tương lai để đưa ra các quyết định có lợi cho mình. Tuy nhiên khi dữ liệu ngày càng tăng lên cả về số lượng và sự phức tạp, thì các cơ quan tổ chức cần một công cụ giúp họ thực hiện được các phân tích trên dữ liệu một cách nhanh hơn và chính xác hơn. Một trong các công cụ hiệu quả nhất tại thời điểm hiện tại là phần mềm R.","code":""},{"path":"giới-thiệu-chung.html","id":"phần-mềm-r-là-gì","chapter":"Chương 2 Giới thiệu chung","heading":"2.3.1 Phần mềm R là gì?","text":"Trước tiên, R là ngôn ngữ lập trình được xây dựng để phục vụ cho toán học và thống kê, đồng thời R cũng là một môi trường phần mềm mã nguồn mở miễn phí cho người sử dụng. R được giới thiệu lần đầu tiên vào năm 1992 bởi các giáo sư Ross Ihaka và Robert Gentleman như một ngôn ngữ lập trình để dạy thống kê tại Đại học Auckland. Tên của ngôn ngữ, R, xuất phát từ chữ cái đầu tiên của các tác giả là Ross và Robert.Trước khi trở thành ngôn ngữ lập trình cho Khoa học dữ liệu, R thường được coi là ngôn ngữ lập trình cho các nhà toán học và thống kê. Sau nhiều năm phát triển, R luôn được coi vẫn là một trong những ngôn ngữ lập trình phổ biến nhất trong giới học thuật vì độ tin cậy. Mỗi thư viện của R đều được phát triển một cách hoàn chỉnh và trải qua quá trình kiểm soát chặt chẽ. Tạp chí R (R Journal) là tạp chí học thuật về các phương pháp tính toán trong toán học và thống kê sử dụng phầm mềm R luôn trong danh sách các tạp chí khoa học uy tín (Science Citation Index Expanded hay SCIE) của Web Science. Chính vì sự uy tín trong học thuật nên đa số các trường đại học và viện nghiên cứu hàng đầu trên thế giới sử dụng như là một ngôn ngữ chính trong đào tạo về tính toán, toán học và thống kê.","code":""},{"path":"giới-thiệu-chung.html","id":"tại-sao-r-lại-được-sử-dụng-trong-khdl","chapter":"Chương 2 Giới thiệu chung","heading":"2.3.2 Tại sao R lại được sử dụng trong KHDL","text":"Trong khoảng hơn 10 năm trở lại đây, R không còn chỉ là ngôn ngữ lập trình thông thường như trước đây nữa. Mặc dù vẫn là một công cụ mạnh mẽ trong tính toán toán học và thống kê, nhưng còn có rất nhiều công cụ tuyệt vời khác mà bạn đọc có thể làm với R, đặc biệt là những ứng dụng trong KHDL. Nguyên nhân chính giúp cho phần mềm R nhanh chóng trở thành ngôn ngữ phổ biến trong KHDL là nền tảng quan trọng nhất của KHDL chính là toán học và thống kê. Đồng thời, ngôn ngữ lập trình R cũng đủ linh hoạt để người sử dụng viết các chương trình yêu cầu tính toán phức tạp trong Khoa học máy tính. Một cách tự nhiên, những nhà toán học, thống kê học và các tổ chức sử đang dụng R như là một ngôn ngữ chính sẽ tìm cách phát triển R để đáp ứng được với yêu cầu xử lý dữ liệu của chính họ. Một nguyên nhân khác khiến cho R phổ biến trong KHDL là đặc thù mã nguồn mở của phần mềm này. Những người làm việc trong lĩnh vực KHDL sử dụng R có thể chia sẻ kiến thức và kinh nghiệm một cách nhanh chóng và rộng rãi.","code":""},{"path":"giới-thiệu-chung.html","id":"r-có-thể-làm-những-gì","chapter":"Chương 2 Giới thiệu chung","heading":"2.3.3 R có thể làm những gì?","text":"Danh sách những việc bạn có thể làm trong R là không thể liệt hết bởi vì phần mềm này vẫn đang được phát triển không ngừng. Dưới đây là một số ứng dụng phổ biến mà R vượt trội hơn với các ngôn ngữ khác:Lập trình trong toán học, tính toán tối ưu, giải tối ưu bằng phương pháp sốLập trình trong toán học, tính toán tối ưu, giải tối ưu bằng phương pháp sốTính toán liên quan đến lý thuyết xác suất.Tính toán liên quan đến lý thuyết xác suất.Thực hiện các kiểm định thống kê.Thực hiện các kiểm định thống kê.Phát triển phần mềm thống kê.Phát triển phần mềm thống kê.Xây dựng mô hình kinh tế lượng.Xây dựng mô hình kinh tế lượng.Mô phỏng ngẫu nhiên.Mô phỏng ngẫu nhiên.Các tính năng của R dành cho Khoa học dữ liệu được liệt kê dưới đây:Thu thập tập dữ liệu, bao gồm cả dữ liệu lớn và không có cấu trúc,Thu thập tập dữ liệu, bao gồm cả dữ liệu lớn và không có cấu trúc,Khai phá dữ liệu,Khai phá dữ liệu,Xử lý, sắp xếp, biến đổi dữ liệu,Xử lý, sắp xếp, biến đổi dữ liệu,Phân tích dữ liệu,Phân tích dữ liệu,Trực quan hóa dữ liệu,Trực quan hóa dữ liệu,Xây dựng các mô hình học máy từ đơn giản đến phức tạpXây dựng các mô hình học máy từ đơn giản đến phức tạp","code":""},{"path":"giới-thiệu-chung.html","id":"tại-sao-cuốn-sách-này-lại-sử-dụng-r","chapter":"Chương 2 Giới thiệu chung","heading":"2.3.4 Tại sao cuốn sách này lại sử dụng R","text":"Mặc dù có một số phần mềm khác có thể được sử dụng thay thế cho R trong phân tích dữ liệu, nhưng chúng tôi lựa chọn ngôn ngữ R vìR là một phần mềm uy tín và đáng tin cậy được sử dụng bởi các trường đại học và các viện nghiên cứu hàng đầu trên thế giới. R cũng được sử dụng rộng rãi trong các công ty công nghệ hàng đầu như Microsoft, Facebook, Google, IBM.R là một phần mềm uy tín và đáng tin cậy được sử dụng bởi các trường đại học và các viện nghiên cứu hàng đầu trên thế giới. R cũng được sử dụng rộng rãi trong các công ty công nghệ hàng đầu như Microsoft, Facebook, Google, IBM.R là ngôn ngữ lập trình rất dễ hiểu cho người mới bắt đầu kể cả với những người không có kinh nghiệm lập trình. Còn nếu bạn đã có kinh nghiệm về một ngôn ngữ lập trình, sẽ chỉ cần một khoảng thời gian ngắn để bạn có thể viết các chương trình với R.R là ngôn ngữ lập trình rất dễ hiểu cho người mới bắt đầu kể cả với những người không có kinh nghiệm lập trình. Còn nếu bạn đã có kinh nghiệm về một ngôn ngữ lập trình, sẽ chỉ cần một khoảng thời gian ngắn để bạn có thể viết các chương trình với R.Phần mềm R là một phần mềm mã nguồn mở, nghĩa là bạn đọc có thể sử dụng R và hơn 12000 thư viện mở rộng mà không cần phải bỏ ra bất kỳ một chi phí nào. Điều này giúp cho R trở nên rất dễ tiếp cận đối với những sinh viên và người học không sẵn sàng chi trả một khoản chi phí để học về KHDL. Đồng thời, giảng viên cũng có thể tận dụng tối đa môi trường phần mềm này khi giảng dạy cho sinh viên.Phần mềm R là một phần mềm mã nguồn mở, nghĩa là bạn đọc có thể sử dụng R và hơn 12000 thư viện mở rộng mà không cần phải bỏ ra bất kỳ một chi phí nào. Điều này giúp cho R trở nên rất dễ tiếp cận đối với những sinh viên và người học không sẵn sàng chi trả một khoản chi phí để học về KHDL. Đồng thời, giảng viên cũng có thể tận dụng tối đa môi trường phần mềm này khi giảng dạy cho sinh viên.Cuối cùng và cũng không kém phần quan trọng, đó là sự hỗ trợ từ công đồng. Với số lượng người dùng lên đến hàng triệu người, nhiều người trong đó là những nhà toán học, thống kê học, giáo sư tại các trường đại học, bạn sẽ luôn tìm thấy sự hỗ trợ mỗi khi gặp bất kỳ vấn đề khi làm việc với R.Cuối cùng và cũng không kém phần quan trọng, đó là sự hỗ trợ từ công đồng. Với số lượng người dùng lên đến hàng triệu người, nhiều người trong đó là những nhà toán học, thống kê học, giáo sư tại các trường đại học, bạn sẽ luôn tìm thấy sự hỗ trợ mỗi khi gặp bất kỳ vấn đề khi làm việc với R.","code":""},{"path":"giới-thiệu-chung.html","id":"các-lựa-chọn-thay-thế-và-bổ-sung-cho-r-trong-khdl","chapter":"Chương 2 Giới thiệu chung","heading":"2.3.5 Các lựa chọn thay thế và bổ sung cho R trong KHDL","text":"Như chúng ta đã thấy, R là một trong những ngôn ngữ lập trình tốt nhất cho người mới bắt đầu bước chân vào lĩnh vực KHDL. Tuy nhiên, bạn đọc cũng có thể tìm thấy các phần mềm/ngôn ngữ có thể sử dụng để thay thế cho R trong quá trình học tập và làm việc:Python - vào thời điểm chúng tôi đang hoàn thành cuốn sách này, Python là một ngôn ngữ lập trình được sử dụng nhiều nhất trong KHDL. Python được giới thiệu lần đầu tiên vào năm 1991 và đã không ngừng tiến hóa và phát triển. Cũng như R, cũng có mã nguồn mở và hoàn toàn miễn phí cho người sử dụng. Câu lệnh của Python cũng rất dễ học và đặc biệt mạnh mẽ trong lập trình hướng đối tượng.Python - vào thời điểm chúng tôi đang hoàn thành cuốn sách này, Python là một ngôn ngữ lập trình được sử dụng nhiều nhất trong KHDL. Python được giới thiệu lần đầu tiên vào năm 1991 và đã không ngừng tiến hóa và phát triển. Cũng như R, cũng có mã nguồn mở và hoàn toàn miễn phí cho người sử dụng. Câu lệnh của Python cũng rất dễ học và đặc biệt mạnh mẽ trong lập trình hướng đối tượng.Julia - xuất hiện lần đầu tiên vào năm 2012, Julia là một trong những ngôn ngữ lập trình được phát hành gần đây nhất và là lựa chọn tối ưu cho các nhà khoa học dữ liệu. Ngôn ngữ lập trình cấp cao và hiệu suất cao này rất năng động và phù hợp để viết bất kỳ loại ứng dụng nào. Mặc dù Python và R vẫn được ưu tiên cho KHDL và học máy nhưng dự báo là Julia sẽ vượt qua cả hai trong tương lai gần. Thật vậy, mặc dù đây là ngôn ngữ lập trình tổng quát hơn nhưng nó có tất cả các đặc điểm cần thiết để xử lý phân tích, thống kê và dữ liệu lớn.Julia - xuất hiện lần đầu tiên vào năm 2012, Julia là một trong những ngôn ngữ lập trình được phát hành gần đây nhất và là lựa chọn tối ưu cho các nhà khoa học dữ liệu. Ngôn ngữ lập trình cấp cao và hiệu suất cao này rất năng động và phù hợp để viết bất kỳ loại ứng dụng nào. Mặc dù Python và R vẫn được ưu tiên cho KHDL và học máy nhưng dự báo là Julia sẽ vượt qua cả hai trong tương lai gần. Thật vậy, mặc dù đây là ngôn ngữ lập trình tổng quát hơn nhưng nó có tất cả các đặc điểm cần thiết để xử lý phân tích, thống kê và dữ liệu lớn.MATLAB - được phát triển bởi MathWorks, ngôn ngữ lập trình này là một môi trường điện toán được phát triển đặc biệt để phân tích số và thống kê. Nhờ có số lượng lớn các thư viện có sẵn cho người dùng, MATLAB cho phép các lập trình viên truy cập dữ liệu, xử lý dữ liệu và tạo các mô hình học máy từ đơn giản đến phức tạp. Mặc dù MATLAB là một hệ thống hiệu suất cao nhưng lại không phải là nguồn mở hoặc miễn phí. Thay vào đó, nó được xây dựng bởi các nhà phát triển chuyên nghiệp vàMATLAB - được phát triển bởi MathWorks, ngôn ngữ lập trình này là một môi trường điện toán được phát triển đặc biệt để phân tích số và thống kê. Nhờ có số lượng lớn các thư viện có sẵn cho người dùng, MATLAB cho phép các lập trình viên truy cập dữ liệu, xử lý dữ liệu và tạo các mô hình học máy từ đơn giản đến phức tạp. Mặc dù MATLAB là một hệ thống hiệu suất cao nhưng lại không phải là nguồn mở hoặc miễn phí. Thay vào đó, nó được xây dựng bởi các nhà phát triển chuyên nghiệp vàJava là một trong những ngôn ngữ lập trình phổ biến nhất và cũng là một lựa chọn cho những người khi mới bước vào lĩnh vực KHDL. Mặc dù vẫn có thể tải xuống miễn phí nhưng một số ứng dụng chỉ có sẵn trong phiên bản trả phí. Cú pháp của Java cũng tương đối dễ học đối với người mới bắt đầu. Nhìn chung Java vẫn là ngôn ngữ có mục đích chung và nó vẫn được các nhà khoa học dữ liệu coi là một lựa chọn bổ sung cho R hoặc Python.Java là một trong những ngôn ngữ lập trình phổ biến nhất và cũng là một lựa chọn cho những người khi mới bước vào lĩnh vực KHDL. Mặc dù vẫn có thể tải xuống miễn phí nhưng một số ứng dụng chỉ có sẵn trong phiên bản trả phí. Cú pháp của Java cũng tương đối dễ học đối với người mới bắt đầu. Nhìn chung Java vẫn là ngôn ngữ có mục đích chung và nó vẫn được các nhà khoa học dữ liệu coi là một lựa chọn bổ sung cho R hoặc Python.Ngoài việc thành thạo R hoặc một phần mềm chuyên dùng trong KHDL, bạn nên bổ sung cho mình các ngôn ngữ lập trình khác để đạt hiệu suất công việc tốt nhất:SQL hay ngôn ngữ truy vấn dữ liệu có cấu trúc. SQL xuất hiện từ năm 1974 đã không ngừng được cải tiến và sửa đổi để giúp cho ngôn ngữ này luôn nằm trong nhóm những ngôn ngữ lập trình được lựa chọn hàng đầu trong KHDL. SQL có cả các phiên bản miễn phí và các phiên bản thương mại mà người sử dụng phải trả chi phí.C và C++ là các ngôn ngữ lập trình hiệu suất cao có thể giúp bạn tăng hiệu quả của các chương trình. Hầu như tất cả các ứng dụng trên hệ điều hành máy tính và điện thoại di động hiện nay đều sử dụng C và C++. Viết các chương trình dưới ngôn ngữ C hoặc C++ sẽ hiệu quả về mặt thời gian hơn nhiều với các ngôn ngữ khác.","code":""},{"path":"giới-thiệu-chung.html","id":"cài-đặt-r-và-rstudio","chapter":"Chương 2 Giới thiệu chung","heading":"2.3.6 Cài đặt R và RStudio","text":"Bạn đọc sẽ bắt đầu bằng cài đặt phần mềm R và sau đó là cài đặt RStudio - một môi trường phát triển phổ biến dành cho R. Phần mềm R dành cho các hệ điều hành MAC OS, Windows, và Linux đều có sẵn để tải xuống từ trang web chính thức:https://cran.r-project.org/Tại thời điểm chúng tôi viết cuốn sách này, R đang là phiên bản 4.1.0. Sau khi tải xuống, chúng ta chỉ cần cài đặt R giống như tất cả các phần mềm khác với tất cả các tùy chọn mặc định.Sau khi cài đặt phần mềm R, bạn đọc cài đặt Rstudio. Chúng ta hoàn toàn có thể sử dụng R mà không cần có Rstudio. Tuy nhiên, Rstudio sẽ hỗ trợ bạn rất nhiều trong quá trình sử dụng R, đó, lời khuyên của chúng tôi là hãy sử dụng Rstudio cùng với R. Để tải xuống Rstudio, bạn truy cập vào trang web chính thức:https://posit.co/download/rstudio-desktop/RStudio là một công cụ linh hoạt giúp bạn tạo các phân tích dễ đọc và giữ mã, hình ảnh, nhận xét và sơ đồ của bạn ở cùng một nơi. Sử dụng RStudio để lập trình và phân tích dữ liệu trong R mang lại nhiều lợi ích. Dưới đây là một vài ví dụ về những gì RStudio cung cấp:Giao diện trực quan cho phép chúng ta theo dõi các đối tượng, tập lệnh và số liệu đã lưu,Giao diện trực quan cho phép chúng ta theo dõi các đối tượng, tập lệnh và số liệu đã lưu,Trình soạn thảo văn bản có các tính năng như tự động gợi ý câu lệnh, hiển thị màu giúp cho việc viết các câu lệnh rõ ràng,Trình soạn thảo văn bản có các tính năng như tự động gợi ý câu lệnh, hiển thị màu giúp cho việc viết các câu lệnh rõ ràng,Hiển thị mô tả hàm số, dữ liệu bằng thao tác đơn giản,Hiển thị mô tả hàm số, dữ liệu bằng thao tác đơn giản,Thanh công cụ có đầy đủ các tính năng trực quan để bạn đọc sử dụng thay vì phải viết câu lệnh,Thanh công cụ có đầy đủ các tính năng trực quan để bạn đọc sử dụng thay vì phải viết câu lệnh,Mỗi khi bạn đọc mở RStudio, R cũng được khởi chạy tự động. Giao diện RStudio rất trực quan và dễ sử dụng. Các cửa sổ quan trọng bao gồm:Cửa số Console là nơi chúng ta có thể chạy các câu lệnh R.Cửa số Console là nơi chúng ta có thể chạy các câu lệnh R.Cửa sổ Environment là chúng ta theo dõi các đối tượng, tập lệnh và số liệu đã lưu.Cửa sổ Environment là chúng ta theo dõi các đối tượng, tập lệnh và số liệu đã lưu.Cửa sổ File là nơi hiển thị địa chỉ thư mục đang làm việc, hiển thị đồ thị trực quan, hoặc hiển thị mô tả dữ liệu, hàm số, thư viện.Cửa sổ File là nơi hiển thị địa chỉ thư mục đang làm việc, hiển thị đồ thị trực quan, hoặc hiển thị mô tả dữ liệu, hàm số, thư viện.Bạn đọc sẽ làm quen dần với giao diện và các cửa sổ làm việc khác nhau của RStudio trong quá trình thực hành trên các câu lệnh và dữ liệu cụ thể. Chúng tôi sẽ không đi quá sâu vào chi tiết tại đây.","code":""},{"path":"giới-thiệu-chung.html","id":"về-cuốn-sách-và-tác-giả","chapter":"Chương 2 Giới thiệu chung","heading":"2.4 Về cuốn sách và tác giả","text":"Cuốn sách được viết với mục tiêu là để thành sách tham khảo chính cho các môn học \\(\\textbf{Phân tích và dự báo}\\) và \\(\\textbf{Khoa học dữ liệu cơ bản}\\) cho sinh viên và học viên cao học ngành Toán Kinh tế tại Đại học Kinh tế Quốc dân. Chúng tôi tin rằng những kiến thức và công cụ được giới thiệu trong cuốn sách này sẽ là những hành trang quan trọng cho những nhà kinh tế và kinh doanh tương lai trước khi bước chân vào thế giới việc làm đầy tính cạnh tranh như hiện nay.","code":""},{"path":"giới-thiệu-chung.html","id":"đôi-lời-từ-tác-giả","chapter":"Chương 2 Giới thiệu chung","heading":"2.4.1 Đôi lời từ tác giả","text":"Tôi không phải là một nhà kinh tế, cũng không phải là một chuyên gia dữ liệu, và cũng không được đào tạo bài bản về máy tính hay lập trình, tôi là một Actuary. Cuốn sách được viết dựa trên kinh nghiệm làm việc và giảng dạy của tôi trong những lĩnh vực khoa học tính toán (actuarial science). Tôi bắt đầu sử dụng R như một phần mềm thống kê khi còn là một sinh viên đại học. Ấn tượng đầu tiên của tôi về R là khi sử dụng phần mềm này để mô phỏng các chuyển động Brown hình học vô cùng bắt mắt. Và R vẫn tiếp tục đồng hành với tôi cho đến nay trong cả môi trường doanh nghiệp và học thuật:Trong thời gian nghiên cứu sinh từ 2011 đến 2014, tôi sử dụng R là công cụ chính để thực hiện các tính toán cho luận án của mình. Với nội dung nghiên cứu tập trung vào tính toán và mô phỏng xác suất của các sự kiện cực hiếm, R là lựa chọn tối ưu vào thời điểm đó. Trong một vài tính toán mà chưa có thư viện hỗ trợ, chẳng hạn như tính toán với độ chính xác siêu nhỏ (dưới \\(10^{-100}\\)), tôi đã tìm đến Python là một giải pháp bổ sung.Trong thời gian nghiên cứu sinh từ 2011 đến 2014, tôi sử dụng R là công cụ chính để thực hiện các tính toán cho luận án của mình. Với nội dung nghiên cứu tập trung vào tính toán và mô phỏng xác suất của các sự kiện cực hiếm, R là lựa chọn tối ưu vào thời điểm đó. Trong một vài tính toán mà chưa có thư viện hỗ trợ, chẳng hạn như tính toán với độ chính xác siêu nhỏ (dưới \\(10^{-100}\\)), tôi đã tìm đến Python là một giải pháp bổ sung.Công việc đầu tiên trong môi trường doanh nghiệp của tôi liên quan đến dữ liệu là xây dựng các thuật toán để đầu tư trên thị trường tài chính tại một quỹ đầu tư. Tôi được làm quen với một kho dữ liệu khổng lồ bao gồm dữ liệu hỗ trợ phân tích kỹ thuật, dữ liệu hỗ trợ phân tích cơ bản, và dữ liệu kiểu tin tức của tất cả các công cụ tài chính có thể sử dụng để giao dịch, bao gồm cổ phiếu, trái phiếu, hợp đồng tương lai, quyền chọn. Các mô hình trên dữ liệu được chúng tôi - những người nghiên cứu thị trường - xây dựng bằng nhiều phương pháp để tìm ra các chiến lược mang lại lợi nhuận cho quỹ đầu tư. Mặc dù không có cơ hội sử dụng R để phân tích dữ liệu các yêu cầu liên quan đến bảo mật, nhưng tôi lại được học những kỹ năng lập trình C++ mà tôi nhận ra là vô cùng quan trọng cho công việc của mình sau này.Công việc đầu tiên trong môi trường doanh nghiệp của tôi liên quan đến dữ liệu là xây dựng các thuật toán để đầu tư trên thị trường tài chính tại một quỹ đầu tư. Tôi được làm quen với một kho dữ liệu khổng lồ bao gồm dữ liệu hỗ trợ phân tích kỹ thuật, dữ liệu hỗ trợ phân tích cơ bản, và dữ liệu kiểu tin tức của tất cả các công cụ tài chính có thể sử dụng để giao dịch, bao gồm cổ phiếu, trái phiếu, hợp đồng tương lai, quyền chọn. Các mô hình trên dữ liệu được chúng tôi - những người nghiên cứu thị trường - xây dựng bằng nhiều phương pháp để tìm ra các chiến lược mang lại lợi nhuận cho quỹ đầu tư. Mặc dù không có cơ hội sử dụng R để phân tích dữ liệu các yêu cầu liên quan đến bảo mật, nhưng tôi lại được học những kỹ năng lập trình C++ mà tôi nhận ra là vô cùng quan trọng cho công việc của mình sau này.Khi bắt đầu công việc như một chuyên gia tính toán, tôi làm việc thường xuyên với dữ liệu trong bảo hiểm. Với những dữ liệu nhỏ, tôi nhận thấy rằng Microsoft Excel kết hợp với lập trình VBA là vừa đủ để xử lý. Khi dữ liệu trở nên ngày càng lớn và phức tạp, Excel và VBA không còn đáp ứng được nhu cầu, đó là lúc tôi quay lại sử dụng R trong công việc của mình. Ngoài sử dụng R như là một công cụ chính để định phí, đánh giá hợp đồng bảo hiểm, tôi còn sử dụng R để thực hiện các công việc liên quan đến dữ liệu như\nThu thập dữ liệu nhận được từ các phòng ban như tài chính, kế toán, nghiệp vụ, , kiểm tra, làm sạch và cập nhập dữ liệu lên cơ sở dữ liệu để phục vụ tính toán. R cho phép xử lý và tính toán những dữ liệu hàng chục triệu dòng với hiệu quả thực sự đáng kinh ngạc.\nTrích xuất dữ liệu từ cơ sở dữ liệu, biến đổi và tính toán để thực hiện các nghiệp vụ như tái bảo hiểm, quản lý tài sản nợ/có, tính toán báo cáo kinh nghiệm.\nXây dựng các dashboard để cập nhật tình hình bồi thường bảo hiểm y tế với thời gian thực.\nXây dựng mô hình để phân loại rủi ro, dự báo những chủ hợp đồng, người được bảo hiểm có khả năng cao là trục lợi trong bảo hiểm y tế.\nKhi bắt đầu công việc như một chuyên gia tính toán, tôi làm việc thường xuyên với dữ liệu trong bảo hiểm. Với những dữ liệu nhỏ, tôi nhận thấy rằng Microsoft Excel kết hợp với lập trình VBA là vừa đủ để xử lý. Khi dữ liệu trở nên ngày càng lớn và phức tạp, Excel và VBA không còn đáp ứng được nhu cầu, đó là lúc tôi quay lại sử dụng R trong công việc của mình. Ngoài sử dụng R như là một công cụ chính để định phí, đánh giá hợp đồng bảo hiểm, tôi còn sử dụng R để thực hiện các công việc liên quan đến dữ liệu nhưThu thập dữ liệu nhận được từ các phòng ban như tài chính, kế toán, nghiệp vụ, , kiểm tra, làm sạch và cập nhập dữ liệu lên cơ sở dữ liệu để phục vụ tính toán. R cho phép xử lý và tính toán những dữ liệu hàng chục triệu dòng với hiệu quả thực sự đáng kinh ngạc.Thu thập dữ liệu nhận được từ các phòng ban như tài chính, kế toán, nghiệp vụ, , kiểm tra, làm sạch và cập nhập dữ liệu lên cơ sở dữ liệu để phục vụ tính toán. R cho phép xử lý và tính toán những dữ liệu hàng chục triệu dòng với hiệu quả thực sự đáng kinh ngạc.Trích xuất dữ liệu từ cơ sở dữ liệu, biến đổi và tính toán để thực hiện các nghiệp vụ như tái bảo hiểm, quản lý tài sản nợ/có, tính toán báo cáo kinh nghiệm.Trích xuất dữ liệu từ cơ sở dữ liệu, biến đổi và tính toán để thực hiện các nghiệp vụ như tái bảo hiểm, quản lý tài sản nợ/có, tính toán báo cáo kinh nghiệm.Xây dựng các dashboard để cập nhật tình hình bồi thường bảo hiểm y tế với thời gian thực.Xây dựng các dashboard để cập nhật tình hình bồi thường bảo hiểm y tế với thời gian thực.Xây dựng mô hình để phân loại rủi ro, dự báo những chủ hợp đồng, người được bảo hiểm có khả năng cao là trục lợi trong bảo hiểm y tế.Xây dựng mô hình để phân loại rủi ro, dự báo những chủ hợp đồng, người được bảo hiểm có khả năng cao là trục lợi trong bảo hiểm y tế.Khi tôi quay trở lại với công việc học thuật vào đầu năm 2018, cũng là lúc mà làn sóng về khoa học dữ liệu bắt đầu ảnh hưởng đến đa số các lĩnh vực khác, bao gồm cả ngành khoa học tính toán. Các chứng chỉ về khoa học dữ liệu là điều kiện bắt buộc đối với những người muốn trở thành thành viên của các Hiệp hội chuyên gia tính toán. Tôi cùng với các đồng nghiệp của mình đưa môn học \\(\\textbf{Phân tích và dự báo}\\) vào trong chương trình đào tạo Định phí bảo hiểm và quản trị rủi ro để cung cấp cho sinh viên các kiến thức và kỹ năng cần thiết khi làm việc với dữ liệu và có thể lấy được chứng chỉ nghề nghiệp của Hiệp hội. Môn học \\(\\textbf{Phân tích và dự báo}\\) sau đó chính thức được giảng dạy cho sinh viên ngành Toán Kinh tế vào năm 2021 với tên gọi \\(\\textbf{Khoa học dữ liệu trong Kinh tế và Kinh doanh}\\). Nội dung của cuốn sách xoay quanh các kiến thức mà tôi đã đang và sẽ giảng dạy cho sinh viên và học viên của mình.Khi tôi quay trở lại với công việc học thuật vào đầu năm 2018, cũng là lúc mà làn sóng về khoa học dữ liệu bắt đầu ảnh hưởng đến đa số các lĩnh vực khác, bao gồm cả ngành khoa học tính toán. Các chứng chỉ về khoa học dữ liệu là điều kiện bắt buộc đối với những người muốn trở thành thành viên của các Hiệp hội chuyên gia tính toán. Tôi cùng với các đồng nghiệp của mình đưa môn học \\(\\textbf{Phân tích và dự báo}\\) vào trong chương trình đào tạo Định phí bảo hiểm và quản trị rủi ro để cung cấp cho sinh viên các kiến thức và kỹ năng cần thiết khi làm việc với dữ liệu và có thể lấy được chứng chỉ nghề nghiệp của Hiệp hội. Môn học \\(\\textbf{Phân tích và dự báo}\\) sau đó chính thức được giảng dạy cho sinh viên ngành Toán Kinh tế vào năm 2021 với tên gọi \\(\\textbf{Khoa học dữ liệu trong Kinh tế và Kinh doanh}\\). Nội dung của cuốn sách xoay quanh các kiến thức mà tôi đã đang và sẽ giảng dạy cho sinh viên và học viên của mình.","code":""},{"path":"giới-thiệu-chung.html","id":"ai-nên-đọc-cuốn-sách-này","chapter":"Chương 2 Giới thiệu chung","heading":"2.4.2 Ai nên đọc cuốn sách này","text":"","code":""},{"path":"giới-thiệu-chung.html","id":"cấu-trúc-của-cuốn-sách","chapter":"Chương 2 Giới thiệu chung","heading":"2.4.3 Cấu trúc của cuốn sách","text":"","code":""},{"path":"giới-thiệu-chung.html","id":"các-ký-hiệu-thông-dụng","chapter":"Chương 2 Giới thiệu chung","heading":"2.4.4 Các ký hiệu thông dụng","text":"","code":""},{"path":"giới-thiệu-chung.html","id":"các-dữ-liệu-sử-dụng-trong-cuốn-sách","chapter":"Chương 2 Giới thiệu chung","heading":"2.4.5 Các dữ liệu sử dụng trong cuốn sách","text":"","code":"## \n## Attaching package: 'dplyr'## The following objects are masked from 'package:stats':\n## \n##     filter, lag## The following objects are masked from 'package:base':\n## \n##     intersect, setdiff, setequal, union## \n## Attaching package: 'kableExtra'## The following object is masked from 'package:dplyr':\n## \n##     group_rows## \n## Attaching package: 'gridExtra'## The following object is masked from 'package:dplyr':\n## \n##     combine## \n## Attaching package: 'pryr'## The following object is masked from 'package:dplyr':\n## \n##     where## Loading required package: shape"},{"path":"neuralnetwork.html","id":"neuralnetwork","chapter":"Chương 3 Mô hình mạng nơ-ron","heading":"Chương 3 Mô hình mạng nơ-ron","text":"Chương sách này thảo luận một chủ đề quan trọng có ứng dụng rộng rãi nhất trong lĩnh vực trí tuệ nhân tạo là mô hình mạng học sâu (deep learning). Tại thời điểm nhóm tác giả viết cuốn sách (2023), học sâu là một lĩnh vực nghiên cứu tích cực nhất không chỉ trong khoa học máy tính, công nghệ thông tin mà còn cả trong các lĩnh vực khác như kinh tế, tài chính, y tế, xây dựng,… Nền tảng của mô hình mạng học sâu là mô hình mạng nơ-ron (hay neural network). Mô hình mạng nơ-ron đã được biết đến đến rộng rãi vào cuối những năm 1980 bởi cách vận hành của mô hình mô tả lại cách thức mà hệ thống thần kinh của con người xử lý thông tin. Mặc dù các đặc tính của mô hình mạng nơ-ron được phân tích bởi những nhà toán học và nhà thống kê nhiều thuật toán liên quan đến mô hình này đã được cải thiện với sự ra đời của các phương pháp học máy khác như SVM, rừng ngẫu nhiên, học tăng cường,…, mà mô hình mạng nơ-ron phần nào không được ưa chuộng.Từ những năm 2010, với nhu cầu xử lý các dữ liệu ngày càng phức tạp và sự ra đời của các kiến trúc máy tính lớn, mô hình mạng nơ-ron đã quay trở lại với tên mới là mạng học sâu (deep learning). Mạng học sâu vượt trội hoàn toàn các mô hình học máy thông thường trong phân loại hình ảnh/video và mô hình hóa ngôn ngữ tự nhiên bao gồm dữ liệu kiểu văn bản và giọng nói (natural langugue processing hay NLP). Các nhà khoa học trong lĩnh vực này tin rằng lý chính cho những thành công của mô hình mạng nơ-ron là càng ngày những người xây dựng mô hình càng chú trọng vào xây dựng các bộ dữ liệu khổng lồ để huấn luyện môn hình và cấu trúc của mô hình cho phép nó đáp ứng được với bất kỳ tập kích thước dữ liệu nào.","code":""},{"path":"neuralnetwork.html","id":"nnonelayer","chapter":"Chương 3 Mô hình mạng nơ-ron","heading":"3.1 Mạng nơ-rơn có một lớp ẩn","text":"Mô hình mạng nơ-ron lấy một véc-tơ đầu vào gồm \\(p\\) biến \\(\\textbf{X} = (X_1, X_2, \\cdots , X_p)\\) và xây dựng một hàm phi tuyến \\(\\hat{f}\\) để dự đoán biến mục tiêu \\(Y\\) . Chúng ta đã xây dựng các mô hình dự đoán phi tuyến trong các chương trước, ví dụ như mô hình cộng tính tổng quát, mô hình cây quyết định, mô hình rừng ngẫu nhiên, mô hình tăng cường. Điều làm nên sự khác biệt của mô hình mạng nơ-ron là cấu trúc xây dựng của mô hình. Hình 3.1 mô tả một mạng nơ-ron chuyển tiếp để mô hình biến mục tiêu \\(Y\\) định lượng từ \\(p = 3\\) biến giải thích là \\(X_1\\), \\(X_2\\), và \\(X_3\\).\nFigure 3.1: Mô hình mạng nơ-ron có p = 3 đơn vị trong lớp đầu vào, một lớp ẩn có năm đơn vị, và một đơn vị đầu ra.\nTheo thuật ngữ chuyên môn, ba biến giải thích \\(X_1\\), \\(X_2\\), và \\(X_3\\) là các đơn vị (unit) của lớp đầu vào (input layer). Các mũi tên được dùng để mô tả rằng mỗi đơn vị đầu vào sẽ chuyển tiếp thông tin vào \\(k = 5\\) đơn vị của lớp ẩn (hidden layer) được ký hiệu là \\(H_i\\) với \\(= 1, 2, \\cdots, k\\). Dạng của hàm \\(f\\) trong mô hình mạng nơ-ron nhân tạo sẽ được viết như sau:\n\\[\\begin{align}\nf(\\textbf{X}) & = \\beta_0 + \\sum\\limits_{= 1}^k \\beta_i \\cdot h_i\\left(\\textbf{X}\\right) \\\\\n& = \\beta_0 + \\sum\\limits_{= 1}^k \\beta_i \\cdot g\\left(w_{,0} + \\sum\\limits_{j=1}^p w_{,j} \\cdot X_j\\right)\n\\tag{3.1}\n\\end{align}\\]\ntrong đó \\(g\\) là một hàm số phi tuyến được xác định trước, được gọi theo thuật ngữ chuyên môn là các hàm kích hoạt (activation function). Các \\(\\beta_i\\) và \\(w_{,j}\\) là các hằng số và cũng là các tham số cần được ước lượng của mô hình. Hàm \\(f\\) trong phương trình (??) được xây dựng theo hai bước:Bước thứ nhất, các đơn vị \\(H_i\\) trong lớp ẩn được tính bằng hàm kích hoạt tính trên tổ hợp tuyến tính của các biến đầu vào:\\[\\begin{align}\nH_i = g\\left(w_{,0} + \\sum\\limits_{j=1}^p w_{,j} \\cdot X_j\\right)\n\\tag{3.2}\n\\end{align}\\]Bước thứ hai, \\(k\\) đơn vị của lớp ẩn là yếu tố đầu vào để tính toán giá trị biến đầu ra định lượng:\n\\[\\begin{align}\nY = \\beta_0 + \\sum\\limits_{= 1}^k \\beta_i \\cdot H_i\n\\tag{3.3}\n\\end{align}\\]Quá trình tính toán đi từ lớp đầu vào qua các lớp ẩn và kết thúc ở mạng đầu ra được gọi là quá trình chuyển tiếp, và hàm \\(f\\) trong phương trình (3.1) được gọi là một mạng nơ-ron chuyển tiếp. Tất cả các tham số \\(\\beta_0\\), \\(\\beta_1\\), \\(\\cdots\\), \\(\\cdots\\) , \\(\\beta_k\\) và \\(w_{1,0}\\) , \\(\\cdots\\) , \\(w_{k,p}\\) được ước lượng từ dữ liệu. Các hàm kích hoạt thường được sử dụng là hàm Sigmoid và hàm ReLU. Hàm sigmoid là hàm được thường xuyên sử dụng trong hồi quy logistic để chuyển hàm tuyến tính thành xác suất giữa 0 và 1 trong khi hàm ReLU là hàm phi tuyến được xây dựng một cách đơn giản nhất nhằm mục đích dễ dàng tuyến tính trong các mạng phức tạp.\n\\[\\begin{align}\n\\text{Sigmoid: } & g(x) = \\cfrac{e^x}{1 + e^{x}} = \\cfrac{1}{1 + e^{-x}} \\\\\n\\text{ReLU: } & g(x) = max(x , 0) = (x)^+\n\\end{align}\\]\nFigure 3.2: Hàm kích hoạt sigmoid và hàm kích hoạt ReLU. Hàm Sigmoid có đạo hàm tại mọi điểm trong khi hàm ReLU không có đạo hàm tại 0\nHình 3.2 mô tả giá trị của hàm Sigmoid và hàm ReLU trên đoạn từ -4 đến 4. Trong thời kỳ đầu của mô hình mạng nơ-ron, hàm Sigmoid được ưa chuộng vì hàm số này có đạo hàm liên tục tại mọi giát trị của \\(x\\). Sau đó, hàm ReLU lại là lựa chọn ưa thích trong các mô hình mạng nơ-ron hiện đại vì hàm số này đơn giản, dễ tính toán và cho hiệu quả tốt hơn với hàm Sigmoid.Có thể tóm tắt lại mô hình được mô tả trong Hình 3.1 như sau: từ 3 biến giải thích ban đầu là \\(X_1\\), \\(X_2\\), và \\(X_3\\) chúng ta tạo ra năm biến giải thích mới là \\(H_1\\), \\(H_2\\), \\(H_3\\), \\(H_4\\) và \\(H_5\\) được tính toán bằng giá trị của hàm kích hoạt \\(g(.)\\) trên các tổ hợp tuyến tính của các biến giải thích ban đầu. Sau đó chúng ta sử dụng năm biến giải thích \\(H_i\\), \\(= 1, 2, 3, 4, 5\\) để xây dựng một mô hình hồi quy tuyến tính mà trong đó biến phụ thuộc là \\(Y\\). Tham số của các mô hình bao gồm các hệ số \\(w_{,j}\\) để tính các biến \\(H_i\\), và các hệ số \\(\\beta_j\\) trong mô hình hồi quy tuyến tính \\(Y\\) theo các biến \\(H\\).Mô hình có tên là mạng nơ-ron bởi vì cấu trúc của mô hình bao gồm các đơn vị \\(H_i\\) hoạt động giống như các tế bào thần kinh trong não bộ của con người. Các đơn vị \\(H_i\\) xấp xỉ hoặc bằng 0 giống như các tế bào thần kinh im lặng (slient neuron), những tế bào ít bị kích hoạt trong quá trình lan truyền thông tin, trong khi các đơn vị \\(H_i\\) lớn (khi sử dụng hàm ReLU), hoặc xấp xỉ 1 (khi sử dụng hàm Sigmoid) giống như những tế bào bị kích hoạt mạng trong quá trình lan truyền thông tin.Sử dụng các hàm kích hoạt \\(g(.)\\) phi tuyến là đặc biệt quan trọng tong vì nếu không hàm \\(f\\) sẽ suy biến thành mô hình tuyến tính thông thường với \\(p = 3\\) biến giải thích trong \\(X_1\\), \\(X_2\\), và \\(X_3\\). Ngoài ra, hàm kích hoạt phi tuyến cho phép mô hình mạng nơ-ron mô tả được những mối liên hệ phi tuyến và phức tạp giữa các biến giải thích \\(\\textbf{X}\\) và biến mục tiêu \\(Y\\).Giả sử trong mô hình mạng nơ-ron được mô tả trong Hình 3.1, chúng ta có hàm kích hoạt \\(g(x) = x^2\\) và giá trị của các hệ số \\(\\boldsymbol{\\beta} = (\\beta_0, \\beta_1, \\beta_2, \\beta_3, \\beta_4, \\beta_5)\\) và \\(w_{,j}\\), với \\(1 \\leq \\leq 5\\) và \\(0 \\leq j \\leq 3\\), được cho như sau\n\\[\\begin{align}\n& \\text{hệ số chặn: } \\beta_0 = w_{1,0} = w_{2,0} = w_{3,0} = w_{4,0} = w_{5,0} 0 \\\\\n& \\\\\n& \\begin{pmatrix}\n\\beta_1 \\\\\n\\beta_2 \\\\\n\\beta_3 \\\\\n\\beta_4 \\\\\n\\beta_5\n\\end{pmatrix} = \\begin{pmatrix}\n0.5 \\\\\n0.5 \\\\\n0.5 \\\\\n1 \\\\\n2\n\\end{pmatrix} \\ \\text{ và } \\ \\begin{pmatrix}\nw_{1,1} & w_{1,2} & w_{1,3} \\\\\nw_{2,1} & w_{2,2} & w_{2,3} \\\\\nw_{3,1} & w_{3,2} & w_{3,3} \\\\\nw_{4,1} & w_{4,2} & w_{4,3} \\\\\nw_{5,1} & w_{5,2} & w_{5,3}\n\\end{pmatrix} = \\begin{pmatrix}\n1 & -1 & 1 \\\\\n1 & 2 & -1 \\\\\n0 & 0 & 0 \\\\\n0 & 0 & 0 \\\\\n0 & 0 & 0\n\\end{pmatrix}\n\\tag{3.4}\n\\end{align}\\]Trước hết, để tránh sự phức tạp chúng tôi cho giá trị các hàng 3, 4, và 5 của ma trận \\(\\boldsymbol{w}\\) đều bằng 0, điều này dẫn đến giá trị tại các đơn vị \\(H_3\\), \\(H_4\\) và \\(H_5\\) của lớp ẩn sẽ bằng 0. Các đơn vị này hoạt động như các tế bào im lặng trong mạng nơ-rơn và không có ảnh hưởng đến biến mục tiêu \\(Y\\). Chúng ta có giá trị tại \\(H_1\\) và \\(H_2\\) được tính theo các biến giải thích và hàm kích hoạt:\n- Giá trị tại \\(H_1\\)\n\\[\\begin{align}\nH_1\\left(X_1, X_2, X_3\\right) & = g\\left(w_{1,1} \\cdot X_1 + w_{1,2} \\cdot X_2 + w_{1,3} \\cdot X_3\\right) \\\\\n& = \\left(w_{1,1} \\cdot X_1 + w_{1,2} \\cdot X_2 + w_{1,3} \\cdot X_3\\right)^2 \\\\\n& = \\left(X_1 - X_2 + X_3\\right)^2\n\\tag{3.5}\n\\end{align}\\]Giá trị tại \\(H_2\\)\n\\[\\begin{align}\nH_2 \\left(X_1, X_2, X_3\\right) & = g\\left(w_{2,1} \\cdot X_1 + w_{2,2} \\cdot X_2 + w_{2,3} \\cdot X_3\\right) \\\\\n& = \\left(w_{2,1} \\cdot X_1 + w_{2,2} \\cdot X_2 + w_{2,3} \\cdot X_3\\right)^2 \\\\\n& = \\left(X_1 + 2 \\cdot X_2 - X_3\\right)^2\n\\tag{3.6}\n\\end{align}\\]Giá trị của hàm số \\(f(\\text{X})\\) là đầu ra của mạng nơ-ron được xác định như sau:\n\\[\\begin{align}\nf(X_1, X_2, X_3) & = 0.5 \\cdot H_1\\left(X_1, X_2, X_3\\right) + 0.5 \\cdot H_2 \\left(X_1, X_2, X_3\\right) \\\\\n& = X_1^2 + 2.5 \\cdot X_2^2 + X_3^2 + X_1 \\cdot X_2 - 3 \\cdot X_2 \\cdot X_3\n\\tag{3.7}\n\\end{align}\\]Bạn đọc có thể thấy rằng, việc sử dụng hàm kích hoạt phi tuyến cho phép chúng ta có hàm đầu ra bao gồm hàm phi tuyến trên các giá trị biến đầu vào, mà còn tính đến cả biến tương tác giữa các biến ban đầu. Trong ví dụ ở phương trình (3.7) là các giá trị \\(X_1 \\cdot X_2\\) và \\(3 \\cdot X_2 \\cdot X_3\\). Trong thực tế, chúng ta sẽ không sử dụng hàm bậc hai hay hàm đa thức cho hàm kích hoạt \\(g(.)\\) hàm kích hoạt đa thức sẽ dẫn tới kết quả cũng chỉ là dạng hàm đa thức. Các hàm kích hoạt Sigmoid hoặc ReLU không bị giới hạn như vậy.Trong ví dụ trên chúng ta đã cho trước các tham số bao gồm các hệ số \\(\\boldsymbol{\\beta}\\) và ma trận \\(\\boldsymbol{w}\\). Tuy nhiên trong thực tế, các tham số này được lựa chọn để giảm thiểu sai số giữa giá trị dự đoán và giá trị quan sát của biến mục tiêu:\n\\[\\begin{align}\n(\\hat{\\boldsymbol{\\beta}},\\hat{\\boldsymbol{w}}) = \\underset{\\boldsymbol{\\beta},\\boldsymbol{w}}{\\operatorname{argmin}} \\sum\\limits_{=1}^n \\left( y_i - f(\\textbf{x}_i) \\right)^2\n\\tag{3.8}\n\\end{align}\\]Chúng ta sẽ thảo luận về ước lượng tham số cho mô hình mạng nơ-ron trong phần 3.3.","code":""},{"path":"neuralnetwork.html","id":"nnmultilayer","chapter":"Chương 3 Mô hình mạng nơ-ron","heading":"3.2 Mạng nơ-ron có nhiều lớp ẩn","text":"Mô hình mạng nơ-ron được sử dụng hiện tại thường có nhiều hơn một lớp ẩn và có nhiều đơn vị trên mỗi lớp. Về lý thuyết, một lớp ẩn duy nhất với số lượng lớn các đơn vị có khả năng xấp xỉ hầu hết các dạng hàm \\(f\\). Tuy nhiên, thực tế chỉ ra rằng xây dựng những cấu trúc nhiều lớp và mỗi lớp có kích thước hợp lý là giải pháp tốt hơn với cấu trúc chỉ có một lớp ẩn và có rất nhiều đơn vị trên cùng một lớp.Cấu trúc có thể mở rộng của mô hình mạng nơ-ron nhân tạo cho phép nó có khả năng mô hình hóa tốt những bộ dữ liệu phức tạp, mà điển hình là dữ liệu dạng ảnh, dạng văn bản, hay dạng tín hiệu. Để minh họa cho khả năng phù hợp của mô hình với những dữ liệu phức tạp, chúng ta sẽ xây dựng một cấu trúc mạng nơ-ron có nhiều lớp ẩn để dự đoán dữ liệu là ảnh chứa các chữ số viết tay. Dữ liệu được sử dụng để huấn luyện mô hình là tập dữ liệu chữ số viết tay nổi tiếng \\(\\textbf{MNIST}\\). Hình 3.3 minh họa một số quan sát trong dữ liệu về các chữ số viết tay được lưu trữ trong dữ liệu \\(\\textbf{MNIST}\\). Mỗi quan sát của dữ liệu sử dụng để xây dựng mô hình là một hình ảnh có kích thước \\(p = 28 \\times 28 = 784\\) pixel và biến mục tiêu là giá trị số của hình ảnh xuất hiện trong biến giải thích. Có tất cả 10 giá trị khác nhau cho biến mục tiêu là các chữ số viết tay tương ứng từ 0 đến 9.\nFigure 3.3: Năm mươi giá trị đầu tiên trong dữ liệu số viết tay MNIST. Mỗi số viết tay là một quan sát trong dữ liệu. Một bức ảnh được lưu dưới dạng một véc-tơ có độ dài p = 784. Mỗi giá trị trong véc-tơ là một số tự nhiên nhận giá trị từ 0 đến 255 cho biết độ tối của điểm ảnh.\nÝ tưởng là xây dựng một mô hình để phân loại các hình ảnh thành chữ số từ 0 đến 9. Chúng ta sẽ sử dụng cấu trúc mạng nơ-ron với hai lớp ẩn được minh họa trong Hình 3.4 để xây dựng mô hình phân loại hình ảnh số viết tay.\nFigure 3.4: Mạng nơ-ron xây dựng trên dữ liệu MNIST có hai lớp ẩn, mỗi lớp ẩn có nhiều đơn vị và lớp đầu ra có 10 đơn vị tương ứng với m = 10 giá trị có thể của các số viết tay từ 0 đến 9. Lớp đầu vào có p = 784 đơn vị tương ứng với 784 điểm ảnh.\nGiá trị đầu ra trong cấu trúc mạng nơ-ron trong Hình 3.4 là biến kiểu factor, được biểu thị bằng véc-tơ \\(\\textbf{Y} = (Y_1, Y_2, \\cdots , Y_{m})\\) với \\(m = 10\\). Dữ liệu có 60 nghìn bức ảnh được sử dụng để huấn luyện mô hình và 10 nghìn bức ảnh được sử dụng để kiểm tra mô hình. Cấu trúc mạng trong Hình 3.4 có hai lớp ẩn thay vì một lớp ẩn giống như trước.Lớp ẩn thứ nhất có \\(k_1\\) đơn vị được tính toán từ \\(p\\) đầu vào ban đầu với hàm kích hoạt \\(g_1(.)\\). Giả sử các nút trong lớp ẩn đầu tiên lần lượt là \\(H^{(1)}_1\\), \\(H^{(1)}_2\\), \\(\\cdots\\), \\(H^{(1)}_{k_1}\\). Ta có \\(H^{(1)}_j\\) được tính toán từ các đầu vào với \\((p+1)\\) tham số \\(w^{(1)}_{j,}\\) với \\(0 \\leq \\leq p\\) như sau:\n\\[\\begin{align}\nH^{(1)}_j = g_1\\left( w^{(1)}_{j,0} + w^{(1)}_{j,1} X_1 + w^{(1)}_{j,2} X_2 + \\cdots + w^{(1)}_{j,p} X_p \\right)\n\\tag{3.9}\n\\end{align}\\]\nCó thể thấy rằng, để tính toán tất cả \\(k_1\\) đơn vị trong lớp ẩn thứ nhất, chúng ta cần sử dụng \\(k_1 \\times (p+1)\\) tham số.Lớp ẩn thứ nhất có \\(k_1\\) đơn vị được tính toán từ \\(p\\) đầu vào ban đầu với hàm kích hoạt \\(g_1(.)\\). Giả sử các nút trong lớp ẩn đầu tiên lần lượt là \\(H^{(1)}_1\\), \\(H^{(1)}_2\\), \\(\\cdots\\), \\(H^{(1)}_{k_1}\\). Ta có \\(H^{(1)}_j\\) được tính toán từ các đầu vào với \\((p+1)\\) tham số \\(w^{(1)}_{j,}\\) với \\(0 \\leq \\leq p\\) như sau:\n\\[\\begin{align}\nH^{(1)}_j = g_1\\left( w^{(1)}_{j,0} + w^{(1)}_{j,1} X_1 + w^{(1)}_{j,2} X_2 + \\cdots + w^{(1)}_{j,p} X_p \\right)\n\\tag{3.9}\n\\end{align}\\]\nCó thể thấy rằng, để tính toán tất cả \\(k_1\\) đơn vị trong lớp ẩn thứ nhất, chúng ta cần sử dụng \\(k_1 \\times (p+1)\\) tham số.Lớp ẩn thứ hai có \\(k_2\\) đơn vị được tính toán từ \\(k_1\\) đơn vị của lớp ẩn thứ nhất và hàm kích hoạt \\(g_2(.)\\). Gọi các nút trong lớp ẩn thứ hai lần lượt là \\(H^{(2)}_1\\), \\(H^{(2)}_2\\), \\(\\cdots\\), \\(H^{(2)}_{k_2}\\). Ta có \\(H^{(2)}_j\\) được tính toán từ lớp ẩn thứ nhất vào với \\((k_1+1)\\) tham số \\(w^{(2)}_{j,}\\) với \\(0 \\leq \\leq k_1\\) như sau:\n\\[\\begin{align}\nH^{(2)}_j = g_2\\left( w^{(2)}_{j,0} + w^{(2)}_{j,1} H^{(2)}_1 + w^{(2)}_{j,2} H^{(2)}_1 + \\cdots + w^{(2)}_{j,k_1} H^{(2)}_{k_1} \\right)\n\\tag{3.10}\n\\end{align}\\]\nĐể tính toán tất cả \\(k_2\\) đơn vị trong lớp ẩn thứ hai, chúng ta cần sử dụng \\(k_2 \\times (k_1+1)\\) tham số.Lớp ẩn thứ hai có \\(k_2\\) đơn vị được tính toán từ \\(k_1\\) đơn vị của lớp ẩn thứ nhất và hàm kích hoạt \\(g_2(.)\\). Gọi các nút trong lớp ẩn thứ hai lần lượt là \\(H^{(2)}_1\\), \\(H^{(2)}_2\\), \\(\\cdots\\), \\(H^{(2)}_{k_2}\\). Ta có \\(H^{(2)}_j\\) được tính toán từ lớp ẩn thứ nhất vào với \\((k_1+1)\\) tham số \\(w^{(2)}_{j,}\\) với \\(0 \\leq \\leq k_1\\) như sau:\n\\[\\begin{align}\nH^{(2)}_j = g_2\\left( w^{(2)}_{j,0} + w^{(2)}_{j,1} H^{(2)}_1 + w^{(2)}_{j,2} H^{(2)}_1 + \\cdots + w^{(2)}_{j,k_1} H^{(2)}_{k_1} \\right)\n\\tag{3.10}\n\\end{align}\\]\nĐể tính toán tất cả \\(k_2\\) đơn vị trong lớp ẩn thứ hai, chúng ta cần sử dụng \\(k_2 \\times (k_1+1)\\) tham số.Trong lớp đầu ra, đây là bài toán phân loại, nên chúng ta sử dụng \\(m = 10\\) đơn vị tương ứng với 10 chữ số viết tay từ 0 đến 9. Trong bài toán phân loại, hàm kích hoạt để tính toán các đơn vị trong lớp đầu ra thường là hàm softmax. Giá trị tại đơn vị \\(Y_j\\) với \\(1 \\leq m\\) trong lớp đầu ra được xác định như sau:\n\\[\\begin{align}\nZ_j &= \\beta_{j,0} + \\beta_{j,1} H^{(2)}_1 + \\beta_{j,2} H^{(2)}_2 + \\cdots +\\beta_{j,k_2} H^{(2)}_{k_2} \\\\\n\\textbf{Y} &= softmax(\\textbf{Z}) \\rightarrow Y_j = \\cfrac{exp(Z_j)}{exp(Z_1) + exp(Z_2) + \\cdots + exp(Z_m)}\n\\tag{3.11}\n\\end{align}\\]\nĐể tính toán \\(m\\) đơn vị trong lớp đầu ra, chúng ta cần \\(m \\times (k_2 + 1)\\) tham số. Lưu ý rằng khi sử dụng hàm softmax thì tổng giá trị của các đơn vị trong lớp đầu ra luôn bằng 1.Trong lớp đầu ra, đây là bài toán phân loại, nên chúng ta sử dụng \\(m = 10\\) đơn vị tương ứng với 10 chữ số viết tay từ 0 đến 9. Trong bài toán phân loại, hàm kích hoạt để tính toán các đơn vị trong lớp đầu ra thường là hàm softmax. Giá trị tại đơn vị \\(Y_j\\) với \\(1 \\leq m\\) trong lớp đầu ra được xác định như sau:\n\\[\\begin{align}\nZ_j &= \\beta_{j,0} + \\beta_{j,1} H^{(2)}_1 + \\beta_{j,2} H^{(2)}_2 + \\cdots +\\beta_{j,k_2} H^{(2)}_{k_2} \\\\\n\\textbf{Y} &= softmax(\\textbf{Z}) \\rightarrow Y_j = \\cfrac{exp(Z_j)}{exp(Z_1) + exp(Z_2) + \\cdots + exp(Z_m)}\n\\tag{3.11}\n\\end{align}\\]\nĐể tính toán \\(m\\) đơn vị trong lớp đầu ra, chúng ta cần \\(m \\times (k_2 + 1)\\) tham số. Lưu ý rằng khi sử dụng hàm softmax thì tổng giá trị của các đơn vị trong lớp đầu ra luôn bằng 1.Như vậy, để tính toán được \\(m = 10\\) giá trị đầu ra cho cấu trúc mạng nơ-ron được trình bày trong Hình 3.4, số lượng tham số cần sử dụng là\n\\[\\begin{align}\nk_1 \\cdot (p + 1) + k_2 \\cdot (k_1 + 1) + m \\cdot (k_2+1)\n\\end{align}\\]Ví dụ, nếu chúng ta sử dụng 512 đơn vị trong lớp ẩn thứ nhất và 256 đơn vị trong lớp ẩn thứ hai, số lượng tham số của mạng nơ-ron với 784 đơn vị đầu vào và 10 đơn vị đầu ra là\n\\[\\begin{align}\n512 \\cdot (784 + 1) + 256 \\cdot (512 + 1) + 10 \\cdot (256 + 1) = 535.818\n\\end{align}\\]\nNói cách khác, mô hình sử dụng hơn 500 nghìn tham số để tính toán 10 đơn vị đầu ra từ 784 đơn vị đầu vào. Các tham số này được tính toán sao cho sai số giữa véc-tơ đầu ra tính toán từ mô hình và giá trị đầu ra quan sát được trên dữ liệu là nhỏ nhất. Trong bài toán phân loại, sai số thường được sử dụng là hàm cross-entropy. Với quan sát thứ \\(\\) của biến giải thích \\(\\textbf{x}_i\\) thì quan sát \\(y_i\\) tương ứng của biến mục tiêu (nhận một trong các giá trị từ \\(1\\) đến \\(m\\)) sẽ được viết dưới dạng véc-tơ đầu ra \\(\\textbf{y}_i = (y^{}_{1},y^i_2,\\cdots,y^i_m)\\) sao cho\n\\[\\begin{align}\ny^i_j &= 1 \\text{ nếu } y_i = j \\\\\ny^i_j &= 0 \\text{ nếu } y_i \\neq j\n\\tag{3.12}\n\\end{align}\\]\nCách biến đổi biến này thường được gọi là one-hot encoding. Với véc-tơ đầu ra được tính toán từ véc-tơ đầu vào \\(\\textbf{x}_i\\) theo cấu trúc mạng nơ-ron bằng các phương trình (3.9), (3.10), và (3.11) là \\(\\hat{\\textbf{y}}_i = (\\hat{y}^{}_{1},\\hat{y}^i_2,\\cdots,\\hat{y}^i_m)\\) thì sai số tính bằng cross-entropy tại quan sát thứ \\(\\) là\n\\[\\begin{align}\n\\sum\\limits_{j=1}^m \\ y^{}_{j} \\cdot \\log(\\hat{y}^{}_{j}) = y^{}_{1} \\cdot \\log(\\hat{y}^{}_{1}) + y^{}_{2} \\cdot \\log(\\hat{y}^{}_{2}) + \\cdots + y^{}_{m} \\cdot \\log(\\hat{y}^{}_{m})\n\\tag{3.13}\n\\end{align}\\]\nvà sai số tính bằng cross-entropy trên \\(n\\) dữ liệu huấn luyện mô hình là\n\\[\\begin{align}\nCE\\_loss = \\sum\\limits_{=1}^n \\sum\\limits_{j=1}^m \\ y^{}_{j} \\cdot \\log(\\hat{y}^{}_{j})\n\\tag{3.14}\n\\end{align}\\]Trong mô hình mạng nơ-ron, để đơn giản hóa ký hiệu, chúng ta sẽ sử dụng ký hiệu dạng ma trận. Cấu trúc mạng nơ-ron có hai lớp ẩn được mô tả trong hình 3.4 có ba ma trận tham số \\(\\boldsymbol{w}_1\\), \\(\\boldsymbol{w}_2\\), và \\(\\boldsymbol{\\beta}\\) được định nghĩa như sau\n\\[\\begin{align}\n\\boldsymbol{\\beta} &= \\begin{pmatrix}\n\\beta_{1,0} & \\beta_{1,1} & \\cdots & \\beta_{1,k_2} \\\\\n\\beta_{2,0} & \\beta_{2,1} & \\cdots & \\beta_{2,k_2} \\\\\n\\cdots & \\cdots & \\cdots & \\cdots \\\\\n\\beta_{m,0} & \\beta_{m,1} & \\cdots & \\beta_{m,k_2}\n\\end{pmatrix} \\\\\n& \\\\\n\\boldsymbol{w}_1 &= \\begin{pmatrix}\nw^{(1)}_{1,0} & w^{(1)}_{1,1} & \\cdots & w^{(1)}_{1,p} \\\\\nw^{(1)}_{2,0} & w^{(1)}_{2,1} & \\cdots & w^{(1)}_{2,p} \\\\\n\\cdots & \\cdots & \\cdots & \\cdots \\\\\nw^{(1)}_{k_1,0} & w^{(1)}_{k_1,1} & \\cdots & w^{(1)}_{k_1,p}\n\\end{pmatrix};\n\\boldsymbol{w}_2 = \\begin{pmatrix}\nw^{(2)}_{1,0} & w^{(2)}_{1,1} & \\cdots & w^{(2)}_{1,k_1} \\\\\nw^{(2)}_{2,0} & w^{(2)}_{2,1} & \\cdots & w^{(2)}_{2,p} \\\\\n\\cdots & \\cdots & \\cdots & \\cdots \\\\\nw^{(2)}_{k_2,0} & w^{(2)}_{k_2,1} & \\cdots & w^{(2)}_{k_2,k_1}\n\\end{pmatrix}\n\\tag{3.15}\n\\end{align}\\]Quá trình ước lượng tham số cho mạng nơ-ron là quá trình tìm các ma trận tham số \\(\\boldsymbol{w}_1\\), \\(\\boldsymbol{w}_2\\), và \\(\\boldsymbol{\\beta}\\) để tối thiểu hóa tổn thất tính bằng cross-entropy\\[\\begin{align}\n(\\hat{\\boldsymbol{w}}_1, \\hat{\\boldsymbol{w}}_2, \\hat{\\boldsymbol{\\beta}}) &= \\underset{\\boldsymbol{w}_1, \\boldsymbol{w}_2, \\boldsymbol{\\beta}}{\\operatorname{argmin}} \\sum_{=1}^n \\sum_{j=1}^m y^{}_{j} \\cdot \\log(\\hat{y}^{}_{j})\n\\label{#eq:nn016}\n\\end{align}\\]Như chúng tôi đã đề cập ở phía trước, các mô hình mạng nơ-ron có nhiều lớp ẩn với số lượng đơn vị trong các lớp ẩn không quá nhiều thường cho kết quả tốt hơn với các mô hình có ít lớp ẩn và sử dụng nhiều đơn vị trong một lớp. Tuy nhiên khi tăng số lớp ẩn lên sẽ làm cho số lượng tham số cần được ước lượng tăng lên rất nhanh, khiến cho mô hình mạng nơ-ron rất dễ rơi vào tình trạng overfitting, nghĩa là sai số trên tập dữ liệu huấn luyện mô hình nhỏ nhưng sai số trên dữ liệu kiểm tra mô hình lại rất lớn. Chính vì thế, trong quá trình ước lượng tham số, người xây dựng mô hình thường sử dụng thêm các ràng buộc tham số, chẳng hạn như ràng buộc tham số kiểu hồi quy ridge. Nghĩa là tổn thất của mô hình tính bằng cross-entropy sẽ được điều chỉnh để cân bằng giữa phương sai và độ lệch của mô hình. Chúng ta sẽ thảo luận về vấn đề này trong phần ước lượng tham số cho mạng nơ-ron.","code":""},{"path":"neuralnetwork.html","id":"nnestimation","chapter":"Chương 3 Mô hình mạng nơ-ron","heading":"3.3 Ước lượng tham số của mạng nơ-ron","text":"Tham số của mạng nơ-ron được chia thành hai nhóm:Nhóm thứ nhất bao gồm các siêu tham số như số lượng lớp ẩn trong cấu trúc mạng và trong mỗi lớp ẩn có bao nhiêu đơn vị. Chẳng hạn như cấu trúc được mô tả trong hình 3.4 có hai lớp ẩn, lớp ẩn thứ nhất có 512 đơn vị, lớp ẩn thứ hai có 256 đơn vị. Trong trường hợp chúng ta có sử dụng ràng buộc tham số, chúng ta có thêm một tham số điều chỉnh sự đánh đổi giữa sai lệch và phương sai giống như tham số \\(\\lambda\\) trong hồi quy ridge.Nhóm thứ nhất bao gồm các siêu tham số như số lượng lớp ẩn trong cấu trúc mạng và trong mỗi lớp ẩn có bao nhiêu đơn vị. Chẳng hạn như cấu trúc được mô tả trong hình 3.4 có hai lớp ẩn, lớp ẩn thứ nhất có 512 đơn vị, lớp ẩn thứ hai có 256 đơn vị. Trong trường hợp chúng ta có sử dụng ràng buộc tham số, chúng ta có thêm một tham số điều chỉnh sự đánh đổi giữa sai lệch và phương sai giống như tham số \\(\\lambda\\) trong hồi quy ridge.Với mỗi lựa chọn cho các tham số trong nhóm thứ nhất, chúng ta có các tham số để tính toán cấu trúc mạng bao gồm các ma trận \\(\\boldsymbol{w}\\) và ma trận \\(\\boldsymbol{\\beta}\\) được định nghĩa trong phương trình (3.15). Quá trình ước lượng các tham số này là quá trình giải bài toán tối thiểu hóa hàm tổn thất dạng tổng sai số bình phương trong bài toán hồi quy hoặc hàm cross-entropy trong bài toán phân loại. Nhìn chung, không thể tính toán được lời giải chính xác cho bài toán tối ưu mà chúng ta sẽ phải ước lượng tham số bằng các phương pháp giải số, mà cụ thể là phương pháp stochastic gradient descent. đó, chúng ta thường phải xác định định thêm các tham số như tốc độ học, số lượng dữ liệu được sử dụng trong mỗi bước tính toán, hay số vòng lặp của thuật toán gradient descent.Với mỗi lựa chọn cho các tham số trong nhóm thứ nhất, chúng ta có các tham số để tính toán cấu trúc mạng bao gồm các ma trận \\(\\boldsymbol{w}\\) và ma trận \\(\\boldsymbol{\\beta}\\) được định nghĩa trong phương trình (3.15). Quá trình ước lượng các tham số này là quá trình giải bài toán tối thiểu hóa hàm tổn thất dạng tổng sai số bình phương trong bài toán hồi quy hoặc hàm cross-entropy trong bài toán phân loại. Nhìn chung, không thể tính toán được lời giải chính xác cho bài toán tối ưu mà chúng ta sẽ phải ước lượng tham số bằng các phương pháp giải số, mà cụ thể là phương pháp stochastic gradient descent. đó, chúng ta thường phải xác định định thêm các tham số như tốc độ học, số lượng dữ liệu được sử dụng trong mỗi bước tính toán, hay số vòng lặp của thuật toán gradient descent.Lựa chọn tham số trong nhóm thứ nhất có thể ảnh hưởng lớn đến kết quả của mô hình mạng nơ-ron, nhưng lại không có phương pháp chính xác nào để xác định các tham số này. Nếu nguồn lực tính toán cho phép, các tham số này sẽ được xác định bằng cách thử nghiệm và lựa chọn. Nếu nguồn lực tính toán không cho phép, người xây dựng mô hình thường lựa chọn các tham số này dựa trên kinh nghiệm và cấu trúc mạng đã có sẵn trên các bộ dữ liệu tương tự.Trong phần này, chúng tôi sẽ tập trung vào các tham số cần được ước lượng trong nhóm thứ hai, nghĩa là tập trung vào ước lượng các ma trận \\(\\boldsymbol{w}\\) và ma trận \\(\\boldsymbol{\\beta}\\) khi chúng ta đã có một cấu trúc mạng cụ thể.","code":""},{"path":"neuralnetwork.html","id":"ước-lượng-tham-số-cho-mạng-nơ-ron-hồi-quy-có-một-lớp-ẩn","chapter":"Chương 3 Mô hình mạng nơ-ron","heading":"3.3.1 Ước lượng tham số cho mạng nơ-ron hồi quy có một lớp ẩn","text":"Quá trình ước lượng mạng nơ-ron đòi hỏi kiến thức và các kỹ thuật toán học khá phức tạp và chúng tôi sẽ cố gắng chỉ trình bày tổng quan và ngắn gọn. Bạn đọc cảm thấy khó khăn về phần này này có thể yên tâm bỏ qua và chuyển sang phần tiếp theo bởi chúng ta có thể sử dụng các thư viện như hay để ước lượng mô hình mà không cần hiểu quá sâu về các chi tiết kỹ thuật trong quy trình xây dựng mô hình.Chúng ta sẽ bắt đầu bằng mô hình mạng nơ-ron hồi quy có một lớp ẩn duy nhất được trình bày trong hình 3.1 và phương trình (3.1). Để mô hình giữ nguyên tính tổng quát, chúng tôi sử dụng \\(p\\) là số tham số đầu vào, \\(k\\) là số lượng đơn vị trong lớp ẩn. Chúng ta cần tìm các tham số là véc-tơ \\(\\boldsymbol{\\beta}\\) và ma trận \\(\\boldsymbol{w}\\) để tối thiểu hóa tổng sai số bình phương:\\[\\begin{align}\nRSS\\left( \\boldsymbol{\\beta}, \\boldsymbol{w} \\right) & = \\cfrac{1}{2} \\ \\sum\\limits_{=1}^n \\ \\left(y_i - f_{\\boldsymbol{\\beta}, \\boldsymbol{w}}\\left(\\textbf{x}_i\\right) \\right)^2 \\\\\n& = \\cfrac{1}{2} \\sum\\limits_{=1}^n \\ \\left(y_i - \\beta_0 - \\sum\\limits_{t = 1}^k \\beta_t \\cdot g\\left(z_{t,}\\right) \\right)^2\n\\tag{3.16}\n\\end{align}\\]\nvới\n\\[\\begin{align}\nz_{,t} = w_{t,0} + \\sum\\limits_{j=1}^p w_{t,j} \\cdot x_{,j}\n\\tag{3.17}\n\\end{align}\\]\nMặc dù hàm \\(RSS\\left( \\boldsymbol{\\beta}, \\boldsymbol{w} \\right)\\) trong phương trình (3.16) không quá phức tạp, nhưng để giải bài toán tối thiểu hóa hàm số này trên dữ liệu \\((\\textbf{x}_i,y_i)\\) với \\(= 1, 2, \\cdots, n\\) và hàm số \\(g\\) cho trước không phải là một nhiệm vụ dễ dàng. Trước hết, mặc dù chúng ta có dạng hàm tường minh cho các tham số, nhưng đây không phải là hàm số lồi theo các tham số, đó quá trình giải bài toán tối ưu thường chỉ cho đáp số là một điểm cực tiểu địa phương chứ không chắc chắn là điểm cực tiểu toàn cục. Thứ hai, không thể có lời giải chính xác cho bài toán tối ưu nên chúng ta sẽ cần tìm lời giải số, trong trường hợp này là phương pháp gradient descent. Việc lựa chọn các tham số cho thuật toán này cũng sẽ là câu hỏi cần được giải đáp cho quá trình xây dựng mô hình. Thứ ba, mô hình mạng nơ-ron có rất nhiều tham số, đó rất dễ dẫn đến hiện tượng mô hình quá khớp với dữ liệu huấn luyện mô hình. Hàm mục tiêu thường sẽ bằng RSS cộng thêm một hàm phạt, khiến cho quá trình giải số trở nên khó khăn hơn.Trước tiên, giả sử bài toán tối ưu trong (3.16) không có ràng buộc tham số, chúng ta cần xác định gradient của RSS theo \\(\\boldsymbol{\\beta}\\) và \\(\\boldsymbol{w}\\). Để đơn giản hóa, chúng ta giả sử bình phương của sai số thứ \\(\\) là \\(RSS_i\\)\n\\[\\begin{align}\nRSS_i = \\cfrac{1}{2} \\ \\left[y_i - \\beta_0 - \\sum\\limits_{t = 1}^k \\beta_t \\cdot g\\left(z_{,t}\\right) \\right]^2\n\\end{align}\\]Đạo hàm của bình phương sai số thứ \\(\\) theo \\(\\beta_l\\), với \\(0 \\leq l \\leq k\\), được xác định như sau\n\\[\\begin{align}\n\\cfrac{\\partial RSS_i}{\\partial \\beta_l} & = \\begin{cases}\n-\\left(y_i - f\\left(\\textbf{x}_i\\right) \\right) & \\text{ nếu } l = 0 \\\\\n-g\\left(z_{,l}\\right)  \\left(y_i - f\\left(\\textbf{x}_i\\right) \\right) & \\text{ nếu } l > 0\n\\end{cases}\n\\tag{3.18}\n\\end{align}\\]Đạo hàm của bình phương sai số thứ \\(\\) theo \\(\\beta_l\\), với \\(0 \\leq l \\leq k\\), được xác định như sau\n\\[\\begin{align}\n\\cfrac{\\partial RSS_i}{\\partial \\beta_l} & = \\begin{cases}\n-\\left(y_i - f\\left(\\textbf{x}_i\\right) \\right) & \\text{ nếu } l = 0 \\\\\n-g\\left(z_{,l}\\right)  \\left(y_i - f\\left(\\textbf{x}_i\\right) \\right) & \\text{ nếu } l > 0\n\\end{cases}\n\\tag{3.18}\n\\end{align}\\]Đạo hàm của bình phương sai số thứ \\(\\) theo \\(w_{l,j}\\), với \\(1 \\leq l \\leq k\\) và \\(0 \\leq j \\leq p\\), được xác định như sauĐạo hàm của bình phương sai số thứ \\(\\) theo \\(w_{l,j}\\), với \\(1 \\leq l \\leq k\\) và \\(0 \\leq j \\leq p\\), được xác định như sau\\[\\begin{align}\n\\cfrac{\\partial RSS_i}{\\partial w_{l,j}} & = - \\cfrac{\\partial  \\beta_l \\cdot g\\left(z_{,l}\\right) }{\\partial w_{l,j}} \\left(y_i - f\\left(\\textbf{x}_i\\right) \\right) \\\\\n& =\n\\begin{cases}\n- \\beta_l \\cdot g^{'}\\left(z_{,l}\\right) \\cdot \\left(y_i - f\\left(\\textbf{x}_i\\right) \\right) & \\text{ nếu } j = 0 \\\\\n- \\beta_l \\cdot x_{,j} \\cdot g^{'}\\left(z_{,l}\\right) \\cdot \\left(y_i - f\\left(\\textbf{x}_i\\right) \\right) & \\text{ nếu } j > 0\n\\end{cases}\n\\tag{3.19}\n\\end{align}\\]Trước tiên, có thể thấy rằng cả hai biểu thức đạo hàm của sai số bình phương này đều chứa phần dư \\(\\left(y_i − f(\\textbf{x})\\right)\\). Trong công thức (3.18) chúng ta thấy rằng giá trị tuyệt đối của gradient theo các \\(\\beta_l\\) bằng phần dư nhân với giá trị \\(g\\left(z_{,l}\\right)\\), chính là giá trị tại nút \\(H_l\\) tính theo đầu vào \\(\\textbf{x}_i\\). Tiếp theo, trong công thức (3.19) chúng ta thấy sự thay đổi của RSS theo tham số \\(w_{l,j}\\), tương ứng với hệ số của đầu vào \\(X_j\\) khi tính toán đơn vị \\(H_l\\) của mạng nơ-ron một lớp ẩn, cũng phụ thuộc vào phần dư. Sự ảnh hưởng của phần dư lên đạo hàm theo từng tham số của mô hình được gọi là quá trình lan truyền ngược trong mô hình mạng nơ-ron.Các công thức đạo hàm ở trên luôn yêu cầu tính toán giá trị của hàm kích hoạt và đạo hàm tại các điểm \\(z_{,l}\\). Về lý thuyết, mọi hàm đơn điệu tăng, có đạo hàm, và không tuyến tính đều có thể được sử dụng làm hàm kích hoạt. Tuy nhiên, nếu dạng hàm quá phức tạp, việc tính toán sẽ trở nên phưc tạp, nhất là khi sử dụng nhiều lớp ẩn và trong mỗi lớp ẩn có nhiều đơn vị. Điều này giải thích tại sao hàm ReLU thường xuyên được sử dụng làm hàm kích hoạt để tính toán gradient là đơn giản nhất có thể. Giả sử hàm \\(g\\) trong các công thức (3.18) và (3.19) là hàm ReLU, chúng ta có thể đơn giản hóa các đạo hàm như sau:Đạo hàm theo hệ số \\(\\beta_l\\):\\[\\begin{align}\n\\cfrac{\\partial RSS_i}{\\partial \\beta_l} & = \\begin{cases}\n-\\left(y_i - f\\left(\\textbf{x}_i\\right) \\right) & \\text{ nếu } l = 0 \\\\\n-\\mathbb{}(z_{,l} > 0) \\cdot z_{,l} \\cdot \\left(y_i - f\\left(\\textbf{x}_i\\right) \\right) & \\text{ nếu } l > 0\n\\end{cases}\n\\tag{3.20}\n\\end{align}\\]Đạo hàm theo \\(w_{l,0}\\),\\[\\begin{align}\n  \\cfrac{\\partial RSS_i}{\\partial w_{l,0}} & = - \\beta_l \\cdot \\mathbb{}(z_{,l} > 0) \\cdot \\left(y_i - f\\left(\\textbf{x}_i\\right) \\right)\n  \\tag{3.21}\n\\end{align}\\]Đạo hàm theo \\(w_{l,j}\\), với \\(j > 0\\) thì\\[\\begin{align}\n\\cfrac{\\partial RSS_i}{\\partial w_{l,j}} & =\n- \\beta_l \\cdot \\mathbb{}(z_{,l} > 0) \\cdot x_{,j} \\cdot \\left(y_i - f\\left(\\textbf{x}_i\\right) \\right)\n\\tag{3.22}\n\\end{align}\\]Gradient của tổng các \\(RSS_i\\) được xác định như sau:Theo \\(\\beta_0\\)\\[\\begin{align}\n\\cfrac{\\partial RSS}{\\partial \\beta_0} &= \\sum\\limits_{=1}^n \\cfrac{\\partial RSS_i}{\\partial \\beta_0} \\\\\n& = - \\sum\\limits_{=1}^n \\left(y_i - f\\left(\\textbf{x}_i\\right) \\right)\n\\tag{3.23}\n\\end{align}\\]Theo \\(\\beta_l\\) với \\(l > 0\\)\\[\\begin{align}\n\\cfrac{\\partial RSS}{\\partial \\beta_l} &= \\sum\\limits_{=1}^n \\cfrac{\\partial RSS_i}{\\partial \\beta_l} \\\\\n& = - \\sum\\limits_{=1}^n \\mathbb{}(z_{,l} > 0) \\cdot z_{,l} \\cdot \\left(y_i - f\\left(\\textbf{x}_i\\right) \\right)\n\\tag{3.24}\n\\end{align}\\]Theo \\(w_{l,0}\\),\\[\\begin{align}\n\\cfrac{\\partial RSS_i}{\\partial w_{l,0}} & =  - \\beta_l \\sum\\limits_{=1}^n \\mathbb{}(z_{,l} > 0) \\cdot \\left(y_i - f\\left(\\textbf{x}_i\\right) \\right)\n\\tag{3.25}\n\\end{align}\\]Theo \\(w_{l,j}\\) với j > 0\n\\[\\begin{align}\n\\cfrac{\\partial RSS_i}{\\partial w_{l,j}} & = - \\beta_l \\sum\\limits_{=1}^n \\mathbb{}(z_{,l} > 0) \\cdot x_{,j} \\cdot \\left(y_i - f\\left(\\textbf{x}_i\\right) \\right)\n\\tag{3.26}\n\\end{align}\\]Sau khi tính toán gradient của RSS theo các tham số, quá trình ước lượng sẽ được thực hiện thông qua thuật toán Stochastic gradient descent. Bạn đọc tham khảo thuật toán này trong Phụ lục ?? của chương Kiến thức R nâng cao. Kết quả của thuật toán Stochastic gradient descent phụ thuộc rất lớn vào giá trị khởi tạo ban đầu của các tham số, và nhất là khi số lượng tham số là rất lớn.Để mô tả quá trình ước lượng tham số của mạng nơ-ron, chúng ta sẽ sử dụng dữ liệu mô phỏng. Ma trận biến giải thích \\(\\textbf{X}\\) có kích thước \\(10^4 \\times 3\\) là các số ngẫu nhiên độc lập có phân phối chuẩn \\(\\mathcal{N}(0,1)\\). Hàm \\(f\\) được tạo bởi mô hình mạng nơ-ron với 3 đơn vị của lớp đầu vào, một lớp ẩn với 5 đơn vị, và một đơn vị trong lớp đầu đầu ra. Hàm kích hoạt được sử dụng là hàm ReLU. Biến giải thích \\(Y\\) được tính toán từ hàm \\(f(\\textbf{X})\\) công thêm một sai số độc lập với \\(\\textbf{X}\\) là một biến ngẫu nhiên phân phối chuẩn với trung bình bằng 0 và độ lệch chuẩn là 0.5. Các tham số dùng để tính toán \\(f\\) được đơn giản hóa: \\(\\beta_l = 2 \\forall l = 0, 1, \\cdots, 5\\) và $w_{l,j} = 1 l = 1, 2, , $ và $j = , $. Quá trình tối thiểu hóa tổng sai số bình phương được mô tả thông qua hình 3.5\nFigure 3.5: Sai số của mô hình mạng nơ-ron giảm dần trong quá trình ước lượng tham số sử dụng thuật toán stochastic gradient descent. Hình bên trái: điểm bắt đầu của các tham số là biến ngẫu nhiên phân phối chuẩn độc lập có trung bình bằng 0 và phương sai bằng 3. Hình bên phải: điểm bắt đầu của các tham số expression{beta} là biến ngẫu nhiên có trung bình bằng 2 và phương sai bằng 1. Điểm bắt đầu của các tham số w là biến ngẫu nhiên có trung bình bằng 1 và phương sai bằng 1\nHình 3.5 mô tả quá trình tối thiểu hóa sai số tính bằng RSS trên dữ liệu mô phỏng bằng phương pháp stochastic gradient descent. Tổng số tham số cần được ước lượng của mô hình là 26, bao gồm 6 giá trị của véc-tơ \\(\\boldsymbol{\\beta}\\) và 20 giá trị của ma trận \\(\\boldsymbol{w}\\). Chúng tôi sử dụng 5 nghìn lần lặp và trong mỗi lần lặp sử dụng 5% dữ liệu để tính các gradient.Hình bên trái mô tả 10 quá trình ước lượng tham số mà các điểm bắt đầu của \\(\\boldsymbol{\\beta}\\) và \\(\\boldsymbol{w}\\) là hoàn toàn ngẫu nhiên. Chúng tôi cho các giá trị ban đầu là các biến ngẫu nhiên phân phối chuẩn với trung bình bằng 0 và phương sai bằng 3. Hình bên phải mô tả 10 quá trình ước lượng tham số với các điểm bắt đầu của \\(\\boldsymbol{\\beta}\\) có giá trị trung bình là 2 bằng với giá trị dùng để mô phỏng dữ liệu và \\(\\boldsymbol{w}\\) có giá trị trung bình là 1 cũng bằng với giá trị dùng để mô phỏng dữ liệu. Phương sai của 26 tham số khởi đầu đều bằng 1. Có thể thấy rằng khi các tham số khởi đầu là hoàn toàn ngẫu nhiên thì về trung bình các quá trình hội tụ về giá ngưỡng nho nhất của RSS chậm hơn khi chúng ta có các giá trị khởi đầu tốt hơn. Có hai trên tám quá trình sau 5000 bước lặp vẫn chưa cho sai số tiệm cận đến giá trị RSS nhỏ nhất. Trong hình bên phải thì các quá trình đều cho kết quả gần với giá trị RSS nhỏ nhất.","code":""},{"path":"neuralnetwork.html","id":"ước-lượng-tham-số-cho-mạng-nơ-ron-phân-loại-có-hai-lớp-ẩn","chapter":"Chương 3 Mô hình mạng nơ-ron","heading":"3.3.2 Ước lượng tham số cho mạng nơ-ron phân loại có hai lớp ẩn","text":"Giả sử cấu trúc mạng nơ-ron có hai lớp ẩn như hình 3.4. Các tham số cần ước lượng của mô hình bao gồm các ma trận \\(\\boldsymbol{w}_1\\), \\(\\boldsymbol{w}_2\\), và \\(\\boldsymbol{\\beta}\\) được cho bởi phương trình (3.15). đây là bài toán phân loại nên hàm mục tiêu được sử dụng là hàm cross-entropy\n\\[\\begin{align}\n(\\hat{\\boldsymbol{w}}_1, \\hat{\\boldsymbol{w}}_2, \\hat{\\boldsymbol{\\beta}}) &= \\underset{\\boldsymbol{w}_1, \\boldsymbol{w}_2, \\boldsymbol{\\beta}}{\\operatorname{argmin}} \\sum\\limits_{=1}^n \\sum\\limits_{j=1}^m \\ y^{}_{j} \\cdot \\log(\\hat{y}^{}_{j}) \\\\\n\\tag{3.27}\n\\end{align}\\]Giá trị của hàm cross-entropy tính trên quan sát thứ \\(\\) có thể được rút gọn như sau\n\\[\\begin{align}\n\\sum\\limits_{j=1}^m \\ y^{}_{j} \\cdot \\log(\\hat{y}^{}_{j}) &= \\sum\\limits_{j=1}^m \\ y^{}_{j} \\cdot \\log\\left(\\cfrac{exp\\left(z_{,j}\\right)}{exp\\left(z_{,1}\\right)+exp\\left(z_{,2}\\right)+\\cdots+exp\\left(z_{,m}\\right)}\\right) \\\\\n& = \\sum\\limits_{j=1}^m \\ y^{}_{j} \\cdot \\left(z_{,j} -  \\log\\left(exp\\left(z_{,1}\\right)+exp\\left(z_{,2}\\right)+\\cdots+exp\\left(z_{,m}\\right)\\right) \\right)\\\\\n& =  \\sum\\limits_{j=1}^m \\ y^{}_{j} \\cdot z_{,j} - \\log\\left(\\sum\\limits_{j=1}^m exp\\left(z_{,j}\\right)\\right)\n\\tag{3.28}\n\\end{align}\\]\nvới \\(z_{,j}\\) là tổ hợp tuyến tính của giá trị các nút ẩn thứ hai tính theo đầu vào \\(\\textbf{x}_i\\)\n\\[\\begin{align}\nz_{,j} = \\beta_{j,0} + \\beta_{j,1} h^{(2)}_{,1} + \\beta_{j,2} h^{(2)}_{,2} + \\cdots + \\beta_{j,k_2} h^{(2)}_{,k_2}\n\\tag{3.29}\n\\end{align}\\]Với mọi tham số \\(\\theta\\) được sử dụng để tính toán giá trị tại các nút đầu ra, đạo hàm của hàm cross-entropy sẽ được tính toán thông qua các \\(z_{,j}\\)\n\\[\\begin{align}\n\\cfrac{\\partial CE\\_Loss_i}{\\partial \\theta} &= \\sum\\limits_{j=1}^m \\ y^{}_{j} \\ \\cfrac{\\partial z_{,j}}{\\partial \\theta} - \\sum\\limits_{j=1}^m \\cfrac{\\partial z_{,j}}{\\partial \\theta} \\cfrac{exp(z_{,j})}{\\sum exp\\left(z_{,j}\\right)} \\\\\n& = \\sum\\limits_{j=1}^m  \\cfrac{\\partial z_{,j}}{\\partial \\theta} \\left(y^{}_{j} - p_{,j}\\right)\n\\tag{3.30}\n\\end{align}\\]\ntrong đó\n\\[\\begin{align}\np_{,j} = \\cfrac{exp(z_{,j})}{\\sum\\limits_{j=1}^m exp\\left(z_{,j}\\right)}\n\\tag{3.31}\n\\end{align}\\]Từ công thức (3.29) có thể thấy rằng: véc-tơ dữ liệu đầu ra \\(\\textbf{y}_i = (y^{}_{1}, y^{}_{2}, \\cdots, y^{}_{m})\\) nhận giá trị bằng 1 tại một vị trí và nhận giá trị bằng 0 tại \\(m-1\\) vị trí còn lại, trong khi đó \\(p_{,j}\\) có thể được hiểu là xác suất mà đầu ra thứ \\(\\) nhận giá trị bằng \\(j\\) được xác định bởi mô hình mạng nơ-ron. Đạo hàm của hàm tổn thất tính bằng cross-entropy theo tham số \\(\\theta\\) bất kỳ, là một phần tử của ma trận \\(\\boldsymbol{\\beta}\\) hoặc các \\(\\boldsymbol{w}\\), phụ thuộc vào sai số giữa véc-tơ xác suất từ dữ liệu quan sát được \\(\\textbf{y}^\\) và véc-tơ xác suất được xác định bởi mô hình mạng nơ-ron \\(\\textbf{p}_{} = (p_{,1}, p_{,2}, \\cdot, p_{,m})\\). Như vậy, gradient của hàm tổn thất theo từng tham số phụ thuộc vào sai số của mô hình hiện tại. Nói một cách khác, sai số của mô hình trong bước hiện tại sẽ tác động đến việc cập nhật tham số trong bước tiếp theo khi chúng ta sử dụng phương pháp gradient descent. Đây là quá trình lan truyền ngược trong mô hình mạng nơ-ron mà chúng tôi đã đề cập đến trong phần mô hình mạng nơ-ron hồi quy có một lớp ẩn.Quá trình tính toán đạo hàm của \\(z_{,j}\\) theo các tham số của ma trận \\(\\boldsymbol{\\beta}\\) là khá hiển nhiên \\(z_{,j}\\) là hàm tuyến tính theo các tham số này. Để tính toán đạo hàm của \\(z_{,j}\\) theo các tham số của các ma trận \\(\\boldsymbol{w}\\), chúng ta sử dụng nguyên tắc chain-rule đã trình bày trong phụ lục của phần Kiến thức R nâng cao. chỉ số của các tham số trong ma trận là quá phức tạp nên chúng tôi sẽ chỉ trình bày cách tính đạo hàm mang tính tổng quát. Ta có mỗi \\(z\\) được tính từ phương trình (3.29) được xác định từ dữ liệu đầu vào \\(x\\) thông qua các hàm kích hoạt \\(g_1\\) và \\(g_2\\) như sau\n\\[\\begin{align}\nz & = \\beta h^{(2)} \\\\\nh^{(2)} &= g_2(w^{(2)}\\cdot h^{(1)}) \\\\\nh^{(1)} &= g_1(w^{(1)}\\cdot x)\n\\tag{3.32}\n\\end{align}\\]Trong đó \\(\\beta\\), \\(w^{(2)}\\), và \\(w^{(1)}\\) là các tham số của mô hình. Giá trị đạo hàm của \\(z\\) theo các tham số được xác định như sauTheo \\(\\beta\\)\n\\[\\begin{align}\n\\cfrac{\\partial z}{\\partial \\beta} = h^{(2)}\n\\tag{3.33}\n\\end{align}\\]Theo \\(\\beta\\)\n\\[\\begin{align}\n\\cfrac{\\partial z}{\\partial \\beta} = h^{(2)}\n\\tag{3.33}\n\\end{align}\\]Theo \\(w^{(2)}\\), chúng ta áp dụng nguyên tắc chain-rule\n\\[\\begin{align}\n\\cfrac{\\partial z}{\\partial w^{(2)}} & = \\cfrac{\\partial z}{\\partial h^{(2)}} \\times \\cfrac{\\partial h^{(2)}}{\\partial w^{(2)}} \\\\\n& =  \\beta \\cdot h^{(1)} \\cdot  g^{'}_2(w^{(2)}\\cdot h^{(1)})\n\\tag{3.34}\n\\end{align}\\]Theo \\(w^{(2)}\\), chúng ta áp dụng nguyên tắc chain-rule\n\\[\\begin{align}\n\\cfrac{\\partial z}{\\partial w^{(2)}} & = \\cfrac{\\partial z}{\\partial h^{(2)}} \\times \\cfrac{\\partial h^{(2)}}{\\partial w^{(2)}} \\\\\n& =  \\beta \\cdot h^{(1)} \\cdot  g^{'}_2(w^{(2)}\\cdot h^{(1)})\n\\tag{3.34}\n\\end{align}\\]Theo \\(w^{(1)}\\), áp dụng nguyên tắc chain-rule\n\\[\\begin{align}\n\\cfrac{\\partial z}{\\partial w^{(1)}} & = \\cfrac{\\partial z}{\\partial h^{(2)}} \\times \\cfrac{\\partial h^{(2)}}{\\partial h^{(1)}} \\times \\cfrac{\\partial h^{(1)}}{\\partial w^{(1)}} \\\\\n& =  \\beta \\cdot w^{(2)} \\cdot g^{'}_2(w^{(2)}\\cdot h^{(1)}) \\cdot x \\cdot g^{'}_1(w^{(1)}\\cdot x)\n\\tag{3.35}\n\\end{align}\\]Theo \\(w^{(1)}\\), áp dụng nguyên tắc chain-rule\n\\[\\begin{align}\n\\cfrac{\\partial z}{\\partial w^{(1)}} & = \\cfrac{\\partial z}{\\partial h^{(2)}} \\times \\cfrac{\\partial h^{(2)}}{\\partial h^{(1)}} \\times \\cfrac{\\partial h^{(1)}}{\\partial w^{(1)}} \\\\\n& =  \\beta \\cdot w^{(2)} \\cdot g^{'}_2(w^{(2)}\\cdot h^{(1)}) \\cdot x \\cdot g^{'}_1(w^{(1)}\\cdot x)\n\\tag{3.35}\n\\end{align}\\]Số lượng tham số cần được tính toán trong mô hình mạng nơ-ron phân loại với \\(p\\) đầu vào, \\(m\\) đầu ra, hai lớp ẩn với số lượng đơn vị lần lượt là \\(k_1\\) và \\(k_2\\) là\\(m \\cdot (k_2+1)\\) tham số trong ma trận \\(\\boldsymbol{\\beta}\\),\\(m \\cdot (k_2+1)\\) tham số trong ma trận \\(\\boldsymbol{\\beta}\\),\\(k_2 \\cdot (k_1 + 1)\\) tham số trong ma trận \\(\\boldsymbol{w}_2\\)\\(k_2 \\cdot (k_1 + 1)\\) tham số trong ma trận \\(\\boldsymbol{w}_2\\)\\(k_1 \\cdot (p + 1)\\) tham số trong ma trận \\(\\boldsymbol{w}_1\\)\\(k_1 \\cdot (p + 1)\\) tham số trong ma trận \\(\\boldsymbol{w}_1\\)Số lượng tham số quá lớn sẽ dẫn đến các vấn đề bao gồm sự phức tạp khi tiến hành tính toán, khối lượng tính toán lớn, và rất dễ dẫn đến overfitting. Để khắc phục vấn đề này, mô hình mạng nơ-ron thường phải sử dụng thêm các kỹ thuật để thêm ràng buộc tham số hoặc loại bỏ đơn vị trong các lớp. Chúng ta sẽ thảo luận các kỹ thuật này trong phần tiếp theo.","code":""},{"path":"neuralnetwork.html","id":"khắc-phục-hiện-tượng-khớp-quá-mức-của-mạng-nơ-ron","chapter":"Chương 3 Mô hình mạng nơ-ron","heading":"3.3.3 Khắc phục hiện tượng khớp quá mức của mạng nơ-ron","text":"Phương pháp trước hết để hạn chế mô hình khớp quá mức là sử dụng ràng buộc tham số. Nguyên tắc ràng buộc tham số trong mô hình mạng nơ-ron cũng tương tự trong hồi quy ridge. Ví dụ, với mô hình mạng nơ-ron hồi quy được trình bày trong phần 3.1 hàm mục tiêu cần được tối thiểu hóa là tổng bình phương sai số cộng thêm một hàm phạt có dạng tổng bình phương các tham số sử dụng trong mô hình.\\[\\begin{align}\n(\\hat{\\boldsymbol{\\beta}},\\hat{\\boldsymbol{w}}) &= \\underset{\\boldsymbol{\\beta},\\boldsymbol{w}}{\\operatorname{argmin}} &= \\sum\\limits_{=1}^n \\ \\left(y_i - f_{\\boldsymbol{\\beta}, \\boldsymbol{w}}\\left(\\textbf{x}_i\\right) \\right)^2 + \\lambda_1 \\cdot \\sum\\limits_{j} \\beta_j^2 + \\lambda_2 \\sum\\limits_{l,j} w_{l,j}^2\n\\tag{3.36}\n\\end{align}\\]Trong mô hình mạng nơ-ron phân loại có hai lớp ẩn trong phần 3.2, hàm mục tiêu tính bằng cross-entropy cũng được biến đổi bằng cách thêm vào một hàm phạt dạng tổng bình phương của các tham số:Người xây dựng mô hình có thể sử dụng các hàm phạt khác như hàm tổng giá trị tuyệt đối trong hồi quy Lasso, hoặc kết hợp giữa Lasso và ridge. Một lưu ý khác khi sử dụng ràng buộc tham số đó là người xây dựng mô hình thường không sử dụng ràng buộc tham số trực tiếp tính lớp đầu ra, nghĩa là thường cho \\(\\lambda_1\\) trong các phương trình (3.36) và (??) bằng 0. Với mỗi giá trị của các tham số \\(\\lambda\\), chúng ta tiến hành ước lượng tham số của mạng nơ-ron giống như đã trình bày trong phần 3.3.Một phương pháp khác để giảm bớt hiện tượng khớp quá mức của mạng nơ-ron là phương pháp loại bỏ đơn vị (unit dropout). Các đơn vị của lớp đầu vào và các lớp ẩn có thể tham gia vào mô hình mạng nơ-ron với xác suất là \\(p\\) và không tham gia vào mô hình với xác suất \\((1-p)\\) trong mỗi bước của quá trình huấn luyện mô hình, trong đó \\(p\\) là tham số của kỹ thuật dropout. Ý tưởng của phương pháp này hoàn toàn tương tự như ý tưởng của thuật toán rừng ngẫu nhiên áp dụng trên cây quyết định. Quá trình ước lượng tham số của mô hình mạng nơ-ron bao gồm hai quá trình: quá trình chuyển tiếp từ đầu vào, qua các lớp ẩn, và kết thúc tại lớp đầu ra, và quá trình lan truyền ngược, khi sai số tính tại lớp đầu ra được sử dụng để tính toán sự thay đổi cho tất cả các tham số của mô hình hiện tại. Nếu trong tất cả các lần chuyển tiếp và lan truyền ngược, chúng ta giữ nguyên cấu trúc của mạng, trong mỗi lớp đầu vào hoặc lớp ẩn có thể có một hoặc một số đơn vị chiếm ưu thế với các đơn vị khác, làm cho kết quả tại đầu ra phụ thuộc rất lớn vào các đơn vị này. Cũng giống như ý tưởng của thuật toán rừng ngẫu nhiên, trong mỗi lần chuyển tiếp và lan truyền ngược tương ứng, tại lớp đầu vào và các lớp ẩn, người xây dựng mô hình chỉ lựa chọn một số ngẫu nhiên các đơn vị vào trong quá trình tính toán tham số. Nói cách khác, mỗi đơn vị được lựa chọn vào quá trình tính toán với xác suất là \\(p\\). Chúng tôi sẽ mô tả cách thực hiện kỹ thuật dropout trong phần thực hành.Hàng thứ hai trong Bảng 10.1 được dán nhãn dropout. Đây là một dạng chính quy hóa tương đối mới bị loại bỏ và hiệu quả, tương tự ở một số khía cạnh với chính quy hóa đường gờ. Lấy cảm hứng từ các khu rừng ngẫu nhiên (Phần 8.2), ý tưởng là loại bỏ ngẫu nhiên một phần φ của các đơn vị trong một lớp khi tạo mô hình. Hình 10.19 minh họa điều này. Việc này được thực hiện riêng biệt mỗi khi xử lý quan sát đào tạo. Các đơn vị còn sống thay thế cho những đơn vị bị thiếu và trọng số của chúng được tăng lên theo hệ số 1/(1 − φ) để bù đắp. Điều này ngăn các nút trở nên chuyên biệt hóa quá mức và có thể được coi là một hình thức chính quy hóa. Trong thực tế, việc bỏ học đạt được bằng cách đặt ngẫu nhiên các kích hoạt cho các đơn vị “bị bỏ” về 0, trong khi vẫn giữ nguyên kiến trúc.","code":""},{"path":"neuralnetwork.html","id":"pickands-dependent-function","chapter":"Chương 3 Mô hình mạng nơ-ron","heading":"3.4 Pickands dependent function","text":"\\[\\begin{align}\nC(u,v) = exp\\left( log(uv) \\cdot \\left(\\cfrac{log(v)}{log(u)+log(v)} \\right) \\right)\n\\end{align}\\]Đạo hàm của \\(C(u,v)\\) tính theo \\(\\)\n\\[\\begin{align}\n\\cfrac{\\partial C(u,v)}{\\partial u} &= C(u,v) \\times \\cfrac{1}{u} \\cdot \\left((t) - ^{'}(t) \\cdot t \\right) \\\\\n\\cfrac{\\partial C(u,v)}{\\partial v} &= C(u,v) \\times \\cfrac{1}{v} \\cdot \\left((t) + ^{'}(t) \\cdot (1-t) \\right) \\\\\n\\end{align}\\]\n\\(t = \\cfrac{log(v)}{log(u)+log(v)}\\)Density của \\(C(u,v)\\)\n\\[\\begin{align}\n\\cfrac{\\partial^2 C(u,v)}{\\partial u \\partial v}  &= C(u,v) \\times \\cfrac{1}{uv} \\cdot \\left[ \\left((t) - ^{'}(t) \\cdot t \\right) \\cdot \\left((t) + ^{'}(t) \\cdot (1-t) \\right) - \\cfrac{^{''}(t) t(1-t)}{log(uv)} \\right]\\\\\n\\end{align}\\]","code":""},{"path":"neuralnetwork.html","id":"nhóm-các-hàm-at-để-cuv-là-một-copula","chapter":"Chương 3 Mô hình mạng nơ-ron","heading":"3.5 Nhóm các hàm A(t) để C(u,v) là một copula","text":"Các điều kiện biên: \\(C(0,v) = 0\\), \\(C(u,v) = 0\\), \\(C(1,v) = v\\), và \\(C(u,1) = u\\)\nKhi \\(u \\rightarrow 0\\) thì \\(t \\rightarrow 0\\) và khi \\(v \\rightarrow 0\\) thì \\(t \\rightarrow 1\\)\nCác điều kiện biên: \\(C(0,v) = 0\\), \\(C(u,v) = 0\\), \\(C(1,v) = v\\), và \\(C(u,1) = u\\)Khi \\(u \\rightarrow 0\\) thì \\(t \\rightarrow 0\\) và khi \\(v \\rightarrow 0\\) thì \\(t \\rightarrow 1\\)\\[\\begin{align}\nC(0,v) = exp\\left( log(0 \\cdot v) \\cdot \\left(0 \\right) \\right) = 0^1 = 0\n\\end{align}\\]\\[\\begin{align}\nC(u,0) = exp\\left( log(u \\cdot 0) \\cdot \\left(0 \\right) \\right) = 0^1 = 0\n\\end{align}\\]Khi \\(u \\rightarrow 1\\) thì \\(t \\rightarrow 1\\) và khi \\(v \\rightarrow 1\\) thì \\(t \\rightarrow 0\\)\\[\\begin{align}\nC(1,v) = exp\\left( log(1 \\cdot v) \\cdot \\left(1 \\right) \\right) = v^1 = v\n\\end{align}\\]\\[\\begin{align}\nC(u,1) = exp\\left( log(u \\cdot 1) \\cdot \\left(0 \\right) \\right) = u^1 = u\n\\end{align}\\]Các hàm \\(C_u(u,v)\\) và \\(C_v(u,v)\\) là hàm phân phối xác suất: \\(C_u(u,v)\\) là hàm phân phối xác suất của biến \\(V\\) với mọi \\(u\\): Khi \\(v \\rightarrow 0\\),\\[\\begin{align}\nC(u,v) \\times \\cfrac{1}{u} \\cdot \\left((t) - ^{'}(t) \\cdot t \\right) &\\rightarrow C(u,0) \\times \\cfrac{1}{u} \\cdot \\left((1) - ^{'}(1) \\cdot 1 \\right)\\\\\n& = 0 \\times \\cfrac{1}{u} \\cdot \\left((1) - ^{'}(1) \\right) \\\\\n& = 0\n\\end{align}\\]Khi \\(v \\rightarrow 1\\),\\[\\begin{align}\nC(u,v) \\times \\cfrac{1}{u} \\cdot \\left((t) - ^{'}(t) \\cdot t \\right) &\\rightarrow C(u,1) \\times \\cfrac{1}{u} \\cdot \\left((0) - ^{'}(0) \\cdot 0 \\right)\\\\\n& = 1 \\times 1 \\\\\n& = 1\n\\end{align}\\]Đạo hàm của hàm \\(C_u(u,v)\\) theo \\(v\\) là hàm tăng theo \\(v\\): Nếu \\(^{''}(t) \\geq 0\\) \\(\\forall t\\) thì\\[\\begin{align}\nC(u,v) \\times \\cfrac{1}{uv} \\cdot \\left[ \\left((t) - ^{'}(t) \\cdot t \\right) \\cdot \\left((t) + ^{'}(t) \\cdot (1-t) \\right) - \\cfrac{^{''}(t) t(1-t)}{log(uv)} \\right] \\geq 0\n\\end{align}\\]\nvới mọi \\(u,v\\)","code":""},{"path":"neuralnetwork.html","id":"cách-thứ-nhất-để-tham-số-hóa-đa-thức-từng-phần","chapter":"Chương 3 Mô hình mạng nơ-ron","heading":"3.5.1 Cách thứ nhất để tham số hóa đa thức từng phần","text":"Hàm \\(f\\) là đa thức từng phần thỏa mãn điều kiện thành pickand dependent function\n\\[\\begin{align}\n\\theta \\[0.5,1] \\\\\n\\lambda_1 \\geq \\theta \\\\\n\\lambda_2 = \\cfrac{ \\left(\\cfrac{\\theta^3}{6}+\\lambda_1 \\cdot \\cfrac{\\theta^2}{2}\\right) \\cdot \\theta - \\left(\\theta+\\lambda_1\\right) \\cdot \\left(\\cfrac{\\theta^3}{6}-\\cfrac{\\theta}{2}+\\cfrac{1}{3}\\right)  }{\\cfrac{(1-\\theta)^2\\cdot(\\theta+\\lambda_1)}{2} - \\left(\\cfrac{\\theta^3}{6}+\\lambda_1 \\cdot \\cfrac{\\theta^2}{2}\\right)}\n\\end{align}\\]","code":""},{"path":"neuralnetwork.html","id":"cách-thứ-hai","chapter":"Chương 3 Mô hình mạng nơ-ron","heading":"3.5.2 Cách thứ hai","text":"Chọn điểm cắt \\(\\theta\\), lựa chọn hàm \\((t)\\) như sau\n\\[\\begin{align}\n(t) = \\mathbb{}_{(t \\leq \\theta)} \\left(a_0 + a_1 \\cdot x + a_2 \\cdot x^2 + a_3 \\cdot x^3\\right) +\n\\mathbb{}_{(t > \\theta)} \\left(b_0 + b_1 \\cdot (1-x) + b_2 \\cdot (1-x)^2 + b_3 \\cdot (1-x)^3\\right)\n\\end{align}\\]\nvới các ràng buộc:Hàm \\((t)\\) có đạo hàm cấp 0,1,2 liên tục tại \\(\\theta\\)\\[\\begin{align}\n& \\begin{pmatrix}\n(1-\\theta) & (1-\\theta)^2 & (1-\\theta)^3 \\\\\n- 1 & - 2\\cdot(1 - \\theta) & -3(1-\\theta)^2 \\\\\n0 & 1 & 3(1-\\theta)\\\\\n\\end{pmatrix} \\times\n\\begin{pmatrix}\nb_1\\\\\nb_2\\\\\nb_3\\\\\n\\end{pmatrix} =\n\\begin{pmatrix}\n\\theta & \\theta^2 & \\theta^3 \\\\\n1 & 2\\theta & 3\\theta^2 \\\\\n0 & 1 & 3\\theta\\\\\n\\end{pmatrix} \\times\n\\begin{pmatrix}\na_1\\\\\na_2\\\\\na_3\\\\\n\\end{pmatrix} \\\\\n& \\\\\n\\rightarrow &\n\\begin{pmatrix}\n(1-\\theta) & (1-\\theta)^2 & (1-\\theta)^3 \\\\\n- 1 & - 2\\cdot(1 - \\theta) & -3(1-\\theta)^2 \\\\\n0 & 1 & 3(1-\\theta)\\\\\n\\end{pmatrix}^{-1}\n\\times \\begin{pmatrix}\n\\theta & \\theta^2 & \\theta^3 \\\\\n1 & 2\\theta & 3\\theta^2 \\\\\n0 & 1 & 3\\theta\\\\\n\\end{pmatrix} \\times\n\\begin{pmatrix}\na_1\\\\\na_2\\\\\na_3\\\\\n\\end{pmatrix} = \\begin{pmatrix}\nb_1\\\\\nb_2\\\\\nb_3\\\\\n\\end{pmatrix}\n& \\\\\n\\rightarrow &\n\\begin{pmatrix}\n\\cfrac{3}{1-\\theta} & 2 & 1 - \\theta \\\\\n- \\cfrac{3}{(1-\\theta)^2} & - \\cfrac{3}{1-\\theta} & -2 \\\\\n\\cfrac{1}{(1-\\theta)^3} & \\cfrac{1}{(1-\\theta)^2} & \\cfrac{1}{(1-\\theta)}\\\\\n\\end{pmatrix}\n\\times \\begin{pmatrix}\n\\theta & \\theta^2 & \\theta^3 \\\\\n1 & 2\\theta & 3\\theta^2 \\\\\n0 & 1 & 3\\theta\\\\\n\\end{pmatrix} \\times\n\\begin{pmatrix}\na_1\\\\\na_2\\\\\na_3\\\\\n\\end{pmatrix} = \\begin{pmatrix}\nb_1\\\\\nb_2\\\\\nb_3\\\\\n\\end{pmatrix}\n& \\\\\n\\rightarrow &\n\\begin{pmatrix}\n\\cfrac{2+\\theta}{1-\\theta} & \\cfrac{2\\theta+1}{1-\\theta} & \\cfrac{3\\theta}{1 - \\theta} \\\\\n- \\cfrac{3}{(1-\\theta)^2} & \\cfrac{\\theta^2-2\\theta-2}{(1-\\theta)^2} & \\cfrac{3\\theta^2-6\\theta}{(1-\\theta)^2} \\\\\n\\cfrac{1}{(1-\\theta)^3} & \\cfrac{1}{(1-\\theta)^3} & \\cfrac{1 - (1-\\theta)^3}{(1-\\theta)^3}\\\\\n\\end{pmatrix}\n\\times\n\\begin{pmatrix}\na_1\\\\\na_2\\\\\na_3\\\\\n\\end{pmatrix} = \\begin{pmatrix}\nb_1\\\\\nb_2\\\\\nb_3\\\\\n\\end{pmatrix}\n\n\n\\end{align}\\]","code":""},{"path":"neuralnetwork.html","id":"thực-hành","chapter":"Chương 3 Mô hình mạng nơ-ron","heading":"3.6 Thực hành:","text":"","code":""},{"path":"neuralnetwork.html","id":"mô-hình-mạng-nơ-ron-trên-dữ-liệu-boston","chapter":"Chương 3 Mô hình mạng nơ-ron","heading":"3.6.1 Mô hình mạng nơ-ron trên dữ liệu Boston","text":"","code":""},{"path":"neuralnetwork.html","id":"mô-hình-mạng-nơ-ron-để-phân-loại-khách-hàng","chapter":"Chương 3 Mô hình mạng nơ-ron","heading":"3.6.2 Mô hình mạng nơ-ron để phân loại khách hàng","text":"","code":""},{"path":"neuralnetwork.html","id":"phụ-lục","chapter":"Chương 3 Mô hình mạng nơ-ron","heading":"3.7 Phụ lục","text":"","code":""},{"path":"neuralnetwork.html","id":"bài-tập","chapter":"Chương 3 Mô hình mạng nơ-ron","heading":"3.8 Bài tập","text":"","code":""}]
