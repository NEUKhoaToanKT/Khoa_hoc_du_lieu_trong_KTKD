[{"path":"index.html","id":"lời-nói-đầu","chapter":"Lời nói đầu","heading":"Lời nói đầu","text":" Khoa học dữ liệu (KHDL) là ngành khoa học kết hợp giữa toán học - xác suất thống kê với khoa học máy tính và kiến thức chuyên môn trong một lĩnh vực cụ thể như kinh tế, tài chính, y học, giáo dục, thể thao, v.v., để khám phá những thông tin hữu ích và có giá trị nằm trong dữ liệu liên quan đến lĩnh vực chuyên môn đó. Những thông tin hữu ích này được sử dụng để hướng dẫn việc ra quyết định và lập kế hoạch chiến lược cho các cơ quan, tổ chức, doanh nghiệp và các cá nhân hoạt động trong lĩnh vực này.Cuốn sách “Khoa học dữ liệu trong Kinh tế và Kinh doanh” được viết xuất phát từ nhu cầu học tập và tìm hiểu về KHDL của những bạn đọc đang học tập, nghiên cứu và làm việc trong lĩnh vực kinh tế, quản lý, quản trị kinh doanh. Cuốn sách ít tập trung vào các khái niệm mang tính kỹ thuật trong toán học hay khoa học máy tính, mà tập trung nhiều hơn vào việc mô tả và áp dụng các phương pháp trên những vấn đề cụ thể. Trong mỗi chương đều sẽ có phần thực hành sử dụng phần mềm thống kê R để minh họa cách triển khai các phương pháp kỹ thuật. Những phần thực hành này sẽ cung cấp cho bạn đọc những trải nghiệm thực tiễn có giá trị.Cuốn sách phù hợp với sinh viên đại học hoặc cao học các ngành kinh tế, khoa học quản lý, quản trị kinh doanh, thương mại, tài chính ngân hàng, bảo hiểm, v.v., muốn nâng cao hiểu biết và tăng cường kinh nghiệm về làm việc với dữ liệu. Cuốn sách có thể làm giáo trình hoặc sách tham khảo cho một môn học kéo dài trong hai học kỳ.Nội dung của cuốn sách bao gồm hầu hết những chủ đề quan trọng trong KHDL: thu thập dữ liệu, tiền xử lý, sắp xếp và biến đổi, trực quan hóa, và xây dựng mô hình trên dữ liệu. Các mô hình được trình bày trong cuốn sách bao gồm cả các mô hình đơn giản như hồi quy tuyến tính hay cây quyết định, và các mô hình phức tạp như mô hình tuyến tính tổng quát, mô hình cộng tính tổng quát, mô hình rừng ngẫu nhiên, học tăng cường hoặc mô hình mạng nơ-ron. Song song với việc trình bày và giải thích các mô hình, chúng tôi sẽ cung cấp các gói lệnh có sẵn để bạn đọc thực hành trên các dữ liệu cụ thể.Đây là phiên bản đầu tiên của cuốn sách nên không thể tránh được những sai sót. Chúng tôi hy vọng rằng sẽ nhận được sự góp ý của bạn đọc về nội dung của cuốn sách để chúng tôi có thể hoàn thiện trong các phiên bản tiếp theo.Xin chân thành cảm ơn các đồng nghiệp tại Khoa Toán Kinh tế nói riêng và Đại học Kinh tế Quốc dân nói chung đã đồng hành cùng với chúng tôi trong suốt quá trình hoàn thành cuốn sách này. Xin cảm ơn các thành viên của Actuarial Sciences Lab đã nỗ lực hết sức để cuốn sách này có thể đến tay người đọc trong thời gian ngắn nhất!","code":""},{"path":"giới-thiệu-về-cuốn-sách.html","id":"giới-thiệu-về-cuốn-sách","chapter":"Chương 1 Giới thiệu về cuốn sách","heading":"Chương 1 Giới thiệu về cuốn sách","text":"","code":""},{"path":"giới-thiệu-về-cuốn-sách.html","id":"giới-thiệu-về-khoa-học-dữ-liệu-và-các-ứng-dụng","chapter":"Chương 1 Giới thiệu về cuốn sách","heading":"1.1 Giới thiệu về Khoa học dữ liệu và các ứng dụng","text":"Khoa học dữ liệu là một ngành liên ngành kết hợp kiến thức từ nhiều lĩnh vực như toán học, thống kê, khoa học máy tính và kỹ thuật dữ liệu để thu thập, xử lý, phân tích và diễn giải dữ liệu. Mục tiêu của khoa học dữ liệu là biến dữ liệu thô thành thông tin hữu ích, hỗ trợ ra quyết định và giải quyết các vấn đề phức tạp trong nhiều lĩnh vực khác nhau. Một quá trình ứng dụng KHDL vào giải quyết một vấn đề cụ thể thường bao gồm các bước được mô tả như Hình 1.1.\nHình 1.1: Quy trình áp dụng Khoa học dữ liệu để giải quyết một vấn đề thực tế\nNhập liệu là quá trình tìm kiếm và thu thập dữ liệu từ các nguồn khác nhau để phục vụ cho mục đích ra quyết định. Có những dự án làm việc trực tiếp trên dữ liệu có sẵn và đã được thiết kế sẵn sàng cho mục tiêu phân tích, nhưng cũng có những dự án mà quá trình nhập liệu lại chiếm phần lớn thời gian và quyết định sự thành công hay thất bại của dự án.Nhập liệu là quá trình tìm kiếm và thu thập dữ liệu từ các nguồn khác nhau để phục vụ cho mục đích ra quyết định. Có những dự án làm việc trực tiếp trên dữ liệu có sẵn và đã được thiết kế sẵn sàng cho mục tiêu phân tích, nhưng cũng có những dự án mà quá trình nhập liệu lại chiếm phần lớn thời gian và quyết định sự thành công hay thất bại của dự án.Sắp xếp (tổ chức) dữ liệu, hay còn được gọi là tiền xử lý dữ liệu, là các bước biến dữ liệu từ dạng thô thành dữ liệu theo đúng như định dạng mong muốn.Sắp xếp (tổ chức) dữ liệu, hay còn được gọi là tiền xử lý dữ liệu, là các bước biến dữ liệu từ dạng thô thành dữ liệu theo đúng như định dạng mong muốn.Biến đổi dữ liệu là quá trình tính toán trên các biến hoặc các quan sát của dữ liệu để dữ liệu có thể đưa vào các công cụ trực quan hoặc các mô hình phân tích.Biến đổi dữ liệu là quá trình tính toán trên các biến hoặc các quan sát của dữ liệu để dữ liệu có thể đưa vào các công cụ trực quan hoặc các mô hình phân tích.Xây dựng mô hình là quá trình tính toán và đánh giá mối liên hệ giữa các biến trong dữ liệu đến các biến mục tiêu, là đầu ra của toàn bộ quá trình. Mô hình thường được xây dựng với một trong hai mục đích: xem xét sự tác động của một biến đến các biến mục tiêu hoặc dự đoán giá trị của các biến mục tiêu.Xây dựng mô hình là quá trình tính toán và đánh giá mối liên hệ giữa các biến trong dữ liệu đến các biến mục tiêu, là đầu ra của toàn bộ quá trình. Mô hình thường được xây dựng với một trong hai mục đích: xem xét sự tác động của một biến đến các biến mục tiêu hoặc dự đoán giá trị của các biến mục tiêu.Mô hình trên dữ liệu được xây dựng dựa trên những nguyên lý của toán học và xác suất thống kê. Dữ liệu được sử dụng để xây dựng mô hình có thể là các dữ liệu nhỏ với một vài cột và vài chục quan sát, nhưng cũng có thể là các dữ liệu lớn với hàng nghìn cột dữ liệu và hàng triệu quan sát. Dữ liệu thậm chí không có dạng bảng biểu như chúng ta gặp hàng ngày mà có thể là các hình ảnh, các văn bản, giọng nói, dạng đồ thị,… Để xử lý các bộ dữ liệu khổng lồ, hay các dữ liệu không có cấu trúc bảng thông thường, người xử lý dữ liệu cần có kiến thức về khoa học máy tính và lập trình để thực hiện các tính toán trên máy tính điện tử. Những ứng dụng của KHDL có thể thuộc về bất kỳ lĩnh vực nào như kinh doanh, y học, vật lý, thiên văn, quản lý nhà nước, chính sách công,… nên đòi hỏi người xây dựng mô hình cũng cần có kiến thức chuyên môn trong lĩnh vực tương ứng để không bị sai định hướng trong quá trình làm việc với dữ liệu.Để minh họa ứng dụng của KHDL trong lĩnh vực kinh tế và kinh doanh, chúng tôi thảo luận ngắn gọn về ba dữ liệu được thu thập trong thế giới thực được xem xét trong cuốn sách này.","code":""},{"path":"giới-thiệu-về-cuốn-sách.html","id":"dữ-liệu-chi-phí-quảng-cáo","chapter":"Chương 1 Giới thiệu về cuốn sách","heading":"1.1.1 Dữ liệu chi phí quảng cáo","text":"Đây là một dữ liệu dạng bảng về chiến dịch quảng cáo của một công ty thu thập được từ 55 cửa hàng trên toàn quốc. Dữ liệu bao gồm thông tin về doanh thu bán sản phẩm và chi phí công ty đã chi cho ba phương thức quảng cáo là quảng cáo qua truyền hình, quảng cáo qua các nền tảng mạng xã hội, và quảng cáo bằng hình thức phát tờ rơi. Mối liên hệ của chi phí cho mỗi phương thức quảng cáo đến doanh thu từ bán sản phẩm được mô tả trong Hình 1.2\nHình 1.2: Doanh thu bán hàng (tỷ đồng) và mối liên hệ với chi phí quảng cáo. Hình bên trái: Chi phí quảng cáo trên Tivi. Hình ở giữa: chi phí quảng cáo qua các nền tảng mạng xã hội. Hình bên phải: quảng cáo theo hình thức phát tờ rơi\nTừ Hình 1.2 có thể đưa ra nhận định khá chắc chắn là chi tiền cho quảng cáo có ý nghĩa trong việc tăng doanh thu bán sản phẩm. Có thể thấy trong các đồ thị ở hàng phía trên rằng có mối liên hệ cùng chiều giữa chi phí chi cho các hình thức quảng cáo đến doanh thu bán sản phẩm. Mối liên hệ này là phù hợp với logic nói chung về quảng cáo sản phẩm: khi công ty chi tiền cho quảng cáo, nhiều khách hàng sẽ có cơ hội tiếp cận về sản phẩm hơn, làm tăng số lượng người mua sản phẩm và tăng doanh thu cho các cửa hàng.Tuy nhiên, vẫn còn","code":""},{"path":"giới-thiệu-về-cuốn-sách.html","id":"dữ-liệu-bảo-hiểm-xã-hội","chapter":"Chương 1 Giới thiệu về cuốn sách","heading":"1.1.2 Dữ liệu bảo hiểm xã hội","text":"","code":""},{"path":"giới-thiệu-về-cuốn-sách.html","id":"dữ-liệu-về-giá-xe-ôtô-cũ","chapter":"Chương 1 Giới thiệu về cuốn sách","heading":"1.1.3 Dữ liệu về giá xe ôtô cũ","text":"","code":""},{"path":"giới-thiệu-về-cuốn-sách.html","id":"sơ-lược-quá-trình-phát-triển-của-xây-dựng-mô-hình-dữ-liệu","chapter":"Chương 1 Giới thiệu về cuốn sách","heading":"1.2 Sơ lược quá trình phát triển của xây dựng mô hình dữ liệu","text":"Mặc dù thuật ngữ xây dựng mô hình trên dữ liệu, hay được gọi một cách kỹ thuật hơn là học máy, còn khá mới mẻ, nhưng những khái niệm nền tảng cho lĩnh vực này đã được phát triển từ lâu. Vào đầu thế kỷ 19, phương pháp bình phương nhỏ nhất đã được phát triển và áp dụng để ước lượng các mô hình hồi quy tuyến tính. Mô hình này lần đầu tiên được áp dụng và cho kết quả thành công trong các vấn đề liên quan đến thiên văn học. Vào đầu thế kỷ 20, mô hình hồi quy tuyến tính được sử dụng để dự đoán các giá trị định lượng, chẳng hạn như mức lương của một cá nhân hoặc để dự đoán các giá trị định tính, chẳng hạn như bệnh nhân sống hay chết, hay thị trường chứng khoán tăng hay giảm. Vào những năm 1940, nhiều tác giả đã đưa ra một cách tiếp cận khác, đó là hồi quy logistic. Vào đầu những năm 1970, thuật ngữ mô hình tuyến tính tổng quát đã được phát triển để mô tả toàn bộ lớp phương pháp học thống kê bao gồm cả hồi quy tuyến tính và hồi quy logistic như các trường hợp đặc biệt. Vào cuối những năm 1970, nhiều kỹ thuật xây dựng mô hình trên dữ liệu đã xuất hiện. Tuy nhiên, các mô hình này chỉ xoay quanh các phương pháp tuyến tính vì việc tạo ra các mối quan hệ phi tuyến tính rất khó khăn về mặt tính toán.Đến những năm 1980, sự phát triển của máy tính điện tử đã hỗ trợ tích cực về mặt tính toán cho các phương pháp phi tuyến tính. Các mô hình phi tuyến được giới thiệu vào đầu những năm 1980 bao gồm mô hình cây quyết định và mô hình cộng tính tổng quát. Những năm cuối thập niên 1980 và đầu thập niên 1990, mô hình mạng nơ-ron được giới thiệu đến cộng đồng nghiên cứu nhưng chưa nhận được nhiều sự quan tâm vì dữ liệu chưa đủ phong phú và sự phổ biến của các mô hình học máy khác.\nHình 1.3: Quá trình phát triển của các mô hình học máy\nGiai đoạn cuối thế kỷ XX và đầu thế kỷ XXI là giai đoạn chiếm ưu thế hoàn toàn của các mô hình học máy rừng ngẫu nhiên và thuật toán học tăng cường. Thuật toán học tăng cường với các biến thể như XGBoost hay LightGBM chiến thắng trong hầu hết các cuộc thi về khoa học dữ liệu.Từ năm 2010, với sự bùng nổ của các thiết bị thông minh và kết nối internet, dữ liệu trở nên phong phú và đa dạng hơn cũng là thời điểm quay trở lại của mô hình mạng nơ-ron, hay còn được gọi với tên gọi khác là mô hình mạng học sâu (deep learning). Mô hình mạng học sâu vượt trội hoàn toàn các mô hình học máy thông thường khi làm việc với dữ liệu kiểu hình ảnh, video, ngôn ngữ tự nhiên bao gồm cả văn bản và giọng nói. Sự kiện đánh dấu sự phát triển vượt bậc của các mô hình mạng học sâu là sự ra đời của ChatGPT vào cuối năm 2022, một mô hình ngôn ngữ lớn cho phép người dùng tương tác, hỏi đáp và trò chuyện một cách hoàn toàn tự nhiên theo định hướng của người sử dụng như phong cách, mức độ chi tiết, hình thức ngôn ngữ. ChatGPT nhanh chóng đạt đến 100 triệu người dùng sau hơn hai tháng phát hành và giúp cho công ty phát hành OpenAI được định giá khoảng 30 tỷ USD. Cho đến thời điểm cuối năm 2023 khi nhóm tác giả bắt đầu viết cuốn sách này, ChatGPT đã được cập nhật đến phiên bản 3.5.","code":""},{"path":"giới-thiệu-về-cuốn-sách.html","id":"tại-sao-lại-sử-dụng-phần-mềm-r","chapter":"Chương 1 Giới thiệu về cuốn sách","heading":"1.3 Tại sao lại sử dụng phần mềm R?","text":"Trong thế giới ngày nay, hầu hết các cơ quan, tổ chức, tập đoàn, doanh nghiệp từ lớn đến nhỏ đều sử dụng một lượng dữ liệu nhất định để phân tích các sự kiện trong quá khứ và cố gắng dự đoán xu hướng trong tương lai để đưa ra các quyết định có lợi cho mình. Tuy nhiên, khi dữ liệu ngày càng tăng lên cả về số lượng và sự phức tạp, các cơ quan tổ chức cần một công cụ giúp họ thực hiện các phân tích trên dữ liệu một cách nhanh hơn và chính xác hơn. Một trong những công cụ hiệu quả nhất hiện nay là phần mềm R.","code":""},{"path":"giới-thiệu-về-cuốn-sách.html","id":"phần-mềm-r-là-gì","chapter":"Chương 1 Giới thiệu về cuốn sách","heading":"1.3.1 Phần mềm R là gì?","text":"Trước tiên, R là ngôn ngữ lập trình được xây dựng để phục vụ cho toán học và thống kê, đồng thời R cũng là một môi trường phần mềm mã nguồn mở miễn phí cho người sử dụng. R được giới thiệu lần đầu tiên vào năm 1992 bởi các giáo sư Ross Ihaka và Robert Gentleman như một ngôn ngữ lập trình để dạy thống kê tại Đại học Auckland. Tên của ngôn ngữ, R, xuất phát từ chữ cái đầu tiên của các tác giả là Ross và Robert.Trước khi trở thành ngôn ngữ lập trình cho khoa học dữ liệu, R thường được coi là ngôn ngữ lập trình cho các nhà toán học và thống kê. Sau nhiều năm phát triển, R luôn được coi là một trong những ngôn ngữ lập trình phổ biến nhất trong giới học thuật vì độ tin cậy. Mỗi thư viện của R đều được phát triển một cách hoàn chỉnh và trải qua quá trình kiểm soát chặt chẽ. Tạp chí R (R Journal) là tạp chí học thuật về các phương pháp tính toán trong toán học và thống kê sử dụng phần mềm R luôn nằm trong danh sách các tạp chí khoa học uy tín (Science Citation Index Expanded hay SCIE) của Web Science. Chính vì sự uy tín trong học thuật nên đa số các trường đại học và viện nghiên cứu hàng đầu trên thế giới sử dụng R như là một ngôn ngữ chính trong đào tạo về tính toán, toán học và thống kê.","code":""},{"path":"giới-thiệu-về-cuốn-sách.html","id":"tại-sao-r-lại-được-sử-dụng-trong-khdl","chapter":"Chương 1 Giới thiệu về cuốn sách","heading":"1.3.2 Tại sao R lại được sử dụng trong KHDL?","text":"Trong khoảng hơn 10 năm trở lại đây, R không còn chỉ là ngôn ngữ lập trình thông thường như trước đây nữa. Mặc dù vẫn là một công cụ mạnh mẽ trong tính toán toán học và thống kê, nhưng còn có rất nhiều công cụ tuyệt vời khác mà bạn đọc có thể làm với R, đặc biệt là những ứng dụng trong khoa học dữ liệu (KHDL). Nguyên nhân chính giúp cho phần mềm R nhanh chóng trở thành ngôn ngữ phổ biến trong KHDL là nền tảng quan trọng nhất của KHDL chính là toán học và thống kê. Đồng thời, ngôn ngữ lập trình R cũng đủ linh hoạt để người sử dụng viết các chương trình yêu cầu tính toán phức tạp trong khoa học máy tính. Một cách tự nhiên, những nhà toán học, thống kê học và các tổ chức sử dụng R như là một ngôn ngữ chính sẽ tìm cách phát triển R để đáp ứng được yêu cầu xử lý dữ liệu của chính họ. Một nguyên nhân khác khiến cho R phổ biến trong KHDL là đặc thù mã nguồn mở của phần mềm này. Những người làm việc trong lĩnh vực KHDL sử dụng R có thể chia sẻ kiến thức và kinh nghiệm một cách nhanh chóng và rộng rãi.","code":""},{"path":"giới-thiệu-về-cuốn-sách.html","id":"r-có-thể-làm-những-gì","chapter":"Chương 1 Giới thiệu về cuốn sách","heading":"1.3.3 R có thể làm những gì?","text":"Danh sách những việc bạn có thể làm trong R là không thể liệt kê hết bởi vì phần mềm này vẫn đang được phát triển không ngừng. Dưới đây là một số ứng dụng phổ biến mà R vượt trội hơn với các ngôn ngữ khác:Lập trình trong toán học, tính toán tối ưu, giải tối ưu bằng phương pháp số.Tính toán liên quan đến lý thuyết xác suất.Thực hiện các kiểm định thống kê.Phát triển phần mềm thống kê.Xây dựng mô hình kinh tế lượng.Mô phỏng ngẫu nhiên.Các tính năng của R dành cho khoa học dữ liệu được liệt kê dưới đây:Thu thập tập dữ liệu, bao gồm cả dữ liệu lớn và không có cấu trúc.Khai phá dữ liệu.Xử lý, sắp xếp, biến đổi dữ liệu.Phân tích dữ liệu.Trực quan hóa dữ liệu.Xây dựng các mô hình từ đơn giản đến phức tạp.","code":""},{"path":"giới-thiệu-về-cuốn-sách.html","id":"tại-sao-cuốn-sách-này-lại-sử-dụng-r","chapter":"Chương 1 Giới thiệu về cuốn sách","heading":"1.3.4 Tại sao cuốn sách này lại sử dụng R?","text":"Mặc dù có một số phần mềm khác có thể được sử dụng thay thế cho R trong phân tích dữ liệu, nhưng chúng tôi lựa chọn ngôn ngữ R vì:R là một phần mềm uy tín và đáng tin cậy được sử dụng bởi các trường đại học và các viện nghiên cứu hàng đầu trên thế giới. R cũng được sử dụng rộng rãi trong các công ty công nghệ hàng đầu như Microsoft, Facebook, Google, và IBM.R là ngôn ngữ lập trình rất dễ hiểu cho người mới bắt đầu, kể cả với những người không có kinh nghiệm lập trình. Còn nếu bạn đã có kinh nghiệm về một ngôn ngữ lập trình, bạn sẽ chỉ cần một khoảng thời gian ngắn để có thể viết các chương trình với R.Phần mềm R là một phần mềm mã nguồn mở, nghĩa là bạn có thể sử dụng R và hơn 12.000 thư viện mở rộng mà không cần phải bỏ ra bất kỳ chi phí nào. Điều này giúp cho R trở nên rất dễ tiếp cận đối với sinh viên và người học không sẵn sàng chi trả một khoản chi phí để học về khoa học dữ liệu. Đồng thời, giảng viên cũng có thể tận dụng tối đa môi trường phần mềm này khi giảng dạy cho sinh viên.Cuối cùng và cũng không kém phần quan trọng, đó là sự hỗ trợ từ cộng đồng. Với số lượng người dùng lên đến hàng triệu người, nhiều trong số đó là những nhà toán học, thống kê học, giáo sư tại các trường đại học, bạn sẽ luôn tìm thấy sự hỗ trợ mỗi khi gặp bất kỳ vấn đề khi làm việc với R.","code":""},{"path":"giới-thiệu-về-cuốn-sách.html","id":"các-lựa-chọn-thay-thế-và-bổ-sung-cho-r-trong-khdl","chapter":"Chương 1 Giới thiệu về cuốn sách","heading":"1.3.5 Các lựa chọn thay thế và bổ sung cho R trong KHDL","text":"Như chúng ta đã thấy, R là một trong những ngôn ngữ lập trình tốt nhất cho người mới bắt đầu bước chân vào lĩnh vực khoa học dữ liệu (KHDL). Tuy nhiên, bạn cũng có thể tìm thấy các phần mềm/ngôn ngữ có thể sử dụng để thay thế cho R trong quá trình học tập và làm việc:Python - Vào thời điểm chúng tôi hoàn thành cuốn sách này, Python là ngôn ngữ lập trình được sử dụng nhiều nhất trong KHDL. Python được giới thiệu lần đầu tiên vào năm 1991 và đã không ngừng tiến hóa và phát triển. Cũng như R, Python có mã nguồn mở và hoàn toàn miễn phí cho người sử dụng. Câu lệnh của Python rất dễ học và đặc biệt mạnh mẽ trong lập trình hướng đối tượng.Julia - Xuất hiện lần đầu tiên vào năm 2012, Julia là một trong những ngôn ngữ lập trình mới nhất và là lựa chọn tối ưu cho các nhà khoa học dữ liệu. Ngôn ngữ lập trình cấp cao và hiệu suất cao này rất năng động và phù hợp để viết bất kỳ loại ứng dụng nào. Mặc dù Python và R vẫn được ưu tiên cho KHDL và học máy, nhưng Julia dự báo sẽ vượt qua cả hai trong tương lai gần. Mặc dù đây là ngôn ngữ lập trình tổng quát, nhưng nó có tất cả các đặc điểm cần thiết để xử lý phân tích, thống kê và dữ liệu lớn.MATLAB - Được phát triển bởi MathWorks, ngôn ngữ lập trình này là một môi trường điện toán được phát triển đặc biệt để phân tích số và thống kê. Nhờ có số lượng lớn các thư viện có sẵn cho người dùng, MATLAB cho phép lập trình viên truy cập dữ liệu, xử lý dữ liệu và tạo các mô hình học máy từ đơn giản đến phức tạp. Mặc dù MATLAB là một hệ thống hiệu suất cao, nhưng lại không phải là nguồn mở hoặc miễn phí. Thay vào đó, nó được xây dựng bởi các nhà phát triển chuyên nghiệp.Java - Là một trong những ngôn ngữ lập trình phổ biến nhất và cũng là một lựa chọn cho những người mới bước vào lĩnh vực KHDL. Mặc dù vẫn có thể tải xuống miễn phí, một số ứng dụng chỉ có sẵn trong phiên bản trả phí. Cú pháp của Java cũng tương đối dễ học đối với người mới bắt đầu. Nhìn chung, Java vẫn là ngôn ngữ có mục đích chung và được các nhà khoa học dữ liệu coi là một lựa chọn bổ sung cho R hoặc Python.Ngoài việc thành thạo R hoặc một phần mềm chuyên dùng trong KHDL, bạn nên bổ sung cho mình các ngôn ngữ lập trình khác để đạt hiệu suất công việc tốt nhất:SQL hay ngôn ngữ truy vấn dữ liệu có cấu trúc. SQL xuất hiện từ năm 1974 và đã không ngừng được cải tiến và sửa đổi để giúp cho ngôn ngữ này luôn nằm trong nhóm những ngôn ngữ lập trình được lựa chọn hàng đầu trong KHDL. SQL có cả các phiên bản miễn phí và các phiên bản thương mại mà người sử dụng phải trả chi phí.C và C++ là các ngôn ngữ lập trình hiệu suất cao có thể giúp bạn tăng hiệu quả của các chương trình. Hầu như tất cả các ứng dụng trên hệ điều hành máy tính và điện thoại di động hiện nay đều sử dụng C và C++. Viết các chương trình dưới ngôn ngữ C hoặc C++ sẽ hiệu quả về mặt thời gian hơn nhiều với các ngôn ngữ khác.","code":""},{"path":"giới-thiệu-về-cuốn-sách.html","id":"cài-đặt-r-và-rstudio","chapter":"Chương 1 Giới thiệu về cuốn sách","heading":"1.3.6 Cài đặt R và RStudio","text":"Bạn đọc sẽ bắt đầu bằng cài đặt phần mềm R và sau đó là cài đặt RStudio - một môi trường phát triển phổ biến dành cho R. Phần mềm R dành cho các hệ điều hành MAC OS, Windows, và Linux đều có sẵn để tải xuống từ trang web chính thức:https://cran.r-project.org/Tại thời điểm chúng tôi viết cuốn sách này, R đang là phiên bản 4.1.0. Sau khi tải xuống, chúng ta chỉ cần cài đặt R giống như tất cả các phần mềm khác với tất cả các tùy chọn mặc định.Sau khi cài đặt phần mềm R, bạn đọc cài đặt Rstudio. Chúng ta hoàn toàn có thể sử dụng R mà không cần có Rstudio. Tuy nhiên, Rstudio sẽ hỗ trợ bạn rất nhiều trong quá trình sử dụng R, đó, lời khuyên của chúng tôi là hãy sử dụng Rstudio cùng với R. Để tải xuống Rstudio, bạn truy cập vào trang web chính thức:https://posit.co/download/rstudio-desktop/RStudio là một công cụ linh hoạt giúp bạn tạo các phân tích dễ đọc và giữ mã, hình ảnh, nhận xét và sơ đồ của bạn ở cùng một nơi. Sử dụng RStudio để lập trình và phân tích dữ liệu trong R mang lại nhiều lợi ích. Dưới đây là một vài ví dụ về những gì RStudio cung cấp:Giao diện trực quan cho phép chúng ta theo dõi các đối tượng, tập lệnh và số liệu đã lưu.Trình soạn thảo văn bản có các tính năng như tự động gợi ý câu lệnh, hiển thị màu giúp cho việc viết các câu lệnh rõ ràng.Hiển thị mô tả hàm số, dữ liệu bằng thao tác đơn giản.Thanh công cụ có đầy đủ các tính năng trực quan để bạn đọc sử dụng thay vì phải viết câu lệnh.Mỗi khi bạn đọc mở RStudio, R cũng được khởi chạy tự động. Giao diện RStudio rất trực quan và dễ sử dụng. Các cửa sổ quan trọng bao gồm:Cửa số Console là nơi chúng ta có thể chạy các câu lệnh R.Cửa sổ Environment là chúng ta theo dõi các đối tượng, tập lệnh và số liệu đã lưu.Cửa sổ File là nơi hiển thị địa chỉ thư mục đang làm việc, hiển thị đồ thị trực quan, hoặc hiển thị mô tả dữ liệu, hàm số, thư viện.Bạn đọc sẽ làm quen dần với giao diện và các cửa sổ làm việc khác nhau của RStudio trong quá trình thực hành trên các câu lệnh và dữ liệu cụ thể. Chúng tôi sẽ không đi quá sâu vào chi tiết tại đây.","code":""},{"path":"giới-thiệu-về-cuốn-sách.html","id":"về-cuốn-sách-và-tác-giả","chapter":"Chương 1 Giới thiệu về cuốn sách","heading":"1.4 Về cuốn sách và tác giả","text":"Cuốn sách được viết với mục tiêu là để thành sách tham khảo chính cho các môn học “Phân tích và dự báo” và “Khoa học dữ liệu cơ bản” cho sinh viên và học viên cao học ngành Toán Kinh tế tại Đại học Kinh tế Quốc dân. Chúng tôi tin rằng những kiến thức và công cụ được giới thiệu trong cuốn sách này sẽ là những hành trang quan trọng cho những nhà kinh tế và kinh doanh tương lai trước khi bước chân vào thế giới việc làm đầy tính cạnh tranh như hiện nay.","code":""},{"path":"giới-thiệu-về-cuốn-sách.html","id":"đôi-lời-từ-tác-giả","chapter":"Chương 1 Giới thiệu về cuốn sách","heading":"1.4.1 Đôi lời từ tác giả","text":"Tôi không phải là một nhà kinh tế, cũng không phải là một chuyên gia dữ liệu, và tôi cũng chưa từng được đào tạo bài bản về máy tính hay lập trình, tôi là một Actuary. Cuốn sách được viết dựa trên kinh nghiệm làm việc và giảng dạy của tôi trong những lĩnh vực khoa học tính toán (Actuarial science). Tôi bắt đầu sử dụng R như một phần mềm thống kê khi còn là một sinh viên đại học. Ấn tượng đầu tiên của tôi về R là khi phần mềm này được sử dụng để mô phỏng các chuyển động Brown hình học vô cùng bắt mắt. Và R vẫn tiếp tục đồng hành với tôi cho đến nay trong cả môi trường doanh nghiệp và học thuật:Trong thời gian nghiên cứu sinh từ 2011 đến 2014, tôi sử dụng R là công cụ chính để thực hiện các tính toán cho luận án của mình. Với nội dung nghiên cứu tập trung vào tính toán và mô phỏng xác suất của các sự kiện cực hiếm, R là lựa chọn tối ưu vào thời điểm đó. Trong một vài tính toán mà chưa có thư viện hỗ trợ, chẳng hạn như tính toán với độ chính xác siêu nhỏ, dưới \\(10^{-100}\\), tôi đã sử dụng Python là một giải pháp bổ sung.Trong thời gian nghiên cứu sinh từ 2011 đến 2014, tôi sử dụng R là công cụ chính để thực hiện các tính toán cho luận án của mình. Với nội dung nghiên cứu tập trung vào tính toán và mô phỏng xác suất của các sự kiện cực hiếm, R là lựa chọn tối ưu vào thời điểm đó. Trong một vài tính toán mà chưa có thư viện hỗ trợ, chẳng hạn như tính toán với độ chính xác siêu nhỏ, dưới \\(10^{-100}\\), tôi đã sử dụng Python là một giải pháp bổ sung.Công việc đầu tiên trong môi trường doanh nghiệp của tôi liên quan đến dữ liệu là xây dựng các thuật toán để đầu tư trên thị trường tài chính tại một quỹ đầu tư. Tôi được làm quen với một kho dữ liệu khổng lồ bao gồm dữ liệu hỗ trợ phân tích kỹ thuật, dữ liệu hỗ trợ phân tích cơ bản, và dữ liệu kiểu tin tức liên quan đến tất cả các công cụ tài chính có thể sử dụng để giao dịch, bao gồm cổ phiếu, trái phiếu, hợp đồng tương lai, quyền chọn. Các mô hình trên dữ liệu được chúng tôi - những người nghiên cứu thị trường - xây dựng bằng nhiều phương pháp để tìm ra các chiến lược mang lại lợi nhuận cho quỹ đầu tư. Mặc dù không có cơ hội sử dụng R để phân tích dữ liệu các yêu cầu liên quan đến bảo mật, nhưng tôi lại được học những kỹ năng lập trình C++ mà tôi nhận ra là vô cùng quan trọng cho công việc của mình sau này.Công việc đầu tiên trong môi trường doanh nghiệp của tôi liên quan đến dữ liệu là xây dựng các thuật toán để đầu tư trên thị trường tài chính tại một quỹ đầu tư. Tôi được làm quen với một kho dữ liệu khổng lồ bao gồm dữ liệu hỗ trợ phân tích kỹ thuật, dữ liệu hỗ trợ phân tích cơ bản, và dữ liệu kiểu tin tức liên quan đến tất cả các công cụ tài chính có thể sử dụng để giao dịch, bao gồm cổ phiếu, trái phiếu, hợp đồng tương lai, quyền chọn. Các mô hình trên dữ liệu được chúng tôi - những người nghiên cứu thị trường - xây dựng bằng nhiều phương pháp để tìm ra các chiến lược mang lại lợi nhuận cho quỹ đầu tư. Mặc dù không có cơ hội sử dụng R để phân tích dữ liệu các yêu cầu liên quan đến bảo mật, nhưng tôi lại được học những kỹ năng lập trình C++ mà tôi nhận ra là vô cùng quan trọng cho công việc của mình sau này.Khi bắt đầu công việc như một chuyên gia tính toán, tôi làm việc thường xuyên với dữ liệu trong bảo hiểm. Với những dữ liệu nhỏ, tôi nhận thấy rằng Microsoft Excel kết hợp với lập trình VBA là vừa đủ để xử lý. Khi dữ liệu trở nên ngày càng lớn và phức tạp, Excel và VBA không còn đáp ứng được nhu cầu, đó là lúc tôi quay lại sử dụng R trong công việc của mình. Ngoài sử dụng R như là một công cụ chính để định phí, đánh giá hợp đồng bảo hiểm, tôi còn sử dụng R để thực hiện các công việc liên quan đến dữ liệu như:\nThu thập dữ liệu nhận được từ các phòng ban như tài chính, kế toán, nghiệp vụ, , kiểm tra; làm sạch và cập nhập dữ liệu lên cơ sở dữ liệu để phục vụ tính toán. R cho phép xử lý và tính toán những dữ liệu hàng chục triệu dòng với hiệu quả thực sự đáng kinh ngạc.\nTrích xuất dữ liệu từ cơ sở dữ liệu, biến đổi và tính toán để thực hiện các nghiệp vụ như tái bảo hiểm, quản lý tài sản nợ/có, tính toán báo cáo kinh nghiệm.\nXây dựng các dashboard để cập nhật tình hình bồi thường bảo hiểm y tế với thời gian thực. Thư viện Shiny là công cụ tuyệt vời để tạo ra những dashboard như vậy. \nXây dựng mô hình để phân loại rủi ro, dự báo những chủ hợp đồng, người được bảo hiểm có khả năng cao là trục lợi trong bảo hiểm y tế.\nKhi bắt đầu công việc như một chuyên gia tính toán, tôi làm việc thường xuyên với dữ liệu trong bảo hiểm. Với những dữ liệu nhỏ, tôi nhận thấy rằng Microsoft Excel kết hợp với lập trình VBA là vừa đủ để xử lý. Khi dữ liệu trở nên ngày càng lớn và phức tạp, Excel và VBA không còn đáp ứng được nhu cầu, đó là lúc tôi quay lại sử dụng R trong công việc của mình. Ngoài sử dụng R như là một công cụ chính để định phí, đánh giá hợp đồng bảo hiểm, tôi còn sử dụng R để thực hiện các công việc liên quan đến dữ liệu như:Thu thập dữ liệu nhận được từ các phòng ban như tài chính, kế toán, nghiệp vụ, , kiểm tra; làm sạch và cập nhập dữ liệu lên cơ sở dữ liệu để phục vụ tính toán. R cho phép xử lý và tính toán những dữ liệu hàng chục triệu dòng với hiệu quả thực sự đáng kinh ngạc.Trích xuất dữ liệu từ cơ sở dữ liệu, biến đổi và tính toán để thực hiện các nghiệp vụ như tái bảo hiểm, quản lý tài sản nợ/có, tính toán báo cáo kinh nghiệm.Xây dựng các dashboard để cập nhật tình hình bồi thường bảo hiểm y tế với thời gian thực. Thư viện Shiny là công cụ tuyệt vời để tạo ra những dashboard như vậy. Xây dựng mô hình để phân loại rủi ro, dự báo những chủ hợp đồng, người được bảo hiểm có khả năng cao là trục lợi trong bảo hiểm y tế.Khi tôi quay trở lại với công việc học thuật vào đầu năm 2018, cũng là lúc mà làn sóng về khoa học dữ liệu bắt đầu ảnh hưởng đến đa số các lĩnh vực khác, bao gồm cả ngành khoa học tính toán. Các chứng chỉ về khoa học dữ liệu là điều kiện bắt buộc đối với những người muốn trở thành thành viên của các Hiệp hội chuyên gia tính toán. Tôi cùng với các đồng nghiệp của mình đưa môn học “Phân tích và dự báo” vào trong chương trình đào tạo Định phí bảo hiểm và quản trị rủi ro để cung cấp cho sinh viên các kiến thức và kỹ năng cần thiết khi làm việc với dữ liệu và có thể lấy được chứng chỉ nghề nghiệp của Hiệp hội. Môn học “Phân tích và dự báo” sau đó chính thức được giảng dạy cho sinh viên ngành Toán Kinh tế vào năm 2021 với tên gọi “Khoa học dữ liệu trong Kinh tế và Kinh doanh”. Nội dung của cuốn sách xoay quanh các kiến thức mà tôi đã đang và sẽ giảng dạy cho sinh viên và học viên của mình.Khi tôi quay trở lại với công việc học thuật vào đầu năm 2018, cũng là lúc mà làn sóng về khoa học dữ liệu bắt đầu ảnh hưởng đến đa số các lĩnh vực khác, bao gồm cả ngành khoa học tính toán. Các chứng chỉ về khoa học dữ liệu là điều kiện bắt buộc đối với những người muốn trở thành thành viên của các Hiệp hội chuyên gia tính toán. Tôi cùng với các đồng nghiệp của mình đưa môn học “Phân tích và dự báo” vào trong chương trình đào tạo Định phí bảo hiểm và quản trị rủi ro để cung cấp cho sinh viên các kiến thức và kỹ năng cần thiết khi làm việc với dữ liệu và có thể lấy được chứng chỉ nghề nghiệp của Hiệp hội. Môn học “Phân tích và dự báo” sau đó chính thức được giảng dạy cho sinh viên ngành Toán Kinh tế vào năm 2021 với tên gọi “Khoa học dữ liệu trong Kinh tế và Kinh doanh”. Nội dung của cuốn sách xoay quanh các kiến thức mà tôi đã đang và sẽ giảng dạy cho sinh viên và học viên của mình.","code":""},{"path":"giới-thiệu-về-cuốn-sách.html","id":"ai-nên-đọc-cuốn-sách-này","chapter":"Chương 1 Giới thiệu về cuốn sách","heading":"1.4.2 Ai nên đọc cuốn sách này","text":"Cuốn sách này dành cho bất kỳ bạn đọc nào quan tâm đến việc sử dụng dữ liệu và các phương pháp thống kê hiện đại để giải quyết các vấn đề gặp phải trong học tập và trong công việc hàng ngày. Độc giả có thể là bất kỳ ai, bao gồm các nhà khoa học, kỹ sư, người phân tích dữ liệu, và người nghiên cứu định lượng, hoặc cũng có thể bao gồm những cá nhân với nền tảng có ít tính kỹ thuật hơn như sinh viên, học viên cao học trong các lĩnh vực không định lượng như khoa học xã hội hoặc kinh doanh. Mặc dù không bắt buộc, chúng tôi kỳ vọng rằng bạn đọc đã từng học ít nhất một khóa học cơ bản về xác suất thống kê và một khóa học về lập trình.Mức độ toán học của cuốn sách này là vừa phải và không cần phải có kiến thức chi tiết về các phép toán phức tạp liên quan đến đại số tuyến tính hay giải tích nhiều chiều. Cuốn sách này cung cấp một phần giới thiệu về ngôn ngữ lập trình thống kê R. Việc đã từng tiếp xúc với một ngôn ngữ lập trình, chẳng hạn như C, C++, hoặc Python, sẽ hữu ích cho bạn đọc nhưng không phải là yêu cầu bắt buộc. Cuốn sách này có thể được sử dụng để giảng dạy cho sinh viên thạc sĩ và tiến sĩ trong các lĩnh vực kinh doanh, kinh tế, sinh học, và nhiều lĩnh vực khác của khoa học tự nhiên và xã hội.Cuốn sách đã được sử dụng để giảng dạy cho sinh viên đại học ở mức độ cơ bản như chúng tôi đã đề cập ở trên, nhằm cung cấp cho người học những kiến thức về xây dựng mô hình học máy cao cấp hơn mô hình hồi quy tuyến tính thông thường. Trong mỗi chương sách, chúng tôi luôn cố gắng thêm vào các luận giải về mặt toán học cho các mô hình, với mục đích để các học viên cao học, nghiên cứu sinh khi tham khảo cuốn sách có hiểu biết cặn kẽ hơn. Việc này giúp cho việc giảng dạy khoa học dữ liệu cho người học có thể được tiếp cận từ các khía cạnh khác nhau.","code":""},{"path":"giới-thiệu-về-cuốn-sách.html","id":"các-ký-hiệu-thông-dụng","chapter":"Chương 1 Giới thiệu về cuốn sách","heading":"1.4.3 Các ký hiệu thông dụng","text":"","code":""},{"path":"giới-thiệu-về-cuốn-sách.html","id":"cấu-trúc-của-cuốn-sách","chapter":"Chương 1 Giới thiệu về cuốn sách","heading":"1.4.4 Cấu trúc của cuốn sách","text":"","code":""},{"path":"giới-thiệu-về-cuốn-sách.html","id":"các-dữ-liệu-sử-dụng-trong-cuốn-sách","chapter":"Chương 1 Giới thiệu về cuốn sách","heading":"1.4.5 Các dữ liệu sử dụng trong cuốn sách","text":"","code":""},{"path":"khdl-và-các-khái-niệm-cơ-bản.html","id":"khdl-và-các-khái-niệm-cơ-bản","chapter":"Chương 2 KHDL và các khái niệm cơ bản","heading":"Chương 2 KHDL và các khái niệm cơ bản","text":"","code":"\nlibrary(dplyr)## \n## Attaching package: 'dplyr'## The following objects are masked from 'package:stats':\n## \n##     filter, lag## The following objects are masked from 'package:base':\n## \n##     intersect, setdiff, setequal, union\nlibrary(kableExtra)## \n## Attaching package: 'kableExtra'## The following object is masked from 'package:dplyr':\n## \n##     group_rows"},{"path":"khdl-và-các-khái-niệm-cơ-bản.html","id":"các-khái-niệm-cơ-bản-trong-khoa-học-dữ-liệu","chapter":"Chương 2 KHDL và các khái niệm cơ bản","heading":"2.1 Các khái niệm cơ bản trong khoa học dữ liệu","text":"Khoa học dữ liệu là ngành khoa học kết hợp toán học và thống kê, lập trình chuyên biệt, kiến thức chuyên môn cụ thể để khám phá những thông tin hữu ích ẩn trong dữ liệu của các cơ quan, tổ chức. Những thông tin hữu ích này có thể được sử dụng để hướng dẫn việc ra quyết định và lập kế hoạch chiến lược cho cơ quan tổ chức đó.Để bắt đầu với Khoa học dữ liệu, chúng ta sẽ đi từ một vài ví dụ đơn giản nhất. Đặt trong trường hợp chúng ta là người phân tích khoa học dữ liệu tại một bệnh viện, một bài toán đơn giản thường được đặt ra là dự đoán khả năng một người bất kỳ nào đó bị bệnh dựa trên các thông tin mà ta biết về người đó. Ví dụ, để tính xác suất của một người bị bệnh tim lần 2, sau có thông tin là người đó đã bị bệnh tim lần 1, dựa vào các thông tin khác như chủng tộc, giới tính, tuổi, tiền sử bệnh lý, lối sống của bệnh nhân đó.Quyển sách này sẽ dạy bạn cách để học từ dữ liệu. Thông thường, chúng ta sẽ có một thước đo kết quả (hay còn gọi là các biến phụ thuộc), thường là theo biến định lượng (ví dụ như giá cổ phiếu, doanh thu của công ty), hoặc biến định tính (ví dụ như có hay không chuyện người đó bị bệnh tim), và chúng ta sẽ dự đoán nó từ các đặc trưng (hay còn gọi là các biến độc lập).Để xây dựng được những mô hình tốt, nghĩa là mô hình có thể đưa ra dự đoán với độ chính xác cao, chúng ta cần phải sử dụng dữ liệu sao cho thật hợp lý. Thử tưởng tượng, mục đích của dự đoán là dự báo những điều chưa được biết. nếu như bạn xây dựng một hàm f để dự đoán kết quả trên toàn bộ dữ liệu bạn có, thì làm sao bạn có thể biết được hàm f đó có hoạt động tốt trên thực tế để chọn ra hàm f phù hợp nhất? Khi hàm f được xây dựng trên toàn bộ tập dữ liệu, các tham số của hàm đó sẽ được tính toán để phù hợp với dữ liệu đó nhất, nhưng có khả năng chúng sẽ lại không phù hợp với các dữ liệu được đưa vào từ bên ngoài.Vậy nên, trong thực tế, chúng ta thường chia dữ liệu ra làm hai hoặc ba phần tuỳ thuộc vào lượng dữ liệu và các đặc tính về dữ liệu đó. Thông thường, nếu dữ liệu đủ lớn, ta sẽ chia dữ liệu ra làm ba thành phần chính: dữ liệu học (train data), dữ liệu xác thực (validation data) và dữ liệu kiểm thử (test data).Các hàm f sẽ được huấn luyện trên dữ liệu học, sau đó các hàm f khác nhau sẽ được đưa vào dữ liệu xác thực để thử dự đoán, và hàm f có những dự đoán chính xác nhất trên dữ liệu xác thực sẽ được chọn để đưa vào dữ liệu kiểm thử. Nếu hàm f dự đoán trên dữ liệu kiểm thử đạt yêu cầu, nghĩa là vượt qua một tiêu chuẩn nhất định. Tiêu chuẩn nhất định cũng sẽ thường phụ thuộc vào bài toán mà hàm f đó được xây dựng để xử lý. Đối với kết quả dự báo là kết quả định lượng, người ta thường đặt tiêu chuẩn là như sai số dự báo nhỏ hơn k cho trước. Còn đối với kết quả dự báo là kết quả định tính, người ta thường đặt tiêu chuẩn là tỉ lệ dự đoán đúng cao hơn một số k cho trước.Trong phương pháp học thống kê, ta thường xây dựng một hàm f dựa trên dữ liệu gồm có nhiều biến khác nhau. Thông thường, biến đầu ra (output) còn được gọi là biến phụ thuộc (dependent variable), thường được ký hiệu là X. Tương tự, biến đầu vào là các biến được sử dụng để dự đoán biến phụ thuộc, loại biến này còn được gọi là biến dự đoán, hay biến độc lập, thường được ký hiệu là Y.Một định nghĩa nữa mà ta cần phải biết đó là định nghĩa về các loại biến khác nhau. Trong quyển sách này, chúng ta sẽ nhắc tới hai loại biến cơ bản nhất là Biến định lượng và biến định tính.\nBiến định lượng là loại biến mà ta có thể sánh các giá trị của biến đó với nhau, và nếu hai giá trị định lượng gần nhau, thì thường chúng cũng sẽ gần nhau trong thực tế. Lấy ví dụ về biến định lượng, ta có thể kể đến chỉ số đường trong máu của một người. Nếu như hai người có chỉ số glucose trong máu chỉ chênh nhau 0.1 mg/dl, thì có nghĩa là mức độ đường trong máu của họ thực tế sẽ giống nhau hơn là hai người có chỉ số glucose trong máu chênh nhau 10 mg/dl. Hoặc đơn giản như là một nhân viên Nói chung là sự khác biệt giữa các giá trị của hai biến định lượng với nhau ta hoàn toàn có thể quan sát được. Biến định lượng cũng sẽ có thể có hai dạng, đó là biến liên tục (continuous) với giá trị quan sát thuộc tập số thực (ví dụ như tốc độ của các xe ô tô trên đường cao tốc), và biến rời rạc (discrete) với các giá trị quan sát thuộc tập số nguyên (ví dụ như tổng số sản phẩm mà một khách hàng mua của công ty).Việc xác định đúng loại biến là rất quan trọng, phục vụ chính xác . Trong thực tế, ta có thể sẽ có trường hợp như sau. Một công ty đưa ra một khảo sát cho khách hàng, khảo sát đó có câu hỏi để đánh giá mức độ hài lòng với chất lượng dịch vụ của công ty với các lựa chọn như sau: “Rất Kém”, “Kém”, “Bình thường”, “Tốt”, “Rất tốt” hoặc đơn thuần là các lựa chọn từ 1 đến 5. Tuy nhiên, trong khảo sát đó, khách hàng cũng có câu hỏi về loại thức ăn mà họ yêu thích, với các lựa chọn là “Cơm”, “Bánh mì”, “Mỳ Ý”. Đối với câu trả lời cho hai câu hỏi trên của khách hàng, thì câu trả lời nào là biến định tính, câu trả lời nào là biến định lượng?Đáp án là không có câu trả lời nào là biến định lượng, toàn bộ các câu trả lời đều là biến định tính. Tại sao lại như vậy? Trong trường hợp hỏi về “loại thức ăn yêu thích”, các câu trả lời không thể sánh được với nhau, ta không thể nói rằng khách hàng thích ăn “Cơm” là tốt hơn, hay kém hơn với khách hàng ăn “Mỳ Ý” hay “Bánh mì”. Nên đương nhiên đáp án ở câu hỏi này là biến định tính. Còn đối với câu hỏi thứ hai, tưởng như ta có thể sánh các lựa chọn của khách hàng với nhau, nhưng sự thực là ta không thể dựa vào các đánh giá trên để kết luận rằng khách hàng này thích dịch vụ của công ty hơn bao nhiêu lần với khách hàng kia. Lấy ví dụ đơn giản, khách hàng có câu trả lời là 3, khách hàng B có câu trả lời là 2, khách hàng C có câu trả lời là 5, ta không thể nói rằng khách hàng thích dịch vụ của công ty hơn gấp rưỡi với khách hàng B, hay khách hàng B thích dịch vụ của công ty bằng 2,5 lần khách hàng C. Điều đó là không hợp lý, ta chỉ có thể nói rằng khách hàng thích dịch vụ của công ty hơn khách hàng B, hay khách hàng B không thích dịch vụ của công ty bằng khách hàng C.Vậy thì có cách nào để ta xác định những trường hợp như vậy là biến định tính, hay biến định lượng? Đối với biến định tính, ta có phải đặt ra tiêu chuẩn phân loại gì không? Ta thường chia ra làm hai loại là biến định lượng có thứ tự (ordinal variable) và biến định lượng không có thứ tự (nominal variable). Biến định lượng có thứ tự là loại biến mà có sự sắp xếp thứ bậc giữa các giá trị trong biến, nhưng không có một thước đo nào phù hợp. Biến định tính không có thứ tự là loại biến có các giá trị mà không có thước đo nào phù hợp giữa chúng, đồng thời cũng không có thước đo nào phù hợp, tương tự như trường hợp “Cơm”, “Bánh mì”, “Mỳ Ý” ở trên.Đối với biến định tính, thường ta có thể chuyển các giá trị của biến định tính sang thành dạng số. Có một phương pháp cơ bản để làm điều này đó được gọi là one-hot encoding hoặc còn gọi là dummy encoding.\nBảng 2.1: Original Dataset\n\nBảng 2.2: One-Hot Encoded Dataset\nDummy variable hay one-hot encoding có cách gán giá trị đơn giản hơn, khi chúng đưa một biến định tính gồm có N giá trị phân loại thành N-1 biến nhị phân khác nhau.Trong trường hợp biến định tính chỉ có hai giá trị duy nhất, chúng ta gọi giá trị đó là biến nhị phân (binary). Các ví dụ cho trường hợp biến nhị phân có thể kể tới như: “Có đi xe đạp” – “Không đi xe đạp”, “Đang đi học” – “Đang không đi học”…. Những trường hợp này ta có thể gán giá trị số 0,1 lần lượt cho hai trường hợp.Trong phần tiếp theo, ta sẽ đi sâu hơn một chút về lý thuyết xây dựng mô hình hồi quy. Để dễ tưởng tượng hơn, ta sẽ bắt đầu với bài toán mà biến dự đoán là biến định lượng. Cho biến Y là biến phụ thuộc, và có các biến độc lập \\(X_1,X_2,X_3,…,X_p\\). Chúng ta giả sử là có sự liên hệ giữa biến Y và biến \\(X = (X_1,X_2,X_3,…,X_p)\\), ta có thể viết chúng dưới dạng tổng quát:\\[\\begin{align}\nY = f(X) + \\epsilon\n\\end{align}\\]Trong đó, f là một hàm cố định, nhưng chưa xác định của \\(X_1,X_2,X_3,…,X_p\\), và bên cạnh đó \\(\\epsilon\\) là random error term, độc lập với X và có giá trị kỳ vọng bằng 0. Trong công thức trên, f thể hiện systematic information mà X cung cấp về Y.Ta có thể lấy ví dụ đơn giản về một hàm f như sau. Giả sử chúng ta muốn tìm mối liên hệ giữa mức lương của một người với số năm kinh nghiệm làm việc và trình độ học vấn, chúng ta có thể viết lại mối liên hệ đó bằng hàm f như sau:\\[\\begin{align}\nMức lương = f(\\text{Số năm kinh nghiệm}, \\text{Trình độ học vấn}) + \\epsilon\n\\end{align}\\]Trong ví dụ này, chúng ta có thể thấy rằng mức lương phụ thuộc một cách có hệ thống vào trình độ học vấn và số năm kinh nghiệm. Theo lẽ tư duy thông thường, một người có số năm kinh nghiệm và trình độ học vấn càng cao thì mức lương họ được hưởng sẽ cao hơn người khác. Tuy nhiên, sự thật là không phải ai cũng được hưởng một mức lương giống hệt nhau, bởi vì có rất nhiều lý khác có thể ảnh hưởng đến mức lương của mỗi người, có thể kể đến như kỹ năng cá nhân, ngành nghề làm việc, vị trí địa lý, cơ cấu lương của doanh nghiệpm,…. Để thể hiện sự khác biệt về mức lương giữa những người khác nhau, chúng ta sử dụng \\(\\epsilon\\), đây có thể được coi là sự chênh lệch một cách ngẫu nhiên về mức lương của những người khác nhau. Khi chúng ta xây dựng một mô hình (hay nói chính xác hơn là hàm f) để thể hiện mối liên hệ giữa biến Y và biến X, thì phần ngẫu nhiên \\(\\epsilon\\) được thêm vào công thức là cần thiết để đảm bảo mô hình của chúng ta được chính xác.","code":""},{"path":"khdl-và-các-khái-niệm-cơ-bản.html","id":"tại-sao-cần-hàm-f","chapter":"Chương 2 KHDL và các khái niệm cơ bản","heading":"2.2 Tại sao cần hàm f?","text":"Có hai lý chính để chúng ta tìm ra hàm f, đó là phục vụ cho mục đích dự báo hoặc suy diễn thống kê. Trước tiên, chúng ta sẽ đi sâu vào mục đích thứ nhất, dự báo.","code":""},{"path":"khdl-và-các-khái-niệm-cơ-bản.html","id":"dự-báo","chapter":"Chương 2 KHDL và các khái niệm cơ bản","heading":"2.2.1 Dự báo","text":"Trong thực tế, có rất nhiều trường hợp, chúng ta có đầy đủ thông tin về biến độc lập X, nhưng chúng ta lại rất khó có đầy đủ thông tin về biến phụ thuộc Y. Trong hệ thống mô hình dự báo của chúng ta, dựa trên việc biến sai số \\(\\epsilon\\) có giá trị trung bình bằng 0, chúng ta có thể dự đoán Y bằng công thức:\\[\\begin{align}\n\\hat{Y} = \\hat{f}(X)\n\\end{align}\\]trong đó \\(\\hat{f}\\) là ước lượng của hàm f, và \\(\\hat{Y}\\) thể hiện kết quả dự báo cho Y. Trong hệ thống này, \\(\\hat{f}\\) thường là dạng hộp đen (black box), có nghĩa là chúng ta thường không quan tâm đến việc hàm \\(\\hat{f}\\) có dạng chính xác như thế nào, mà chỉ quan tâm đến độ chính xác của hàm \\(\\hat{f}\\) khi dự đoán giá trị của Y.Một ví dụ cho việc sử dụng hàm f với biến dự báo là biến định lượng có thể được trình bày như sau. Giả dụ chúng ta đang có các dữ liệu về thông tin cá nhân của một khách hàng mua bảo hiểm của công ty bảo hiểm (thể hiện bằng các biến \\(X_1, X_2, ... X_p\\)), Y là số tiền mô hình dự đoán công ty bảo hiểm sẽ phải chi trả cho khách hàng đó trong tương lai. Việc dự đoán Y bằng việc sử dụng X là khá hợp lý, khi chúng ta có thể dự đoán số tiền bảo hiểm mà một khách hàng sẽ đòi công ty bảo hiểm trong tương lai bằng chính những thông tin cá nhân của người đó, giúp cho công ty bảo hiểm nắm được thông tin để chuẩn bị đủ tiền trả cho các khách hàng.Độ chính xác của \\(\\hat{Y}\\) trong việc dự báo Y dựa vào hai yếu tố: Đó là sai số có thể giảm (reducible error) và sai số không thể giảm (irreducible error). Cần phải hiểu rằng \\(\\hat{f}\\) không dự đoán một cách chính xác hoàn toàn giá trị của f, mà chúng luôn luôn tồn tại sai số trong đó.Sai số này có thể giảm thiểu được, bởi vì chúng ta có thể tăng sự chính xác của mô hình \\(\\hat{f}\\) bằng cách sử dụng những mô hình thống kê tiên tiến, phù hợp với dữ liệu để ước lượng hàm f. Tuy nhiên, nếu chúng ta có thể dự đoán đúng giá trị chính xác của f, nghĩa là ước lượng của chúng ta sẽ có dạng \\(\\hat{Y} = f(X)\\), thì chúng ta vẫn có một vài sai số trong đó. Bởi vì Y là hàm có chứa \\(\\epsilon\\), mà theo định nghĩa, ta không thể dự đoán \\(\\epsilon\\) theo X. Vậy nên, sự biến động liên quan tới \\(\\epsilon\\) cũng vẫn gây ảnh hưởng đến độ chính xác của dự báo. \\(\\epsilon\\) được biết đến như là sai số không thể giảm, bởi vì dù cho chúng ta có ước lượng hàm f tốt đến mức nào, ta vẫn không thể giảm sai số được thể hiện bởi \\(\\epsilon\\).Vậy tại sao sai số không thể giảm lại lớn hơn 0? Giá trị \\(\\epsilon\\) có thể chứa những thông tin chưa được thu thập, nhưng có giá trị trong việc dự đoán Y. Bởi vì chúng ta không có số liệu của những biến \\(X_{p+1}, X_{p+2},...\\) đó, hàm f không thể sử dụng chúng trong việc dự đoán Y. Trong trường hợp khác, ta không thể giảm \\(\\epsilon\\) bởi vì thực sự chúng có chứa những biến động không thể đo lường được. Ví dụ như khi ta xây mô hình để dự đoán rủi ro khi sử dụng một loại thuốc lên người bệnh nhân, thì rủi ro đó có thể thay đổi tuỳ vào chất lượng sản xuất của viên thuốc đó, hoặc đơn giản là thể trạng bệnh nhân vào ngày hôm đó, thường đó là những rủi ro mà ta gần như không thể xác định trước được.Cho một hàm ước lượng \\(\\hat{f}\\) và một bộ các biến dự báo (predictors), mà từ đó ta sẽ có được những giá trị dự báo \\(\\hat{Y} = \\hat{f}(X)\\). Giả sử rằng \\(\\hat{f}\\) và X đều giữ nguyên, và sai số chỉ đến từ \\(\\epsilon\\), ta có công thức:\\[\\begin{align}\nE(Y-\\hat{y})^2 &= E[f(X) + \\epsilon - \\hat{f}(X)]^2 \\\\\n&= \\underbrace{[f(X) - \\hat{f}(X)]^2}_{Sai \\ số \\ có \\ thể \\ giảm} \\quad + \\underbrace{Var(\\epsilon)}_{Sai \\ số \\ không \\ thể \\ giảm}\n\\label{eq:re_irre_error}\n\\end{align}\\]trong đó E(Y-\\(\\hat{y}\\))^2 thể hiện giá trị trung bình, hoặc giá trị kỳ vọng của giá trị bình phương của chênh lệch giữa giá trị dự đoán và giá trị thực tế. \\(Var(\\epsilon)\\) thể hiện phương sai của \\(\\epsilon\\).Quyển sách này sẽ tập trung vào các kỹ thuật thống kê giúp ước lượng hàm f với mục tiêu là giảm tối đa sai số có thể giảm. Tuy nhiên, chúng ta luôn cần chú ý rằng sai số không thể giảm sẽ vẫn luôn luôn tạo ra một giới hạn trên cho sự chính xác của dự báo. Và giới hạn trên này luôn luôn là một ẩn số trong thực tế.","code":""},{"path":"khdl-và-các-khái-niệm-cơ-bản.html","id":"suy-diễn","chapter":"Chương 2 KHDL và các khái niệm cơ bản","heading":"2.2.2 Suy diễn","text":"Ngoài mục đích xây dựng hàm f để dự đoán các giá trị của Y. Người ta còn muốn hiểu hơn về mối liên hệ giữa Y và \\(X_1, X_2, X_3,...,X_p\\). Trong trường hợp này, chúng ta vẫn sẽ phải ước lượng hàm f, tuy nhiên mục tiêu của chúng ta không phải là dự báo giá trị của Y. Bây giờ, hàm \\(\\hat{f}\\) không thể được coi là hộp đen (black box) nữa, mà chúng ta cần phải biết chính xác dạng của hàm \\(\\hat{f}\\) đó là gì. Khi thực hiện các phương pháp thống kê suy diễn, chúng ta thường tập trung vào trả lời các câu hỏi như sau:Biến độc lập (predictor) nào có mối quan hệ với biến phụ thuộc? Câu hỏi này thường được đặt ra trong trường hợp dữ liệu của chúng ta chỉ có một lượng nhỏ biến độc lập X có liên quan đến biến Y. Việc tìm ra một số lượng nhỏ các biến độc lập X liên hệ với biến Y giữa một số lượng biến độc lập X lớn hơn có rất nhiều tác dụng trong thực tế. Ví dụ như trong bài toán tính toán khả năng bị sốc phản vệ của bệnh nhân khi dùng thuốc, khi đó biến Y thể hiện khả năng bệnh nhân đó bị sốc phản vệ, các biến X chứa thông tin cá nhân của bệnh nhân như tuổi, cân nặng, tiền sử bệnh; trong trường hợp này nếu như chúng ta tìm ra được yếu tố (ở đây được thể hiện bởi biến X) nào có ảnh hưởng nhiều đến biến Y thì có thể các bác sĩ sẽ tập trung tìm ra cách cải thiện những yếu tố đó, giúp cho bệnh nhân giảm khả năng bị sốc phản vệ.Mối liên hệ giữa biến phụ thuộc và các biến độc lập là gì? Trong thực tế, một vài biến độc lập X sẽ có mối quan hệ đồng biến với biến Y, có nghĩa là nếu giá trị các biến độc lập đó càng lớn, thì giá trị của Y sẽ càng lớn. Một vài biến độc lập khác cũng có thể có mối quan hệ nghịch biến với Y. Phụ thuộc vào sự phức tạp của hàm f và của dữ liệu, mối liên hệ giữa biến phụ thuộc Y và một biến độc lập bất kỳ có thể cũng phụ thuộc vào giá trị của các biến độc lập khác.Liệu mối quan hệ giữa biến phụ thuộc Y và các biến độc lập có thể được biểu diễn dưới dạng tuyến tính, hay là còn ở các dạng khác phức tạp hơn? Xuyên suốt lịch sử của các mô hình thống kê, hầu hết các phương pháp ước lượng hàm \\(f\\) đều có dạng tuyến tính, đi kèm với đó là những giả thuyết nhất định về dữ liệu, mà đôi khi chúng rất phù hợp với thực tế dữ liệu. Tuy nhiên trong một vài trường hợp, những giả thuyết của mô hình dạng tuyến tính lại không còn phù hợp nữa, vì dữ liệu thực tế không tuân theo những giả thuyết đó mà nó lại phức tạp hơn. Khi ấy, mô hình tuyến tính có thể sẽ đưa ra những kết quả không chính xác về mối liên hệ giữa biến độc lập và biến phụ thuộc. Một vài mô hình khác với mô hình tuyến tính mà ta có thể tham khảo như: Mô hình logit, mô hình probit, mô hình hồi quy nghịch đảo, mô hình hồi quy mũ…Một ví dụ cho việc tạo mô hình thống kê suy diễn trong lĩnh vực kinh tế là mô hình giữa chi phí tiêu dùng (Consumption) và Mức thu nhập (Income), Số lượng người phụ thuộc. Theo suy nghĩ thông thường, những gia đình có mức thu nhập cao thường có xu hướng chi tiêu nhiều hơn mức bình thường. Tuy nhiên, hoàn toàn có khả năng những người đó sẽ chi tiêu tiết kiệm với mức thu nhập. Để có thể xác nhận thiên kiến này, người ta thường sẽ phải xây dựng những mô hình dựa trên dữ liệu. Ví dụ người ta nghĩ rằng khi mức thu nhập của một người tăng lên 10 triệu, người đó sẽ có xu hướng tiêu thêm 7 triệu nữa, và người ta thu thập thông tin về chi phí tiêu dùng, mức thu nhập và số lượng thành viên trong gia đình, kết quả của mô hình ra như sau:\\[\\begin{align}\nChi phí tiêu dùng = 2.5 + 2.3 * Mức thu nhập + 5 * Số lượng thành viên gia đình\n(Đơn vị của chi phí tiêu dùng và mức thu nhập là (triệu đồng))\n\\end{align}\\]Công thức bên trên có nghĩa là nếu mức thu nhập của một người tăng lên 10 triệu đồng, thì chi phí tiêu dùng của người đó sẽ tăng lên khoảng 2.3 triệu đồng. Mô hình thống kê này thể hiện sự khác biệt với quan điểm trước đó là chi phí tiêu dùng sẽ tăng lên 7 triệu nếu thu nhập tăng lên 10 triệu, vậy nên ta có thể thấy rằng mô hình toán học dựa trên dữ liệu, với định hướng thống kê suy diễn đã giúp chúng ta tránh những sai sót khi tìm cách áp dụng một quan điểm nào đó.","code":""},{"path":"khdl-và-các-khái-niệm-cơ-bản.html","id":"cách-ước-lượng-hàm-f","chapter":"Chương 2 KHDL và các khái niệm cơ bản","heading":"2.3 Cách ước lượng hàm f","text":"Trong quyển sách này, chúng ta sẽ được học những phương pháp tuyến tính và phi tuyến tính để ước lượng hàm f. Tuy các phương pháp này khác nhau, nhưng chúng đều có một vài điểm chung nhất định, và tôi sẽ trình bày các điểm chung đó trong phần này. Trước tiên, giả sử chúng ta có một bộ số liệu gồm n quan sát khác nhau. Các quan sát này được gọi là dữ liệu huấn luyện (training data) bởi vì chúng ta sẽ sử dụng những dữ liệu này cùng với các phương pháp thống kê để ước lượng hàm f. \nĐể thuận tiện, ta có thể coi \\(x_{ij}\\) là giá trị biến độc lập thứ j của quan sát thứ , với \\(= 1,2,...,n\\) và \\(j = 1,2,...,p\\). Tương tự, ta cũng coi \\(y_i\\) là biến phụ thuộc của quan sát thứ . Vậy nên, dữ liệu huấn luyện sẽ bao gồm \\({(x_1, y_1), (x_2, y_2), ... ,(x_n, y_n)}\\) với \\(x = (x_{i1}, x_{i2}, ... , x_{ip})^T\\).Mục tiêu cao nhất của húng ta là sử dụng các phương pháp thống kê với dữ liệu đào tạo để ước lượng hàm f, hay nói cách khác là tìm một hàm \\(\\hat{f}\\) sao cho \\(Y \\approx \\hat{f}(X)\\) với mọi quan sát (X,Y). Các phương pháp ước lượng với dữ liệu này, nói một cách tổng quát hơn, có thể chia ra làm hai loại là các phương pháp sử dụng tham số (parametric methods) và các phương pháp không sử dụng tham số (non-parametric methods).","code":""},{"path":"khdl-và-các-khái-niệm-cơ-bản.html","id":"ước-lượng-tham-số-parametric-methods","chapter":"Chương 2 KHDL và các khái niệm cơ bản","heading":"2.3.1 Ước lượng tham số (parametric methods)","text":"Phương pháp ước lượng tham số có thể được chia ra làm hai bước chính:Trước tiên, chúng ta phải đưa ra một giả thiết về dạng của hàm f. Ví dụ như hàm f là hàm tuyến tính, hoặc là hàm nghịch đảo, ở đây tác giả sẽ lấy ví dụ hàm f là hàm tuyến tính của X có dạng:\\[\\begin{align}\nf(X) = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + ... + \\beta_p X_p\n\\end{align}\\]Một khi chúng ta giả định rằng hàm f là hàm tuyến tính, thì vấn đề ước lượng hàm f lại đơn giản hơn rất nhiều. Đó là thay vì chúng ta phải ước lượng một hàm f(X) bất kỳ có p chiều, vốn sẽ làm chậm tốc độ tính toán của máy tính đi rất nhiều, thì chúng ta chỉ cần phải ước lượng p+1 tham số \\(\\beta_0, \\beta_1, \\beta_2, ... , \\beta_p\\).Sau khi chúng ta lựa chọn được kiểu hàm mà chúng ta muốn, chúng ta sẽ cần thực hiện thêm các quy trình kỹ thuật thống kê khác nhau với dữ liệu đào tạo để cho ra mô hình phù hợp với dữ liệu nhất.Hàm bên trên là hàm tuyến tính, một loại hàm vô cùng phổ biến và cơ bản mà hầu hết người xây dựng mô hình phải biết khi xây dựng hàm f. Người ta hướng đến việc tham số \\(\\beta_0, \\beta_1, \\beta_2, ... , \\beta_p\\) rất hiệu quả với mục tiêu là tìm ra các tham số sao cho:\\[\\begin{align}\nY \\approx \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + ... + \\beta_p X_p\n\\end{align}\\]Một khi chúng ta giả thiết rằng hàm f là hàm tuyến tính, việc ước lượng các tham số của hàm f khá là đơn giản và các nhà nghiên cứu đã chỉ ra rất nhiều phương pháp để ước lượng, chúng ta sẽ tìm hiểu kỹ hơn về cách ước lượng các tham số \\(\\beta\\) trong phần sau.Những phương pháp dựa vào việc giả thiệt mô hình trước khi thực hiện tính toán như vừa mô tả thường được gọi là phương pháp ước lượng có tham số. Phương pháp này giúp làm giảm độ phức tạp của bài toán bởi vì việc ước lượng một bộ tham số \\(\\beta_0, \\beta_1, \\beta_2, ... , \\beta_p\\) nhất định thường dễ dàng hơn là ước lượng một hàm f bất kỳ cho phù hợp với dữ liệu. Tuy nhiên, phương pháp ước lượng có tham số này vẫn có điểm hạn chế, đó là khi hàm số chúng ta giả thiết thường không trùng với hàm f thực tế (không bao giờ biết được hàm f thực tế là gì), được thể hiện qua các giá trị sai số lớn khi chúng ta thử mô hình giả thiệt trên các tập dữ liệu. Để hạn chế những sai số lớn này, người ta sẽ sử dụng những mô hình với ít điều kiện hơn là hàm tuyến tính (hàm tuyến tính là hàm yêu cầu nhiều giả thiết khá khó đạt được trong thực tế), từ đó có thể giảm thiểu sai lệch trong những ước lượng của mình. Những mô hình với ít điều kiện hơn thông thường sẽ phù hợp với nhiều dạng hàm f thực tế hơn. Tuy nhiên, nếu như chúng ta sử dụng những mô hình phức tạp, thường thì chúng ta sẽ phải mất nhiều thời gian và tài nguyên để ước lượng ra các tham số. Ngoài ra, ta cũng có thể gặp phải trường hợp quá khớp (overfitting), có nghĩa là mô hình cho ra sai số rất nhỏ trên tập dữ liệu đào tạo bởi vì mô hình đã tự điều chỉnh tham số sao cho khớp với cả nhiễu trong dữ liệu đào tạo, nhưng chúng lại cho ra sai số rất lớn trên tập dữ liệu kiểm thử, hoặc khi ứng dụng ngoài thực tế.","code":""},{"path":"khdl-và-các-khái-niệm-cơ-bản.html","id":"uớc-lượng-phi-tham-số-non---parametric-methods","chapter":"Chương 2 KHDL và các khái niệm cơ bản","heading":"2.4 Uớc lượng phi tham số (Non - parametric methods)","text":"Khác với phương pháp ước lượng tham số, phương pháp ước lượng phi tham số không đưa ra bất kỳ giả thiết nào về dạng hàm của f. Thay vào đó, phương pháp này sẽ đưa ra một giá trị ước lượng của f sao cho giá trị đó càng gần với các giá trị quan sát (hay còn gọi là các điểm dữ liệu) càng tốt, nhưng các giá trị ước lượng đó vẫn được tính toán sao cho chúng không quá khớp (overfitting) hay ít khớp (underfitting). Cách tiếp cận phi tham số này có lợi thế khá lớn ở chỗ chúng có thể khớp các dạng hàm khác nhau của hàm f trong thực tế một cách chính xác hơn với cách tiếp cận tham số, chúng không đặt ra trước bất kỳ giả thuyết nào về hàm f, nên có thể tránh việc các tính toán ước lượng của chúng ta bị sai lệch mô hình không phù hợp. Khi người ta sử dụng phương pháp phi tham số, có nghĩa là họ đã cho rằng dữ liệu của họ (thể hiện một phần hàm f thực tế) có nhiều khả năng sẽ không phù hợp với các mô hình thông thường đã được biết trước (tuyến tính, nghịch đảo, hồi quy mũ). Phương pháp ước lượng phi tham số hoàn toàn tránh được rủi ro ước lượng sai giả thiết về hàm f không chính xác, tuy nhiên chúng cũng có những hạn chế nhất định khiến người ta phải cân nhắc trước khi áp dụng vào thực tế. Hạn chế đó nằm ở khả năng tính toán, bởi vì các bài toán ước lượng hàm f theo phương pháp phi tham số không được chuyển thành bài toán ước lượng một dãy tham số, có nghĩa là phương pháp này phải sử dụng một lượng quan sát lớn hơn nhiều với phương pháp tham số để có thể ước lượng hàm f với độ chính xác cao.Thêm một ví dụ nói về việc overfitting khi sử dụng ước lượng phi tham sốTrên đây là những điểm mạnh, điểm yếu của hai phương pháp ước lượng có tham số và ước lượng phi tham số, chúng ta sẽ tìm hiểu sâu thêm về một vài điển hình của các phương pháp đó trong quyển sách này.","code":""},{"path":"khdl-và-các-khái-niệm-cơ-bản.html","id":"đánh-đổi-giữa-khả-năng-dự-báo-chính-xác-prediction-accuracy-và-khả-năng-diễn-giải-mô-hình-model-interpretability","chapter":"Chương 2 KHDL và các khái niệm cơ bản","heading":"2.4.1 Đánh đổi giữa khả năng dự báo chính xác (prediction accuracy) và khả năng diễn giải mô hình (model interpretability)","text":"Trong các phương pháp được tác giả sử dụng ở trong quyển sách này, có những phương pháp linh hoạt (flexible, định nghĩa flexible chưa được nhắc đến), nhưng cũng có phương pháp hạn chế (restrictive) hơn thông thường, có nghĩa là các mô hình sử dụng sẽ giả thiết một dạng hàm f nào đó linh hoạt, hoặc khắt khe hơn thông thường. Ví dụ, hàm tuyến tính là một dạng hàm khá đợn giản, dễ giải thích, còn những mô hình mạng neural (được sử dụng nhiều trong trí tuệ nhân tạo) lại rất phức tạp vì bản chất của mô hình mạng neural cho phép người sử dụng tạo ra một hàm f với các hình dạng linh hoạt hơn rất nhiều và khiến con người khó giải thích sự liên quan giữa các biến trong dữ liệu. Một câu hỏi thường được đặt ra là: Tại sao người ta lại phải sử dụng mô hình linh hoạt, có khả năng giải thích kém hơn thay vì những mô hình hạn chế, nhưng lại dễ giải thích hơn?. Lý là bởi vì, dựa trên thực nghiệm trên rất nhiều dữ liệu khác nhau, hàm tuyến tính lại thường đưa ra những dự đoán kém chính xác hơn với mô hình mạng neural, vậy nên đôi khi người ta phải đánh đổi giữa khả năng dự báo chính xác và khả năng diễn giải khi xây dựng mô hình dựa trên dữ liệu. Chúng ta cũng cần phải nhớ rằng, tuỳ thuộc vào mục đích xây dựng mô hình, người ta sẽ có những lựa chọn ưu tiên khả năng dự báo hoặc khả năng diễn giải. Đối với mục đích mô hình là suy diễn, tìm sự liên hệ giữa các biến, thì người ta sẽ lựa chọn phương pháp hạn chế để dễ diễn giải hơn. Ngược lại, nếu như ta sử dụng mô hình với mục đích dự đoán, thì người ta sẽ không quan tâm đến khả năng diễn giải nữa mà hoàn toàn có thể ưu tiên sử dụng mô hình linh hoạt hơn, với kỳ vọng mang lại dự đoán chính xác hơn.","code":""},{"path":"khdl-và-các-khái-niệm-cơ-bản.html","id":"phương-pháp-học-có-giám-sát-và-phương-pháp-học-không-giám-sát","chapter":"Chương 2 KHDL và các khái niệm cơ bản","heading":"2.4.2 Phương pháp học có giám sát và phương pháp học không giám sát","text":"Hầu hết các phương pháp học thống kê hiện nay đều chia ra làm hai loại là phương pháp học có giám sát và và phương pháp học không giám sát. Ta có thể miêu tả phương pháp học có giám sát như sau: Cho một dữ liệu, đối với mỗi quan sát chúng ta sẽ có các biến dự báo (predictors) \\(x_i\\), = 1,2,… và biến mục tiêu (response) y tương ứng với mỗi quan sát. Chúng ta muốn tìm một mô hình cho thấy sự liên quan giữa biến mục tiêu (response) và biến dự báo (predictors), với mục tiêu là đoán các giá trị tương lai (dự báo) hoặc để hiểu rõ hơn mối quan hệ giữa biến mục tiêu và biến dự báo (suy diễn). Ta có thể liệt kê ra rất nhiều phương pháp học có giám sát như: hồi quy tuyến tính, hồi quy logistic, GAM, boosting, Support Vector Machine (SVM).Còn đối với phương pháp học không giám sát, phương pháp này có chút khó khăn hơn với phương pháp học có giám sát bởi dữ liệu của chúng không hề chỉ rõ ra đâu là biến mục tiêu \\(y_i, = 1,2,...\\) mà chỉ có các biến \\(x_i\\). Trong trường hợp này, chúng ta không thể sử dụng các phép biến đổi mô hình tuyến tính như trên vì không xác định được biến mục tiêu. Nói một cách đơn giản, khi sử dụng phương pháp học không giám sát, chúng ta xây dựng các phương pháp thống kê mà không có sự rõ ràng từ đầu, giống như là chúng ta đi trong một khu rừng dữ liệu nhưng mà bị bịt mắt và không nhìn thấy đường đi. Câu hỏi đặt ra là: Những phương pháp phân tích thống kê nào là khả dĩ trong trường hợp này? Một hướng đi phổ biến đó là sử dụng dữ liệu để tìm ra mối liên hệ giữa các biến trong dữ liệu đó, và một phương pháp rất cơ bản mà chúng ta cần phải biết đó là phương pháp phân cụm (cluster analysis hoặc clustering). Mục tiêu của phương pháp phân cụm là phương pháp sử dụng thống kê để tăng sự chắc chắn khi phân nhóm các quan sát vào các nhóm khác nhau, dựa trên những biến và quan sát mà chúng ta đang có. Trong bài toán phân loại khách hàng, thông thường, chúng ta có một nhóm các khách hàng, và chúng ta biết được các thông tin cơ bản của các vị khách đó như tuổi, nghề nghiệp, lương, sở thích…, thì chúng ta có thể sử dụng phương pháp phân cụm để phân nhóm các khách hàng đó vào các loại khác nhau, chẳng hạn như Khách hàng giá trị cao, Khách hàng giá trị trung bình, Khách hàng giá trị thấp. Cần chú ý rằng, với phương pháp học không giám sát này, chúng ta đang giả định các khách hàng đó thuộc ba nhóm Khách hàng giá trị cao, Khách hàng giá trị trung bình, Khách hàng giá trị thấp bởi vì chúng ta không có thông tin chính xác về mức độ chi tiêu của các khách hàng này. Nếu chúng ta có những thông tin đó, chúng ta có thể sử dụng phương pháp học có giám sát với biến mục tiêu (response) là biến “Loại khách hàng”. Tuy nhiên, bởi vì những thông tin đó bị ẩn đi, nên chúng ta phải sử dụng phương pháp phân cụm để phân loại các nhóm. Việc phân loại này rất có ích bởi vì chúng ta có thể đưa ra những chiến lược kinh doanh khác nhau cho từng đối tượng, phục vụ cho mục đích của chúng ta, thường là để tối đa hoá lợi nhuận thu được.Phương pháp phân cụm có thể được thể hiện qua hình [Điền tên hình]. Như các bạn có thể thấy, chúng ta chỉ có thể biểu diễn phương pháp phân cụm với hai trục (2 biến), hoặc tối đa là ba trục (ba biến). Để thực hiện phương pháp phân cụm bằng mắt với một dữ liệu nhiều chiền hơn, ta không thể sử dụng phương pháp phân cụm bằng đồ thị. Trong đồ thị [Điền tên đồ thị], giả sử chúng ta có một tập dữ liệu có p biến, thì chúng ta sẽ cần vẽ \\(\\dfrac{p(p-1)}{2}\\) đồ thị để xác định các cụm bằng mắt. Tuy nhiên, con người thường không thể phân cụm được các quan sát chỉ bằng cách nhìn vào các đồ thị. Vậy nên, việc phát triển các phương pháp phân cụm tự động, tiên tiến hơn là điều rất cần thiết và quan trọng. Trong phần [Điền tên phần], quyển sách này sẽ giới thiệu kỹ hơn về các phương pháp phân cụm nói riêng, và các phương pháp học thống kê phi tham số nói chung.Rất nhiều bài toán ứng dụng thực tế rơi vào bối cảnh của thuật toán học có giám sát hoặc không giám sát. Tuy nhiên, có một vài trường hợp bài toán có thể không được phân loại rõ ràng vào loại học có giám sát hay không giám sát. Đơn giản như ta có một tập dữ liệu với n quan sát, với m (m < n) quan sát đầu tiên, chúng ta có cả biến mục tiêu (response) và biến dự báo (predictors), nhưng với n - m quan sát còn lại, chúng ta chỉ có các biến dự báo (predictors) mà không còn biến mục tiêu (response) nữa. Những dữ liệu như vậy có thể xuất hiện trong trường hợp chi phí để thu thập dữ liệu cho các biến dự báo khá rẻ, trong khi đó các biến mục tiêu để thu thập thì cần tốn kém hơn rất nhiều. Chúng ta có thể gọi trường hợp này là bài toán học bán giám sát (semi-supervised learning problem). Đối với trường hợp này, chúng ta sẽ cần một phương pháp xử lý dữ liệu thống kê hiệu quả cho cả m quan sát có biến mục tiêu và n - m quan sát không có biến mục tiêu, tuy nhiên trong quyển sách này chúng tôi sẽ không giới thiệu các phương pháp đó.","code":"\n# Load necessary libraries\nlibrary(MASS)## \n## Attaching package: 'MASS'## The following object is masked from 'package:dplyr':\n## \n##     select\nlibrary(ggplot2)\nlibrary(dplyr)\n\n# Generate synthetic data\nset.seed(123) # For reproducibility\n\n# Parameters for the clusters\nmeans <- list(\n  c(2, 3, 4, 5, 6, 7),   # Mean for cluster 1\n  c(5, 7, 6, 5, 4, 3),   # Mean for cluster 2\n  c(8, 9, 7, 6, 5, 4),   # Mean for cluster 3\n  c(3, 2, 1, 2, 3, 4)    # Mean for cluster 4\n)\n\nsigma <- matrix(c(1,0.5,0,0,0,0,  0.5,1,0.5,0,0,0,  0,0.5,1,0.5,0,0,  0,0,0.5,1,0.5,0,  0,0,0,0.5,1,0.5,  0,0,0,0,0.5,1), 6) # Covariance matrix\n\n# Generate observations\ndata_list <- lapply(means, function(mu) mvrnorm(n = 25, mu = mu, Sigma = sigma))\ndata <- do.call(rbind, data_list)\ndata <- as.data.frame(data)\ndata$cluster <- factor(rep(1:4, each = 25))\n\n# Plotting function\nplot_pair <- function(df, var1, var2) {\n  ggplot(df, aes_string(x = var1, y = var2, color = \"cluster\")) + \n    geom_point() + \n    theme_minimal() + \n    labs(x = var1, y = var2, title = paste(\"Scatter plot of\", var1, \"vs\", var2)) +\n    scale_color_manual(values=c(\"red\", \"green\", \"blue\", \"orange\")) +\n    theme(legend.title = element_blank())\n}\n\n# Create plots\nplot_list <- list()\nvariable_names <- names(data)[1:6]\n\nfor (i in 1:(length(variable_names)-1)) {\n  for (j in (i+1):length(variable_names)) {\n    plot_list[[paste(variable_names[i], variable_names[j], sep = \"_\")]] <- plot_pair(data, variable_names[i], variable_names[j])\n  }\n}## Warning: `aes_string()` was deprecated in ggplot2 3.0.0.\n## ℹ Please use tidy evaluation idioms with `aes()`.\n## ℹ See also `vignette(\"ggplot2-in-packages\")` for more information.\n## This warning is displayed once every 8 hours.\n## Call `lifecycle::last_lifecycle_warnings()` to see where this warning was\n## generated.\n# Display the first plot as an example\nprint(plot_list[[1]])\n# To view other plots, you can print them one by one:\n# print(plot_list[[\"V1_V2\"]]) # For example\n\nlibrary(gridExtra)## \n## Attaching package: 'gridExtra'## The following object is masked from 'package:dplyr':\n## \n##     combine\n# Assuming 'plot_list' contains all 15 ggplot objects from the previous example\n# Arrange plots in a 3x5 grid\ngrid_layout <- do.call(grid.arrange, c(plot_list, ncol=5, nrow=3))"},{"path":"khdl-và-các-khái-niệm-cơ-bản.html","id":"bài-toán-hồi-quy-và-bài-toán-phân-loại","chapter":"Chương 2 KHDL và các khái niệm cơ bản","heading":"2.4.3 Bài toán hồi quy và bài toán phân loại","text":"Các biến thường được chia ra làm hai loại rõ ràng là biến định lượng và biến định tính. Biến định lượng có thể liệt kê ra một vài ví dụ tiêu biểu như tuổi, thu nhập, số lượng nhà đang sở hữu…. Trong khi đó, biến định tính là các biến nhận một giá trị trong K loại khác nhau, ví dụ như tình trạng hôn nhân (Đã kết hôn/Chưa kết hôn), tình hình tín dụng (Đã vỡ nợ/Chưa vỡ nợ), tình hình bệnh tiểu đường (Không bị/Tuýp 1/Tuýp 2/Tuýp 3). Chúng ta thường phân các bài toán mà biến mục tiêu (response) thuộc loại biến định lượng là bài toán hồi quy, trong khi biến mục tiêu thuộc loại biến định tính thì gọi là bài toán phân loại. Dù trong nhiều trường hợp việc phân loại này hoạt động rất tốt, rất rõ ràng, nhưng đôi khi cũng tồn tại vài trường hợp mà các bài toán không được phân loại thực sự rõ ràng vào hai loại này. Các bài toán không được phân loại hẳn vào một loại ta có thể lấy một vài ví dụ như bài toán hồi quy logistic (chương 4) với biến mục tiêu là biến định tính, vốn luôn được coi là thuộc lớp bài toán phân loại, nhưng vì bản chất chúng tính toán xác suất mà biến mục tiêu rơi vào loại k (ví dụ như xác suất người bị tiểu đường bị tiểu đường Tuýp 2), nên chúng cũng có thể coi là bài toán hồi quy. Hay như các phương pháp như K - mean neighboring (KNN) hoặc Boosting đều cũng đều có thể sử dụng cho cả bài toán phân loại và bài toán hồi quy.Thông thường, chúng ta sẽ dựa vào các biến mục tiêu là biến định tính hay biến định lượng để xác định bài toán của chúng ta là bài toán phân loại hay bài toán hồi quy, còn những biến dự báo (predictors) thuộc loại gì thì thường không ảnh hưởng đến cách chúng ta phân loại bài toán. Thực tế, hầu hết những phương pháp thống kê trong quyển sách này đều sử dụng được bất kể biến dụ báo là biến định lượng hay biến định tính, nếu là biến định tính chúng ta thường có phương pháp mã hoá biến trước khi thực hiện xây dựng mô hình thống kê.","code":""},{"path":"khdl-và-các-khái-niệm-cơ-bản.html","id":"đánh-giá-sự-chính-xác-của-mô-hình","chapter":"Chương 2 KHDL và các khái niệm cơ bản","heading":"2.5 Đánh giá sự chính xác của mô hình","text":"Mục đích chính của quyển sách này là đưa người đọc đi sâu hơn vào thế giới của thống kê, nơi mà những phương pháp đã được xây dựng vượt xa hoàn toàn phương pháp hồi quy tuyến tính truyền thống. Tại sao chúng ta cần phải học nhiều phương pháp thống kê thay vì chỉ học một phương pháp thống kê tốt nhất? Bởi vì không bao giờ có gì là miễn phí hoàn toàn (free lunch) trong thống kê, có nghĩa là không có một mô hình nào vượt trội hoàn toàn với các mô hình khác trong mọi loại dữ liệu. Trong một bộ dữ liệu, có một phương pháp thống kê nào đó cho ra kết quả tốt nhất, nhưng cũng có thể sẽ có một phương pháp thống kê khác cho ra kết quả tốt hơn trên một bộ dữ liệu khác gần như tương tự. Vậy nên, một trong những công việc quan trọng của các nhà thống kê đó là chọn lựa phương pháp thống kê tốt nhất khi gặp phải một bộ dữ liệu bất kỳ, đây là một trong những công việc khó và phức tạp nhất trong thực tế. Trong phần này, chúng tôi sẽ thảo luân về một vài khái niệm sẽ xuất hiện khi chúng ta lựa chọn một mô hình thống kê nào đó phù hợp với dữ liệu của chúng ta, và chúng tôi cũng sẽ lồng ghép các khái niệm được thể hiện ở đây vào các bài toán thực tế trong phần sau của quyển sách này.","code":""},{"path":"khdl-và-các-khái-niệm-cơ-bản.html","id":"đo-độ-khớp-của-mô-hình","chapter":"Chương 2 KHDL và các khái niệm cơ bản","heading":"2.5.1 Đo độ khớp của mô hình","text":"Để đánh giá hiệu quả của một phương pháp thống kê trên một tập dữ liệu nhất định, chúng ta cần phải theo dõi xem các dự đoán của mô hình khớp với các giá trị dữ liệu quan sát được đến đâu. Từ đó, người ta đã nghĩ ra các phương pháp để lượng hoá sự gần sát của các dự đoán và các giá trị dữ liệu quan sát thực tế. Trong các bài toán hồi quy, giá trị đo thường được người ta sử dụng nhất là trung bình bình phương sai số (MSE), được cho bởi công thức\\[\\begin{align}\nMSE = \\dfrac{1}{n} \\sum\\limits_{= 1}^{n} \\left[y_i - \\hat{f}(x_i)\\right]^2\n\\tag{2.1}\n\\end{align}\\]trong đó \\(\\hat{f}(x_i)\\) là dự đoán của hàm đối với quan sát thứ . Giá trị trung bình bình phương sai số (MSE) sẽ càng trở nên nhỏ nếu như giá trị dự đoán càng gần với giá trị dữ liệu quan sát thực tế, và càng lớn nếu như có một vài quan sát có kết quả dự đoán cách xa với giá. trị dữ liệu quan sát thực tế. Phương trình (2.1) được tính dựa trên tập dữ liệu huấn luyện mà được sử dụng để tạo ra hàm khớp với mô hình. Vậy nên nó thường được gọi bằng cái tên chính xác hơn là trung bình bình phương sai số của tập dữ liệu huấn luyện (training MSE). Dù vậy, trong thực tế, chúng ta không quá quan tâm đến sự chính xác của mô hình trên dữ liệu huấn luyện mà câu hỏi quan trọng hơn là: Sự chính xác của mô hình khi chúng ta áp dụng nó vào các quan sát trong tập dữ liệu kiểm thử liệu có còn tốt hay không?\nChúng ta quan tâm đến các số liệu này bởi vì những người xây dựng mô hình để dự báo luôn muốn xây dựng một mô hình có khả năng dự đoán tương lai tốt nhất. Ta lấy ví dụ những chuyên gia muốn xây dựng một mô hình để dự đoán giá cổ phiếu trong tương lai, dựa vào dữ liệu giá cổ phiếu trong 6 tháng trước. Khi đó, sẽ không có nhiều ý nghĩa nếu mô hình cho ra kết quả dự đoán dữ liệu giá cổ phiếu 2 tuần trước gần với giá trị đã được ghi nhận, bởi vì đó là thông tin chúng ta và cả mô hình đều được biết rồi, mà quan trọng là mô hình đó phải dự báo được khá chính xác giá cổ phiếu trong tương lai (thông tin chưa ai biết) để các nhà đầu tư có thể đưa ra chiến lược phù hợp. Tương tự, đối với bài toán dự đoán khả năng bị tiểu đường của các bệnh nhân, chúng ta cũng thường chỉ quan tâm đến khả năng mô hình được xây dựng dự báo chính xác những bệnh nhân trong tương lai, chứ không phải là dự đoán xem những bệnh nhân trước đó bị tiểu đường loại nào. Thực tế, ta không hề có dữ liệu về những quan sát ở trong tương lai, nên để kiểm tra mô hình ta thường chia dữ liệu của chúng ta thành ít nhất hai tập dữ liệu huấn luyện và tập dữ liệu kiểm thử. Bởi vì khi tập dữ liệu kiểm thử không được đưa vào để huấn luyện mô hình, có nghĩa là những dữ liệu đó sẽ được coi như là dữ liệu mới chưa bao giờ gặp trong mô hình, nên nếu như mô hình có thể dự đoán với độ chính xác cao những quan sát trong tập dữ liệu kiểm thử thì người ta kỳ vọng rằng nó cũng sẽ chạy tốt khi áp dụng với những quan sát trong tương lai.Để diễn tả một cách toán học, giả sử chúng ta đang muốn khớp một phương pháp thống kê nào đó dựa trên tập dữ liệu huấn luyện \\({(x_1, y_1), (x_2, y_2),...,(x_n, y_n)}\\), kết quả chúng ta sẽ thu được một hàm \\(\\hat{f}\\). Từ đó chúng ta có thể tính các giá trị \\(\\hat{f}(x_1), \\hat{f}(x_2),..., \\hat{f}(x_n)\\). Nếu những giá trị đó mà gần với các giá trị \\(y_1, y_2,...,y_n\\) thì trung bình bình phương sai số của tập dữ liệu huấn luyện (training MSE) được cho bởi công thức (2.1) sẽ rất nhỏ. Nhưng chúng ta lại quan tâm hơn đến liệu giá trị \\(\\hat{f}(x_0)\\) có xấp xỉ bằng \\(y_0\\) hay không, với \\((x_0, y_0)\\) là một quan sát thuộc tập dữ liệu kiểm thử, chưa được đưa vào để tạo ra hàm \\(\\hat{f}\\). Các chuyên gia thường muốn lựa chọn được mô hình có trung bình bình phương sai số trên tập dữ liệu kiểm thử (test MSE) nhỏ nhất. Nói cách khác, nếu chúng ta có một lượng lớn các dữ liệu trong tập dữ liệu kiểm thử, chúng ta có thể tính\\[\\begin{align}\nAverage\\left[y_0 - \\hat{f}(x_0)\\right]^2\n\\end{align}\\]giá trị trung bình bình phương sai số trên tập dữ liệu kiểm thử với các quan sát \\((x_0, y_0)\\), và chúng ta thường lựa chọn mô hình sao cho giá trị này càng nhỏ càng tốt.","code":""},{"path":"khdl-và-các-khái-niệm-cơ-bản.html","id":"sự-đánh-đổi-giữa-độ-chệch-và-phương-sai","chapter":"Chương 2 KHDL và các khái niệm cơ bản","heading":"2.6 Sự đánh đổi giữa độ chệch và phương sai","text":"Dáng đường cong chữ U (U-shape) của đường sai số bình phương trung bình trên tập dữ liệu kiểm thử thể hiện hai tính chất rất quan trọng của một phươung pháp học thống kê hiện đại. Cần phải nhắc lại rằng trong quyển sách này chúng tôi sẽ không đưa ra những chứng minh toán học quá phức tạp, nhưng để thể hiện tính chất đánh đổi giữa độ chệch và phương sai này, chúng ta vẫn có thể đi từ một công thức đơn giản liên quan đến giá trị kỳ vọng của sai số bình phương trung bình của một giá trị \\(x_0\\). Chúng ta có thể tách giá trị kỳ đó ra thành ba phần chính: phương sai của \\(\\hat{f}(x_0)\\), độ chệch bình phương của \\(\\hat{f}(x_0)\\), và phương sai của sai số \\(\\epsilon\\) như sau:\\[\\begin{equation}\nE\\left(y_0 - \\hat{f}(x_0)\\right)^2 = Var\\left(\\hat{f}(x_0)\\right) + \\left[Bias(\\hat{f}(x_0))\\right]^2 + Var(\\epsilon)\n\\label{eq:bias_variance}\n\\end{equation}\\]Trong đó, \\(E\\left(y_0 - \\hat{f}(x_0)\\right)^2\\) được định nghĩa là giá trị kỳ vọng của trung bình bình phương sai số trên tập dữ liệu kiểm thử với quan sát \\(x_0\\). Nếu chúng ta lặp đi lặp lại quá trình ước lượng các hàm f này với một lượng lớn tập dữ liệu huấn luyện, và test mỗi mô hình với quan sát \\(x_0\\). Giá trị kỳ vọng tổng thể của trung bình bình phương sai số (overall expected test MSE) có thể được tính bằng cách lấy trung bình của \\(E\\left(y_0 - \\hat{f}(x_0)\\right)^2\\) với tất cả các quan sát \\(x_0\\) trong tập dữ liệu kiểm thử.Phương trình \\(\\ref{eq:bias_variance}\\) cho chúng ta biết rằng nếu muốn giảm giá trị sai số kỳ vọng trên tập dữ liệu kiểm thử, chúng ta cần chọn phương pháp học thống kê sao cho đồng thời đạt được cả hai mục tiêu là phương sai thấp và độ chệch thấp. Chúng ta cũng cần lưu ý rằng giá trị MSE kỳ vọng không bao giờ thấp hơn Var(\\(\\epsilon\\)), đó là phần sai số không thể giảm trong công thức \\(\\ref{eq:re_irre_error}\\).","code":""},{"path":"khdl-và-các-khái-niệm-cơ-bản.html","id":"phương-sai-của-mô-hình","chapter":"Chương 2 KHDL và các khái niệm cơ bản","heading":"2.6.1 Phương sai của mô hình","text":"Vậy thì rốt cuộc, chúng ta nên hiểu phương sai (variance) và độ chệch (bias) của phương pháp học thống kê như thế nào cho đúng? Phương sai (variance) nói đến lượng thay đổi của hàm \\(\\hat{f}\\) khi chúng ta ước lượng hàm \\(\\hat{f}\\) bằng một dữ liệu huấn luyện khác. Bởi vì dữ liệu huấn luyện được sử dụng để ước lượng hàm \\(\\hat{f}\\), nên việc chúng ta sử dụng dữ liệu huấn luyện khác nhau sẽ cho ra hàm \\(\\hat{f}\\) khác nhau. Tuy nhiên, trong trường hợp lý tưởng, ước lượng cho hàm f không nên chênh lệch quá nhiều giữa các dữ liệu huấn luyện khác nhau. Nếu một phương pháp học thống kê nào đó có sự thay đổi lớn về hàm \\(\\hat{f}\\) dù chúng ta chỉ thay đổi dữ liệu đầu vào một chút, có nghĩa là phương pháp học thống kê đó có phương sai cao (high variance). Nhìn chung, các phương pháp học thống kê càng linh hoạt (ví dụ như neural networks, decision tree, random forests) thường có phương sai càng cao.Ở đây tiếp tục simulate ra một dữ liệu nữa để thể hiện phương sai cao của phương pháp flexible","code":"\n# Define the number of variables\nnumber_of_variables <- 1:15\n\n# Train MSE decreases exponentially from 7 to below 0.3\ntrain_mse <- 7 * exp(-0.5 * number_of_variables)\n# Manually set the last value to ensure it's just below 0.3\ntrain_mse[length(train_mse)] <- 0.3\n\n# Test MSE: starts at 7.5, decreases to a point, then increases again\ntest_mse <- numeric(length(number_of_variables))\ntest_mse[1] <- 7.5 # Starting point for Test MSE\n\n# Calculate Test MSE values\nfor (i in 2:length(test_mse)) {\n  if (i <= 4) {\n    # Decreasing phase for the first 4 variables\n    test_mse[i] <- test_mse[i-1] - 0.5\n  } else {\n    # Increasing phase after the 4th variable\n    test_mse[i] <- 6 + 0.1 * (i - 4)^2\n  }\n}\n\n# Ensure Test MSE is always at least 2 units greater than Train MSE\ntest_mse <- pmax(test_mse, train_mse + 2)\n\n# Plotting\nplot(number_of_variables, train_mse, type='l', pch=19, col='blue', ylim=c(0, max(test_mse)), xlim = c(1,14), ylab='MSE', xlab='Number of Variables', main='Train vs Test MSE')\nlines(number_of_variables, test_mse, type='l', pch=19, col='red', lty=2)\n#points(number_of_variables, test_mse, pch=19, col='red') # Adding points for test MSE\n\nlegend(\"topright\", legend=c(\"Train MSE\", \"Test MSE\"), col=c(\"blue\", \"red\"), pch=19, lty=1:2)\n# Required libraries\nlibrary(caret)## Loading required package: lattice\nset.seed(42)\n\n# Simulating data with 15 independent variables\nn <- 200 # Number of observations\np <- 15 # Number of variables\n\n# Simulate relevant variables with stronger signal\nnum_relevant <- 5\nX_relevant <- matrix(rnorm(n * num_relevant), ncol = num_relevant)\nbeta_relevant <- runif(num_relevant, -2, 2) # Random coefficients for relevant variables\n\n# Simulate irrelevant variables with weaker signal (noise)\nnum_irrelevant <- p - num_relevant\nX_irrelevant <- matrix(rnorm(n * num_irrelevant), ncol = num_irrelevant)\nbeta_irrelevant <- runif(num_irrelevant, -0.1, 0.1) # Random coefficients for irrelevant variables\n\n# Combine relevant and irrelevant variables\nX <- cbind(X_relevant, X_irrelevant)\nbeta <- c(beta_relevant, beta_irrelevant)\n\n# Noise\nepsilon <- rnorm(n)\n\n# Response variable\nY <- X %*% beta + epsilon\n\n# Splitting the data into training and testing sets\nset.seed(42)\ntrainIndex <- createDataPartition(Y, p = .8, list = FALSE)\nX_train <- X[trainIndex, ]\nY_train <- Y[trainIndex]\nX_test <- X[-trainIndex, ]\nY_test <- Y[-trainIndex]\n\n# Initialize vectors to store metrics\ntrain_mse <- numeric(p)\ntest_mse <- numeric(p)\n\n# Fit models and calculate MSE for training and testing sets\nfor (i in 1:p) {\n  model <- lm(Y_train ~ X_train[, 1:i])\n  train_predictions <- predict(model, data.frame(X_train[, 1:i]))\n  test_predictions <- predict(model, data.frame(X_test[, 1:i]))\n  \n  # Calculating MSE\n  train_mse[i] <- mean((Y_train - train_predictions)^2)\n  test_mse[i] <- mean((Y_test - test_predictions)^2)\n}## Warning: 'newdata' had 40 rows but variables found have 160 rows\n\n## Warning: 'newdata' had 40 rows but variables found have 160 rows\n\n## Warning: 'newdata' had 40 rows but variables found have 160 rows\n\n## Warning: 'newdata' had 40 rows but variables found have 160 rows\n\n## Warning: 'newdata' had 40 rows but variables found have 160 rows\n\n## Warning: 'newdata' had 40 rows but variables found have 160 rows\n\n## Warning: 'newdata' had 40 rows but variables found have 160 rows\n\n## Warning: 'newdata' had 40 rows but variables found have 160 rows\n\n## Warning: 'newdata' had 40 rows but variables found have 160 rows\n\n## Warning: 'newdata' had 40 rows but variables found have 160 rows\n\n## Warning: 'newdata' had 40 rows but variables found have 160 rows\n\n## Warning: 'newdata' had 40 rows but variables found have 160 rows\n\n## Warning: 'newdata' had 40 rows but variables found have 160 rows\n\n## Warning: 'newdata' had 40 rows but variables found have 160 rows\n\n## Warning: 'newdata' had 40 rows but variables found have 160 rows\n# Plotting\nplot(1:p, train_mse, type = \"b\", pch = 19, col = \"blue\", xlab = \"Number of Variables\", ylab = \"MSE\", ylim = c(min(c(train_mse, test_mse)), max(c(train_mse, test_mse))), main = \"MSE, Bias, and Variance vs Model Complexity\")\npoints(1:p, test_mse, type = \"b\", pch = 23, col = \"red\", bg = \"red\")\nlegend(\"topright\", legend = c(\"Train MSE\", \"Test MSE\"), col = c(\"blue\", \"red\"), pch = c(19, 23))\nset.seed(42)\nlibrary(MASS) # For true randomness in simulation\n\n# Simulating data\nn <- 1000 # Number of observations\np <- 15 # Number of independent variables\nX <- matrix(rnorm(n * p), ncol = p)\ntrue_beta <- rnorm(p, mean = 0.2, sd = 0.5) # True coefficients\nY <- X %*% true_beta + rnorm(n) # Response with noise\n\n# Train-test split\ntrain_proportion <- 0.5\ntrain_sample <- sample(1:n, size = floor(n * train_proportion))\nX_train <- X[train_sample, ]\nY_train <- Y[train_sample]\nX_test <- X[-train_sample, ]\nY_test <- Y[-train_sample]\n\n# Calculate squared bias, variance, and MSE\ncalc_metrics <- function(i, X_train, Y_train, X_test, Y_test, true_beta) {\n  # Bootstrap predictions for variance estimation\n  n_bootstraps <- 100\n  n_test <- length(Y_test)\n  predictions_test <- matrix(nrow = n_bootstraps, ncol = n_test)\n  \n  for (b in 1:n_bootstraps) {\n    sample_indices <- sample(nrow(X_train), nrow(X_train), replace = TRUE)\n    X_train_b <- X_train[sample_indices, , drop = FALSE]\n    Y_train_b <- Y_train[sample_indices]\n    model_b <- lm(Y_train_b ~ X_train_b[, 1:i, drop = FALSE])\n    # Ensuring that newdata has the correct number of columns\n    predictions_test[b, ] <- predict(model_b, newdata = as.data.frame(X_test[, 1:i, drop = FALSE]))\n  }\n  \n  # Bias^2 = (E[Ŷ] - Y_true)^2 where Y_true is the true response\n  Y_true <- X_test[, 1:i, drop = FALSE] %*% true_beta[1:i]\n  expected_predictions <- rowMeans(predictions_test)\n  bias_squared <- mean((mean(predictions_test) - Y_true)^2, na.rm = TRUE)\n  \n  # Variance = E[(Ŷ - E[Ŷ])^2]\n  variance <- mean((predictions_test - mean(predictions_test, na.rm = TRUE)^2))\n  \n  # Test MSE\n  mse_test <- mean((expected_predictions - Y_test)^2, na.rm = TRUE)\n  \n  return(c(bias_squared, variance, mse_test))\n}\n\n# Initialize matrices to store metrics\nresults <- matrix(ncol = 3, nrow = p)\ncolnames(results) <- c(\"Bias^2\", \"Variance\", \"MSE\")\n\n# Calculate metrics for models with increasing complexity\nfor (i in 1:p) {\n  results[i, ] <- calc_metrics(i, X_train, Y_train, X_test, Y_test, true_beta)\n}\n\n# Adding the irreducible error to MSE\nirreducible_error <- var(Y - X %*% true_beta)\nresults[, \"MSE\"] <- results[, \"Bias^2\"] + results[, \"Variance\"] + irreducible_error## Warning in results[, \"Bias^2\"] + results[, \"Variance\"] + irreducible_error: Recycling array of length 1 in vector-array arithmetic is deprecated.\n##   Use c() or as.vector() instead.\n# Plotting the results\nplot(1:p, results[, \"Bias^2\"], type = \"l\", col = \"blue\", ylim = c(0, max(results)), ylab = \"Metric\", xlab = \"Number of Variables\", main = \"Model Complexity vs Error Components\")\nlines(1:p, results[, \"Variance\"], col = \"orange\")\nlines(1:p, results[, \"MSE\"], col = \"red\")\nabline(h = irreducible_error, lty = 2, col = \"black\")\nlegend(\"topright\", legend = c(\"Squared Bias\", \"Variance\", \"Test MSE\", \"Irreducible Error\"), col = c(\"blue\", \"orange\", \"red\", \"black\"), lty = c(1, 1, 1, 2))"},{"path":"khdl-và-các-khái-niệm-cơ-bản.html","id":"độ-chệch-của-mô-hình","chapter":"Chương 2 KHDL và các khái niệm cơ bản","heading":"2.6.2 Độ chệch của mô hình","text":"Ở một khía cạnh khác, độ chệch (bias) là từ được dùng để nhắc tới sai số được tạo ra khi ước lượng một vấn đề thực tế (có thể rất phức tạp), bằng một mô hình hàm \\(\\hat{f}\\) đơn giản. Ví dụ, mô hình hồi quy tuyến tính thường giả định rằng có mối liên hệ tuyến tính giữa \\(Y\\) và các giá trị \\(X_1, X_2, \\cdot, X_p\\). Tuy nhiên, hầu như không có hiện tượng nào trong thực tế đều có một mối quan hệ hồi quy tuyến tính hoàn toàn, vậy nên mô hình hồi quy tuyến tính sẽ luôn có độ chệch khi ước lượng hàm f. Nhìn chung, phương pháp càng linh hoạt (flexible) thường cho ra kết quả có độ chệch càng thấp.","code":""},{"path":"khdl-và-các-khái-niệm-cơ-bản.html","id":"sự-đánh-đổi-giữa-độ-chệch-và-phương-sai-1","chapter":"Chương 2 KHDL và các khái niệm cơ bản","heading":"2.6.3 Sự đánh đổi giữa độ chệch và phương sai","text":"Sự đánh đổi này xuất hiện như một quy luật mà chúng ta cần phải nhớ, rằng khi chúng ta sử dụng những mô hình linh hoạt hơn, thì phương sai (variance) sẽ tăng và độ chệch (bias) sẽ giảm. Sự thay đổi của hai giá trị liên quan tới nhau, và sẽ xác định giá trị MSE của tập dữ liệu kiểm thử tăng hay giảm. Khi chúng ta tăng sự phức tạp (flexibility) của mô hình, độ chệch (bias) có xu hướng giảm nhanh hơn là phương sai (variance), dẫn đến giá trị kỳ vọng của MSE trên tập dữ liệu kiểm thử giảm. Tuy nhiên, đến một thời điểm nào đó, việc tăng sự phức tạp (flexibility) của mô hình chỉ giảm độ chệch (bias) đi một chút, nhưng lại làm tăng phương sai (variance) nhanh hơn nhiều, và kết quả là giá trị kỳ vọng của MSE trên tập dữ liệu kiểm thử tăng.Mối liên hệ giữa độ chệch và phương sai khi thực hiện các mô hình học thống kê còn được gọi là sự đánh đổi giữa độ chệch và phương sai (bias-variance trade ). Thông thường, những phương pháp học thống kê cho ra hiệu quả cao trên tập dữ liệu kiểm thử thường là những phương pháp cho ra độ chệch thấp và phương sai thấp. Hiện tượng này được gọi là đánh đổi (trade ), bởi vì rất dễ để chúng ta có một mô hình với độ chệch thấp (low bias) và phương sai cao (high variance), hoặc một mô hình với độ chệch cao (high bias) và phương sai thấp (low variance). Vấn đề khó nhất là tìm ra phương pháp học thống kê nào mà cho ra được phương sai (variance) và độ chệch bình phương (squared bias) thấp nhất. Đó là mục tiêu chính của quyển sách này.Trong những vấn đề thực tế, chúng ta không bao giờ quan sát được hàm f thật, vậy nên là nhìn chung chúng ta không thể tính ra giá trị MSE của tập dữ liệu kiểm thử, độ chệch (bias) và phương sai (variance) của một phương pháp học thống kê nào đó. Tuy nhiên, bạn đọc hãy luôn phải nhớ rằng có sự đánh đổi giữa độ chệch và phương sai (bias-variance trade ) trong các phương pháp thống kê. Trong quyển sách này, chúng tôi sẽ giới thiệucác phương pháp thống kê rất phức tạp (flexible) và có thể giúp chúng ta giảm rất nhiều độ chệch (bias). Tuy nhiên, điều đó, không có nghĩa rằng những phương pháp của chúng tôi sẽ tốt hơn những phương pháp đơn giản (hồi quy tuyến tính chẳng hạn). Một ví dụ đơn giản có thể chỉ ra như …Trong phần tiếp theo, tác giả sẽ giới thiệu về phương pháp cross-validation, là một cách để ước lượng giá trị MSE của tập dữ liệu kiểm thử trên tập dữ liệu huấn luyện, đây là một phương pháp rất hứa hẹn để xử lý các vấn đề liên quan đến sự đánh đổi giữa độ chệch (bias) và phương sai (variance).Tạo ra một tập dữ liệu nữa illustrate cái sự tăng và giảm MSE theo Bias và Variance này","code":""},{"path":"khdl-và-các-khái-niệm-cơ-bản.html","id":"sự-đánh-đổi-giữa-độ-chệch-và-phương-sai-của-phương-pháp-k-fold-cross-validation","chapter":"Chương 2 KHDL và các khái niệm cơ bản","heading":"2.6.4 Sự đánh đổi giữa độ chệch và phương sai của phương pháp k-fold cross validation","text":"Ở phần bên trên, chúng ta đã nói về việc phương pháp k-fold CV có lợi thế hơn phương pháp LOOCV về tốc độ tính toán số phép tính mà phương pháp k-fold CV phải thực hiện là ít hơn. Ngoài lợi thế về tốc độ tính toán, phương pháp k-fold CV còn được ưa chuộng hơn bởi phương pháp này cho ra kết quả ước lượng sai số trên tập dữ liệu kiểm thử chính xác hơn phương pháp LOOCV. Đây là một ví dụ điển hình cho hiện tượng đánh đổi giữa độ chệch và phương sai của mô hình.Như chúng ta đã nhắc đến ở phần trước (viết trang 205 nhưng trong quyển sách này chưa nhắc đến cái gì ở trước), phương pháp sử dụng tập dữ liệu xác thực (validation set approach) có xu hướng overestimate sai số trên tập dữ liệu kiểm thử, nghĩa là độ chệch sẽ tương đối lớn, bởi vì phương pháp này chỉ sử dụng một nửa lượng dữ liệu để ước lượng. Cùng với một logic đó, không khó để chúng ta nhận ra rằng phương pháp LOOCV sẽ cho ra một ước lượng gần như không chệch (approximately unbiased estimates) của sai số trên tập dữ liệu kiểm thử, bởi vì phương pháp LOOCV sử dụng n - 1 trên n quan sát trong dữ liệu, nghĩa là gần như toàn bộ dữ liệu chúng ta có. Khi chúng ta thực hiện phương pháp k-fold CV, giả sử với trường hợp k = 5 và k = 10, kết quả cho ra một mô hình với độ chệch nằm giữa hai phương pháp vừa liệt kê bên trên, phương pháp này chỉ sử dụng \\(\\frac{(k-1)n}{k}\\) quan sát cho tập dữ liệu huấn luyện, ít hơn phương pháp LOOCV nhưng nhiều hơn phương pháp sử dụng tập dữ liệu xác thực (validation set approach). Vậy nên, từ góc nhìn của các chuyên gia, nếu mục tiêu là làm giảm độ chệch, rõ ràng phương pháp LOOCV là phương pháp vượt trội hơn hẳn với phương pháp k-fold CV.Tuy nhiên, chúng ta đều biết rằng độ chệch không phải là mối quan tâm duy nhất trong quá trình chúng ta ước lượng mô hình; chúng ta cũng cần phải quan tâm đến cả phương sai của mô hình nữa. Những nghiên cứu cho thấy rằng phương pháp LOOCV có phương sai cao hơn phương pháp k-fold CV với k<n. Tại sao lại như vậy? Khi chúng ta thực hiện phương pháp LOOCV, chúng ta thực ra đang tính trung bình sai số từ n mô hình ước lượng từ dữ liệu, và mỗi mô hình được huấn luyện trên một tập dữ liệu gần như y hệt nhau; điều này tạo ra những kết quả sai số có tương quan dương (highly positively correlated) lớn. Ở chiều ngược lại, khi chúng ta sử dụng phương pháp k-fold CV với k<n, chúng ta sẽ tính trung bình sai số của k mô hình ước lượng từ dữ liệu, nhưng các sai số này sẽ tương quan với nhau ít hơn, các tập dữ liệu huấn luyện của k mô hình này ít trùng nhau hơn. trung bình của các phần tử có tương quan lớn sẽ có phương sai lớn hơn trung bình của các phần tử có tương quan nhỏ hơn, nên giá trị sai số trên tập dữ liệu kiểm thử từ phương pháp LOOCV sẽ có xu hướng cho ra phương sai lớn hơn là với sai số trên tập dữ liệu kiểm thử từ phương pháp k-fold CV.Tóm lại, chúng ta có sự đánh đổi giữa độ chệch và phương sai liên quan tới sự lựa chọn k trong k-fold CV. Từ những kết quả thực nghiệm trên rất nhiều dữ liệu, thường các chuyên gia xây dựng mô hình sẽ chọn thực hiện phương pháp k-fold CV với k = 5 hoặc k = 10, để tránh trường hợp sai số trên tập dữ liệu kiểm thử gặp phải tình trạng độ chệch quá cao hoặc phương sai quá cao.","code":""},{"path":"khdl-và-các-khái-niệm-cơ-bản.html","id":"lớp-bài-toán-phân-loại","chapter":"Chương 2 KHDL và các khái niệm cơ bản","heading":"2.6.4.1 Lớp bài toán phân loại","text":"Từ đầu quyển sách, chúng ta đã thảo luận về độ chính xác của mô hình trong lớp bài toán mô hình hồi quy. Tuy nhiên, rất nhiều thuật ngữ, phương pháp vừa được giới thiệu bên trên, ví dụ như sự đánh đổi giữa độ chệch và phương sai, cũng có thể được áp dụng cho lớp bài toán phân loại. Lớp bài toán phân loại có một sự khác biệt cơ bản với lớp bài toán hồi quy đó là biến phụ thuộc \\(y_i\\) của bài toán phân loại thường là biến định tính, còn biến \\(y_i\\) của lớp bài toán hồi quy là biến định lượng. Trong bài toán phân loại, giả sử chúng ta phải tìm hàm f dựa trên các quan sát \\({(x_1, y_1),...,(x_n, y_n)}\\), với \\(y_1,...,y_n\\) là giá trị định tính. Phương pháp đơn giản nhất để để đánh giá độ chính xác của hàm ước lượng \\(\\hat{f}\\) là sử dụng tỷ lệ sai sót trên tập dữ liệu huấn luyện, được tính bằng công thức:\\[\\begin{align*}\n\\text{Training Error Rate} = \\dfrac{1}{n} \\sum\\limits_{= 1}^{n} (y_i \\neq \\hat{y_i})\n\\tag{2.2}\n\\end{align*}\\]Với \\(\\hat{y_i}\\) là kết quả phân loại quan sát sử dụng hàm \\(\\hat{f}\\), và \\((y_i \\neq \\hat{y_i})\\) là biến phân loại nhận giá trị 1 nếu \\(y_i \\neq \\hat{y_i}\\) và 0 nếu \\(y_i = \\hat{y_i}\\). Nếu ($y_i ) = 0, có nghĩa là hàm \\(\\hat{f}\\) đã phân loại chính xác quan sát thứ , và bằng 1 nếu hàm \\(\\hat{f}\\) phân loại sai quan sát thứu . Công thức (2.2) cho\nthấy tỷ lệ sai sót trên tập dữ liệu huấn luyện bằng số quan sát bị phân loại sai chia cho tổng các quan sát được phân loại.Biểu thức (2.2) được gọi là tỷ lệ sai sót trên mô hình huấn luyện bởi vì giá trị sai số được tính trên dữ liệu được sử dụng để huấn luyện hàm \\(\\hat{f}\\). Tương tự như lớp bài toán hồi quy, trong lớp bài toán phân loại chúng ta cũng quan tâm cả tỷ lệ những quan sát trên tập dữ liệu kiểm thử mà mô hình dự đoán sai. Tỷ lệ sai sót trên tập dữ liệu kiểm thử với các quan sát có dạng {\\((x_i, y_i\\)} (chú ý những dữ liệu trong tập dữ liệu kiểm thử phải khác tập dữ liệu huấn luyện) được cho bởi công thức:\\[\\begin{align*}\n\\text{Test Error Rate} = \\dfrac{1}{m} \\sum\\limits_{= 1}^{m} (y_i \\neq \\hat{y_i})\n\\tag{2.3}\n\\end{align*}\\]Trong đó, \\(\\hat{y_m}\\) là giá trị phân loại được gán cho quan sát thứ m, với các biến dự báo (predictors) \\(x_m\\), được xác định bởi hàm f. Một hàm phân loại tốt là hàm cho giá trị biểu thức (2.3) nhỏ nhất.","code":""},{"path":"khdl-và-các-khái-niệm-cơ-bản.html","id":"phân-loại-bayes","chapter":"Chương 2 KHDL và các khái niệm cơ bản","heading":"2.6.4.2 Phân loại Bayes","text":"Ta có thể dễ dàng chứng minh (nhưng việc chứng minh nằm ngoài phạm vi của quyển sách này) rằng tỷ lệ sai sót trên tập dữ liệu kiểm thử trong công thức (2.3) là nhỏ nhất, bởi một hàm phân loại khá đơn giản với ý tưởng là gán cho mỗi quan sát giá trị có khả năng đúng nhất, dựa trên những biến dự báo (predictors) của quan sát đó. Nói cách khác, chúng ta sẽ gán cho quan sát \\((x_m, y_m)\\) trên dữ liệu kiểm thử với biến dự báo (predictors) \\(x_m\\) giá trị phân loại j nếu:\\[\\begin{align*}\nP(y_m = j|x_m)\n\\tag{2.4}\n\\end{align*}\\]là lớn nhất. Công thức (2.4) là công thức xác suất có điều kiện, có nghĩa là xác suất mà \\(y_m\\) được phân loại thành loại j, với điều kiện các biến dự báo (predictors) là \\(x_m\\). Phương pháp phân loại này được gọi là phương pháp phân loại Bayes (Bayes classifier).Phương pháp phân loại Bayes cho ra một trong những tỷ lệ sai sót trên tập dữ liệu kiểm thử thấp nhất có thể, được gọi là Bayes error rate. Thực tế, hàm phân loại Bayes luôn luôn chọn giá trị phân loại sao cho (2.4) là lớn nhất, tỷ lệ sai sót sẽ là \\(1 - max_{j}Pr(y_m = j|X = x_m)\\) tại \\(X = x_m\\). Tổng quát hơn, ta có Bayes error rate được cho bởi công thức:\\[\\begin{align*}\n1 - E(\\underbrace{max}_{j} Pr(Y = j|X))\n\\tag{2.5}\n\\end{align*}\\]với ký hiệu E() thể hiện giá trị kỳ vọng trung bình của xác suất trên tất cả các giá trị có thể của X. Giá trị Bayes error rate này thường luôn lớn hơn 0, và chúng có giá trị khá tương đồng với sai số không thể giảm.","code":""},{"path":"khdl-và-các-khái-niệm-cơ-bản.html","id":"k-nearest-neighbors","chapter":"Chương 2 KHDL và các khái niệm cơ bản","heading":"2.6.4.2.1 K-Nearest Neighbors","text":"Trên lý thuyết, chúng ta luôn muốn dự đoán biến định tính sử dụng phương pháp Bayes. Tuy nhiên trong thực tế, dữ liệu của chúng ta không cho chúng ta biết phân phối của Y với điều kiện X, vậy nên chúng ta không thể tính toán hàm phân loại Bayes. Vậy nên, trong thế giới thống kê, hàm phân loại Bayes đóng vai trò như một tiêu chuẩn vàng không thể đạt tới, và người ta sẽ sử dụng phương pháp phân loại Bayes để sánh với các phương pháp khác. Rất nhiều phương pháp đã được sử dụng để ước lượng \\(P(Y|X)\\) và sau đó cho gán cho quan sát phân loại được dự đoán có xác suất đúng cao nhất. Một trong những phương pháp đó là, phương pháp phân loại K-nearest neighbors (KNN). Phương pháp này được trình bày như sau: Cho một số dương K và một quan sát \\(x_0\\), hàm KNN trước tiên xác định K điểm trong trong tập dữ liệu huấn luyện gần với quan sát \\(x_0\\) nhất, gọi các điểm đó là \\(N_0\\). Tiếp đó, hàm KNN ước lượng giá trị xác suất có điều kiện của loại j bằng công thức sau:\\[\\begin{align*}\nP(Y = j|X = x_0) = \\dfrac{1}{K} \\sum\\limits_{\\N_0} (y_i = j)\n\\tag{2.6}\n\\end{align*}\\]Ta có thể thấy rằng, xác suất có điều kiện của loại j trong công thức (2.6) được tính bằng cách lấy số điểm trong \\(N_0\\) chia cho tổng số điểm trong dữ liệu, và sau đó hàm KNN phân loại quan sát \\(x_0\\) thành loại nhận giá trị xác suất lớn nhất trong công thức (2.6). Dù cho KNN là một phương pháp khá đơn giản, nhưng chúng lại thể hiện kết quả gần giống với phương pháp Bayes classifier một cách bất ngờ.Tương tự với lớp bài toán hồi quy, trong lớp bài toán phân loại, chúng ta cũng không có mối liên hệ mạnh mẽ giữa sai số trên tập dữ liệu huấn luyện và sai số trên tập dữ liệu kiểm thử. Ví dụ với phương pháp KNN, nếu chúng ta cho K bằng 1, thì sai số trên tập dữ liệu huấn luyện sẽ bằng 0, nhưng sai số trên tập dữ liệu kiểm thử vẫn cao. Nhìn chung, nếu chúng ta sử dụng phương pháp phân loại linh hoạt (flexible) hơn, thì có khả năng sai số trên tập dữ liệu huấn luyện sẽ giảm, nhưng sai số trên tập dữ liệu kiểm thử vẫn tăng. Các phương pháp phân loại cũng gặp vấn đề về hiện tượng quá khớp và chưa khớp tương tự như các phương pháp hồi quy.Tóm lại, trong cả lớp bài toán hồi quy và lớp bài toán phân loại, việc lựa chọn mức độ linh hoạt (flexibility) chính xác có vai trò tối quan trọng đến sự thành công của phương pháp học thống kê của chúng ta. Hiện tượng đánh đổi giữa độ chệch và phương sai khiến cho việc lựa chọn mức độ linh hoạt của mô hình trở nên khó khăn, tuy nhiên những phương pháp sử dụng dữ liệu xác thực và\nđặc biệt là k-fold cross validation đã giải quyết phần nào vấn đề này.","code":""},{"path":"khdl-và-các-khái-niệm-cơ-bản.html","id":"k-fold-cross-validation","chapter":"Chương 2 KHDL và các khái niệm cơ bản","heading":"2.7 K-fold cross validation","text":"Ta lại đặt ra thêm một câu hỏi nữa, nếu như chúng ta không có dữ liệu kiểm thử thì chúng ta sẽ xây dựng mô hình như thế nào? Câu trả lời là sử dụng chính dữ liệu đào tạo để kiểm tra sự chính xác của mô hình. Tuy nhiên, chúng ta sẽ không sử dụng toàn bộ tập dữ liệu kiểm thử, mà chúng ta sẽ chia tập dữ liệu huấn luyện thành k phần khác nhau. Sau đó, chúng ta sẽ xây dựng mô hình trên k-1 tập dữ liệu huấn luyện con, và sử dụng tập dữ liệu huấn luyện con còn lại (thường gọi là tập dữ liệu xác thực) để kiểm tra sự chính xác của mô hình được xây dựng trên k-1 tập dữ liệu huấn luyện kia. Quá trình này được lặp lại k lần, để bất kỳ phần nào của tập dữ liệu huấn luyện cũng đều được làm tập dữ liệu xác thực. Cuối cùng, chúng ta tính trung bình của sai số trên toàn bộ các tập dữ liệu xác thực và sánh. Mô hình nào cho giá trị trung bình sai số trên các tập xác thực khác nhau này nhỏ nhất thì sẽ là mô hình được chúng ta chọn. Phương pháp này thường được gọi là k-fold cross validation.Diễn giải một cách toán học, giả sử thước đo cho sai số của chúng ta là MSE, và chúng ta có k ước lượng của sai số trên tập dữ liệu xác thực, lần lượt là \\(MSE_1, MSE_2, MSE_3, ... , MSE_k\\). Khi đó ước lượng giá trị k-fold cross validation sẽ được tính bằng công thức:\\[\\begin{align}\nCV_{(k)} = \\dfrac{1}{k} \\sum\\limits_{=1}^{k} MSE_i\n\\end{align}\\]Khi ta để giá trị k bằng với số các quan sát trong dữ liệu, khi đó ta sẽ có n tập dữ liệu xác thực, mỗi tập có duy nhất một quan sát. Trường hợp này gọi là Leave-one-cross validation (LOOCV) và công thức ước lượng giá trị LOOCV trong trường hợp này là\\[\\begin{align}\nCV_{(n)} = \\dfrac{1}{n} \\sum\\limits_{=1}^{n} MSE_i\n\\end{align}\\]Hiện giờ quyển sách này đang giới thiệu hai phương pháp để tính toán giá trị cross-validation, vậy ưu và nhược điểm của hai phương pháp này là gì, và chúng ta sử dụng hai phương pháp này như thế nào trong phù hợp? Trong thực tế, để có thể tính toán ra được những con số này yêu cầu chúng ta phải thực hiện bằng máy tính, và đối với những bộ dữ liệu lớn thì tốc độ tính toán là hết sức quan trọng. Phương pháp LOOCV yêu cầu thuật mô hình thống kê của chúng ta phải chạy n lần, và điều này khả năng gây ra gánh nặng rất lớn về mặt tính toán (trừ trường hợp ta sử dụng mô hình ước lượng bình phương tối thiểu, có công thức tính nhanh chính xác là \\(CV_{(n)} = \\dfrac{1}{n} \\sum\\limits_{=1}^{n} \\left(\\dfrac{y_i - \\hat{y_i}}{1 - h_i}\\right)^2\\).\nPhương pháp cross-validation là phương pháp được áp dụng rất rộng rãi, phù hợp hầu như với bất kỳ các mô hình thống kê đã biết. Trong những phương pháp đó, có những phương pháp yêu cầu cách tính toán rất phức tạp chứ không đơn giản như phương pháp ước lượng bình phương tối thiểu, và nếu như sử dụng LOOCV thì máy tính sẽ xử lý mất thời gian hơn rất nhiều, đặc biệt là nếu số lượng quan sát trong dữ liệu lớn. Ngược lại, việc thực hiện k-fold cross validation (thường là với k = 8 hoặc k = 10) thì quá trình xây dựng mô hình thống kê sẽ chỉ phải thực hiện 8 - 10 lần, điều này là khả thi hơn nếu lương dữ liệu được sử dụng lớn.Ngoài lợi thế về mặt tính toán, phương pháp k-fold cross validation còn có một ưu điêm vượt trội nữa với các phương pháp khác, nằm ở sự đánh đổi giữa bias và variance (bias-variance trade ). Phương pháp k-fold cross validation sẽ cho ra kết quả ước lượng của sai số trên tập dữ liệu kiểm thử chính xác hơn với phương pháp LOOCV. Chúng ta đã từng nhắc đến việc phương pháp chọn mô hình dựa trên tập dữ liệu xác thực (validation test) có thể overestimate sai số trên tập dữ liệu kiểm thử, khi mà phương pháp này chỉ sử dụng lượng dữ liệu huấn luyện bằng khoảng một nửa tổng dữ liệu chúng ta có. Với logic này, rất đơn giản để chúng ta nhận ra rằng phương pháp LOOCV sẽ cho ra ước lượng gần như không chệch (approximately unbiased estimates) của tập dữ liệu kiểm thử, khi mỗi tập dữ liệu huấn luyện có tới n - 1 quan sát (tổng số quan sát trong dữ liệu là n). Tương tự, khi chúng ta chạy mô hình và sử dụng phương pháp k-fold cross validation (với các giá trị k khá nhau, có thể là k = 5 hoặc k = 10), chúng ta sẽ có được mức độ chệch vừa phải đối với sai số trên tập dữ liệu kiểm thử, vì khi đó mỗi tập dữ liệu huấn luyện sẽ có khoảng \\(\\dfrac{(k-1)n}{k}\\) quan sát, ít hơn với phương pháp LOOCV nhưng lại nhiều hơn phương pháp chỉ sử dụng một dữ liệu xác thực. Vậy nên, đứng trên góc độ của một người ưu tiên giảm độ chệch của ước lượng, rõ ràng phương pháp LOOCV được ưa chuộng hơn só với phương pháp k-fold cross validation.Tuy nhiên, giảm độ chệch của ước lượng không phải là tất cả những gì chúng ta quan tâm khi ước lượng một mô hình. Một yếu tố rất quan trọng nữa mà chúng ta phải quan tâm đó là phương sai (variance) của mô hình. Nếu như sánh về phương sai, phương pháp LOOCV hay k-fold cross validation sẽ cho ra kết quả tốt hơn? Câu trả lời là phương pháp k-fold cross validation (với điều kiện k<n). Lý là vì khi chúng ta thực hiện phương pháp LOOCV, thực tế chúng ta đang tính trung bình các kết quả của n mô hình ước lượng, nhưng mỗi mô hình đều được huấn luyện dựa trên một tập dữ liệu gần như giống hệt nhau (chỉ khác nhau nhiều nhất là một quan sát); vậy nên, kết quả ước lượng của các mô hình thường có mối tương quan dương rất cao. Ngược lại, phương pháp k-fold cross validation (với điều kiện k < n) sẽ cho ra giá trị trung bình của kết quả của k mô hình ước lượng, và mức độ tương quan của các kết quả này sẽ ít hơn với phương pháp LOOCV, vì số lượng các quan sát trùng nhau ở tập dữ liệu huấn luyện là nhỏ hơn. Bởi vì khi sánh các chỉ số, thì giá trị trung bình của các chỉ số có mối tương quan cao sẽ luôn có phương sai cao hơn giá trị trung bình của các chỉ số có mối tương quan thấp, vậy nên là ước lượng sai số trên tập dữ liệu kiểm thử của phương pháp LOOCV có phương sai cao hơn với phương pháp k-fold cross validation.Tóm lại, phương pháp k-fold cross validation sẽ thể hiện được sự cân bằng tốt giữa độ chệch và phương sai, nhưng sự cân bằng này liên quan đến việc lựa chọn tham số k. Theo như các kết quả thưucj nghiệm cho thấy, khi chúng ta chọn giá trị k = 5 hoặc k = 10, thì phương pháp k-fold cross validation sẽ cho ra được ước lượng sai số trên tập dữ liệu kiểm thử không gặp phải tình trạng độ chệch hoặc phương sai quá cao.\n(đang viết đến trang 205). Chúng ta sẽ sử dụng một dữ liệu được sinh ngẫu nhiên để thể hiện sự vượt trội của phương pháp k -fold cross-validation. Lý là bởi vì khi chúng ta sử dụng dữ liệu thực tế, chúng ta sẽ không biết giá trị MSE thật của tập dữ liệu kiểm thử là bao nhiêu, và điều đó sẽ gây khó khăn nếu như chúng ta muốn đánh giá độ chính xác ước lượng cross-validation của mô hình. Còn nếu như chúng ta sử dụng dữ liệu sinh ngẫu nhiên, chúng ta có thể tính được giá trị MSE thật của tập dữ liệu kiểm thử.need generate simulation data shows k-fold cross validation better Leave-one--cross-validationKhi chúng ta xây dựng mô hình với sai số nhỏ nhất tính theo cross validation, mục tiêu của chúng ta là xác định xem phương pháp thống kê mà ta lựa chọn được kỳ vọng thể hiện tốt như thế nào trên các dữ liệu mới được đưa vào; trong trường hợp này, chúng ta quan tâm đến ước lượng thực tế (actual estimate) của trung bình bình phương sai số trên tập dữ liệu kiểm thử. Tuy nhiên, trong một vài trường hợp khác, có thể chúng ta chỉ đơn thuần quan tâm đến vị trí của điểm cực tiểu trên đường ước lượng trung bình bình phương sai số trên tập dữ liệu kiểm thử. Lý là bởi vì chúng ta đang thực hiện cross-validation với một vài phương pháp thống kê khác nhau, hoặc là với một phương pháp thống kê nhưng có các mức độ linh hoạt (flexibility) khác nhau để tìm ra phương pháp có sai số trên dữ liệu kiểm thử thấp nhất trong các phương pháp được thử. Với mục đích này, vị trí trên đường sai số bình phương trung bình mang lại giá trị sai số bình phương trung bình nhỏ nhất được quan tâm hơn là giá trị của sai số bình phương trung bình đó.Lấy ví dụ đồ thị cho thấy rằng dù các mô hình cho ra test error khác nhau, thì correct level flexibility cũng thường khá gần nhau","code":"\n# # Load necessary libraries\n# library(caret) # For cross-validation\n# library(MASS)  # For synthetic data generation\n# library(ggplot2) # For visualization\n# \n# # Step 1: Data Simulation\n# # Generate a synthetic dataset\n# set.seed(123) # Set seed for reproducibility\n# n <- 250 # Number of observations\n# p <- 5 # Number of predictors\n# X <- matrix(rnorm(n * p), nrow = n, ncol = p) # Generate predictors\n# beta <- runif(p, -2, 2) # Random coefficients\n# epsilon <- rnorm(n, 0, 1) # Noise\n# y <- X %*% beta + epsilon # Generate target variable\n# data <- as.data.frame(cbind(X, y))\n# \n# # Step 2: Model Selection\n# # Linear regression model for this simulation\n# model_formula <- as.formula(paste(\"V1 ~\", paste(\"V\", 2:(p+1), sep=\"\", collapse=\"+\")))\n# \n# # Step 3: Cross-validation Implementations\n# # k-fold cross-validation\n# set.seed(123)\n# k_fold_results <- train(model_formula, data=data, method=\"lm\",\n#                         trControl=trainControl(method=\"cv\", number=10, \n#                                                savePredictions = TRUE))\n# \n# # Leave-one-out cross-validation (LOOCV)\n# set.seed(123)\n# loocv_results <- train(model_formula, data=data, method=\"lm\",\n#                        trControl=trainControl(method=\"LOOCV\", \n#                                               savePredictions = TRUE))\n# \n# # Step 4: Performance Comparison\n# # Extracting RMSE from each fold/iteration\n# k_fold_rmse <- k_fold_results$resample$RMSE\n# \n# # Variance of the RMSE\n# k_fold_variance <- var(k_fold_rmse)\n# \n# # Comparing computational efficiency\n# k_fold_time <- k_fold_results$times$total\n# loocv_time <- loocv_results$times$total\n# \n# # Step 5: Visualization and Interpretation\n# # Plotting the distribution of RMSE for both methods\n# rmse_data <- data.frame(Method = rep(c(\"K-Fold\", \"LOOCV\"), each=nrow(data)),\n#                         RMSE = c(k_fold_rmse, rep(mean(loocv_rmse), nrow(data))))\n# \n# ggplot(rmse_data, aes(x=Method, y=RMSE, fill=Method)) +\n#   geom_boxplot() +\n#   ggtitle(\"Distribution of RMSE for K-Fold vs LOOCV\") +\n#   xlab(\"Cross-Validation Method\") +\n#   ylab(\"RMSE\") +\n#   theme_minimal()\n# \n# # Displaying computational times\n# print(paste(\"K-Fold Computational Time:\", k_fold_time, \"seconds\"))\n# print(paste(\"LOOCV Computational Time:\", loocv_time, \"seconds\"))\n# \n# # Conclusion: This script should help illustrate the differences in variance and\n# # computational efficiency between k-fold and LOOCV cross-validation techniques."},{"path":"khdl-và-các-khái-niệm-cơ-bản.html","id":"cross-validation-cho-lớp-bài-toán-phân-loại","chapter":"Chương 2 KHDL và các khái niệm cơ bản","heading":"2.8 Cross-validation cho lớp bài toán phân loại","text":"Cross-validation không chỉ phù hợp với lớp bài toán hồi quy, phương pháp này còn rất phù hợp với lớp bài toán phân loại. Khác với lớp bài toán hồi quy, nơi mà người ta thường sử dụng MSE, MAE là giá trị đánh giá sai số trên tập dữ liệu kiểm thử, lớp bài toán phân loại sử dụng số lượng các quan sát bị phân loại sai khi áp dụng mô hình vào dữ liệu. Ví dụ, trong lớp bài toán phân loại, sai số LOOCV có dạng như sau:\\[\\begin{align*}\nCV_{(n)} = \\dfrac{1}{n} \\sum\\limits_{= 1}^{n} Err_i\n\\label{eq:class_cv}\n\\end{align*}\\]với \\(Err_i = (y_i \\neq \\hat{y_i})\\). Sai số của phương pháp k-fold cross validation và phương pháp sử dụng tập dữ liệu xác thực cũng được tính tương tự như sai số LOOCV.","code":""},{"path":"khdl-và-các-khái-niệm-cơ-bản.html","id":"phương-pháp-bootstrap","chapter":"Chương 2 KHDL và các khái niệm cơ bản","heading":"2.9 Phương pháp Bootstrap","text":"Phương pháp Bootstrap là một phương pháp thống kê rất mạnh, được ứng dụng rộng rãi để lượng hoá sự không chắc chắn của một ước lượng hoặc một phương pháp học thống kê. Một ví dụ đơn giản của phương pháp bootstrap là để ước lượng sai số chuẩn (standard errors) của tham số trong mô hình hồi quy tuyến tính. Trong trường hợp hồi quy tuyến tính (linear regression) thông thường, phương pháp này thường không có ích lắm, khi hầu như mọi gói lệnh thống kê đã tự động tính ra ước lượng sai số chuẩn (standard errors). Tuy nhiên, sức mạnh của phương pháp bootstrap đến từ việc nó có thể dễ dàng được áp dụng với rất nhiều phương pháp học thống kê khác nhau, trong đó có cả những phương pháp mà việc đo phương sai (measure variablitiy) là rất khó khăn, và không được tính sẵn bởi những gói lệnh thống kê.Trong phần này, chúng ta sẽ miêu tả về các bước thực hiện quá trình lấy mẫu để thực hiện phương pháp bootstrap bằng một ví dụ đơn giản như sau:Bước 1: Chọn ngẫu nhiên một quan sát từ dữ liệu gốc vào dữ liệu mẫu.Bước 2: Đưa lại quan sát được chọn ở bước 1 vào dữ liệu gốc.Bước 3: Lặp lại bước 1 và bước 2 n lần (n là số lượng quan sát trong dữ liệu gốc), ta thu được dữ liệu mẫu có n quan sát.Phương pháp boostrap thường cho chúng ta một kết quả ước lượng phù hợp cho phương sai của tham số hoặc một thước đo nào đó cho một tập dữ liệu. Ở đây, chúng ta sẽ tạm gọi giá trị tham số/thước đo cần được ước lượng phương sai là \\(\\hat{\\theta}_n = g(X_1, ... , X_n)\\), với \\(X_i\\) là giá trị của quan sát thứ trong mẫu VÀ g() là hàm để tính tham số/thước đo từ mẫu.Thuật toán ước lượng phương sai bằng phương pháp bootstrap có thể được trình bày như sau:Bước 1: Lấy ra các quan sát mẫu \\(X_1^*, ... , X_n^*\\), chúng ta tính \\(\\hat{\\theta}_n^* = g(X_1^*, ... , X_n^*)\\)Bước 2: Lặp lại bước 1 B lần, chúng ta thu được các ước lượng \\(\\hat{\\theta_{n,1}^*},...,\\hat{\\theta_{n,B}^*}\\)Bước 3: Tínhvới \\(\\bar{\\theta} = \\dfrac{1}{B} \\sum\\limits_{j = 1}^B \\hat{\\theta}_{n,j}^*\\)Bước 4: Thu được kết quả .Khi thoả mãn các điều kiện thông thường, chúng ta có \\(\\dfrac{s^2}{Var(\\hat{\\theta}_n)}\\) hội tụ xác suất đến 1, khi n \\(\\rightarrow \\infty\\). Khi đó, giá trị \\(\\hat{s}^2\\) được chứng minh xấp xỉ bằng phương sai của _n. Thông thường, giả sử ta tính ra giá trị \\(\\hat{s}\\) bằng 0.15, chúng ta có thể nói rằng, chúng ta kỳ vọng \\(\\hat{\\theta}\\) sẽ cách \\(\\theta\\) một khoảng bằng 0.15. Trong xấp xỉ này, chúng ta có hai nguồn gây ra sai số. Nguồn sai số đầu tiên đến từ việc n là hữu hạn, và sai số thứ hai đến từ việc B là hữu hạn. Chúng ta không thể thay đổi giá trị của n, nhưng chúng ta có thể thay đổi giá trị của B. Nếu chúng ta cho giá trị B đủ lớn (trong thực tế ta thường lấy B = 10000), chúng ta có thể bỏ qua sai số sinh ra B là hữu hạn.Khi đã tìm ra phương sai của tham số/thước đo cho tập dữ liệu, chúng ta có thể xác định khoảng tin cậy của tham số/thước đo đó theo các bước sau:Bước 1: Lấy ra các quan sát mẫu \\(X_1^*, ... , X_n^*\\), chúng ta tính \\(\\hat{\\theta}_n^* = g(X_1^*, ... , X_n^*)\\)Bước 2: Lặp lại bước 1 B lần, chúng ta thu được các ước lượng \\(\\hat{\\theta_{n,1}^*},...,\\hat{\\theta_{n,B}^*}\\)Bước 3: Ta tính giá trị:\n\\[\\begin{align*}\n\\hat{F}(t) = \\dfrac{1}{B} \\sum\\limits_{j = 1}^{B} (\\sqrt{n}(\\hat{\\theta}_{n,j}^* - \\hat{\\theta}_n) \\leq t)\n\\end{align*}\\]Bước 4: Sử dụng hàm (t), ta tính được khoảng tin cậy của \\(\\hat{\\theta}_n\\) theo công thức sau:\n\\[\\begin{align*}\nC_n = \\left[ \\hat{\\theta}_n - \\dfrac{t_{1- \\alpha/2}}{\\sqrt{n}}, \\hat{\\theta}_n - \\dfrac{t_{\\alpha/2}}{\\sqrt{n}}  \\right]\n\\end{align*}\\]\nvới \\(t_{\\alpha/2} = \\hat{F}^{-1}(\\alpha/2)\\) và \\(t_{1 - \\alpha/2} = \\hat{F}^{-1}(1 - \\alpha/2)\\).Bước 5: Trả kết quả \\(C_n\\), đây chính là khoảng tin cậy của tham số/thước đo được tính từ dữ liệu.Điểm mạnh của phương pháp bootstrap nằm ở chỗ chúng ta có thể lấy được mẫu dữ liệu B lần (với B không giới hạn) để đánh giá độ chính xác của ước lượng \\(\\theta\\). Có một phương pháp tương đương với cách chọn dữ liệu bootstrap, đó là chúng ta sinh ra B bộ dữ liệu mới khác nhau với các đặc điểm giống với dữ liệu thực tế. Tuy nhiên, với dữ liệu thực tế, chúng ta lại không thể sinh ra một tập mẫu mới từ dữ liệu gốc, vậy nên ta không thể sử dụng được phương pháp kể trên. Với mục tiêu đạt được giá trị \\(\\hat{s}\\) tốt nhất, phương pháp bootstrap vẫn được đánh giá là một phương pháp cho kết quả chính xác, khả năng ứng dụng tốt với các loại mô hình khác nhau.","code":""},{"path":"kiến-thức-r-cơ-bản.html","id":"kiến-thức-r-cơ-bản","chapter":"Chương 3 Kiến thức R cơ bản","heading":"Chương 3 Kiến thức R cơ bản","text":"","code":""},{"path":"kiến-thức-r-cơ-bản.html","id":"làm-quen-với-các-dòng-lệnh-cơ-bản","chapter":"Chương 3 Kiến thức R cơ bản","heading":"3.1 Làm quen với các dòng lệnh cơ bản","text":"Đây là cuốn sách dành cho sinh viên và học viên các ngành kinh tế, quản lý, và quản trị kinh doanh muốn tìm hiểu sang lĩnh vực khoa học dữ liệu. Những bạn đọc đã có nền tảng kiến thức cơ bản về toán học, xác suất thống kê, và lập trình sẽ cảm thấy dễ dàng hơn khi bắt đầu. Cuốn sách sử dụng R làm ngôn ngữ và công cụ chính để thực hiện các thao tác trên dữ liệu.Nếu bạn đọc đã có nền tảng cơ bản về lập trình thì cuốn sách này không giúp bạn lập trình tốt hơn. Mục đích chính của cuốn sách là giúp bạn có thể sử dụng được R và thực hiện được các thao tác trên dữ liệu trong môi trường R một cách nhanh nhất. Theo quan điểm của chúng tôi, R không phải là một ngôn ngữ thích hợp để bắt đầu cho học lập trình. Muốn trở thành một lập trình viên giỏi, bạn đọc nên bắt đầu với các ngôn ngữ lập trình cơ bản như Pascal, C++, Java, hay cũng có thể bắt đầu với ngôn ngữ Python.Cách viết các dòng lệnh của R có thể nói là khá tùy tiện, thậm chí có thể làm cho những người có chuyên môn về lập trình cảm thấy khó chịu. Tuy nhiên, như đã đề cập trong phần giới thiệu của cuốn sách, R có các thế mạnh riêng mà các ngôn ngữ khác không có được và chúng tôi tin rằng R có thể giải quyết được tất cả những yêu cầu của bạn đọc từ những yêu cầu đơn giản đến những yêu cầu phức tạp nhất.Cuốn sách hướng đến cả các bạn đọc chưa từng làm quen với lập trình. Với những ai đã có kinh nghiệm với lập trình có thể bỏ qua các phần không cần thiết như biến, vòng lặp, viết hàm số và có thể bắt đầu ngay kể từ phần phân tích dữ liệu.","code":""},{"path":"kiến-thức-r-cơ-bản.html","id":"sử-dụng-r-như-một-máy-tính-cầm-tay","chapter":"Chương 3 Kiến thức R cơ bản","heading":"3.1.1 Sử dụng R như một máy tính cầm tay","text":"Để R hiểu và thực hiện được các yêu cầu của mình, bạn đọc cần phải viết các câu lệnh dưới ngôn ngữ của phần mềm. Hãy bắt đầu với câu lệnh đầu tiên và đơn giản nhất: hiển thị một giá trị lên cửa sổ Console. Trước hết bạn đọc hãy nhấp chuột vào cửa sổ Console, sau đó gõ trực tiếp đoạn câu lệnh như ở dưới và kết thúc câu lệnh bằng cách sử dụng phím Enter:Bạn đọc có thể bắt đầu làm quen với các dòng lệnh của R bằng cách viết lên cửa sổ Console các công thức để thực hiện tính toán các phép toán dưới đây. R có thể được sử dụng đơn giản như một máy tính cầm tay:Một vài lưu ý bạn đọc có thể nhận thấy khi gõ các dòng lệnh ở trên: các phép tính cộng trừ nhân chia, dấu thập phân, dấu lũy thừa,…, hoàn toàn giống như khi sử dụng máy tính cầm tay. Các hàm số quen thuộc như hàm logarit, hàm lũy thừa cơ số tự nhiên cũng không có gì đặc biệt.Bạn đọc có thể tiếp tục thực hành các câu lệnh cơ bản bằng cách sử dụng R để tính toán kết quả của các biểu thức dưới đây:\\[\\begin{align}\n) \\ \\cfrac{1}{4^{1/6}} \\ \\ \\ b) \\ \\cfrac{7 - 4}{12 - 7} \\ \\ \\ c) \\ \\sqrt{\\cfrac{4}{22}} \\ \\ \\ d) (12-5)^{4/3} \\ \\ \\ e) \\ \\log\\left( \\cfrac{2 + 4}{2^5 -1} \\right)\n\\end{align}\\]Khi viết câu lệnh trên cửa sổ Console, R luôn thực hiện câu lệnh mỗi khi bạn đọc sử dụng phím Enter. Khi muốn viết hai hay nhiều câu lệnh trên một dòng trên cửa sổ Console, bạn đọc hãy ngăn cách các câu lệnh bằng dấu  ’’ ; ” . Hãy thử câu lệnh ở dưới và quan sát R cách trả kết quả:Khi bạn đọc viết các câu lệnh đơn giản thì sử dụng nhiều phép tính trên một dòng lệnh có thể hạn chế việc dùng phím Enter nhiều lần. Tuy nhiên chúng tôi khuyên bạn đọc khi muốn thực hiện nhiều câu lệnh khác nhau hãy sử dụng cửa sổ Script thay vì viết câu lệnh trực tiếp lên cửa sổ Console. Phần tiếp theo của chương chúng tôi sẽ thảo luận về cách viết và thực thi câu lệnh trên cửa sổ Script.","code":"\nprint(\"I am MFEer\")\n1+0.001\n2*pi - 3 # Số pi trong R được viết là pi\nexp(1)-exp(-1) # Hàm lũy thừa cơ số tự nhiên.\nlog(3.2) # Hàm logarit cơ số tự nhiên.\nlog(1000,10) # Hàm logarit cơ số 10.\n2*pi - 3; exp(1)-exp(-1) # Một dòng lệnh thực hiện hai câu lệnh.## [1] 3.283185## [1] 2.350402"},{"path":"kiến-thức-r-cơ-bản.html","id":"sử-dụng-cửa-sổ-script-để-viết-câu-lệnh","chapter":"Chương 3 Kiến thức R cơ bản","heading":"3.1.2 Sử dụng cửa sổ Script để viết câu lệnh","text":"Cách tốt nhất để bạn đọc viết, quản lý, và thực thi câu lệnh đó là sử dụng cửa sổ Script. Để mở cửa sổ Script trên Rstudio, bạn đọc có thể tìm trên thanh công cụ theo trình tự File \\(\\rightarrow\\) New file \\(\\rightarrow\\) R Script, hoặc bạn đọc sử dụng tổ hợp phím tắt Ctrl + Shift + N. Khi viết câu lệnh trên cửa sổ Script, R chỉ thực hiện câu lệnh khi bạn đọc yêu cầu. đó, bạn đọc có thể sử dụng cửa sổ Script để viết các chương trình lớn với nhiều dòng lệnh kế tiếp nhau.Sau khi mở của sổ Script, bạn có thể viết các dòng lệnh và sử dụng Enter để xuống dòng và không cần quan tâm đến việc R có thực thi câu lệnh đó hay không. Trong một dòng lệnh trên cửa sổ Script mỗi khi bạn đọc sử dụng dấu ngắt câu lệnh “;” R vẫn hiểu rằng bạn đọc đang viết hai câu lệnh khác nhau trên một dòng. Bạn đọc có thể bắt đầu mở cửa sổ Script và gõ các dòng lệnh dưới đây:Phím Enter cho phép xuống dòng và không thực thi câu lệnh khi bạn đọc viết trên cửa sổ Script. Thay vào đó, để thực thi các dòng lệnh, bạn đọc có hai lựa chọn: thứ nhất, sử dụng con trỏ bấm vào nút Run nằm ở phía góc trên bên phải của cửa sổ này, và thứ hai các bạn sử dụng tổ hợp phím tắt Ctrl + Enter nếu bạn sử dụng R trên hệ điều hành Windows và Cmd + Enter nếu bạn sử dụng R trên hệ điều hành MacOS. Để thực thi một dòng lệnh riêng lẻ trên cửa sổ Script, bạn đọc di chuyển con trỏ đến dòng lệnh đó và thực hiện thao tác như ở trên. Để thực thi nhiều dòng lệnh bạn đọc sử dụng chuột trái lựa chọn các dòng lệnh và sau đó thực hiện thao tác chạy. Khi lựa chọn nhiều dòng lệnh một lúc để thực thi, R sẽ thực hiện các câu lệnh lần lượt theo thứ tự từ trên xuống dưới và từ bên trái qua bên phải nếu một dòng có nhiều câu lệnh.Lưu ý, khi bạn đọc viết một chương trình bao gồm nhiều dòng lệnh, bạn thường phải sử dụng ngôn ngữ thông thường và dễ hiểu bằng tiếng Việt, hoặc tiếng Anh, để ghi chú lại các dòng lệnh hoặc nhóm các dòng lệnh đó có ý nghĩa là gì. Việc này giúp cho bản thân bạn khi xem lại các dòng lệnh của mình và cho những người tiếp nhận khi đọc các dòng lệnh hiểu được ý nghĩa của các câu lệnh. Các câu ghi chú đó theo ngôn ngữ lập trình được gọi là các câu ghi chú, hay comment. Bạn đọc sử dụng dấu Script bắt đầu trước câu ghi chú:Phần tiếp theo của chương sẽ thảo luận về các khái niệm cơ bản nhất trong lập trình, đó là khái niệm về các kiểu biến.","code":"\n1+0.001 ; 2*pi - 3 ; exp(1)-exp(-1)\nlog(3.2)\nlog(1000,10)\n# Đây là cách tính xấp xỉ số e\nn<-1000\ncat(\"e = \", (1+1/n)^n) # Khi n càng lớn thì kết quả càng chính xác"},{"path":"kiến-thức-r-cơ-bản.html","id":"biến-trong-ngôn-ngữ-r","chapter":"Chương 3 Kiến thức R cơ bản","heading":"3.2 Biến trong ngôn ngữ R","text":"Biến là khái niệm cơ bản nhất trong mọi ngôn ngữ lập trình. Có bốn loại biến cơ bản trong R bao gồmBiến kiểu số, được gọi trong ngôn ngữ R là kiểu numeric;\nBiến kiểu số, được gọi trong ngôn ngữ R là kiểu numeric;Biến kiểu ký tự, hay chuỗi ký tự, được gọi trong ngôn ngữ R là kiểu character;\nBiến kiểu ký tự, hay chuỗi ký tự, được gọi trong ngôn ngữ R là kiểu character;Biến kiểu logic, được gọi trong ngôn ngữ R là kiểu logical;\nBiến kiểu logic, được gọi trong ngôn ngữ R là kiểu logical;Biến kiểu thời gian, được gọi trong ngôn ngữ R là kiểu Date và kiểu POSIXct.\nBiến kiểu thời gian, được gọi trong ngôn ngữ R là kiểu Date và kiểu POSIXct.Nhiều sách tham khảo khi viết về kiểu biến trong ngôn ngữ lập trình R phân loại biến thành nhiều kiểu hơn, chẳng hạn như có thêm kiểu số nguyên (integer), kiểu factor,… Tuy nhiên theo quan điểm của chúng tôi, phân loại biến quá chi tiết sẽ gây khó khăn cho bạn đọc, nhất là với bạn đọc mới làm quen với lập trình. đó, chúng tôi phân chia kiểu biến trong ngôn ngữ R thành bốn kiểu như ở trên. Trong các phần tiếp theo của chương chúng tôi sẽ thảo luận chi tiết về từng kiểu biến cụ thể và các kiểu biến khác có thể có liên quan.Để tạo một biến trong R và gán giá trị cho biến đó, bạn đọc sử dụng một trong ba cách như sau:Trong các dòng lệnh ở trên, tenbien là tên của biến mà bạn muốn đặt, giatri là giá trị mà bạn muốn gán cho biến. Ký tự <- là ký tự gán giá trị được sử dụng trong các phiên bản R đầu tiên. Gán giá trị cho biến sử dụng ký tự -> khi bạn đọc viết tên biến sang phía bên phải. Cách viết này hiếm khi được dùng. Từ các phiên bản 3.0 của phần mềm R trở đi, dấu = cũng có thể được sử dụng để gán giá trị cho biến. Tuy nhiên dấu = có thể gây nhầm lẫn khi sau này bạn đọc sử dụng cùng lúc với ký hiệu sánh ==. Đồng thời ký hiệu = cũng được sử dụng trong truyền giá trị cho tham số khi viết hàm số. đó, trong cuốn sách này, chúng tôi luôn sử dụng <- để gán giá trị cho biến.Dưới đây là một vài ví dụ về tạo biến và gán giá trị cho biến:Trong các câu lệnh ở trên, x, y hay z là tên biến. Quy tắc đặt tên biến hay tên một đối tượng trong R cần tuân theo các quy tắc sau:Tên biến có thể là tổ hợp của tất cả các chữ cái viết hoa, chữ cái viết thường và các chữ số.\nTên biến có thể là tổ hợp của tất cả các chữ cái viết hoa, chữ cái viết thường và các chữ số.Trong tên biến có thể chứa hai ký tự đặc biệt là “.” và “_“.\nTrong tên biến có thể chứa hai ký tự đặc biệt là “.” và “_“.Tên biến không được phép bắt đầu bằng số hoặc ký tự “_“.\nTên biến không được phép bắt đầu bằng số hoặc ký tự “_“.Không được dùng từ khóa của R để đặt tên biến.\nKhông được dùng từ khóa của R để đặt tên biến.Để kiểm tra các quy tắc ở trên, bạn đọc có thể thử thực thi các câu lệnh tạo biến dưới đây và xem dòng lệnh nào báo lỗi và dòng lệnh nào không báo lỗi:Lưu ý rằng R có phân biệt chữ viết hoa với chữ viết thường trong tên biến. Ví dụ như khi bạn đọc sử dụng x để đặt tên và sau đó dùng X để đặt tên thì R sẽ hiểu đây là hai biến khác nhau:Để biết danh sách các tên biến đang tồn tại trên môi trường làm việc và giá trị của các biến đó, ngoài cách giá trị biến lên cửa sổ Console, bạn đọc có thể sử dụng cửa sổ Environment ở góc phía trên bên phải, của Rstudio. Để xóa một biến hoặc một đối tượng đang tồn tại trong môi trường làm việc hiện tại, bạn đọc sử dụng câu lệnh rm():Một điều cũng cần lưu ý khi đặt tên biến hay khi đặt tên các đối tượng khác trong R là tên biến không được phép trùng với các từ khóa. Danh sách các từ khóa thường sử dụng trong R nằm trong bảng ??Trong phần tiếp theo, chúng ta sẽ thảo luận chi tiết về từng kiểu biến.","code":"\n# Cách thứ nhất\ntenbien <- giatri # dấu \"<-\" là dấu gán giá trị\n\n# Cách thứ hai\ngiatri -> tenbien\n\n# Cách thứ ba\ntenbien = giatri # dấu \"=\" cũng được sử dụng để gán giá trị\n# Cách thứ nhất:\nx <- 3  # Tạo một biến tên là x có giá trị 3.\n\n# Cách thứ hai:\n\"MFE\" -> y # Tạo một biến tên y có giá trị bằng ký tự \"MFE\".\n\n# Cách thứ ba:\nz = 1 + 2\n# Tạo một biến tên z và nhận giá trị bằng kết quả của phép cộng.x1 <- 3 # Biến tên x1 sẽ được tạo với giá trị bằng 3.\n1x <- 3 # Sẽ có lỗi, tên biến không được phép bắt đầu bằng số.\n.x <- 3 # Biến tên .x hợp lệ.\n_x <- 3 # Sẽ báo lỗi, tên biến không được phép bắt đầu bằng _\nx<-3 # Tạo một biến tên x, giá trị bằng 3.\nX<-5 # Tạo một biến tên X, giá trị bằng 5.\nX-x # Hiệu số cho kết quả bằng 2 do x và X là khác nhau.\nx # R trả lại giá trị bằng 3.\nrm(x) # Xóa biến x khỏi môi trường làm việc.\nx # Sẽ báo lỗi vì biến x không còn tồn tại."},{"path":"kiến-thức-r-cơ-bản.html","id":"biến-kiểu-số","chapter":"Chương 3 Kiến thức R cơ bản","heading":"3.2.1 Biến kiểu số","text":"Biến kiếu số, được gọi trong R là kiểu numeric, là các biến nhận giá trị kiểu số thập phân. Để tạo một biến kiểu số, bạn đọc gán một giá trị kiểu số bất kỳ cho tên biến mà bạn muốn đặt. Đây cũng là cách tạo biến chung trong R. Câu lệnh dưới đây khởi tạo một biến tên x kiểu số và nhận giá trị bằng 5:Để kiểm tra một biến tên x có phải là biến kiểu số không, bạn đọc sử dụng hàm .numeric(). Hàm số này trả lại giá trị là kiểu logic. Giá trị TRUE cho biết biến được hỏi đúng là kiểu số trong khi giá trị FALSE cho biết biến được hỏi không phải là kiểu số. Ngoài sử dụng hàm .numeric(), bạn đọc cũng có thể sử dụng hàm class(). Hàm class() cho biết kiểu giá trị của một đối tượng bất kỳ trong R. Cách sử dụng hai hàm này như sau:Trong phép gán cho giá trị của biến x ở trên, mặc dù giá trị khởi tạo 5 là số nguyên nhưng R vẫn cho rằng x là kiểu số thập phân. Để tạo một biến kiểu số nguyên, bạn đọc cần phải sử dụng thêm chữ L phía sau số nguyên đó. Chữ L là viết tắt cho Long nghĩa là số nguyên kiểu Long trong các ngôn ngữ lập trình cơ bản như Pascal hay C. Số nguyên kiểu Long là các số nguyên cần 32 bytes để lưu và nhận \\(2^{32}\\) giá trị từ −2,147,483,648 (\\(-2^{31}\\)) đến 2,147,483,647 (\\(2^{31}-1\\)). Để tạo biến x nhận giá trị là số nguyên bằng 5 chúng ta viết như sau:Phân biệt số nguyên và số thập phân trong các ngôn ngữ lập trình có ý nghĩa khi bạn đọc cần tiết kiệm bộ nhớ cho chương trình. Trong R, khi sử dụng số thập phân thay cho số nguyên, dung lượng bộ nhớ máy tính sẽ tăng gấp 2 lần. Hình vẽ dưới đây mô tả dung lượng bộ nhớ cần sử dụng cho các véc-tơ chứa các số nguyên và các véc-tơ chứa các số thập phân với số lượng phần tử chạy từ 1 đến 100. Từ hình 3.1 bạn đọc có thể thấy rằng không có sự khác biệt về bộ nhớ với các véc-tơ có độ dài dưới 10 nhưng khi véc-tơ có độ dài từ 10 trở lên, véc-tơ kiểu số thập phân cần trung bình khoảng 2 lần bộ nhớ với véc-tơ kiểu số nguyên.\nHình 3.1: Sự khác nhau về dung lượng bộ nhớ cần sử dụng để lưu véc-tơ kiểu số nguyên và véc-tơ kiểu số thập phân.\nBiến kiểu số được sử dụng chủ yếu để thực hiện trong các phép tính toán. Các phép tính toán thông thường được liệt kê trong bảng ??Lưu ý rằng các phép toán lấy phần dư %%, và phép lấy thương số trong phép chia %/%, cũng có thể thực hiện được với cả số kiểu thập phân. Bạn đọc quan sát kết quả của các phép toán này trong ví dụ dưới đây:Một cách viết số kiểu khoa học và các giá trị kiểu số đặc biệt được liệt kê trong bảng ??.Bạn đọc cần đặc biệt lưu ý khi thực hiện các phép tính toán có kết quả là các giá trị đặc biệt. Dưới đây là các ví dụ:","code":"\nx <- 5 # 5 là giá trị kiểu số nên R tự hiểu x là biến kiểu số.\nis.numeric(x) # 5 là giá trị kiểu số nên R trả lại TRUE.## [1] TRUE\nclass(x) # Cho biết kiểu giá trị của đối tượng x.## [1] \"numeric\"\ny<-\"abc\" # Khởi tạo một biến y kiểu ký tự.\nis.numeric(y) # Kết quả là FALSE vì y không phải số.## [1] FALSE\nx <- 5L # 5L nghĩa là số nguyên 5, L là viết tắt của Long.\nclass(x) # x là số tự nguyên (integer)## [1] \"integer\"\nis.numeric(x) # x không còn là số thập phân, nhưng vẫn là kiểu số.## [1] TRUE\n6.5 %% 2 # Phần dư của phép chia 6.5 cho 2## [1] 0.5\n6.5 %/% 2 # Phần nguyên của phép chia 6.5 cho 2## [1] 3\n1/0 # Kết quả của 1/0 là dương vô cùng.\n(-1)/0 # Kết quả của -1/0 là âm vô cùng.\nInf - 10^10 # Trong các phép tính có Inf sẽ có kết quả Inf.\n1/0 + (-1)/0 # Inf + (-Inf) là không thể xác định được.\nlog(-2) # Kết quả của các phép tính vô nghĩa là NaN."},{"path":"kiến-thức-r-cơ-bản.html","id":"biến-kiểu-logical","chapter":"Chương 3 Kiến thức R cơ bản","heading":"3.2.2 Biến kiểu logical","text":"Biến kiểu logic, được gọi là kiểu logical, là kiểu biến đơn giản nhất nhưng lại quan trọng nhất trong R nói riêng và trong đa số các ngôn ngữ lập trình nói chung. Biến kiểu logical chỉ nhận một trong hai giá trị là TRUE hoặc FALSE. R phân biệt chữ viết hoa và chữ viết thường nên khi viết giá trị cho biến kiểu logic bạn đọc cần viết bao gồm toàn bộ các chữ cái viết hoa. Để tạo một biến kiểu logic, chúng ta chọn tên biến và gán một trong hai giá trị logic. Việc này hoàn toàn giống như khi tạo một biến kiểu số:Biến kiểu logic có thể đặt trong các phép tính toán giống như biến kiểu số. Khi gặp một công thức có bao gồm cả biến kiểu số và biến kiểu logic, R sẽ đổi biến kiểu logic có giá trị TRUE thành giá trị 1 và biến kiểu logic có giá trị FALSE thành giá trị 0 rồi thực hiện phép tính toán:Trong thực tế, ít khi chúng ta khởi tạo giá trị cho biến kiểu logic như trên. Biến kiểu logic thường nhận được từ kết quả các phép sánh. Các phép toán sánh thường được sử dụng trong R được liệt kê trong bảng ??Ngoài ra, các biến kiểu logic còn là kết quả của việc kết hợp \\(|\\) nhiều biến kiểu logic khác bằng các toán tử logic. Các toán tử logic bao gồm có toán tử “và”, toán tử “hoặc”, và toán tử “phủ định”. Các toán tử này được liệt kê trong bảng ??Bạn đọc cần lưu ý rằng các biến logic khi kết hợp với nhau bằng các toán tử trong bảng ?? sẽ cho kết quả là một biến kiểu logic. Quy tắc kết hợp các biến kiểu logic bằng các toán tử logic được tổng hợp trong bảng ??Như chúng tôi đã đề cập ở trên, các biến kiểu logic khi đặt trong các biểu thức tính toán sẽ được tự động đổi sang biến kiểu số trước khi thực hiện phép tính. Ngược lại, khi biến kiểu số xuất hiện trong các biểu thức có toán tử logic, biến kiểu số cũng sẽ được chuyển sang kiểu logic. R sẽ mặc định quy tắc đổi từ số sang kiểu logic như sau: chỉ có số 0 khi đặt trong biểu thức có toán tử logic mới được chuyển thành FALSE, mọi số khác 0 khi đổi sang kiểu logic đều được chuyển thành TRUE. Bạn đọc hãy quan sát nguyên tắc đổi từ biến kiểu số sang logic trong các phép toán dưới đâyBạn đọc có thể thực hành việc tính toán trên các toán tử logic như ở dưới đây. Trước khi sử dụng R để xem kết quả, hãy thử suy nghĩ xem các biểu thức từ 1. đến 4. dưới đây cho kết quả như thế nào?","code":"\nx <- TRUE\nFALSE + TRUE * 10 # Sẽ cho kết quả giống như 0 + 1 * 10## [1] 10\n10 & -0.1 # Tương đương với TRUE & TRUE## [1] TRUE\n0 & -0.1 # Tương đương với FALSE & TRUE## [1] FALSE\n0 | 0.2 # Tương đương với FALSE | TRUE## [1] TRUE\n# 1.\n(1<=2) | (2<=3)\n# 2.\n(1<=2) + (2<=3)\n# 3.\n((1<=2) * (2^2 == 4)) | (2!=3) #\n# 4.\n!((1<=2) * (2^2 == 4)) & !(2!=3) #\n# 5.\n((2 + 2) | (2 - 2)) & !(2 ^ 2) #"},{"path":"kiến-thức-r-cơ-bản.html","id":"biến-kiểu-chuỗi-ký-tự","chapter":"Chương 3 Kiến thức R cơ bản","heading":"3.2.3 Biến kiểu chuỗi ký tự","text":"Biến kiểu ký tự hay chuỗi ký tự còn được gọi trong ngôn ngữ R là kiểu character. Biến kiểu chuỗi ký tự tương tự như biến kiểu xâu ký tự hay kiểu string trong các ngôn ngữ lập trình cơ bản. Biến kiểu chuỗi ký tự có thể chỉ ngắn gọn là một ký tự trống, một chữ cái, nhưng đôi khi có thể là một câu, hoặc cũng có thể là cả một đoạn văn bản. Khi làm việc với biến kiểu chuỗi ký tự, bạn đọc hãy luôn ghi nhớ rằng R phân biệt chữ viết hoa và chữ viết thường.Để tạo một biến có kiểu ký tự trong R, bạn đọc chọn tên cho biến và gán giá trị kiểu chuỗi ký tự. R sẽ hiểu một biến là chuỗi ký tự khi chuỗi ký tự đó nằm trong dấu ngoặc kép \" \" hoặc trong dấu ngoặc đơn ' '.Để biết một biến có phải kiểu chuỗi ký tự không, bạn đọc sử dụng hàm .character() hoặc hàm class(). Hàm .character() trả lại giá trị TRUE nếu một biến có kiểu chuỗi ký tự và trả lại giá trị FALSE trong các trường hợp còn lại. Hàm class(), như chúng tôi đã giới thiệu ở trên, cho biết một đối tượng bất kỳ là đối tượng có kiểu như thế nào:Khi xử lý biến kiểu chuỗi ký tự, bạn đọc nên sử dụng các hàm số đã được xây dựng sẵn. Bảng ?? liệt kê các hàm thường được sử dụng khi xử lý biến kiểu chuỗi ký tự và kết quả của các hàm này.Bạn đọc có thể thực thi các hàm liệt kê trong bảng ?? và quan sát giá trị trả ra của các hàm đó trong các câu lệnh dưới đây để hiểu cách sử dụng của các hàm này:Nhìn chung, trong bất kỳ ngôn ngữ lập trình nào, xử lý biến kiểu chuỗi ký tự sẽ luôn khó khăn hơn với xử lý biến kiểu số. Để thực hiện được các yêu cầu phức tạp hơn, bạn đọc có thể kết hợp các hàm số ở trên để có hiệu quả tốt hơn. Có các thư viện được phát triển dành riêng cho việc xử lý các biến kiểu chuỗi ký tự mà tiêu biểu là thư viện stringr. Các hàm hữu ích trong thư viện stringr sẽ được thảo luận chi tiết trong phần phân tích dữ liệu.","code":"\nx <- \"Ice cream\" # \"Ice cream\" với chữ I viết hoa.\nx == \"ice cream\" # Kết quả trả lại là FALSE## [1] FALSE\nis.character(x)\nclass(x)\n# Khởi tạo hai biến kiểu chuỗi ký tự x1 và x2\nx1 <- \"I am an Actuary\"; x2<-\"I am Vietnamese\"\nnchar(x1) # Cho biết x1 có bao nhiêu ký tự.\npaste(x1, x2, sep = \" and \") # Ghép x1 và x2 lại và thêm \" and \" vào giữa.\ntoupper(x1); tolower(x1) # Chuyển tất cả các ký tụ sang viết hoa/viết thường\nchartr(\"an\",\"bm\",x1) # Thay tất cả các chữ \"a\" trong x1 bằng \"b\" và \"n\" bằng \"m\"\nsubstr(x1, 9, 15) # Lấy ra đoạn ký tự từ ký tự thứ 9 (chữ A) đến ký tự thứ 15 (chữ \"y\")\nsub(\"a\", \"XYZ\", x1) # Thay chữ \"a\" đầu tiên trong x1 bằng đoạn \"XYZ\"\ngsub(\"a\", \"XYZ\", x1) # Thay tất cả chữ \"a\" trong x1 bằng đoạn \"XYZ\"\ngrepl(\"Vietnam\", x2) # Cho biết đoạn ký tự \"Vietnam\" có nằm trong x2 hay không"},{"path":"kiến-thức-r-cơ-bản.html","id":"biến-kiểu-thời-gian","chapter":"Chương 3 Kiến thức R cơ bản","heading":"3.2.4 Biến kiểu thời gian","text":"Trong ngôn ngữ R có hai kiểu biến thời gian là biến kiểu ngày tháng, được gọi trong ngôn ngữ R là kiểu Date) và biến kiểu thời gian chi tiết, được gọi trong ngôn ngữ R là kiểu POSIXct. Thời gian POSIX hay còn được biết đến với tên gọi là thời gian Unix là một cách quy ước về thời gian của một thời điểm cụ thể được tính bằng số giây từ cột mốc thời gian Unix đến thời điểm đó. Cột mốc thời gian Unix được các kỹ sư xây dựng hệ điều hành Unix lựa chọn là thời điểm 0 giờ, 0 phút, 0 giây, ngày 01 tháng 01 năm 1970 theo giờ phối hợp quốc tế (giờ UTC). Chữ “ct” là viết tắt của canlendar time. Bạn đọc cũng có thể gặp biến kiểu thời gian chi tiết trong R dưới dạng POSIXlt trong đó lt là chữ viết tắt của local time. Sự khác biệt của biến kiểu POSIct và POSIXlt chỉ là cách R lưu trữ các biến này dưới dạng số nguyên hay dưới dạng véc-tơ. Trong cuốn sách này khi nói đến biến kiểu thời gian chúng tôi luôn sử dụng biến kiểu POSIXct.Để tạo biến kiểu thời gian trong R, bạn đọc sử dụng hàm .Date() cho biến kiểu ngày tháng và hàm .POSIXct() cho biến kiểu thời gian chi tiết:Khi xử lý biến kiểu thời gian, bạn đọc nên đổi sang dạng số hoặc lưu biến kiểu thời gian dưới dạng một véc-tơ số lưu lại các thành phần của thời gian theo một thứ tự nhất định. Hàm .numeric() sẽ đổi các biến kiểu ngày tháng hoặc thời gian chi tiết ra thành số ngày đối với biến kiểu ngày tháng, hoặc số giây đối với biến kiểu thời gian chi tiết, tính từ mốc thời gian Unix.múi giờ UTC của Việt Nam là UTC + 7 nên thời điểm tính làm mốc sẽ là 7 giờ, 0 phút, 0 giây, ngày 01 tháng 01 năm 1970. Điều này giải thích tại sao khi đổi biến time2 dưới đây thành dạng số ta sẽ thu được kết quả là 30 giây.Khi sử dụng các hàm .Date() hoặc .POSIXct() giá trị được đưa vào phải là biến dạng chuỗi ký tự được viết theo đúng quy tắc YYYY-MM-DD và YYYY-MM-DD hh:mm:ss. Trong trường hợp chuỗi ký tự được đưa vào không đúng định dạng, bạn đọc cần phải thông báo cho R biết định dạng của biến chuỗi ký tự đó bằng cách sử dụng thêm tham số format. Bạn đọc có thể tham khảo cách khai báo định dạng của biến chuỗi ký tự trong các hàm .Date() hoặc .POSIXct() như sau:Trong rất nhiều trường hợp, biến kiểu thời gian sẽ được lấy từ các nguồn khác nhau vào R và được lưu dưới dạng số tự nhiên. Điển hình là khi bạn đọc lấy dữ liệu từ các file được lưu từ phần mềm Microsoft Excel. Các hàm .Date() và .POSIXct() cũng có thể chuyển giá trị số biến kiểu ngày tháng và biến kiểu thời gian chi tiết. Bạn đọc cần sử dụng thêm tham số biến origin trong các hàm này để quy định mốc thời gian.Sau khi thực thi các câu lệnh ở trên, biến date1 tương ứng với ngày thứ 19,000 tính từ mốc ngày 1 tháng 1 năm 1970 và biến time1 tương ứng với thời điểm giây thứ 1 tỷ tính từ 07 giờ 00 phút 00 giây ngày 1 tháng 1 năm 1970.Vấn đề thường gặp phải khi lấy dữ liệu từ các nguồn ngoài là cách chuyển đổi từ thời gian thành số của phần mềm lưu dữ liệu gốc có mốc thời gian khác với R. Chẳng hạn như biến kiểu thời gian từ Microsoft Excel khi chuyển đổi thành số sử dụng mốc thời gian là ngày 30 tháng 12 năm 1899. Giả sử khi bạn đọc lấy một biến thời gian từ Microsoft Excel vào R và thấy giá trị là 45678. Nếu không sử dụng mốc thời gian của Microsoft Excel để chuyển đổi, giá trị thời gian nhận được sẽ không đúng. Để biến ngày tháng nhận giá trị đúng, chúng ta cần khai báo tham số origin như sau:Nguyên tắc cơ bản khi xử lý và tính toán với biến kiểu thời gian trong R là luôn luôn đổi biến sang kiểu số nguyên hoặc đổi một biến kiểu thời gian thành một véc-tơ chứa các thành phần ngày, tháng, năm, giờ, phút, giây dưới dạng số. Để tách biến kiểu ngày tháng ra thành ngày, tháng, năm bạn đọc có thể sử dụng hàm sub.str() để lấy ra các đoạn ký tự chứa giá trị ngày, tháng, và năm rồi sau đó sử dụng hàm .numeric() để đổi các biến thành biến kiểu số:Xử lý biến kiểu ngày tháng và biến kiểu thời gian phức tạp hơn với xử lý biến kiểu số và thường cần thêm các thư viện bổ sung. Thư viện chúng tôi thường sử dụng khi làm việc với biến kiểu thời gian là thư viện lubridate hoặc thư viện hms. Bạn đọc sẽ sử dụng các thư viện này để thực hành với biến kiểu thời gian trong chương phân tích dữ liệu.","code":"\n# Tạo biến date1 nhận giá trị là ngày 31/08/2023\ndate1 <- as.Date(\"2023-08-31\")\n\n# Biến time1 là 16 giờ, 41 phút, 30 giây ngày 31/08/2023\ntime1 <- as.POSIXct(\"2023-08-31 16:41:30\")\n# Cho biết số ngày tính từ 01/01/1970 đến date1\nas.numeric(date1)## [1] 19600\n# Cho biết số giây tính từ 7 giờ, 0 phút, 0 giây ngày 01/01/1970 đến time1\nas.numeric(time1)## [1] 1693474890\n# Cho biết số giây tính từ 7 giờ, 0 phút, 0 giây ngày 01/01/1970 đến time2\ntime2 <- as.POSIXct(\"1970-01-01 07:00:30\")\nas.numeric(time2)## [1] -3570\n# date1 là ngày 27 tháng 02 năm 1992\ndate1 <- as.Date(\"02/27/92\", format = \"%m/%d/%y\")\n\n# date2 là ngày 02 tháng 01 năm 2010\ndate2 <- as.Date(\"02 Jan 2010\", format = \"%d %b %Y\")\ndate1 <- as.Date(19000, origin = \"1970-01-01\")\ntime1 <- as.POSIXct(10^9, origin = \"1970-01-01 07:00:00\")\n# 45678 là giá trị của biến lấy từ nguồn Microsoft Excel\ndate1 <- as.Date(45678, origin = \"1970-01-01\")\ndate1 # date1 sai do sử dụng mốc thời gian ngày 01 tháng 01 năm 1970.## [1] \"2095-01-23\"\ndate2 <- as.Date(45678, origin = \"1899-12-30\")\ndate2 # date2 có giá trị ĐÚNG do dùng đúng mốc thời gian của Excel.## [1] \"2025-01-21\"\n# Lấy ra giá trị năm trong biến date2\nyear <- as.numeric(substr(date2,1,4))\n\n# Lấy ra giá trị tháng trong biến date2\nmonth <- as.numeric(substr(date2,6,7))\n\n# Lấy ra giá trị ngày trong biến date2\nday <- as.numeric(substr(date2,9,10))"},{"path":"kiến-thức-r-cơ-bản.html","id":"véc-tơ","chapter":"Chương 3 Kiến thức R cơ bản","heading":"3.3 Véc-tơ","text":"Trong phần này của cuốn sách chúng tôi giới thiệu các khái niệm cơ bản về véc-tơ và giải thích tại sao xử lý véc-tơ lại là thế mạnh của ngôn ngữ R. Chúng tôi sẽ không đi quá chi tiết vào các kỹ thuật xử lý véc-tơ mà chỉ tập trung vào các công cụ cơ bản. Các yêu cầu về xử lý véc-tơ sẽ được lặp lại trong tất cả các chương tiếp sau của cuốn sách, đó đi quá sâu vào chi tiết trong phần này là không cần thiết.","code":""},{"path":"kiến-thức-r-cơ-bản.html","id":"tại-sao-xử-lý-véc-tơ-là-thế-mạnh-của-r","chapter":"Chương 3 Kiến thức R cơ bản","heading":"3.3.1 Tại sao xử lý véc-tơ là thế mạnh của R?","text":"Véc-tơ là một tập hợp các phần tử có cùng kiểu được sắp xếp theo một thứ tự nhất định. Thứ tự của một phần tử trong véc-tơ được gọi là chỉ số của phần tử đó. Phần tử đầu tiên trong một véc-tơ của R có chỉ số là 1. Bạn đọc lưu ý điều này bởi trong một vài ngôn ngữ khác chỉ số của phần tử đầu tiên trong véc-tơ sẽ là 0. Véc-tơ là đối tượng quan trọng nhất trong R và xử lý vec-tơ chính là một thế mạnh của R, giúp cho R có thể thực hiện được những phân tích mà nhiều ngôn ngữ khác không đáp ứng được.Khi bạn đọc làm việc với dữ liệu, các thao tác biến đổi hay phân tích dữ liệu thường sẽ là được thực hiện đồng thời trên các giá trị trên cùng một hàng hoặc một cột dữ liệu. Hiếm khi các thao tác này được thực hiện với một giá trị riêng lẻ. Đối tượng véc-tơ là một công cụ hiệu quả để thực hiện các công việc này. Hiệu quả ở đây không chỉ bao gồm sự tiện lợi khi viết các câu lệnh, mà còn hiệu quả ở cả thời gian thực hiện tính toán. Trong phần Lập trình với R, chúng tôi sẽ thảo luận kỹ hơn về hiệu quả về thời gian tính toán. Hãy nói về sự tiện lợi khi sử dụng véc-tơ trước. Chúng ta hãy lấy ví dụ khi thực hiện một phân tích trên dữ liệu có tên là trump_tweets bằng cách thực thi một đoạn lệnh sauDữ liệu trump_tweets trong thư viện dslabs là dữ liệu chứa 20.761 câu trạng thái trên các nền tảng mạng xã hội của cựu tổng thống Mỹ Donald Trump trong khoảng thời gian từ năm 2009 đến năm 2017. Đoạn câu lệnh trên thực hiện một phân tích cho biết cựu tổng thống Donald Trump có thói quen viết các câu trạng thái trên các nền tảng mạng xã hội vào các khoảng thời gian nào trong ngày. Kết quả này thu được bằng việc thực hiện 1 chuỗi các phép biến đổi và tính toán trên cột dữ liệu có tên là created_at. Các biến đổi và tính toán được liệt kê như sau:Lấy ra đoạn ký tự chứa giá trị là giờ của cột created_at. Được thực hiện bằng cách dùng hàm substr();Chuyển đổi dữ liệu kiểu chuỗi ký sang kiểu số sử dụng hàm .numeric();Chuyển đổi dữ liệu kiểu số sang kiểu factor sử dụng hàm .factor();Tổng hợp lại dữ liệu kiểu factor theo các nhóm bằng hàm table();Vẽ đồ thị kiểu Barplot để người đọc hiểu về dữ liệu một cách nhanh chóng và trực quan hơn.Để biến đổi từ cột dữ liệu created_at có kiểu .POSIXct() đến kết quả là đồ thị Barplot mà chỉ cần một dòng lệnh là việc gần như không thể đối với đa số các ngôn ngữ lập trình. Các ngôn ngữ lập trình cơ bản chỉ cho phép người sử dụng tác động đển từng phần tử của véc-tơ một cách lần lượt và riêng lẻ. Điều thú vị là khi bạn đọc thực hiện một phép biến đổi hay tính toán trên đối tượng là véc-tơ trong R, các phép tính toán hay biến đổi này sẽ được thực hiện một cách đồng thời cho tất cả các phần tử trong véc-tơ. Ngoài việc giúp cho các câu lệnh trở nên đơn giản, dể hiểu, R cũng được phát triển để những tính toán trên véc-tơ được thực hiện theo cơ chế song song. Cơ chế song song hiểu một cách đơn giản là việc thực hiện các phép toán trên các phần tử của một véc-tơ sẽ diễn ra cùng một lúc chứ không thực hiện một cách lần lượt. Việc này cho phép rút ngắn được thời gian tính toán.Hầu hết các hàm số trên R đều được phát triển theo cơ chế lập trình véc-tơ, nghĩa là các hàm số được dùng cho một biến kiểu số đều có thể áp dụng được cho một véc-tơ kiểu số, các hàm số được dùng cho một biến kiểu chuỗi ký tự đều có thể áp dụng được cho một véc-tơ kiểu chuỗi ký tự, tương tự với các véc-tơ kiểu logic hay kiểu ngày tháng. Trong ví dụ trên, cột (véc-tơ) created_at của dữ liệu trump_tweets là một véc-tơ kiểu thời gian chi tiết đã liên tục được biến đổi bằng các hàm số như substr(), .numeric(),… Các hàm này đều cho phép đầu vào là một véc-tơ và trả lại giá trị cũng là một véc-tơ có độ dài tương ứng.Ngoài việc thực hiện tính toán trên các véc-tơ riêng lẻ, cơ chế hoạt động của ngôn ngữ R cũng cho phép thực hiện tính toán tương tác giữa các véc-tơ với nhau. Tương tác giữa hai hay nhiều véc-tơ với nhau luôn được thực hiện trên nguyên tắc các phần tử có cùng chỉ số của các véc-tơ sẽ tương tác với nhau. Thậm chí các véc-tơ tương tác với nhau có thể không có cùng kích thước mà vẫn cho kết quả. Chi tiết sẽ được thảo luận trong các phần tiếp theo.","code":"\nlibrary(dslabs) # cần gọi thư viện dslabs chứa dữ liệu trump_tweets\nbarplot(table(as.factor(as.numeric(substr(trump_tweets$created_at,12,13)))),\n        main = \"Tổng thống Trump viết tweet vào thời gian nào trong ngày\",\n        col = \"white\", border = \"#640514\", xlabs = \"Giờ trong ngày\")"},{"path":"kiến-thức-r-cơ-bản.html","id":"khởi-tạo-véc-tơ-và-các-phép-toán-trên-véc-tơ","chapter":"Chương 3 Kiến thức R cơ bản","heading":"3.3.2 Khởi tạo véc-tơ và các phép toán trên véc-tơ","text":"","code":""},{"path":"kiến-thức-r-cơ-bản.html","id":"khởi-tạo-véc-tơ-bằng-các-hàm-có-sẵn","chapter":"Chương 3 Kiến thức R cơ bản","heading":"3.3.2.1 Khởi tạo véc-tơ bằng các hàm có sẵn","text":"Để khởi tạo một véc-tơ trong R, bạn đọc có thể sử dụng bất kỳ một hàm số có sẵn miễn là có đầu ra là một véc-tơ với kiểu giá trị phù hợp. Hàm số thông dụng nhất được dùng để tạo véc-tơ trong R là hàm c(); c là viết tắt của concatenate, hoặc một vài tài liệu cho rằng c là viết tắt của combine. Về mặt ý nghĩa, hàm c() tập hợp các đối tượng được liệt kê trong dấu ngoặc \\(()\\) lại để tạo thành một véc-tơ duy nhất. Nếu các phần tử được liệt kê ra có cùng kiểu dữ liệu, đối tượng tượng tạo thành sẽ là một véc-tơKhi các biến được liệt kê bên trong hàm c() không cùng kiểu, R sẽ cố gắng phân tích các giá trị đó để đưa ra kết quả phù hợp. Nguyên tắc chung là nếu các giá trị được liệt kê bên trong hàm c() bao gồm kiểu số, kiểu logic, và kiểu thời gian thì véc-tơ được tạo thành sẽ là véc-tơ kiểu số. Trong trường hợp có 1 biến được liệt kê ra là kiểu chuỗi ký tự, véc-tơ được tạo thành sẽ là véc-tơ kiểu chuỗi ký tự. Bạn đọc có thể kiểm tra giá trị của các véc-tơ sau:Các giá trị bên trong hàm c() cũng có thể là một véc-tơ khác, thậm chí có thể là một ma trận (matrix), hoặc là một đối tượng kiểu mảng (array). Giá trị đầu ra của hàm c() luôn luôn là một véc-tơ. Nếu là ma trận hoặc mảng hàm c() sẽ chuyển hóa các phần tử ra thành 1 véc-tơ theo thứ tự các cột bắt đầu từ cột có chỉ số 1. Chúng ta sẽ quay lại vấn đề này khi thảo luận về ma trận và mảng trong phần kiến thức R nâng cao.Như chúng tôi đã nói ở trên, bất kỳ hàm số sẵn có nào có đầu ra là một véc-tơ đều có thể dùng để tạo thành véc-tơ. Các hàm mà chúng tôi hay sử dụng để khởi tạo véc-tơ, ngoài hàm c(), còn có các hàm rep() và hàm seq().Hàm số rep() có thể được sử dụng để lặp đi lặp lại một biến, một véc-tơ nào đó nhiều lần. Cụ thể rep(x,n) với n là một số nguyên dương và x là một biến hoặc một véc-tơ sẽ cho kết quả là một véc-tơ được tạo thành bằng cách lặp lại giá trị x theo thứ tự từ trái sang phải n lần.Hàm số seq() được dành riêng cho véc-tơ kiểu số. Câu lệnh seq(= , = b,length = n) tạo thành một dãy số tăng dần (hoặc giảm dần) bắt đầu từ kết thúc tại b và véc-tơ có độ dài là n.Các hàm rep() và seq() được sử dụng như sau:Đầu ra của hàm seq() luôn là một véc-tơ kiểu số. Nếu không sử dụng tham số length, bạn đọc có thể sử dụng tham số là khoảng cách giữa hai số liên tiếp trong dãy số như ví dụ dưới đâyKhi thực hiện các phép tính toán tổng hợp các véc-tơ kiểu số, chúng ta thường quan tâm đến các giá trị như tổng, giá trị trung bình, tích, độ lệch chuẩn, hoặc quan tâm đến việc sắp xếp các giá trị trong véc-tơ đó. Các hàm số được liệt kê trong bảng ?? là các hàm số thường sử dụng để thực hiện các tính toán như vậy,Bạn đọc lưu ý rằng còn nhiều hàm số hữu ích khác được xây dựng sẵn khi tính toán với véc-tơ mà chúng tôi không liệt kê ở đây. Đồng thời, mỗi hàm số trong bảng kể trên còn có các tham số để có thể sử dụng trong các hoàn cảnh khác nhau. Chẳng hạn khi trong véc-tơ \\(x\\) có các giá trị không có ý nghĩa NaN hoặc không quan sát được NA thì các hàm như sum(x), mean(x), … sẽ không thực hiện tính toán được. Trong trường hợp này, bạn đọc cần sử dụng thêm tham số na.rm=TRUE để R hiểu rằng các phép tính toán chỉ thực hiện trên các giá trị có ý nghĩa.Cách tốt nhất để hiểu và sử dụng hiệu quả và đúng mục đích các hàm số liệt kê ở trên là đọc hướng dẫn của hàm số đó. Trong cuốn sách này chúng tôi chỉ nhấn mạnh những hàm số và tham số của nó mà chúng tôi cho rằng quan trọng khi ứng dụng các hàm số trong thực tế.Các hàm số sử dụng trên các véc-tơ kiểu số như sum(), mean(), hay thậm chí cả var() hay sd() đều có thể hoạt động trên véc-tơ kiểu thời gian hoặc kiểu logic. Nếu phép toán thực hiện không thể giữ nguyên kiểu dữ liệu của véc-tơ, R sẽ đổi véc-tơ kiểu thời gian và véc-tơ kiểu logic sang kiểu số để thực hiện tính toán như ví dụ dưới đây:Ngoài các nguyên tắc tính toán thông thường, bạn đọc thấy rằng R có thể sắp xếp thứ tự các phần tử trong một véc-tơ bất kỳ tăng dần hoặc giảm dần bằng hàm sort(). Hoặc R cũng có thể lấy ra giá trị lớn nhất hoặc nhỏ nhất của một véc-tơ bằng hàm max() hoặc hàm min(). Điều này là khá hiển nhiên với các véc-tơ kiểu số. Trong trường hợp véc-tơ là véc-tơ kiểu logic hay kiểu ngày tháng, R sẽ đổi giá trị của véc-tơ đó sang kiểu số để tiến hành sắp xếp hay tìm ra giá trị lớn nhất hoặc giá trị nhỏ nhất. Điều gì sẽ xảy ra nếu véc-tơ cần được sắp xếp, hay tìm giá trị lớn nhất và nhỏ nhất là véc-tơ kiểu chuỗi ký tự. Đây là một vấn đề phức tạp liên quan đến việc mã hóa các ký tự trên máy tính và vượt quá phạm vi của cuốn sách. Bạn đọc chỉ cần ghi nhớ các nguyên tắc sau khi sắp xếp một véc-tơ kiểu chuỗi ký tự:Nếu véc-tơ kiểu chuỗi ký tự được biến đổi thành kiểu factor thì thứ tự sắp xếp tăng dần sẽ phụ thuộc vào cách định nghĩa các mức độ (level) của véc-tơ kiểu factor.\nNếu véc-tơ kiểu chuỗi ký tự được biến đổi thành kiểu factor thì thứ tự sắp xếp tăng dần sẽ phụ thuộc vào cách định nghĩa các mức độ (level) của véc-tơ kiểu factor.Khi sánh hai chuỗi ký tự, phép sánh sẽ được thực hiện ở ký tự thứ nhất trước, nếu hai ký tự đầu tiên giống nhau thì sẽ sánh ký tự tiếp theo, và tiếp tục như thế đến khi có sự khác biệt.\nKhi sánh hai chuỗi ký tự, phép sánh sẽ được thực hiện ở ký tự thứ nhất trước, nếu hai ký tự đầu tiên giống nhau thì sẽ sánh ký tự tiếp theo, và tiếp tục như thế đến khi có sự khác biệt.Các ký tự đặc biệt luôn được xếp trước (nhỏ hơn), sau đó đến các ký tự là các số, rồi đến chữ cái. Thứ tự sắp xếp của các ký tự số theo đúng thứ tự tăng dần từ 0 đến 9 trong khi thứ tự sắp xếp của các chữ cái là tăng dần theo bảng chữ cái. Chữ viết thường được viết trước (nhỏ hơn) chữ viết hoa của chữ cái đó. Chữ viết hoa của chữ cái đứng trước lại “nhỏ hơn” chữ viết thường của chữ đứng sau trong bảng chữ cái.\nCác ký tự đặc biệt luôn được xếp trước (nhỏ hơn), sau đó đến các ký tự là các số, rồi đến chữ cái. Thứ tự sắp xếp của các ký tự số theo đúng thứ tự tăng dần từ 0 đến 9 trong khi thứ tự sắp xếp của các chữ cái là tăng dần theo bảng chữ cái. Chữ viết thường được viết trước (nhỏ hơn) chữ viết hoa của chữ cái đó. Chữ viết hoa của chữ cái đứng trước lại “nhỏ hơn” chữ viết thường của chữ đứng sau trong bảng chữ cái.Trước khi sử dụng R để ra kết quả, bạn đọc hãy thử đoán xem R sẽ cho kết quả như thế nào khi chạy các câu sắp xếp các véc-tơ sau theo thứ tự tăng dần:Hàm sort() nếu không sử dụng thêm tham số sẽ luôn sắp xếp véc-tơ theo thứ tự tăng dần. Để sắp xếp véc-tơ theo thứ tự giảm dần, bạn đọc có thể sử dụng thêm tham số decreasing = TRUE hoặc ngắn gọn hơn là decreasing = T trong hàm sort().","code":"\n# Khởi tạo x là một vec-tơ kiểu số\nx <- c(1,1,2,3,5,8,13,21)\n\n# Khởi tạo \"qua\" là vec-tơ chứa tên các loại quả\nqua = c(\"chuối\", \"táo\", \"cam\", \"chanh\")\n# Véc-tơ bao gồm kiểu số và logic\nx <- c(1,TRUE, FALSE)\nclass(x)## [1] \"numeric\"\n# Véc-tơ bao gồm kiểu logic và ngày tháng\nx <- c(TRUE, as.Date(\"2023-12-31\"))\n\n# Véc-tơ bao gồm kiểu số,logic,ngày tháng, và chuỗi ký tự\nx <- c(1, TRUE, as.Date(\"2023-12-31\"),\"MFE\")\nclass(x)## [1] \"character\"\n# Khai báo véc-tơ x\nx <- c(1, TRUE, as.Date(\"2023-12-31\"),\"MFE\")\n\n# Khai báo véc-tơ y chứa x\ny <- c(x,\"Actuary\",x) # dùng véc-tơ x trong khai báo véc-tơ y\n# Véc-tơ x kiểu số, các giá trị đều là 1, độ dài 1000\nx <- rep(1,10^3)\n\n# Véc-tơ y kiểu chuỗi ký tự, độ dài 200 vì lặp lại véc-tơ (\"a\",\"b\") 100 lần\ny <- rep(c(\"a\",\"b\"),100) #\n\n# z là cấp số cộng từ 0 đến 1, độ dài là 101\nz <- seq(from = 0,to = 1,length = 101)\n# z1 là cấp số cộng từ 0 đến 1, tăng dần 0.01\nz1 <- seq(from = 0,to = 1, 0.01)\n\n# z2 là cấp số trừ từ 1 về 0, giảm dần 0.01\nz2 <- seq(from = 1,to = 0, -0.01)\n# Khởi tạo véc-tơ x có chứa NA\nx <- c(rep(1,10),2,3,NA)\n\n# Hàm sum trả lại giá trị NA\nsum(x)## [1] NA\n# tham số na.rm = TRUE loại NA ra khỏi tổng\nsum(x,na.rm=TRUE)## [1] 15\n# Khởi tạo x là véc-tơ kiểu thời gian\nx <- c(as.Date(\"2023-01-01\"),as.Date(\"2023-12-31\"))\n\n# Hàm mean trên véc-tơ kiểu thời gian\nmean(x)## [1] \"2023-07-02\"\n# Hàm sd trên véc-tơ kiểu thời gian\nsd(x)## [1] 257.3869\n# Xếp véc-tơ kiểu ký tự tăng dần\nsort(c(\"a\",\"az\",\"z\")) # chữ cái đầu tiên để so sánh\n\n# Xếp chữ cái đứng trước trong bảng chữ cái nhỏ hơn\nsort(c(\"a\",\"az\",\"z\",\"A\",\"Z\"))\n\n# Xếp số đứng trước chữ cái\nsort(c(\"a\",\"az\",\"z\",\"A\",\"Z\",\"1a\"))\n\n# Xếp ký tự đặc biệt trước ký tự số\nsort(c(\"a\",\"az\",\"z\",\"A\",\"Z\",\"1a\",\"@a\"))\n\n# ký tự đặc biệt trước số, số trước chữ cái\nsort(c(\"a\",\"az\",\"z\",\"A\",\"Z\",\"1a\",\"@a\", \"0123\"))\n# Xếp véc-tơ theo thứ tự giảm dần\nsort(c(1,1,2,3,5,8,13,21), decreasing = TRUE)## [1] 21 13  8  5  3  2  1  1\n# Viết ngắn gọn TRUE bằng T\nsort(c(\"a\",\"az\",\"z\",\"A\",\"Z\",\"1a\",\"@a\", \"0123\"),decreasing = T)## [1] \"Z\"    \"z\"    \"az\"   \"A\"    \"a\"    \"1a\"   \"0123\" \"@a\""},{"path":"kiến-thức-r-cơ-bản.html","id":"tính-toán-trên-véc-tơ","chapter":"Chương 3 Kiến thức R cơ bản","heading":"3.3.2.2 Tính toán trên véc-tơ","text":"Như chúng tôi đã đề cập ở trên, R là ngôn ngữ lập trình véc-tơ, nghĩa là bạn đọc có thể sử dụng véc-tơ như một đối tượng trong các phép tính toán hoặc sánh mà không cần phải tác động đến từng phần tử riêng lẻ của véc-tơ đó. Đây chính là thế mạnh của R mà nhiều ngôn ngữ lập trình khác không thực hiện được.Trước hết, chúng ta có thể cho một véc-tơ x kiểu số vào trong các phép tính toán thông thường như cộng, trừ, nhân, chia, lũy thừa với các số thực. Kết quả thu được sẽ là một véc-tơ có độ dài bằng với véc-tơ ban đầu:Quan sát kết quả được ra, bạn đọc có thể nhận thấy rằng nguyên tắc R thực hiện phép tính nhân véc-tơ x với số 2, hay bất kỳ phép tính nào khác giữa véc-tơ x với một số, là lấy lần lượt các phần tử riêng lẻ trong véc-tơ x nhân lên 2 và tạo thành một véc-tơ mới theo đúng thứ tự như trong x. Tương tự như phép tính toán, phép sánh cũng có thể thực hiện giữa một véc-tơ với một biến để cho kết quả là một véc-tơ kiểu logic:Hầu hết các hàm số sẵn có trong R, hoặc các hàm số được phát triển trong các thư viện của R, đều có thể áp dụng trên đối tượng là véc-tơ và nguyên tắc áp dụng hàm số trên véc-tơ cũng tương tự như nguyên tắc tính toán giữa véc-tơ với một số. Việc thực hiện tính toán sẽ được thực hiện trên các phần tử riêng lẻ của véc-tơ và sau đó lưu lại trong một véc-tơ mới có độ dài bằng với véc-tơ ban đầu. Ví dụ như hàm nchar() cho biết một biến kiểu chuỗi ký tự có bao nhiêu ký tự. Khi sử dụng hàm nchar() với một véc-tơ kiểu chuỗi ký tự sẽ cho kết quả là một véc-tơ kiểu số mà mỗi phần tử là số ký tự của phần tử tương ứng trong véc-tơ kiểu chuỗi ký tựBằng cách kết hợp các hàm số trên véc-tơ và tương tác giữa véc-tơ với một biến, bạn đọc có thể tự tạo ra các hàm số, các phương pháp của riêng mình để giải quyết các vấn đề phức tạp hơn. Chẳng hạn như chúng ta muốn biết có bao nhiêu phần tử trong véc-tơ thỏa mãn một điều kiện nào đó, chúng ta có thể kết hợp hàm sum() với một biểu thức sánh giữa véc-tơ với một số:Khi thực hiện phép sánh x > 10 , x là một véc-tơ kiểu số nên phép sánh sẽ trả lại giá trị TRUE tại các vị trí mà kết quả sánh là đúng và trả lại giá trị FALSE tại các vị trí còn lại. Khi kết hợp với hàm sum(), các giá trị TRUE sẽ được đổi thành số 1 và FALSE được đổi thành 0. Kết quả thu được sẽ là số lượng các giá trị TRUE trong phép sánh, hay nói một cách khác, là số các phần tử trong x thỏa mãn điều kiện lớn hơn 10.Tất nhiên với véc-tơ x có độ dài nhỏ như ở trên, bạn đọc có thể nhìn được một cách trực quan mà không cần hỗ trợ của máy tính. Nhưng thực tế thì các véc-tơ mà chúng ta cần thực hiện tính toán trên thực tế sẽ có độ dài lớn hơn rất nhiều và bạn đọc không thể tính toán trực quan như với véc-tơ x kể trên. Chẳng hạn như bạn muốn biết có bao nhiêu câu đăng trạng thái của cựu tổng thống Donald Trump mà có nhiều hơn 10.000 lượt yêu thích. Bạn có thể kết hợp sum() với biểu thức sánh để trả lời câu hỏi này. Lưu ý rằng véc-tơ chứa số lượt yêu thích với mỗi câu đăng trạng thái là cột favorite_count trong dữ liệu:Thay vì tính số tổng, bạn đọc cũng có thể tính tỷ lệ số câu đăng trạng thái có số lượt yêu thích nhiều hơn 10.000 bằng cách kết hợp thêm với hàm length() :Có rất nhiều cách kết hợp các hàm số trong bảng ?? lại để đạt được kết quả mong muốn. Một kết quả phân tích có thể đạt được bằng các cách kết hợp khác nhau. Để sử dụng thành thạo các hàm này chỉ có một cách duy nhất là hãy thực hành nhiều trên R và tự đúc kết kinh nghiệm cho chính mình!","code":"\n# Khởi tạo véc-tơ kiểu số x\nx <- 1:5\n\n# Nhân véc-tơ x với một số\nx * 2## [1]  2  4  6  8 10\n# Phép lũy thừa trên đối tượng véc-tơ\nx ^ 2## [1]  1  4  9 16 25\n# Phép lấy phần dư trên đối tượng véc-tơ\nx %% 2## [1] 1 0 1 0 1\n# Khởi tạo véc-tơ kiểu số x\nx <- c(1,1,2,3,5,8,13,21)\n\n# So sánh véc-tơ x với một số, kết quả là véc-tơ logic\nx == 1## [1]  TRUE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE\n# Kết hợp các véc-tơ logic bằng toán tử logic\n(x > 10) | (x < 3)## [1]  TRUE  TRUE  TRUE FALSE FALSE FALSE  TRUE  TRUE\n# Khởi tạo véc-tơ s kiểu chuỗi kỹ tự\ns <- c(\"a\",\"az\",\"z\",\"A\",\"Z\",\"1a\",\"@a\", \"0123\")\n\n# So sánh s với một biến kiểu chuỗi ký tự\ns == \"a\"## [1]  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n# Khởi tạo véc-tơ s kiểu chuỗi kỹ tự\ns <- c(\"a\",\"az\",\"z\",\"A\",\"Z\",\"1a\",\"@a\", \"0123\")\n\n# Áp dụng hàm nchar trên véc-tơ s\nnchar(s)## [1] 1 2 1 1 1 2 2 4\n# Khởi tạo véc-tơ x kiểu số\nx <- c(1,1,2,3,5,8,13,21)\n\n# Đếm xem có phần tử trong x lớn hơn 10\nsum(x > 10)## [1] 2\n# Tính tỷ lệ số phần tử lớn hơn 10\nsum(x > 10)/length(x)## [1] 0.25\n# Khởi tạo véc-tơ x chứa số lượt yêu thích\nx <- trump_tweets$favorite_count\n\n# Đếm xem có bao nhiêu phần tử trong x lớn hơn 10^4\nsum(x>10^4)## [1] 4958\n# Tính tỷ lệ phần tử trong x lớn hơn 10^4\nsum(x>10^4)/length(x)## [1] 0.2388132"},{"path":"kiến-thức-r-cơ-bản.html","id":"lấy-các-thành-phần-con-từ-một-véc-tơ","chapter":"Chương 3 Kiến thức R cơ bản","heading":"3.3.3 Lấy các thành phần con từ một véc-tơ","text":"Khi làm việc với véc-tơ, chúng ta thường phải lấy các phần tử của véc-tơ ra theo một thứ tự hoặc lấy các phần tử con thỏa mãn các điều kiện nào đó và lưu kết quả vào một véc-tơ mới. Các kỹ thuật này liên quan đến chỉ số của các phần tử trong véc-tơ và sẽ được thảo luận trong phần dưới đây.","code":""},{"path":"kiến-thức-r-cơ-bản.html","id":"hai-cách-lấy-véc-tơ-con-từ-một-véc-tơ","chapter":"Chương 3 Kiến thức R cơ bản","heading":"3.3.3.1 Hai cách lấy véc-tơ con từ một véc-tơ","text":"Để lấy một phần tử con của một véc-tơ x chúng ta sử dụng dấu ngoặc vuông [] . Chẳng hạn như để lấy ra phần tử thứ nhất của véc-tơ, chúng ta sử dụng x[1] . Số 1 nằm trong dấu ngoặc vuông được gọi là chỉ số. Nhắc lại với bạn đọc rằng chỉ số của các phần tử của một véc-tơ trong R bắt đầu từ 1 và phần tử cuối cùng trong véc-tơ có chỉ số bằng với độ dài của véc-tơ đó. Nếu chúng ta sử dụng chỉ số lớn hơn độ dài của véc-tơ, R sẽ trả lại giá trị là NA.Bạn đọc có thể đặt câu hỏi là điều gì xảy ra nều sử dụng chỉ số 0 hoặc chỉ số là số âm. Hãy nói về chỉ số 0 trước; khi gọi phần tử ở vị trí thứ 0 trong một véc-tơ bất kỳ bạn sẽ nhận được một phần tử rỗng. Khái niệm rỗng có thể hiểu giống như khái niệm rỗng khi nói về một tập hợp không có phần tử. Tùy theo kiểu giá trị của véc-tơ ta sẽ có một phần tử rỗng với kiểu giá trị tương ứng như bảng ??Khi sử dụng chỉ số âm đối với véc-tơ, R hiểu rằng chúng ta đang loại đi các phần tử. Thật vậy, x[-1] sẽ trả lại kết quả là một véc-tơ giống với véc-tơ x sau khi loại đi phần tử thứ nhất. Với số tự nhiên \\(k, (k \\\\mathbb{N}),\\), câu lệnh x[-k] sẽ trả lại kết quả là véc-tơ x sau khi loại đi phần tử thứ \\(k\\). Nếu \\(k\\) lớn hơn độ dài của véc-tơ x, véc-tơ nhận được sẽ vẫn là x. Sử dụng chỉ số âm cũng có thể hiểu là một cách để lấy một véc-tơ con từ một véc-tơ ban đầu bằng cách sử dụng véc-tơ chỉ số kiểu số nguyên.Ngoài cách lấy một véc-tơ con từ một véc-tơ bằng chỉ số kiểu số nguyên, chúng ta cũng có thể lấy một véc-tơ con từ một véc-tơ bằng cách sử dụng chỉ số là một véc-tơ kiểu logic. Hai cách lấy véc-tơ con sẽ được thảo luận ngay dưới đây.Trước hết, từ véc-tơ x ban đầu, để lấy ra một véc-tơ con, trong trường hợp chúng ta đã biết chính xác các vị trí và thứ tự của các phần tử con mà chúng ta muốn lấy ra, chúng ta có thể lưu vị trí, hay chỉ số của các phần tử con này vào một véc-tơ tạm gọi là véc-tơ y. Véc-tơ y còn được gọi là véc-tơ chỉ số. Sau đó, chúng ta chỉ cần sử dụng câu lệnh x[y] để lấy ra các phần tử của x tại các vị trí được lưu ở véc-tơ y. Thật vậy, hãy thử quan sát ví dụ sauNếu trong véc-tơ chỉ số có giá trị lớn hơn độ dài của véc-tơ ban đầu, R sẽ trả lại giá trị là NA tại vị trí đóNếu chúng ta sử dụng véc-tơ chỉ số là số âm, R sẽ hiểu rằng chúng ta đang muốn loại đi một hay một số phần tử nào đó.Bạn đọc cần lưu ý là R sẽ báo lỗi nếu véc-tơ chỉ số y chứa cả số âm và số dương. Trong thực tế thì hiếm khi chúng ta biết chính xác vị trí mà chúng ta muốn lấy ra, hay nói cách khác chúng ta không thể trực tiếp khai báo giá trị cho véc-tơ chỉ số y giống như ví dụ trên. Thông thường véc-tơ chỉ số y sẽ là kết quả của các hàm số tạo chỉ số. Các hàm () và hàm match() được thảo luận ở phần tiếp theo của chương là các phương pháp tuyệt vời để tạo ra các véc-tơ chỉ số kiểu số như vậy.Phương pháp thứ hai để lấy một véc-tơ con từ véc-tơ x đó là sử dụng véc-tơ chỉ số kiểu logic. Cách lấy này sẽ rất thuận tiện khi bạn đọc muốn lấy ra một véc-tơ con của x bao gồm các phần tử thỏa mãn một điều kiện nào đó. Véc-tơ chỉ số, tạm gọi tên là véc-tơ y, được tạo ra từ một phép sánh, sau đó câu lệnh x[y] sẽ trả lại giá trị là một véc-tơ con của x bao gồm các phần tử mà vị trí tương ứng của nó trong véc-tơ y là TRUE. Lấy véc-tơ con bằng cách này, bạn đọc hãy luôn để độ dài của véc-tơ y bằng độ dài của véc-tơ x. Khi độ dài của y không bằng độ dài của x, câu lệnh x[y] vẫn trả lại kết quả, tuy nhiên hiểu được kết quả là khá phức tạp. đó chúng tôi khuyên bạn đọc hãy luôn đảm bảo rằng véc-tơ chỉ số kiểu logic và véc-tơ ban đầu luôn có cùng độ dài.Giả sử với véc-tơ x[y] chứa tên các loại quả, chúng ta muốn lấy ra tên các loại quả có tên dài hơn 3 ký tự. Chúng ta không biết chính xác các quả này nằm ở vị trí nào trong x[y] nên không thể tạo véc-tơ chỉ số kiểu số. Trong trường hợp này, chúng ta sẽ tạo một véc-tơ chỉ số y kiểu logic như sauĐây là cách lấy ra véc-tơ con rất hiệu quả khi làm việc với dữ liệu. Các cột dữ liệu là các véc-tơ có cùng độ dài, đó chỉ số y có thể được tạo thành từ phép sánh một cột dữ liệu trong khi véc-tơ x cần lấy các phần tử con lại là một cột dữ liệu khác. Chẳng hạn như chúng ta muốn lấy ra các câu đăng trạng thái của cựu tổng thống Donald Trump có nhiều hơn 10.000 lần like và sau đó lưu vào một véc-tơ mới có tên là z, chúng ta chỉ cần thực hiện như sau:Điều gì xảy ra nếu độ dài của y không giống như độ dài của x. Trong trường hợp y có độ dài nhỏ hơn độ dài của x, R sẽ tạo ra một véc-tơ, tạm gọi là y1 có độ dài bằng với độ dài của x bằng cách lặp lại giá trị của y cho đến khi véc-tơ thu được có độ dài bằng x. Khi chúng ta gọi câu lệnh x[y], R sẽ lấy chỉ số là véc-tơ y1. Hãy quan sát ví dụ sauKết quả thu được tương tự như khi chúng ta thực hiện phép lấy véc-tơ con thông qua một véc-tơ chỉ số y1 có độ dài bằng 5 như sauNếu độ dài của véc-tơ chỉ số y lớn hơn độ dài của x, tại các vị trí của y mà chỉ số vẫn nhỏ hơn hoặc bằng chiều dài của x, việc lấy ra phần tử con vẫn theo quy tắc thông thường, nghĩa là lấy ra các phần tử tương ứng với giá trị TRUE và bỏ qua các phần tử tương ứng với giá trị FALSE. Tại các vị trí của y mà chỉ số lớn hơn chiều dài của x, R sẽ bỏ qua các phần tử có giá trị là FALSE và sẽ trả lại giá trị là NA mỗi khi gặp giá trị TRUE. Bạn đọc có thể quan sát ví dụ sauDo sự phức tạp khi tương tác giữa các véc-tơ có không cùng độ dài nên chúng tôi khuyên bạn đọc hãy luôn luôn thực hiện các phép tính toán với các véc-tơ có cùng độ dài để kiểm soát được kết quả khi làm việc với R. Trong phần tiếp theo chúng ta sẽ thảo luận về các hàm số để tạo ra véc-tơ chỉ số.","code":"\n# Khởi tạo véc-tơ x kiểu số\nx <- c(1,1,2,3,5,8,13,21)\n\n# Lấy ra phần tử thứ nhất trong x\nx[1]## [1] 1\n# Lấy ra phần tử có chỉ số lớn hơn độ dài\nx[length(x)+1]## [1] NA\n# Khởi tạo véc-tơ kiểu chuỗi ký tự\nx <- c(\"cam\",\"táo\",\"kiwi\",\"chuối\",\"nho\")\n\n# Tạo véc-tơ chỉ số y tại các vị trí muốn lấy\ny <- c(3,5,2,3,1)\n\n# Lấy ra các phần tử con của x theo chỉ số y\nx[y] # thứ thự là x[3] -> x[5] -> x[2] -> x[3] -> x[1]## [1] \"kiwi\" \"nho\"  \"táo\"  \"kiwi\" \"cam\"\n# Khởi tạo véc-tơ kiểu chuỗi ký tự\nx <- c(\"cam\",\"táo\",\"kiwi\",\"chuối\",\"nho\")\n\n# Tạo véc-tơ chỉ số y tại các vị trí muốn lấy\ny <- c(3,5,2,10,3,1) # Có chỉ số 10 lớn hơn độ dài của x\n\n# Lấy ra các phần tử con của x theo chỉ số y\nx[y] # Vị trí thứ tư (chỉ số 10) là NA## [1] \"kiwi\" \"nho\"  \"táo\"  NA     \"kiwi\" \"cam\"\n# Khởi tạo véc-tơ kiểu chuỗi ký tự\nx <- c(\"cam\",\"táo\",\"kiwi\",\"chuối\",\"nho\")\n\n# Tạo véc-tơ chỉ số y gồm các chỉ số âm\ny <- c(-3,-5,-2,-3)\n\n# Lọc ra các phần tử con của x theo chỉ số y\nx[y] # loại đi phần tử thứ 2, 3, 5 của x (chỉ còn x[1] rồi x[4])## [1] \"cam\"   \"chuối\"\n# Khởi tạo véc-tơ kiểu chuỗi ký tự\nx <- c(\"cam\",\"táo\",\"kiwi\",\"chuối\",\"nho\")\n\n# Tạo véc-tơ chỉ số y kiểu logic\ny <- (nchar(x)>3) # y có giá trị TRUE tại vị trí phần tử có độ dài > 3\n\n# Hiển thị giá trị của y\nprint(y)## [1] FALSE FALSE  TRUE  TRUE FALSE\n# Lấy phần tử con của x theo chỉ số y\nx[y] # Trả lại phần tử trong x có độ dài > 3## [1] \"kiwi\"  \"chuối\"\n# Khởi tạo véc-tơ x chứa các câu status\nx <- trump_tweets$text\n\n# Tạo véc-tơ chỉ số kiểu logic y\ny <- trump_tweets$favorite_count > 10^4\n\n# Lấy ra các câu status có nhiều hơn 10.000 like\nz <- x[y]\n# Khởi tạo véc-tơ kiểu chuỗi ký tự độ dài 5\nx <- c(\"cam\",\"táo\",\"kiwi\",\"chuối\",\"nho\")\n\n# Khởi tạo véc-tơ chỉ số độ dài 2\ny <- c(TRUE,FALSE) # y có độ dài là 2\n\n# Lấy chỉ số của x theo y\nx[y] # Là véc-tơ có độ dài 5## [1] \"cam\"  \"kiwi\" \"nho\"\n# Tạo véc-tơ chỉ số độ dài 6\ny1<-rep(y,3) # lặp lại y cho đến khi có độ dài lớn hơn x\n\n# Cho độ dài của chỉ số bằng độ dài của x\ny1<-y1[1:length(x)]\n\n# Lấy véc-tơ con theo chỉ số y1\nx[y1] # cho kết quả giống như x[y]## [1] \"cam\"  \"kiwi\" \"nho\"\n# Khởi tạo véc-tơ kiểu chuỗi ký tự độ dài 5\nx <- c(\"cam\",\"táo\",\"kiwi\",\"chuối\",\"nho\")\n\n# Khởi tạo véc-tơ chỉ số độ dài 7\ny <- c(nchar(x)>3,FALSE,TRUE) # chỉ số thứ 6 là FALSE, thứ 7 là TRUE\n\n# Lấy véc-tơ con của x theo chỉ số y\nx[y]## [1] \"kiwi\"  \"chuối\" NA"},{"path":"kiến-thức-r-cơ-bản.html","id":"các-hàm-tạo-chỉ-số","chapter":"Chương 3 Kiến thức R cơ bản","heading":"3.3.4 Các hàm tạo chỉ số","text":"Có một nhóm các hàm số thường được sử dụng khi làm việc với chỉ số của các phần tử trong véc-tơ. Các hàm số này có thể được phỏng theo bằng cách kết hợp một vài kỹ thuật chỉ số đã đề cập đến ở chương trước. Tuy nhiên chúng tôi khuyên bạn đọc nên sử dụng các hàm có sẵn được trình bày trong phần này bởi sự tiện lợi và sự dễ hiểu của các dòng lệnh. Các hàm số liên quan đến chỉ số của véc-tơ được liệt kê trong bảng ??","code":""},{"path":"kiến-thức-r-cơ-bản.html","id":"hàm-which","chapter":"Chương 3 Kiến thức R cơ bản","heading":"3.3.4.1 Hàm which()","text":"Hàm () áp dụng trên một véc-tơ kiểu logic và cho biết các vị trí nào trong véc-tơ có giá trị là TRUE. Có hai biến thể của hàm () thường được sử dụng là .min() và .max() cho biết chỉ số của giá trị lớn nhất và chỉ số của giá trị nhỏ nhất.Trong trường hợp x có nhiều giá trị bằng với giá trị lớn nhất, hoặc có nhiều giá trị bằng với giá trị nhỏ nhất, các hàm .min() và .max() luôn luôn trả lại giá trị là chỉ số nhỏ hơn.Bạn đọc sử dụng hàm () để tạo ra véc-tơ chỉ số khi muốn lấy ra các phần tử của một véc-tơ thỏa mãn một điều kiện nào đó. Ví dụ như chúng ta muốn lấy ra các các câu đăng trạng thái của cựu tổng thống Donald Trum có nhiểu hơn 10.000 lượt yêu thích bằng một véc-tơ chỉ số:","code":"\n# Khởi tạo véc-tơ x kiểu số\nx <- c(20,40,60,50,30,10)\n\n# Các chỉ số trong véc-tơ x có giá trị > 40\nwhich(x>40)## [1] 3 4\n# Chỉ số của số nhỏ nhất\nwhich.min(x)## [1] 6\n# Chỉ số của số lớn nhất\nwhich.max(x)## [1] 3\n# Khởi tạo véc-tơ x kiểu số\nx <- c(20,40,60,50,30,10,60,10)\n\n# Số nhỏ nhất nằm ở vị trí nào đầu tiên\nwhich.min(x)## [1] 6\n# Số lớn nhất nằm ở vị trí nào đầu tiên\nwhich.max(x)## [1] 3\n# Tạo véc-tơ chứa tất cả các câu tweet\nx <- trump_tweets$text\n\n# Tạo chỉ số kiểu số cho các câu nhiều hơn 10.000 like\ny <- which(trump_tweets$favorite_count>10^4)\n\n# Lấy ra véc-tơ con của x theo chỉ số y\nz <- x[y] # z chứa các câu tweet nhiều hơn 10.000 like"},{"path":"kiến-thức-r-cơ-bản.html","id":"hàm-match-và-toán-tử-in","chapter":"Chương 3 Kiến thức R cơ bản","heading":"3.3.4.2 Hàm match() và toán tử %in%","text":"Hàm match() là hàm số cho phép tương tác giữa hai véc-tơ có độ dài khác nhau. Với x và y là hai véc-tơ có cùng kiểu biến, câu lệnh z <- match(y,x) sẽ trả lại giá trị là một véc-tơ, tạm gọi là z có các tính chất sau đâyCó độ dài bằng với độ dài của véc-tơ yGiá trị z[] cho biết y[] có chỉ số, hay nằm ở vị trí nào, trong véc-tơ x; ví dụ, z[1] bằng 3 nghĩa là y[1] nằm ở vị trí, hay chỉ số thứ 3, trong véc-tơ xGiá trị z[] bằng NA có nghĩa là y[] không xuất hiện trong x.Bạn đọc quan sát ví dụ về hàm match() như sau:Chúng ta có thể thấy rằng giá trị 70 không xuất hiện trong x nên giá trị thứ 3 trong véc-tơ kết quả là NA. Lưu ý rằng hàm match() luôn luôn tìm đến chỉ số đầu tiên trong véc-tơ x có giá trị khớp với giá trị của véc-tơ y, nghĩa là nếu trong x có nhiều hơn một giá trị khớp với giá trị của y, hàmmatch()cho kết quả là chỉ số nhỏ hơn. Bạn đọc quan sát ví dụ dưới đây khi véc-tơ y có các phần tử xuất hiện nhiều hơn một lần trong x:Các giá trị 10 và 20 của y xuất hiện hai lần trong x, tuy nhiên hàmmatch()sẽ trả lại giá trị là 6 và 1 bởi vì số 10 xuất hiện lần đầu tiên ở vị trí thứ 6 trong x và số 20 xuất hiện lần đầu tiên ở vị trí thứ 1 trong x.Hàm match() trả lại kết quả là véc-tơ chỉ số nên sẽ phù hợp với việc lấy véc-tơ con theo chỉ số kiểu số. Một phương pháp khác để tạo chỉ số của véc-tơ là toán tử %%. Toán tử %% được sử dụng để cho biết mỗi phần tử của một véc-tơ có nằm trong một véc-tơ khác hay không. Câu lệnh y %% x sẽ trả lại giá trị là một véc-tơ kiểu logic, tạm gọi là z, có độ dài bằng với độ dài của y, đồng thời z[] nhận giá trị là TRUE nếu y[] có xuất hiện trong x, và nhận giá trị là FALSE nếu y[] không xuất hiện trong x. Bạn đọc hãy quan sát ví dụ dưới đây:Những bạn đọc mới làm quen với R thường dễ nhầm lẫn cách sử dụng hàm match() và toán tử %%. Cách duy nhất để hiểu và thực hiện thành thạo đó là thực hành trên dữ liệu thực tế nhiều lần. Hình 3.2 dưới đây minh họa sự khác nhau trong kết quả được trả ra của hàm match()và toán tử %%\nHình 3.2: Sự khác nhau giữa hàm match() và toán tử %%\nHàm match() và toán tử %% cho phép tương tác giữa các véc-tơ có độ dài khác nhau nên rất hiệu quả khi bạn đọc muốn kết nối nhiều dữ liệu khác nhau. Bạn đọc hãy đọc ví dụ dưới đây để hình dung cách sử dụng hàmmatch()khi kết nối hai dữ liệu.Giả sử chúng ta có danh sách điểm học tại trường đại học của ba sinh viên ngành actuary có mã sinh viên lần lượt là MSV001, MSV002, MSV003 khi học các môn học Xác suất, Toán tài chính, và Đầu tư và thị trường tài chính. Thông tin được lưu trong một dữ liệu tên là diem_hoc_DH. Sinh viên ngành actuary ngoài các môn học ở trường đại học có thể thi các môn học tại các hiệp hội nghề nghiệp actuary để lấy chứng chỉ hành nghề. Thông tin về điểm thi chứng chỉ được lưu trong dữ liệu có tên là diem_chung_chi_Actuary. Khi xét tốt nghiệp, sinh viên có quyền lấy điểm thi chứng chỉ tại các hiệp hội để thay thế cho điểm học tại trường đại học của môn học tương ứng nếu điểm thi chứng chỉ cao hơn. Dữ liệu về điểm thi tại trường đại học và thi chứng chỉ hành nghề như sau:Để tìm được điểm thi chứng chỉ của sinh viên trong bảng diem_hoc_DH chúng ta phải kết nối bảng này với bảng “diem_chung_chi_Actuary” thông qua mã sinh viên và tên môn học bằng cách sử dụng hàm match(). Việc kết nối sẽ được thực hiện bằng cách tạo ra trên mỗi bảng một biến có tên là key là tổ hợp của mã sinh viên và tên môn học.Trước hết bạn đọc có thể tạo hai dữ liệu dựa trên thông tin của hai bảng bằng đoạn lệnh như dưới đây:Chúng ta tạo ra biến key trêm hai bảng để kết nối hai bảng; véc-tơ tạo ra bằng cách kết hợp từ véc-tơ chứa mã sinh viên và véc-tơ tên môn họcToán tử %% sẽ cho chúng ta biết những phần tử nào trong véc-tơ diem_hoc_DH_key nằm trong véc-tơ diem_chung_chi_Actuary_key hay nói một cách khác, sinh viên nào trong bảng diem_hoc_DH có thi chứng chỉ tương ứng với môn học ở trường đại học:Chỉ số y là kết quả của toán tử %% nên sẽ là véc-tơ kiểu logic. y có độ dài là 9 bằng với số quan sát của dữ liệu diem_hoc_DH và cho biết tương ứng mỗi sinh viên có thi chứng chỉ môn học tương ứng hay không. Chẳng hạn như muốn tạo ra danh sách thi chứng chỉ của sinh viên lớp Actuary chúng ta sử dụng cách lấy véc-tơ con với chỉ số y như sauĐể tìm được điểm thi chứng chỉ của các sinh viên lớp Actuary chúng ta cần biết kết nối mã sinh viên và môn học từ bảng diem_hoc_DH đến bảng diem_chung_chi_Actuary bằng cách sử dụng hàm match()Véc-tơ y có độ dài bằng 9, cho biết mỗi quan sát của dữ liệu diem_hoc_DH tương ứng với dòng thứ bao nhiêu (chỉ số) của dữ liệu diem_chung_chi_Actuary. Giá trị NA trong y có ý nghĩa là quan sát đó của dữ liệu diem_hoc_DH không xuất hiện trong diem_chung_chi_Actuary, hay nói cách khác sinh viên không thi chứng chỉ môn học tương ứng. Chúng ta có thể thêm một cột (véc-tơ) gọi là diem_CT cho bảng diem_hoc_DHNhư vậy chúng ta đã có một dữ liệu với điểm học trên lớp và điểm thi chứng chỉ của các sinh viên","code":"\n# Khởi tạo véc-tơ x và véc-tơ y kiểu số\nx <- c(20,40,60,50,30,10)\ny <- c(60,10,70)\n\n# Chỉ số của các phần tử của y trong véc-tơ x\nmatch(y,x)## [1]  3  6 NA\n# Khởi tạo véc-tơ x và véc-tơ y kiểu số\nx<-c(20,40,60,50,30,10,20,10) # 10 và 20 xuất hiện nhiều lần\ny<-c(10,20) # các phần tử của y xuất hiện nhiều lần trong x\n\n# Chỉ số của các phần tử của y trong véc-tơ x\nmatch(y,x)## [1] 6 1\n# Khởi tạo véc-tơ x và véc-tơ y kiểu số\nx <- c(20,40,60,50,30,10)\ny <- c(60,10,70)\n\n# Cho biết các phần tử của y có nằm trong x hay không\ny %in% x## [1]  TRUE  TRUE FALSE\n# du lieu diem_hoc_DH\nMSV <- rep(c( \"MSV001\", \"MSV002\", \"MSV003\"),3)\nMon_hoc <- c(rep(\"Xác suất\",3),rep(\"Toán tài chính\",3),rep(\"Đầu tư và thị trường tài chính\",3))\nDiem <- c(5,7,9,10,6,8,9,5,10)\ndiem_hoc_DH <- data.frame(MSV, Mon_hoc, Diem)\n\n# du lieu diem_chung_chi_Actuary\nMSV <- c(\"MSV005\", \"MSV002\", \"MSV004\", \"MSV003\", \"MSV002\", \"MSV001\")\nMon_hoc <- c(\"Xác suất\", \"Xác suất\", \"Xác suất\", \"Toán tài chính\", \"Toán tài chính\", \"Đầu tư và thị trường tài chính\")\nDiem <- c(8,9,10,10,9,8)\ndiem_chung_chi_Actuary <- data.frame(MSV, Mon_hoc, Diem)\ndiem_hoc_DH_key<- paste(diem_hoc_DH$MSV, diem_hoc_DH$Mon_hoc)\ndiem_chung_chi_Actuary_key<-paste(diem_chung_chi_Actuary$MSV, diem_chung_chi_Actuary$Mon_hoc)\ny<-diem_hoc_DH_key %in% diem_chung_chi_Actuary_key\n# Lọc véc-tơ cột MSV bằng véc-tơ kiểu logic y\n# Lọc véc-tơ cột tên môn học bằng véc-tơ kiểu logic y\ndata.frame(MSV = diem_hoc_DH$MSV[y],\n           Diem = diem_hoc_DH$Mon_hoc[y])##      MSV                           Diem\n## 1 MSV002                       Xác suất\n## 2 MSV002                 Toán tài chính\n## 3 MSV003                 Toán tài chính\n## 4 MSV001 Đầu tư và thị trường tài chính\ny<-match(diem_hoc_DH_key,diem_chung_chi_Actuary_key)\n# Lọc véc-tơ con bằng véc-tơ chỉ số y kiểu số\ndiem_hoc_DH$diem_CT<-diem_chung_chi_Actuary$Diem[y]"},{"path":"kiến-thức-r-cơ-bản.html","id":"hàm-rank-và-hàm-order","chapter":"Chương 3 Kiến thức R cơ bản","heading":"3.3.4.3 Hàm rank() và hàm order()","text":"Hàm rank() trả lại giá trị là thứ tự (rank) của một phần tử trong véc-tơ x khi sắp xếp x theo thứ tự tăng dần. Thứ tự tăng dần ở đây được hiểu đối với cả các véc-tơ kiểu chuỗi ký tự. Bạn đọc có thể quan sát kết quả của hàm rank() trong ví dụ dưới đây:Lưu ý rằng hàm rank() có một tham số quan trọng là ties.method. Khi bạn đọc không sử dụng tùy chọn này, giá trị mặc định là average. Tham số ties.method chỉ có ý nghĩa khi trong véc-tơ x có các giá trị giống nhau. Nếu các phần tử trong x là đôi một khác nhau, bất kỳ tùy chọn nào đối với ties.method cũng trả lại một kết quả duy nhất.Khi x có các giá trị bị lặp lại, các giá trị khác nhau của ties.method sẽ cho kết quả khác nhau. Bạn đọc hãy quan sát ví dụ sau để thấy sự khác biệt:Khi ties.method nhận giá trị là first, giá trị trả lại là 1, 2, 3, 4, 5. Ba giá trị 10 liền nhau ở phần đầu của véc-tơ x được xếp thứ tự theo nguyên tắc số nào xuất hiện trước là có thứ tự nhỏ hơn, đó thứ tự của ba giá trị trong véc-tơ x khi xếp x theo thứ tự tăng dần là 1, 2, 3. Tương tự với hai giá trị 20 ở cuối vec-tơ x, số 20 xuất hiện trước được hiểu là có thứ tự trước số 20 xuất hiện sau, đó thứ tự của hai giá trị 20 sẽ là 4, 5Khi ties.method nhận giá trị là last giá trị trả lại là 3, 2, 1, 5, 4. Ba giá trị 10 liền nhau ở phần đầu của véc-tơ x được xếp thứ tự theo nguyên tắc số nào xuất hiện trước là có thử tự lớn hơn, đó thứ tự của ba số 10 này trong véc-tơ x khi xếp x theo thứ tự tăng dần là 3, 2, 1. Tương tự với hai số 20 ở cuối vec-tơ x, số 20 xuất hiện trước được hiểu là có thứ tự lớn hơn số 20 xuất hiện sau, đó thứ tự của hai số 20 sẽ là 5, 4Khi ties.method nhận giá trị là min, giá trị trả lại là 1, 1, 1, 4, 4. Ba số 10 liền nhau ở phần đầu của véc-tơ x có thứ tự bằng nhau là 1. Đây chính là thứ tự nhỏ nhất của ba số khi xếp các số này theo tùy chọn ties.method nhận giá trị là first. Tương tự ta có thứ tự của hai số 20 tiếp theo bằng nhau và bằng 4.Tham số ties.method nhận giá trị max sẽ cho kết quả ngược lại với min. Thứ tự của ba số 10 đầu tiên trong x đều bằng 3 - là số lớn nhất trong (1, 2, 3) đồng thời thứ tự của hai số 20 tiếp theo đều là 5 - là số lớn nhất trong (4,5).Khi ties.method nhận giá trị là average, cũng là giá trị mặc định khi sử dụng hàm rank(), thứ tự của ba số 10 ở đầu véc-tơ x được tính là trung bình của thứ tự khi xếp theo giá trị first. Thứ tự của ba giá trị 10 khi ties.method nhận giá trị là first là 1, 2, 3. Thứ tự khi ties.method nhận giá trị là average là\n\\[\\begin{align}\n\\cfrac{1 + 2 + 3}{3} = 2\n\\end{align}\\]\nvà thứ tự của hai số 20 ở cuối véc-tơ là\\[\\begin{align}\n\\cfrac{4 + 5}{2} = 4.5\n\\end{align}\\]Cuối cùng, khi ties.method nhận giá trị là random, thứ tự của ba số 10 ở đầu véc-tơ x là một hoán vị ngẫu nhiên của (1,2,3) cũng là thứ tự của ba số khi ties.method nhận giá trị là first. Bạn đọc có thể thấy rằng hai lần gọi hàm rank() với tham số ties.method = random có thể cho kết quả là khác nhau.Một hàm số khác trả lại giá trị là chỉ số của véc-tơ là hàm order(). Câu lệnh y <- order(x) trả lại giá trị là một véc-tơ y là các chỉ số của x sao cho:y[1] là chỉ số của số nhỏ nhất trong véc-tơ x;y[1] là chỉ số của số nhỏ nhất trong véc-tơ x;y[2] là chỉ số của số nhỏ thứ hai trong véc-tơ x; …y[2] là chỉ số của số nhỏ thứ hai trong véc-tơ x; …Số cuối cùng trong véc-tơ y là chỉ số của số lớn nhất trong véc-tơ x.Số cuối cùng trong véc-tơ y là chỉ số của số lớn nhất trong véc-tơ x.Khi muốn lấy chỉ số của véc-tơ x nhưng theo thứ tự giảm dần, bạn đọc sử dụng tham số decreasing = TRUE trong hàm order(). Khái niệm tăng dần và giảm dần cũng có thể hiểu cho các véc-tơ kiểu thời gian, kiểu factor hay kiểu chuỗi ký tự.Câu lệnh order(x) cho kết quả là 6 tại vị trí thứ nhất có nghĩa là số nhỏ nhất trong x nằm ở vị trí thứ sáu trong véc-tơ này (số 10). Vị trí thứ hai trong order(x) nhận giá trị là 1 có nghĩa là số nhỏ thứ hai trong x nằm ở vị trí thứ nhất trong véc-tơ này, và cứ tiếp tục như thế. Vị trí cuối cùng trong order(x) có giá trị là 3 có nghĩa là số lớn nhất trong véc-tơ x nằm ở vị trí thứ 3 trong véc-tơ này.Câu lệnh order(x) có thể được phỏng theo được bằng cách khớp chỉ số của véc-tơ x với véc-tơ được tạo thành từ câu lệnh rank(x, ties.method = ‘first’), thật vậy:Sử dụng hàm order() bạn đọc có thể dễ dàng lấy ra các giá trị nhỏ (hoặc lớn) thứ k trong một véc-tơ. Chẳng hạn như bạn đọc muốn lấy ra câu status có số lượt yêu thích nhiều thứ hai của cựu tổng thống Donald Trump trong dữ liệu trump_tweet, bạn có thể sử dụng hàm order() như sau:Như vậy, chúng tôi đã giới thiệu với bạn đọc những khái niệm cơ bản nhất của R, bao gồm có biến, véc-tơ và các phép tính toán trên các đối tượng này. Trong phần tiếp theo, chúng tôi sẽ nói về lập trình với ngôn ngữ R. Mặc dù R được phát triển để các câu lệnh được viết dưới dạng đơn giản nhất có thể, nhưng trong đa số các dự án liên quan đến dữ liệu, yêu cầu về viết các vòng lặp, hay viết hàm số là không thể tránh khỏi. Phần tiếp theo của cuốn sách sẽ thảo luận về các khái niệm cơ bản về cách lập trình trong ngôn ngữ R.","code":"\n# Khởi tạo véc-tơ x kiểu số\nx<-c(20,40,60,50,30,10)\n\n# Áp dụng hàm rank trên véc-tơ x\nrank(x) # số lớn nhất (60) có chỉ số 6, số nhỏ nhất (10) có chỉ số 1## [1] 2 4 6 5 3 1\n# Khởi tạo véc-tơ x kiểu số\nx<-c(10,10,10,20,20)\n\n# ties.method = \"first\" -> giá trị xuất hiện TRƯỚC có rank NHỎ hơn\nrank(x,ties.method = \"first\")## [1] 1 2 3 4 5\n# ties.method = \"last\" -> giá trị xuất hiện TRƯỚC có rank LỚN hơn\nrank(x,ties.method = \"last\")## [1] 3 2 1 5 4\n# ties.method = \"min\" -> các số bằng nhau có rank bằng nhau và bằng min\nrank(x,ties.method = \"min\")## [1] 1 1 1 4 4\n# ties.method = \"max\" -> các số bằng nhau có rank bằng nhau và bằng max\nrank(x,ties.method = \"max\")## [1] 3 3 3 5 5\n# ties.method = \"average\" -> các số bằng nhau có rank bằng nhau và bằng mean\nrank(x,ties.method = \"average\")## [1] 2.0 2.0 2.0 4.5 4.5\n# ties.method = \"average\" -> các số bằng nhau có rank là hoán vị ngẫu nhiên\nrank(x,ties.method = \"random\")## [1] 1 2 3 4 5\n# Khởi tạo véc-tơ x kiểu số\nx<-c(20,40,60,50,30,10)\n\n# Áp dụng hàm order trên x\norder(x) # chỉ số khi xếp x theo thứ tự TĂNG dần## [1] 6 1 5 2 4 3\n# Áp dụng hàm order trên x, với tham số decreasing = TRUE\norder(x, decreasing = TRUE) # chỉ số khi xếp x theo thứ tự GIẢM dần## [1] 3 4 2 5 1 6\n# Khởi tạo véc-tơ x kiểu số\nx<-c(20,20,10,10,10) # véc-tơ x kiểu số có các giá trị lặp lại\n\n# Khởi tạo chỉ số tăng dần\nchiso<-1:length(x)\n\n# match véc-tơ chiso với rank của x để được order của x\nmatch(chiso,rank(x, ties.method = \"first\"))## [1] 3 4 5 1 2\n# Áp dụng order trên x cho kết quả tương tự\norder(x)## [1] 3 4 5 1 2\n# Cột chứa số lượng like là cột favorite_count\n# Chỉ số của câu status được like nhiều thứ hai\ny<-order(trump_tweets$favorite_count, decreasing = T)[2]\n\n# Lấy ra câu status tại vị trí y\ntrump_tweets$text[y] # là câu tweet được like nhiều thứ hai## [1] \"Why would Kim Jong-un insult me by calling me \\\"old,\\\" when I would NEVER call him \\\"short and fat?\\\" Oh well, I try so hard to be his friend - and maybe someday that will happen!\""},{"path":"kiến-thức-r-cơ-bản.html","id":"ngôn-ngữ-lập-trình-r","chapter":"Chương 3 Kiến thức R cơ bản","heading":"3.4 Ngôn ngữ lập trình R","text":"Để viết các chương trình phức tạp, bạn đọc sẽ cần kiểm soát tốt trình tự mà các dòng lệnh thực thi. Một cách cơ bản để làm được việc này là thực hiện một số câu lệnh nhất định phụ thuộc vào một hoặc một số điều kiện hay còn gọi là viết các câu lệnh rẽ nhánh. Một cách kiểm soát khác là sử dụng vòng lặp nhằm lặp lại một nhóm các câu lệnh một số lần nhất định. Trong phần này, chúng ta sẽ khám phá những kiến thức lập trình cơ bản này trong ngôn ngữ lập trình R. Các kiến thức về lập trình bao gồm có cách sử dụng câu lệnh rẽ nhánh (-else), cách sử dụng vòng lặp (, , và repeat) và một vài cấu trúc khác giúp bạn đọc điều khiển được cách thực hiện các dòng lệnh của mình. Bạn đọc đã có kiến thức về lập trình trong bất kỳ ngôn ngữ nào khác có thể chỉ cần xem qua để nắm được cách viết ngôn ngữ lập trình của R. Với các bạn đọc mới làm quen với lập trình, hãy cố gắng đọc kỹ từng phần dưới đây để có thể tự viết được các chương trình phức tạp.","code":""},{"path":"kiến-thức-r-cơ-bản.html","id":"câu-lệnh-điều-kiện","chapter":"Chương 3 Kiến thức R cơ bản","heading":"3.4.1 Câu lệnh điều kiện","text":"","code":""},{"path":"kiến-thức-r-cơ-bản.html","id":"câu-lệnh-if-và-if-else","chapter":"Chương 3 Kiến thức R cơ bản","heading":"3.4.1.1 Câu lệnh if và if-else","text":"Bạn đọc sử dụng câu lệnh điều kiệu để thông báo cho R biết một câu lệnh, hay một nhóm câu lệnh chỉ thực hiện khi một điều kiện nào đó được thực thi. Dưới đây là cách viết của câu lệnh trong ngôn ngữ RBạn đọc có thể thực hiện một đoạn lệnh có biểu thức điều kiện cụ thể như sau trên cửa sổ ConsoleKhi thực hiện nhóm các câu lệnh ở trên, dòng lệnh thứ 3 chỉ được thực hiện nếu biểu thức điều kiện được viết trong dấu ngoặc () sau từ khóa ở dòng lệnh thứ 2 nhận giá trị là TRUE. Nếu biểu thức điều kiện đó nhận giá trị là FALSE, R sẽ không thực hiện các dòng lệnh thứ 3. Sau khi R thực thi các dòng lệnh 1, biến x nhận giá trị là 1 nên phép sánh (x<10) sẽ cho kết quả là TRUE. đó, dòng lệnh 3 gán giá trị mới bằng 4 cho biến y sẽ được thực hiện. Bạn đọc có thể kiểm tra được rằng sau khi thực hiện đoạn lệnh ở trên, giá trị của biến y sẽ bằng 4 chứ không phải là 2 như khởi tạo ở dòng lệnh số 1.Khi sử dụng câu lệnh điều kiện , sẽ không có câu lệnh nào được thực hiện trong trường hợp biểu thức điều kiện nhận giá trị là sai. Trong thực tế, đa phần các đoạn lệnh rẽ nhánh sẽ có các câu lệnh phải thực thi khi biểu thức điều kiện nhận giá trị là sai. Để thực hiện được việc này, bạn đọc sử dụng câu lệnh kết hợp với else như sauBạn đọc có thể quan sát sự thay đổi giá trị của biến y sau khi thực hiện đoạn lệnh như sauDo biểu thức điều kiện (x==10) nhận giá trị là FALSE nên R sẽ không thực hiện dòng lệnh số 3 mà chuyển qua thực hiện dòng lệnh số 5. Giá trị của y sau khi thực hiện dòng lệnh thứ 5 ở trên sẽ là 8. Nếu trong dòng lệnh 1, bạn đọc sửa giá trị của x thành 10 thay vì 1, dòng lệnh 3 sẽ được thực hiện và dòng lệnh số 5 không được thực hiện đó giá trị của y sau khi thực hiện đoạn lệnh lúc này sẽ là 4.Biểu thức điều kiện trong dấu ngoặc () đứng sau phải là một biến kiểu logic. Nếu sơ ý, biểu thức điều kiện là một véc-tơ của các biến kiểu logic, câu lệnh sẽ chỉ tính đến giá trị đầu tiên trong véc-tơ.Bạn đọc có thể sẽ gặp câu lệnh ifelse() trong các đoạn câu lệnh của R. Tuy nhiên đây không phải là cách viết của câu lệnh rẽ nhánh. Hàm ifelse() được sử dụng khi muốn tạo ra một véc-tơ từ hai véc-tơ dựa trên giá trị của một véc-tơ kiểu logic. Cách sử dụng ifelse() được minh họa thông qua ví dụ dưới đâyHàm ifelse() ở trên sẽ tạo ra một véc-tơ có độ dài bằng với véc-tơ x và tương ứng với các vị trí x chia hết cho 2 sẽ cho kết quả là “chẵn” và tương ứng với các vị trí mà x không chia hết cho 2 sẽ cho kết quả là “lẻ”.Khi sử dụng câu lệnh rẽ nhánh để thực hiện các yêu cầu phức tạp hơn, bạn đọc thường phải sử dụng các câu lệnh và else lồng vào nhau để có được kết quả. Hãy lấy một ví dụ như sau: để viết một đoạn câu lệnh để trả lại giá trị giá vé vào rạp chiếu phim của một khách hàng dựa trên hai yếu tố là độ tuổi và việc có thẻ thành viên hay không, bạn đọc không thể chỉ dùng một câu lệnh điều kiện duy nhất.Giả sử biến Age là biến kiểu số cho biết độ tuổi của khách hàng và biến Member là biến kiểu logic nhận giá trị TRUE nếu khách hàng là thành viên và FALSE nếu khách hàng không phải là thành viên. Bạn đọc có thể sử dụng câu lệnh điều kiện để ra màn hình giá vé của khách hàng đó bằng một trong hai cách như sau","code":"\nif (\"Biểu thức điều kiện\"){\n  \"Nhóm các câu lệnh thực hiện khi biểu thức điều kiện là ĐÚNG\"\n}\nx<-1; y<-2 # Dòng lệnh 1: tạo biến x = 1 và biến y = 2\nif (x<10){ # Dòng lệnh 2: Nếu x < 10 thì thực hiện các câu lệnh nằm trong {}\n  y<-4 # Dòng lệnh 3: Gán giá trị y bằng 4\n} # Dòng lệnh 4: kết thúc câu lệnh if\nif (\"Biểu thức điều kiện\"){\n  \"Nhóm các câu lệnh thực hiện khi biểu thức điều kiện là ĐÚNG\"\n} else {\n  \"Nhóm các câu lệnh thực hiện khi biểu thức điều kiện là SAI\"\n}\nx<-1; y<-2 # Dòng lệnh 1: tạo biến x = 1 và biến y = 2\nif (x==10){ # Dòng 2: Nếu x bằng 10 thì thực hiện các câu lệnh của if\n  y<-4 # Dòng 3: Gán giá trị y bằng 4\n} else { # Dòng 4: Nếu x KHÁC 10 thì thực hiện các câu lệnh của else\n  y<-8 # Dòng 5: gán giá trị y bằng 4\n} # Dòng lệnh 6: kết thúc câu lệnh if-else\ndieukien<-c(TRUE,FALSE,FALSE)\nif (dieukien){ # dieukien là một véc-tơ kiểu logic\n  print(\"Xin chào\") #R CÓ chạy dòng lệnh này\n} # kết thúc câu lệnh if\n# Khởi tạo véc-tơ x là số tự nhiên từ 1 đến 10\nx<-1:10\n\n# Áp dụng hàm ifelse trên véc-tơ x\nifelse(x%%2==0,\"chẵn\",\"lẻ\")##  [1] \"lẻ\"   \"chẵn\" \"lẻ\"   \"chẵn\" \"lẻ\"   \"chẵn\" \"lẻ\"   \"chẵn\" \"lẻ\"   \"chẵn\"\n# Cách thứ nhất: sử dụng bốn câu lệnh if\nAge<-50; Member<-TRUE # tạo giá trị cho các biến Age, Member\nif ((Age < 6) & Member){ # nếu khách hàng dưới 6 tuổi và là thành viên\n  print(\"70.000 đồng\")\n}\nif ((Age < 6) & Member){ # nếu khách hàng trên 6 tuổi và là thành viên\n  print(\"100.000 đồng\")\n}\nif ((Age < 6) & Member){ # nếu khách hàng dưới 6 tuổi và không phải thành viên\n  print(\"120.000 đồng\")\n}\nif ((Age < 6) & Member){ # nếu khách hàng trên 6 tuổi và không phải thành viên\n  print(\"150.000 đồng\")\n}\n# Cách thứ hai: câu lệnh if-else\nAge<-50; Member<-TRUE # tạo giá trị cho các biến Age, Member\nif (Age<6){\n  if(Member){\n    print(\"70.000 đồng\")\n  } else {\n    print(\"100.000 đồng\")\n  }\n} else {\n  if(Member){\n    print(\"120.000 đồng\")\n  } else {\n    print(\"150.000 đồng\")\n  }\n}"},{"path":"kiến-thức-r-cơ-bản.html","id":"viết-vòng-lặp","chapter":"Chương 3 Kiến thức R cơ bản","heading":"3.4.2 Viết vòng lặp","text":"Vòng lặp là một cơ chế lập trình với mục đích để R lặp lại việc chạy một dòng lệnh hay một đoạn lệnh cụ thể. Có hai kiểu viết lặp đó là vòng lặp hoạt động theo cách cho một phần tử nhận lần lượt từng giá trị trong một véc-tơ và vòng lặp hoạt động theo cách lặp lại một đoạn mã cho đến khi một điều kiện cụ thể nhận giá trị là FALSE. Cách thức hoạt động kiểu vòng lặp cũng có thể được áp dụng khi sử dụng nhóm các hàm apply() trong R và sẽ được thảo luận ở một phần riêng của cuốn sách.","code":""},{"path":"kiến-thức-r-cơ-bản.html","id":"vòng-lặp-for","chapter":"Chương 3 Kiến thức R cơ bản","heading":"3.4.2.1 Vòng lặp for","text":"Những câu lệnh sau dùng để ra màn hình tất cả các giá trị nằm trong véc-tơ qua bằng cách sử dụng một vòng lặp forCác dòng lệnh bắt đầu từ đến kết thúc dấu ngoặc {} của vòng lặp có ý nghĩa như sau: cho một biến ten nhận lần lượt các giá trị trong véc-tơ qua từ giá trị ở vị trí thứ nhất đến giá trị ở vị trí cuối cùng; với mỗi giá trị mà biến ten nhận được, đoạn lệnh thực hiện nhóm các câu lệnh nằm trong dấu ngoặc {} của vòng lặp một lần. Trong đoạn lệnh ở trên các câu lệnh được lặp lại là câu lệnh print() với tham số là biến ten.Bạn đọc hãy thử một ví dụ khó hơn một chút, chẳng hạn như bạn muốn tính tổng các số trong một véc-tơ x và không sử dụng hàm sum(). Bạn có thể thực hiện việc này bẳng một vòng lặp như sau:Cho biến tên tong nhận giá trị bằng 0. tong sẽ là giá trị của tổng sau khi kết thúc vòng lặpCho một biến tên là gia_tri nhận lần lượt các giá trị trong véc-tơ bắt đầu từ vị trí thứ nhất, tại mỗi lần lặp tăng giá trị biến tong lên đúng bằng giá trị của gia_triSau khi vòng lặp chạy qua tất cả các giá trị trong véc-tơ cần tính tổng, biến tong sẽ chứa giá trị của tổng các số trong véc-tơ.Giả sử x là véc-tơ Airpassengers là một dữ liệu có sẵn trong R. Đây là một véc-tơ kiểu số, ở dạng chuỗi thời gian chứa thông tin về số lượng khách hàng đi máy bay hàng tháng, đơn vị là nghìn người, tính từ tháng 1 năm 1949 đến tháng 12 năm 1960. Chúng ta sử dụng vòng lặp để tính tổng các số trong véc-tơ sau đó sánh kết quả với hàm sum() có sẵn.Mặc dù đây là chương giới thiệu về viết vòng lặp, nhưng lời khuyên của chúng tôi là bạn đọc hãy luôn cố gắng viết câu lệnh trên các đối tượng vec-tơ nếu có thể thay vì viết vòng lặp. Sử dụng véc-tơ trong R hiệu quả hơn nhiều cả về thời gian chạy lẫn sự đơn giản của các dòng lệnh. Để đánh giá về hiệu quả thời gian, bạn đọc có thể xem ví dụ dưới đây khi sử dụng vòng lặp cho những véc-tơ có độ dài lớn và sánh với thời gian tính toán nếu viết dưới dạng đối tượng véc-tơ. Véc-tơ được sử dụng để kiểm tra tính hiệu quả là véc-tơ có độ dài \\(10^8\\):Bạn đọc có thể thấy rằng trên máy tính của chúng tôi, sử dụng vòng lặp để tính tổng các số trong véc-tơ có độ dài \\(10^8\\) mất khoảng 2 giây trong khi dùng hàm sum() trực tiếp trên véc-tơ chỉ mất 0.1 giây. Nghĩa là thời gian để thực hiện tính toán trên véc-tơ nhanh hơn gấp 20 lần nếu bạn đọc viết dưới dạng đối tượng véc-tơ. Ngoài ra, sử bạn đọc cũng có thể thấy rằng câu lệnh gọi hàm sum() là đơn giản hơn nhiều với viết vòng lặp.Trong các ví dụ ở trên, chúng tôi sử dụng trực tiếp giá trị trong véc-tơ để thực hiện vòng lặp. Bạn đọc cũng có thể sử dụng vòng lặp theo chỉ số của véc-tơ và cho kết quả tương tự. Chẳng hạn như đối với véc-tơ qua, bạn đọc có thể cho một chỉ số nhận giá trị lần lượt từ 1 đến độ dài của véc-tơ qua để lấy từng phần tử của véc-tơ qua:Trong nhiều trường hợp, bạn đọc cần phải sử dụng một vòng lặp nằm trong một vòng lặp khác để giải quyết được vấn đề của mình. Ví dụ như bạn cần ra tất cả các cách kết hợp giữa hai cách pha chế là Nước ép và Sinh tố với bốn loại quả ở trên. Bạn đọc cần sử dụng 2 vòng lặp lồng nhau để làm được việc này:Trong ví dụ ở trên, tổng số lần câu lệnh print() được lặp là \\(4 \\times 2 = 8 (\\text{lần})\\). Mỗi khi viết vòng lặp , đặc biệt là khi viết các vòng lặp lồng vào nhau, bạn đọc hãy luôn cân nhắc thời gian R chạy vòng lặp. Một cách để kiểm tra thời gian vòng lặp chạy là thay vì cho chỉ số chạy qua độ dài của cả véc-tơ thì hãy cho vòng lặp thực hiện với một số lượng nhỏ chỉ số ban đầu để ước tính ra tổng thời gian. Nói một cách đơn giản, vòng lặp chạy qua 100 giá trị ban đầu của véc-tơ sẽ mất thời gian bằng khoảng \\(\\cfrac{1}{100}\\) thời gian để chạy vòng lặp qua 10.000 giá trị của toàn bộ véc-tơ. Thời gian để thực hiện các vòng lặp lồng nhau sẽ tăng lên theo cấp số nhân.","code":"\n# Khởi tạo véc-tơ qua chứa tên các loại quả\nqua = c(\"chuối\", \"táo\", \"cam\", \"chanh\")\n\n# Viết vòng lặp for\n# Cho một biến ten nhận lần lượt các giá trị trong qua\nfor (ten in qua){\n  print(ten) # in ten ra màn hình\n} # kết thúc vòng lặp for## [1] \"chuối\"\n## [1] \"táo\"\n## [1] \"cam\"\n## [1] \"chanh\"\ntong<-0\nfor (gia_tri in x){\n  tong<-tong + gia_tri\n}\nprint(tong)\n# Tạo biến có tên tong nhận giá trị 0\ntong<-0\n# Dùng vòng lặp for\n# Cho biến gia_tri nhận lần lượt các giá trị trong Airpassengers\nfor (gia_tri in AirPassengers){\n  tong<-tong + gia_tri # tăng tong thêm giá trị bằng gia_tri\n} # kết thúc vòng lặp\ntong # in tong ra màn hình## [1] 40363\n# So sánh kết quả ở trên với hàm sum()\nsum(AirPassengers) # cho kết quả tương tự## [1] 40363\n# Khời tạo véc-tơ my_vector có độ dài 10^8\nmy_vector<-rep(1,10^8)\n\n## Tính tổng véc-tơ có độ dài 10^8 bằng vòng lặp\nstart<-proc.time()\ntong<-0\nfor (value in my_vector){\n  tong<-tong+value\n}\nproc.time()-start##    user  system elapsed \n##   1.037   0.038   1.075\n## Tính tổng véc-tơ có độ dài 10^8 bằng véc-tơ\nstart<-proc.time()\ntong<-sum(my_vector)\nproc.time()-start##    user  system elapsed \n##   0.163   0.000   0.163\nfor (i in 1:length(qua)){ # i sẽ nhận giá trị lần lượt 1,2,3,4\n  print(qua[i]) # in ra giá trị thứ i trong véc-tơ qua\n} # kết thúc vòng lặp## [1] \"chuối\"\n## [1] \"táo\"\n## [1] \"cam\"\n## [1] \"chanh\"\npha_che<-c(\"Nước ép\", \"Sinh tố\") # 2 cách pha chế\nfor (i in 1:length(pha_che)){ # i sẽ nhận giá trị lần lượt 1,2\n  for (j in 1:length(qua)){ # VỚI MỐI i, j sẽ nhận giá trị lần lượt 1,2,3,4\n    print(paste(pha_che[i],qua[j],sep=\" \")) # in ra màn hình pha chế và quả\n  } # kết thúc vòng lặp của j với mỗi i\n} # kết thúc vòng lặp của i## [1] \"Nước ép chuối\"\n## [1] \"Nước ép táo\"\n## [1] \"Nước ép cam\"\n## [1] \"Nước ép chanh\"\n## [1] \"Sinh tố chuối\"\n## [1] \"Sinh tố táo\"\n## [1] \"Sinh tố cam\"\n## [1] \"Sinh tố chanh\""},{"path":"kiến-thức-r-cơ-bản.html","id":"vòng-lặp-while","chapter":"Chương 3 Kiến thức R cơ bản","heading":"3.4.2.2 Vòng lặp while","text":"Vòng lặp được gọi là vòng lặp xác định vì nếu không có thêm các câu lệnh đặc biệt, người viết câu lệnh sẽ biết trước được số lần vòng lặp thực hiện. Một cách khác để thực hiện vòng lặp là sử dụng vòng lặp . Đây là kiểu vòng lặp không xác định, nghĩa là trong nhiều trường hợp người viết câu lệnh sẽ không biết trước được sẽ vòng lặp sẽ được thực hiện bao nhiêu lần. Trước khi nói kỹ hơn về khái niệm không xác định, bạn đọc hãy làm quen với cấu trúc của vòng lặp trước. Cách viết một vòng lặp như sauNguyên tắc hoạt động của vòng lặp là tiếp tục thực hiện Đoạn câu lệnh nằm giữa dấu ngoặc {} nếu giá trị của y là TRUE và bỏ qua vòng lặp nếu giá trị của y là FALSE. Nếu y nhận giá trị là TRUE và trong Đoạn câu lệnh không có các dòng lệnh tác động làm thay đổi giá trị của y, thì y sẽ luôn luôn nhận giá trị là TRUE và khi đó vòng lặp sẽ lặp vô hạn, nghĩa là không bao giờ dừng lại được.Vòng lặp dưới đây sẽ ra tên các phần tử của véc-tơ qua bằng cách sử dụng một chỉ số tăng dần và chỉ thoát ra khỏi vòng lặp nếu chỉ số đó vượt qua độ dài của véc-tơ:Trong ví dụ ở trên chúng ta đã biết chính xác khi nào chúng ta sẽ dừng lại vòng lặp nên việc sử dụng vòng lặp sẽ phức tạp hơn vòng lặp . Vòng lặp sẽ phát huy hiệu quả khi bạn đọc không biết chính xác khi nào chúng ta nên dừng việc thực hiện lặp các câu lệnh.Hãy lấy ví dụ khi bạn đọc muốn kiểm tra xem một số tự nhiên \\(n\\) bất kỳ có phải là số nguyên tố hay không. Xin được nhắc lại rằng số nguyên tố là các số tự nhiên chỉ có hai ước số là số 1 và chính nó. Để kiểm tra xem số \\(n\\) có phải là số nguyên tố hay không, bạn đọc cần kiểm tra xem \\(n\\) có chia hết cho số nguyên dương nào từ 2 đến số tự nhiên là phần nguyên của \\(\\sqrt{n}\\) hay không. Phần nguyên của \\(\\sqrt{n}\\) được ký hiệu là \\([\\sqrt{n}]\\). Nếu \\(n\\) chia hết cho một số bất kỳ từ 2 đến \\([\\sqrt{n}]\\), \\(n\\) không phải là số nguyên tố. Theo nguyên tắc này bạn đọc có thể viết một vòng lặp bắt đầu từ \\(2\\) đến \\([\\sqrt{n}]\\) và kiểm tra xem \\(n\\) có chia hết cho số nào trong dãy này không. Tuy nhiên vòng lặp như vậy sẽ luôn luôn phải lặp lại chính xác \\([\\sqrt{n}] - 1\\) lần. Viết vòng lặp trong trường hợp này sẽ hiệu quả hơn rất nhiều bởi chỉ cần \\(n\\) chia hết cho 1 số nào đó chúng ta có thể kết thúc ngay vòng lặp và kết luận \\(n\\) không phải là số nguyên tố.Hãy thử áp dụng vòng lặp trên một ví dụ khác liên quan đến dữ liệu trump_tweet. Chẳng hạn như bạn đọc muốn tìm ra thời điểm đầu tiên mà một câu đăng trạng thái được yêu thích nhiều hơn 10.000 lượt. Câu hỏi này khá dễ nếu chúng ta sử dụng đối tượng véc-tơ. Tuy nhiên chúng tôi muốn bạn đọc suy nghĩ theo hướng sử dụng vòng lặp. Chúng ta sẽ sử dụng một chỉ số tăng dần từ 1 và kiểm tra xem câu đăng trạng thái đó có nhiều hơn 10.000 lượt yêu thích hay không và chỉ dừng lại việc kiểm tra nếu gặp câu có nhiều hơn 10.000 yêu thích. Chúng ta không biết chính xác khi nào sẽ dừng lại, đó sử dụng vòng lặp sẽ hợp lý trong trường hợp nàyMặc dù phần này của cuốn sách đang viết về vòng lặp nhưng chúng tôi muốn nhắc lại rằng bạn đọc hãy cố gắng sử dụng véc-tơ để tìm lời giải thay vì sử dụng vòng lặp khi có thể. Cùng câu hỏi như trên, chúng ta có thể cho lời giải đơn giản hơn bằng cách sử dụng hàm match().Khi làm việc với vòng lặp những người mới làm quen với lập trình rất dễ rơi vào trạng thái vòng lặp vô hạn. Dưới đây là một ví dụ về một vòng lặp như vậy. Biến kiem_tra nhận giá trị ban đầu là TRUE và trong các câu lệnh nằm trong vòng lặp không có câu lệnh nào tác động đến giá trị của biến đó. Bạn đọc sẽ thấy giá trị được ra tăng dần và không bao giờ dừng lại. Bạn đọc chỉ có thể dừng chương trình chạy bằng cách nhấn vào biểu tượng STOP màu đỏ phía trên - bên phải cửa sổ Console.Kinh nghiệm của chúng tôi khi sử dụng vòng lặp không xác định là luôn luôn sử dụng một biến, tạm gọi là , không liên quan đến chương trình chạy và được gán cho giá trị tăng dần trong vòng lặp. Trong biển thức điều kiện luôn luôn kèm thêm một điều kiện là nhỏ hơn số lần lặp tối đa mà người lập trình quy định. Bạn đọc có thể quan sát đoạn lệnh sau:Các đoạn câu lệnh kiểu trên sẽ lặp tối đa là 10.000 lần chúng ta sử dụng thêm điều kiện (<= loop_max)","code":"\n# y là một biến kiểu logic\nwhile (y){\n  \"Đoạn câu lệnh\"\n}\nqua = c(\"chuối\", \"táo\", \"cam\", \"chanh\") # Véc-tơ chứa tên các loại quả\ni<-1\nwhile (i <= length(qua)){ # TRUE cho đến khi i = 5\n  print(qua[i]) # In ra màn hình phần tử thứ i\n  i<-i+1 # Tăng i lên dần để thoát ra khỏi vòng lặp\n} # Kết thúc vòng lặp while## [1] \"chuối\"\n## [1] \"táo\"\n## [1] \"cam\"\n## [1] \"chanh\"\nprint(i) # Kiểm tra giá trị của i khi thoát ra khỏi vòng lặp## [1] 5\n# Nhập số n để kiểm tra\nn<-123454321\n\n# Biến điều kiện ket_qua\nket_qua<-TRUE # ket_qua sẽ đổi thành FALSE khi n chia hết cho 1 số\nuoc_so<-2 # Ước số đầu tiên cần kiểm tra\n\n# Vòng lặp ko xác định, lặp lại nếu ket_qua = TRUE VÀ ước số < n^0.5\nwhile( ket_qua & (uoc_so < n^0.5) ){\n  if(n %% uoc_so == 0){\n    ket_qua<-FALSE # Thay đổi ket_qua nếu n chia hết cho uoc_so\n  }\n  uoc_so<-uoc_so + 1 # Tăng ước số thêm 1\n}\n\n# In ra kết quả cuối cùng\nket_qua # TRUE nến n nguyên tố## [1] FALSE\n# Biến điều kiện của vòng lặp while\nkiem_tra<-TRUE\n\n# Chỉ số dòng\ni<-0\n\n# Thực hiện vòng lặp, dừng lại khi kiem_tra là FALSE\nwhile(kiem_tra){\n  i<-i+1 # tăng chỉ số i\n  kiem_tra<-trump_tweets$favorite_count[i] <= 10^4\n}\n\n# In ra kết quả\ntrump_tweets$favorite_count[i]## [1] 15457\ntrump_tweets$created_at[i]## [1] \"2011-12-21 15:36:36 EST\"\n# Sử dụng hàm match trên véc-tơ logic\n# vitri là chỉ số nhỏ nhất mà số like nhiều hơn 10.000\nvitri<-match(TRUE,trump_tweets$favorite_count>10^4)\n\n# In ra kết quả\ntrump_tweets$favorite_count[i]## [1] 15457\ntrump_tweets$created_at[vitri]## [1] \"2011-12-21 15:36:36 EST\"\n# HÃY CẨN THẬN VÌ ĐÂY LÀ VÒNG LẶP VÔ HẠN\n\n# Biến điều kiện trong vòng lặp không xác định\nkiem_tra<-TRUE\n\n# Bắt đầu lặp\nwhile (kiem_tra){\n  # In ra màn hình phần tử thứ i\n  print(paste0(\"Giá trị của i hiện tại: \", i))\n}\n# Quy định số lần lặp tối đa, biến i tăng dần sau mỗi lần lặp\nloop_max<-10^4\ni<-1\n\n# Biến điều kiện trong vòng lặp\nkiem_tra<-TRUE\n\n# Thực thi vòng lặp\nwhile (kiem_tra & (i<= loop_max)){\n  i<-i+1 # luôn luôn tăng i\n  print(paste0(\"Giá trị của i hiện tại: \", i))\n}"},{"path":"kiến-thức-r-cơ-bản.html","id":"điều-khiển-vòng-lặp","chapter":"Chương 3 Kiến thức R cơ bản","heading":"3.4.2.3 Điều khiển vòng lặp","text":"Khi bạn đọc viết các vòng lặp hoặc , R cung cấp các từ khóa để bạn đọc có thể điều khiển vòng lặp. Các từ khóa đó bao gồm break và next. Ý nghĩa của các từ khóa này như sauBạn đọc quan sát giá trị trả ra màn hình của đoạn câu lệnh sau đề hiểu cách sử dụng next trong vòng lặp dưới đâyCó thể thấy rằng trong các loại quả được ra màn hình không có giá trị cam bởi vì khi biến ten bằng giá trị này từ khóa next đã kết thúc vòng lặp hiện tại, bỏ qua dòng lệnh print() và đi đến vòng lặp tiếp theo. Vẫn các câu lệnh như trên nhưng thay next bằng break, chúng ta có thể quan sát R trả ra kết quả như sau:R chỉ trả ra tên hai loại quả là chuối và táo bởi vì khi gặp giá trị cam từ khóa break đã kết thúc vòng lặp .Trong R còn có một kiểu viết vòng lặp không xác định khác với vòng lặp đó là viết vòng lặp sử dụng câu lệnh repeat. Khi sử dụng vòng lặp repeat bạn đọc luôn luôn phải sử dụng từ khóa break để kết thúc vòng lặp và tránh bị lặp vô hạn. Cách sử dụng repeat trong R như sauTrong vòng lặp repeat ở trên, chúng tôi sử dụng điều kiện là bằng độ dài của véc-tơ để kết thúc vòng lặp. Những bạn đọc mới làm quen với lập trình sẽ dễ bị nhầm lẫn về cách kết thúc vòng lặp của và repeat. Cách hoạt động của hai vòng lặp này là tương đương nhau nên chúng tôi cho rằng những bạn đọc chưa quen với lập trình nên chỉ chọn một trong hai cách viết trong quá trình viết câu lệnh.","code":"\nqua = c(\"chuối\", \"táo\", \"cam\", \"chanh\") # Vec-tơ chứa tên các loại quả\nfor (ten in qua){ # cho biến ten nhận lần lượt các giá trị trong vec-tơ qua\n  if (ten == \"cam\"){\n    next # nếu ten là \"cam\" thì chuyển qua vòng lặp tiếp theo\n  }\n  print(ten) # in ten ra màn hình\n} # kết thúc vòng lặp for## [1] \"chuối\"\n## [1] \"táo\"\n## [1] \"chanh\"\nqua = c(\"chuối\", \"táo\", \"cam\", \"chanh\") # Vec-tơ chứa tên các loại quả\nfor (ten in qua){ # cho biến ten nhận lần lượt các giá trị trong vec-tơ qua\n  if (ten == \"cam\"){\n    break # nếu ten là \"cam\" thì kết thúc vòng lặp ngay lập tức\n  }\n  print(ten) # in ten ra màn hình\n} # kết thúc vòng lặp for## [1] \"chuối\"\n## [1] \"táo\"\nqua = c(\"chuối\", \"táo\", \"cam\", \"chanh\") # Vec-tơ chứa tên các loại quả\ni<-0\nrepeat{\n  i<-i+1 # luôn luôn tăng i\n  print(qua[i]) # in tên ra màn hình\n  if (i== length(qua)){break}\n} # kết thúc vòng lặp repeat## [1] \"chuối\"\n## [1] \"táo\"\n## [1] \"cam\"\n## [1] \"chanh\""},{"path":"kiến-thức-r-cơ-bản.html","id":"hàm-số","chapter":"Chương 3 Kiến thức R cơ bản","heading":"3.4.3 Hàm số","text":"Hàm số có vai trò quan trọng trong bất kỳ ngôn ngữ lập trình nào. Trước hết, hàm số công cụ hữu hiệu đảm bảo sự chính xác và tiện lợi khi lập trình. Thứ hai hàm số là phương pháp chuyển giao kiến thức và kinh nghiệm từ người dùng này đến người dùng khác một cách vô cùng hiệu quả.Hàm số đặc biệt có ý nghĩa khi bạn phải thực hiện một đoạn câu lệnh một cách lặp đi lặp lại và sự thay đổi của một số yếu tố đầu vào. Thay vì phải làm đi làm lại công việc đó một cách thủ công, bạn hãy viết quy trình đó thành một hàm số.Hàm số đặc biệt có ý nghĩa khi bạn phải thực hiện một đoạn câu lệnh một cách lặp đi lặp lại và sự thay đổi của một số yếu tố đầu vào. Thay vì phải làm đi làm lại công việc đó một cách thủ công, bạn hãy viết quy trình đó thành một hàm số.Khi chúng ta muốn chuyển giao kinh nghiệm, kiến thức của mình cho một người khác, hãy viết chương trình của bạn dưới dạng hàm số và chuyển giao. Người dùng có thể không hiểu được ý nghĩa của chương trình của bạn ngay thì ít nhất cũng có thể sử dụng được kiến thức của bạn. Nếu bạn đọc để ý, các thư viện cài đặt thêm trên R đều là tập hợp của các hàm số.Khi chúng ta muốn chuyển giao kinh nghiệm, kiến thức của mình cho một người khác, hãy viết chương trình của bạn dưới dạng hàm số và chuyển giao. Người dùng có thể không hiểu được ý nghĩa của chương trình của bạn ngay thì ít nhất cũng có thể sử dụng được kiến thức của bạn. Nếu bạn đọc để ý, các thư viện cài đặt thêm trên R đều là tập hợp của các hàm số.Hàm số trên R ngoài các hàm sẵn có còn có các hàm số nằm trong các thư viện mà bạn đọc cài đặt bổ sung và các hàm số mà bạn đọc tự định nghĩa.","code":""},{"path":"kiến-thức-r-cơ-bản.html","id":"hàm-số-do-người-dùng-tự-định-nghĩa.","chapter":"Chương 3 Kiến thức R cơ bản","heading":"3.4.3.1 Hàm số do người dùng tự định nghĩa.","text":"Từ khóa để khai báo một hàm số là function(). Để tự tạo một hàm số tên là f nhận giá trị bằng \\(x^2\\) thì bạn đọc sử dụng đoạn câu lệnh như sau:Thay vì sử dụng từ khóa return, bạn đọc cũng có thể sử dụng tên hàm số đển gán cho giá trị trả lại:Đôi khi bạn đọc sẽ gặp các đoạn lệnh khai báo hàm số không có từ khóa return và cũng không có phần gán giá trị cho hàm số. Khi đó R sẽ luôn luôn lấy giá trị được trả ra cuối cùng để gán giá trị cho hàm số đó.Cách viết này chỉ phù hợp cho các hàm số ngắn gọn và chúng tôi khuyên bạn đọc hãy luôn sử dụng từ khóa return khi trả lại giá trị cho hàm số để tránh sự nhầm lẫn.Sau khi đã chạy các đoạn lệnh khai báo hàm số \\(f(x) = x^2\\), R sẽ lưu đối tượng có tên f là kiểu hàm số lên môi trường làm việc chung. Để gọi hàm số và thực hiện tính toán, bạn đọc cần viết đúng tên hàm và cho tham số x giá trị phù hợp.Từ khóa return được sử dụng để trả lại giá trị cho hàm số f và R sẽ gán giá trị cho hàm số f ngay lập tức khi gặp hàm câu lệnh return. Nếu trong đoạn câu lệnh của hàm số f có nhiều từ khóa return, giá trị của f sẽ được gán bằng từ khóa return đầu tiên. Hãy quan sát ví dụ sau:Cách đặt tên hàm số cũng giống như đặt tên biến trong R, bạn đọc cần lựa chọn tên hợp lệ và tránh các từ khóa. Biến x trong phần khai báo hàm số ở trên được gọi là tham số. Hàm số trong R có thể không có tham số nào hoặc có thể có rất nhiều tham số, mỗi tham số là một kiểu đối tượng khác nhau, việc này hoàn toàn tùy thuộc vào người lập trình. Bên trong dấu ngoặc {} của từ khóa function() được gọi là môi trường cục bộ, R sẽ luôn ưu tiên biến nằm trong môi trường này trước tất cả các môi trường khác. Vấn đề sẽ được thảo luận ở phần tiếp theo. Một điểu cần lưu ý là khi viết hàm số hãy luôn luôn có tài liệu hoặc mô tả đi kèm rõ ràng để người sử dụng khác, hoặc chính mình khi sử dụng có thể hiểu hay nhớ được hàm số được sử dụng như thế nào và với mục đích gì.Tham số hay biến số là phần thiết yếu của các hàm trong R. Trong phần tiếp theo, chúng ta sẽ xem xét cách tham số trong hàm số hoạt động như thế nào, chẳng hạn như cách tạo giá trị mặc định cho tham số, cách xử lý các giá trị tham số bị thiếu, cách bổ sung vào tham số bằng cách sử dụng dấu ba chấm (…)Để tạo giá trị mặc định cho tham số bạn đọc cần gán giá trị phù hợp khi khai báo hàm số. Tạo giá trị mặc định cho tham số là quan trọng khi bạn đọc viết các hàm số có nhiều tham số bởi vì khi bạn gọi hàm số và quên gán giá trị cho một vài tham số nào đó, R sẽ sử dụng giá trị mặc định để tính toán và không báo lỗi. Hãy xem xét ví dụ sau: bạn muốn viết một hàm số để tính giá trị hiện tại (present value) của một dòng tiền được quan sát theo năm và được lưu trong một véc-tơ tên là CF. Lãi suất tính theo kiểu lãi gộp là biến . Chúng ta sẽ sử dụng giá trị mặc định 5% để gán cho iGiả sử dòng tiền có giá trị là 1 nghìn USD tại thời điểm cuối năm thứ nhất và tăng thêm 1 nghìn USD mỗi năm và lên đến 10 nghìn USD tại cuối năm thứ 10. Mức lãi suất gộp = 10%/năm. Giá trị hiện tại của dòng tiền được tính bằng hàm PV như sauKhi chúng ta quên không gán giá trị cho tham số khi gọi hàm PV, R sẽ cho nhận giá trị mặc định là 5%Sử dụng dấu ba chấm (…) khi khai báo tham số của một hàm số là phương pháp để người lập trình sử dụng tham số có sẵn của một hàm số khác. Nguyên tắc hoạt động của cách khai báo tham số này thể hiện qua ví dụ sau: hàm PV được xây dựng ở trên chỉ tính được dòng tiền tại các thời điểm cuối các năm. Bạn đọc muốn hàm PV có thể tính được giá trị hiện tại của dòng tiền trong cả hai trường hợp: dòng tiền bắt đầu từ thời điểm đầu năm hoặc dòng tiền bắt đầu từ cuối năm. Chúng ta thực hiện việc đó bằng cách thêm vào một tham số tên là bat_dau: khi bat_dau nhận giá trị bằng 0 thì thời điểm bắt đầu là đầu năm thứ nhất và khi bat_dau nhận giá trị bằng 1 thì thời điểm bắt đầu là cuối năm thứ nhất. Thay vì sửa lại hàm số PV chúng ta có thể viết một hàm mới, tạm gọi là PV1, và sử dụng tham số của hàm PV:Khi gọi hàm PV1 chúng ta cần gọi đầy đủ tham số:Bạn đọc cũng có thể sử dụng cách mượn tham số này để sử dụng các hàm số có sẵn trong R. Trong ví dụ dưới đây, chúng tôi tự xây dựng một hàm có tên là myplot() để vẽ đồ thị phân tán của một véc-tơ kiểu số x theo chỉ số của véc-tơ đó đồng thời và mượn các tham số main, xlab, ylab, của hàm plot():Chúng ta sẽ sử dụng hàm myplot() để vẽ đồ thị phân tán của véc-tơ x nhận giá trị bằng véc-tơ kiểu chuỗi thời gian Airpassengers.Bạn đọc có thể tham khảo cách sử dụng hàm plot() trong phần đồ thị cơ bản trong cuốn sách này.","code":"\nf<-function(x){ # Là một hàm số của biến x\n  return(x^2) # Trả lại giá trị của hàm số là x^2\n}\nf<-function(x){ # Là một hàm số của biến x\n  f<-x^2 # Trả lại giá trị của hàm số là x^2\n}\nf<-function(x) x^2\nclass(f) # Kiểu của đối tượng f là function## [1] \"function\"\nf(10) # Cho tham số x giá trị bằng 10## [1] 100\nf<-function(x){ # là một hàm số của biến x\n  return(x^2) # Trả lại giá trị của hàm số là x^2 khi gặp return\n  return(x^3) # R sẽ không chạy câu lệnh này\n}\nf(10) # trả lại gái trị là 100\nPV<-function(i = 0.05, CF){  # Hàm PV có hai tham số là i và CF\n  n<-length(CF)\n  discount_factor<-(1+i)^(-(1:n))\n  return (sum(discount_factor * CF))\n}\nMyCF<-seq(1000,10000,length=10)\nPV(i = 0.1, MyCF) # Giá trị hiện tại của MyCF tại i = 10%## [1] 29035.91\nPV(CF = MyCF) # Giá trị hiện tại của MyCF tại i = 5%## [1] 39373.78\nPV1<-function(bat_dau,i,...){ # chúng ta chỉ sử dụng tham số i của PV, các tham số khác khai báo bằng ...\n  if (bat_dau==1) {\n    return (PV(i,...)) # PV1 sử dụng các tham số còn lại của PV\n  } else {\n    return ((1+i)*PV(i,...)) # PV1 sử dụng các tham số còn lại của PV\n  }\n}\nPV1(bat_dau = 0,i = 0.1, CF = MyCF) # dòng tiền bắt đầu từ đầu năm thứ 1## [1] 31939.5\nPV1(bat_dau = 1,i = 0.1, CF = MyCF) # dòng tiền bắt đầu từ cuối năm thứ 1## [1] 29035.91\nmyplot<-function(x,...){ # hàm myplot vẽ đồ thị phân tán\n  n<-length(x) # độ dài của véc-tơ x\n  plot(1:n,x,...)\n}\nmyplot(AirPassengers,main=\"Số lượng hành khách các tháng\",\n       ylab = \"Số lượng hành khách\", # tham số ylab của hàm plot()\n       xlab = \"\", # tham số xlab của hàm plot()\n       type = \"l\", color = \"red\") # tham số type và color của hàm plot"},{"path":"kiến-thức-r-cơ-bản.html","id":"hàm-số-được-xây-dựng-sẵn","chapter":"Chương 3 Kiến thức R cơ bản","heading":"3.4.3.2 Hàm số được xây dựng sẵn","text":"Hàm số được xây dựng sẵn là các hàm số được phát triển sẵn trong R và các hàm số trong các thư viện mà bạn đọc cài đặt thêm. Để biết R hiện đang có các thư viện nào đang sẵn sàng để sử dụng, bạn đọc sử dụng câu lệnhNgoài việc liệt kê danh sách các đối tượng, thư viện đang sẵn trên môi trường đang làm việc, bạn đọc còn biết được thứ tự ưu tiên của các thư viện này. Chúng tôi sẽ giải thích về thứ tự ưu tiên ở phía dưới. Đa số các phiên bản R đều có sẵn các thư viện như stats, graphics, utils, … Để biết cụ thể hơn trong một thư viện cụ thể có những đối tượng (hàm số, nhóm các hàm số) nào khác, chúng ta sử dụng câu lệnhBạn đọc sẽ thấy cửa sổ Script liệt kê ra danh sách các hàm số hoặc tên các đối tượng lưu chứa nhóm các hàm số đã được phát triển sẵn trong thư viện stats. Một vài đối tượng được liệt kê ra trong danh sách là các hàm số: hàm AIC(), hàm ARMAacf(),… Một số đối tượng là nhóm các hàm số, chẳng hạn như Beta hay Binomal. Khi bạn đọc gọi Beta trên cửa sổ Console sẽ gặp lỗi vì đó không phải là tên chính xác của hàm số. Thay vì thế hãy chạy câu lệnh ? Beta để thấy rằng trong đối tượng Beta của thư viện stats có một nhóm các hàm số liên quan đến phân phối xác suất \\(Beta\\): hàm dbeta(), hàm pbeta(), hàm qbeta(), và hàm rbeta().Chúng tôi không bàn đến việc làm thế nào để biết sử dụng hàm số nào trong một trường hợp cụ thể bởi vì đương nhiên không có câu trả lời chung cho câu hỏi này. Việc này tùy thuộc vào chuyên môn, hiểu biết, khả năng tìm kiếm của bạn đọc. Chúng tôi muốn tập trung vào việc đảm bảo bạn đọc gọi đúng hàm số mà bạn mong muốn. Sẽ không có vấn đề lớn nếu tên hàm số bạn cần gọi là duy nhất trên cửa số R bạn đang làm việc. Tuy nhiên, khi có một vài đối tượng khác có tên giống như tên hàm số bạn đang sử dụng, bạn sẽ gặp vấn đề.Để làm được việc này bạn đọc nên hiểu một chút về môi trường làm việc và thứ tự ưu tiên khi gọi tên một đối tượng trong R. Khi bạn làm việc trên R, có ba môi trường mà R sử dụng để lưu trữ các đối tượng. Môi trường thứ nhất tạm gọi là môi trường chung (thuật ngữ công nghệ thông tin gọi là toàn cục), thứ hai là môi trường các thư viện, và cuối cùng là môi trường trong một hàm số cụ thể (thuật ngữ CNTT gọi là cục bộ). Khi bạn gọi tên một đối hay một hàm số, R sẽ luôn luôn ưu tiên theo thứ tự là: môi trường cục bộ \\(\\rightarrow\\) môi trường chung (toàn cục) \\(\\rightarrow\\) môi trường các thư viện. có nhiều thư viện cùng mở trên R nên để biết thứ tự ưu tiên của các thư viện bạn đọc sử dụng hàm search(). Các thư viện được ưu tiên hơn sẽ có chỉ số nhỏ hơn trong danh sánh được liệt kê bằng hàm search().Nhìn chung các thư viện cài đặt thêm sẽ thường được ưu tiên hơn các thư viện có sẵn. Nếu một hàm trong thư viện cài đặt thêm trùng tên với một hàm trong thư viện có sẵn, R ưu tiên thư viện cài đặt thêm. Thật vậy, hàm số tên filter() là một hàm được xây dựng sẵn trong thư viện stats. Tuy nhiên trong thư viện dplyr cũng có một hàm tên là filter(). Trước khi gọi thư viện dplyr, mỗi khi bạn đọc gọi hàm filter(), R sẽ luôn hiểu đây là hàm filter() của thư viện stats.Sau khi chúng ta gọi thư viện dplyr, chúng ta sẽ thấy thư viện này xuất hiện trước thư viện stats theo thứ tự ưu tiên.Trong thư viện dplyr cũng có một hàm tên là filter(). Theo thứ tự ưu tiên nếu bạn đọc gọi hàm filter() thì R sẽ hiểu đây là hàm của thư viện dplyr. Lúc này muốn sử dụng hàm filter() của thư viện stats bạn đọc cần phải sử dụng tên thư viện viết trước hàm này stats::filter().Như đã nói ở phần trước, môi trường chung cũng là môi trường được ưu tiên trước môi trường các thư viện. Bạn đọc có thể thấy từ kết quả hàm search(), môi trường chung, ký hiệu .GlobalEnv, luôn xuất hiện trước tiên. Môi trường chung chính là nơi lưu trữ tất cả các hàm số hay đối tượng mà bạn đọc tự định nghĩa. Môi trường chung luôn được ưu tiên trước môi trường thư viện. Điều này có nghĩa là nếu bạn đọc tự định nghĩa một biến, một véc-tơ, hay hàm số có tên là filter, R sẽ ưu tiên tên filter cho đối tượng mà bạn đọc tự định nghĩa. Như vậy, nếu bạn đọc sử dụng tên filter cho một hàm bạn tự định nghĩa, bạn sẽ cần phải sử dụng thêm tên thư viện để gọi hàm filter() từ các thư viện như dplyr hoặc stats.Còn một môi trường khác, tạm gọi là môi trường cục bộ, sẽ được ưu tiên hơn môi trường chung. Môi trường cục bộ mô tả môi trường bên trong một hàm số mà bạn đọc tự định nghĩa. Giả sử sau khi bạn đọc tự định nghĩa một hàm filter() trên môi trường chung và sau đó tự định nghĩa một hàm số f có sử dụng một tham số, có thể là biến hoặc hàm số, có tên là filter thì mỗi khi bạn đọc gọi hàm số f, đối tượng tên filter sẽ luôn được hiểu là tham số của hàm số f. Môi trường bên trong hàm f được gọi là môi trường cục bộ. Bạn đọc hãy quan sát ví dụ dưới đây để hiểu hơn về môi trường chung và môi trường cục bộTrước hết chúng ta định nghĩa một hàm tên là filter() trong môi trường chung luôn nhận giá trị bằng hằng số \\(\\pi\\). Lúc này khi chúng ta gọi filter(), R sẽ hiểu rằng đây là hàm filter() chúng ta tự định nghĩa. Sau đó chúng ta định nghĩa một hàm số tên f đồng thời bên trong hàm f chúng ta định nghĩa một hàm filter() khác nhận giá trị là 10. Hàm filter() bên trong hàm f() được gọi là hàm số trong môi trường cục bộ.Khi chúng ta gọi hàm số f, hàm số này lại gọi một hàm số tên là filter() được định nghĩa bên trong hàm số nó. Bởi vì R ưu tiên môi trường cục bộ trước nên hàm filter() bên trong f() có giá trị bằng 10. Bên ngoài hàm số f(), chúng ta lại gọi filter() thì giá trị trả lại là \\(\\pi\\) vì đây là môi trường chung.Tất cả các hàm số mà bạn đọc thường xuyên sử dụng hãy lưu trong các file và mỗi khi cần sử dụng bạn đọc chỉ cần gọi tên file đó thay vì copy toàn bộ các câu lệnh của các hàm số vào cửa sổ Script. Hàm số để gọi một file lên cửa sổ R bạn đang sử dụng là hàm source(). Chẳng hạn như tất cả các hàm số bạn đọc tự định nghía được lưu ở một file có tên myfunction.R, bạn chỉ cần sử dụng câu lệnh sau để gọi tất cả các hàm số lên cửa sổ đang làm việc:","code":"\nsearch()##  [1] \".GlobalEnv\"         \"package:dslabs\"     \"package:stringr\"   \n##  [4] \"package:ggrepel\"    \"package:pryr\"       \"package:gridExtra\" \n##  [7] \"package:grid\"       \"package:forcats\"    \"package:ggplot2\"   \n## [10] \"package:kableExtra\" \"package:knitr\"      \"package:dplyr\"     \n## [13] \"package:readxl\"     \"package:stats\"      \"package:graphics\"  \n## [16] \"package:grDevices\"  \"package:utils\"      \"package:datasets\"  \n## [19] \"package:methods\"    \"Autoloads\"          \"package:base\"\nlibrary(help = \"stats\") # liệt kê các đối tượng trong thư viện stats\n? filter # nếu chưa gọi thư viện dplyr, filter là hàm của thư viện stats\nlibrary(dplyr) # gọi thư viện dplyr\nsearch() # sau khi gọi thư viện dplyr, thư viện này được ưu tiên trước stats\nfilter<-function(){return(pi)} #Tự định nghĩa hàm filter trong môi trường chung\nfilter() # hàm filter trong .GlobalEnv luôn bằng pi## [1] 3.141593\nf<-function(){\n  filter<-function(){return(10)}\n  # bên trong hàm f, định nghĩa lại hàm filter bằng 10\n\n  return(filter())\n}\nf() # trả lại giá trị là 10 vì hàm filter bên trong f nhận giá trị 10## [1] 10\nfilter() # trả lại giá trị pi## [1] 3.141593\nsource(\"Đường dẫn đến file/myfunction.R\")"},{"path":"kiến-thức-r-cơ-bản.html","id":"đồ-thị-cơ-bản","chapter":"Chương 3 Kiến thức R cơ bản","heading":"3.5 Đồ thị cơ bản","text":"Có nhiều hàm trong R để tạo ra các đồ thị từ cơ bản đến phức tạp. Trong phần này chúng tôi sẽ giới thiệu về các hàm cơ bản để tạo đồ thị trong R. Mục đích là để bạn đọc làm quen với các phương pháp vẽ đồ thị phổ biến và hiểu cách tùy chỉnh tham số đồ thị cơ bản. Hiểu về cách vẽ các đồ thị cơ bản sẽ giúp ích rất nhiều khi bạn đọc tìm hiểu về các kỹ thuật trực quan hóa dữ liệu được giới thiệu trong các chương tiếp theo.","code":""},{"path":"kiến-thức-r-cơ-bản.html","id":"đồ-thị-phân-tán","chapter":"Chương 3 Kiến thức R cơ bản","heading":"3.5.1 Đồ thị phân tán","text":"Đồ thị phân tán, được gọi là Scatter plot, là đồ thị được sử dụng để mô tả sự phân tán của các điểm trên một mặt phẳng. Đồ thị phân tán có thể sử dụng để mô tả mối liên hệ của một véc-tơ kiểu số theo chỉ số của nó, hoặc để mô tả mối liên hệ giữa hai véc-tơ kiểu số. Hàm số dùng để vẽ đồ thị phân tán là hàm plot(). Một số tham số thường được sử dụng trong hàm này làTham số type cho biết hình dạng sẽ xuất hiện, chẳng hạn như type = ‘p’ cho biết sẽ sử dụng các điểm, type = ‘l’ cho biết sẽ sử dụng các đường.Tham số type cho biết hình dạng sẽ xuất hiện, chẳng hạn như type = ‘p’ cho biết sẽ sử dụng các điểm, type = ‘l’ cho biết sẽ sử dụng các đường.Tham số main được sử dụng để viết tiêu đề của đồ thị, xlab là tiêu đề của trục x, ylab là tiêu đề của trục y.Tham số main được sử dụng để viết tiêu đề của đồ thị, xlab là tiêu đề của trục x, ylab là tiêu đề của trục y.Tham số col cho biết màu sắc của các điểm, đường sử dụng trong đồ thị, chẳng hạn như col = ‘red’ cho biết sẽ sử dụng màu đỏ để vẽ hình.Tham số col cho biết màu sắc của các điểm, đường sử dụng trong đồ thị, chẳng hạn như col = ‘red’ cho biết sẽ sử dụng màu đỏ để vẽ hình.Khi đối tượng trong hàm plot() là một véc-tơ duy nhất, R sẽ vẽ đồ thị phân tán với trục x là chỉ số của các phần tử trong véc-tơ và trục y là giá trị của số trong véc-tơ. Câu lệnh dưới đây sử dụng hàm plot() để vẽ véc-tơ cột Temp từ dữ liệu airquality:\nHình 3.3: Vẽ đồ thị phân tán sử dụng hàm plot()\nNgoài vẽ đồ thị một véc-tơ, đồ thị phân tán còn được sử dụng để mô tả mối liên hệ giữa hai biến liên tục. Đồ thị dưới đây mô tả mối liên hệ giữa hai cột Temp và Ozone trong của dữ liệu airquality:\nHình 3.4: Mô tả mối liên hệ giữa hai biến liên tục sử dụng hàm plot()\nHàm plot() còn có thể được sử dụng để mô tả sự biến động của một véc-tơ theo thời gian. Bạn đọc chỉ cần sử dụng tham số type = ‘l’ trong hàm plot(). Hình vẽ dưới đây mô tả sự biến động của véc-tơ dữ liệu AirPassengers theo thời gian:\nHình 3.5: Mô tả một biến liên tục theo thời gian sử dụng hàm plot()\nHàm lines() được sử dụng để vẽ thêm các đường vào trong một đồ thị có sẵn. Bạn đọc quan sát các sử dụng hàm lines() trong đoạn câu lệnh dưới đây. Lưu ý rằng khi chúng ta mô tả nhiều biến trên cùng một đồ thị thì cần có thêm chú giải và điều chỉnh cá giá trị giới hạn trên trục y để đồ thị hiển thị đầy đủ:\nHình 3.6: Mô tả nhiều biến liên tục theo thời gian sử dụng hàm plot() và lines()\nBạn đọc có thể nhận thấy rằng chúng tôi sử dụng tham số ylim = c(1500,8500) để cho biết giới hạn trên và giới hạn dưới của giá trị trục y trong đồ thị. Hàm lines() được sử dụng để vẽ (thêm) chỉ số chứng khoán SMI vào trong đồ thị đã được tạo bàng hàm plot() trước đó. Sau cùng chúng ta sử dụng hàm legend() để thêm chú giải cho biết đâu là chỉ số chứng khoáng DAX, đâu là chỉ số chứng khoán SMI.","code":"\nplot(airquality$Temp,\n     type = \"p\",\n     main = \"Biến nhiệt độ (temp) - dữ liệu airquality\",\n     xlab = \"Chỉ số véc-tơ\",\n     ylab = \"Nhiệt độ\",\n     col = \"#640514\")\nplot(airquality$Ozone,airquality$Temp,\n     type = \"p\",\n     main = \"Mối liên hệ giữa Ozone và Temp trong dữ liệu airquality\",\n     xlab = \"Chỉ số Ozone\",\n     ylab = \"Nhiệt độ\",\n     col = \"#640514\")\nplot(AirPassengers,\n     type = \"l\",\n     main = \"Số lượng hành khách trung bình theo tháng\",\n     xlab = \"Thời gian\",\n     ylab = \"Số lượng hành khách\",\n     col = \"#640514\")\n# Vẽ đồ thị chỉ số chứng khoán DAX bằng plot()\nplot(EuStockMarkets[,1],type = \"l\",\n     main = \"Chỉ số chứng khoán DAX và SMI\",\n     col = \"#640514\",\n     ylab = (\"\"), xlab = (\"Năm\"),\n     ylim = c(1500,8500))\n\n# Vẽ (thêm) chỉ số chứng khoán SMI bằng lines()\nlines(EuStockMarkets[,2], col = \"#005478\")\n\n# Dùng hàm legend để thêm chú giải vào vị trí\n# (x,y) == (1992,8000)\nlegend(1992, 8000, legend=c(\"Chỉ số DAX\", \"Chỉ số SMI\"),\n       fill = c(\"#005478\",\"#640514\")\n)"},{"path":"kiến-thức-r-cơ-bản.html","id":"đồ-thị-cột","chapter":"Chương 3 Kiến thức R cơ bản","heading":"3.5.2 Đồ thị cột","text":"Đồ thị cột hay còn gọi là biểu đồ tần số được sử dụng để mô tả phân phối của một véc-tơ. Hàm số để vẽ đồ thị cột là hàm hist(). Chúng ta sử dụng đồ thị dạng cột để mô tả véc-tơ lợi suất của chỉ số DAX như sau\nHình 3.7: Mô tả phân phối của một biến bằng hàm hist()\nNgoài các tham số thường được sử dụng như main, xlab, ylab, hay col, hàm hist() có một tham số quan trọng là breaks. Tham số này cho biết vị trí các điểm mà ở đó chúng ta sẽ tính toán các cột tần suất. Véc-tơ có độ dài càng lớn thì cần càng nhiều điểm được sử dụng để mô tả phân phối dữ liệu. Bạn đọc có thể thấy rằng đồ thị tần xuất mô tả lợi suất của chỉ số DAX ở trên không có nhiều thông tin các điểm breaks quá nhỏ với độ dài véc-tơ. Chúng ta điều chỉnh lại đồ thị tần suất và sử dụng 50 điểm breaks cách đều từ -0.1 đến 0.1 để có hiển thị tốt hơn\nHình 3.8: Sử dụng tham số break trong hàm hist()\nBiểu đồ tần suất thường được chuẩn hóa về đồ thị xác suất và sử dụng với đồ thị hàm mật độ (density) để mô tả tốt hơn phân phối xác suất của véc-tơ kiểu số. Để chuyển đồ thị tần số thành đồ thị xác suất, bạn đọc sử dụng tham số freq và cho giá trị bằng FALSE. Đoạn câu lệnh dưới đây mô tả cách sử dụng biểu đồ xác suất kết hợp với đồ thị hàm mật độ của lợi suất của chỉ số DAX\nHình 3.9: Sử dụng đồng thời đồ thị tần suất và hàm mật độ để mô tả phân phối xác suất\n","code":"\n# Tính return trên chỉ số DAX\nDAX<-EuStockMarkets[,1]\nn<-length(DAX)\nr_DAX<-log(DAX[2:n]/DAX[1:(n-1)])\n\n# Dùng hàm hist để vẽ đồ thị tần số\nhist(r_DAX, main = \"Tần suất lợi suất của chỉ số DAX\",\n     xlab = \"\", ylab = \"\",\n     border = \"#640514\", col = \"white\")\n# Dùng hàm hist để vẽ đồ thị tần số\nhist(r_DAX, main = \"Tần suất lợi suất của chỉ số DAX\",\n     xlab = \"\", ylab = \"\",\n     breaks = seq(-0.1,0.1,length=100),\n     border = \"#640514\", col = \"white\")\n# Dùng hàm hist để vẽ đồ thị tần số\nhist(r_DAX, main = \"Phân phối xác suất lợi suất của chỉ số DAX\",\n     xlab = \"\", ylab = \"\", freq = FALSE,\n     breaks = seq(-0.1,0.1,length=50),\n     border = \"#640514\", col = \"white\")\n\n# Kết hợp thêm với đồ thị hàm mật độ\nlines(density(r_DAX), col = \"darkblue\")"},{"path":"kiến-thức-r-cơ-bản.html","id":"đồ-thị-hình-hộp","chapter":"Chương 3 Kiến thức R cơ bản","heading":"3.5.3 Đồ thị hình hộp","text":"Đồ thị hình hộp, hay còn được gọi là box--whisker, hoặc boxplot, dùng để mô tả phân phối xác suất của một biến liên tục hoặc để mô tả mối liên hệ giữa một biến liên tục với một biến rời rạc. Đồ thị hình hộp sử dụng 5 giá trị đặc trưng của một véc-tơ để mô tả hình dạng phân phối xác suất và các điểm có nhiều khả năng là giá trị ngoại lai trong một véc-tơ. Chúng tôi sẽ thảo luận kỹ hơn về cách sử dụng boxplot trong chương Trực quan hóa dữ liệu. Trong phần này, chúng tôi chỉ mô tả cách vẽ một đồ thị hình hộp cơ bản.Hàm boxplot() được sử dụng để vẽ đồ thị hình hộp. Đồ thị dưới đây mô tả phân phối xác suất của véc-tơ nhiệt độ (biến Temp) trong dữ liệu airquality bằng đồ thị kiểu hình hộp:\nHình 3.10: Đồ thị dạng hộp mô tả phân phối của biến Temp\nĐể mô tả phân phối xác suất của nhiệt độ theo tháng (cột Month) trong dữ liệu airquality chúng ta sử dụng đồ thị hình hộp như sau\nHình 3.11: Đồ thị dạng hộp mô tả phân phối của biến Temp theo tháng (Month)\nCòn nhiều hàm vẽ đồ thị hữu ích trong R bạn đọc có thể tự tìm hiểu như barplot(), dotchart(), hay coplot(). Như chúng tôi đã đề cập, trọng tâm của phần vẽ đồ thị sẽ ở trong chương Trực quan hóa dữ liệu, đó các đồ thị được liệt kê trong phần này chỉ mang tính tham khảo.","code":"\nboxplot(airquality$Temp,\n        border = \"#640514\",col = \"white\")\nboxplot(Temp~Month, data = airquality, border = \"#640514\",col = \"white\")"},{"path":"kiến-thức-r-cơ-bản.html","id":"phụ-lục","chapter":"Chương 3 Kiến thức R cơ bản","heading":"3.6 Phụ lục","text":"","code":""},{"path":"kiến-thức-r-cơ-bản.html","id":"biểu-thức-chính-quy-và-thư-viện-stringr","chapter":"Chương 3 Kiến thức R cơ bản","heading":"3.6.1 Biểu thức chính quy và thư viện stringr","text":"Biểu thức chính quy (Regular Expression hay Regex) là công cụ hiệu quả khi chúng phải xử lý một đoạn văn bản dài hoặc một véc-tơ kiểu chuỗi ký tự. Sử dụng biểu thức chính quy giúp cho các câu lệnh phân tích cú pháp, tìm kiếm, và thay thế trong các chuỗi ký tự hay đoạn văn bản trở nên đơn giản hơn rất nhiều. Khái niệm biểu thức chính quy xuất hiện trong hầu hết các ngôn ngữ lập trình nhưng mỗi ngôn ngữ lập trình lại có thể có một cách viết biểu thức chính quy khác nhau. Trong phần này, chúng tôi chỉ thảo luận về biểu thức chính quy trong R và cách sử dụng các biểu thức này trong thư viện chuyên xử lý biến kiểu chuỗi ký tự là thư viện stringr.Biểu thức chính quy là nhóm các quy tắc và cú pháp viết hoặc miêu tả các chuỗi ký tự khác, thường là phức tạp và tổng quát hơn, bằng cách sử dụng các chuỗi ký tự đơn giản. Ví dụ, biểu thức chính quy đơn giản nhất là . đại diện cho tất cả các ký tự có thể xuất hiện trong một chuỗi ký tự. Giả sử chúng ta có biến kiểu chuỗi ký tự là câu trạng thái thứ 20.000 của cựu tổng thống Donald Trump và chúng ta muốn xem trong chuỗi ký tự đó, có những đoạn ký tự nào được bắt đầu bằng chữ “” và theo sau đó là một ký tự bất kỳ, chúng ta sử dụng biểu thức chính quy \".\". Hàm số sử dụng để hiển thị tất cả đoạn ký tự là hàm str_view_all() của thư viện stringrCó thể thấy rằng có sáu đoạn ký tự trong biến str0 được bắt đầu bằng “” và theo sau là một ký tự bất kỳ. Để miêu tả ký tự . trong chuỗi ký tự và phân biệt với biểu thức chính quy, chúng ta sử dụng \\\\. để mô tả dấu .. Thật vậy, hãy quan sát ví dụ sau:","code":"\nstr0<-trump_tweets$text[20000]\nstr_view_all(str0,\"A.\")## [1] │ M<AK>E <AM>ERIC<A >GRE<AT> <AG><AI>N!\nstr0<-\"I am an actuary. What is your profession?\"\nstr_view_all(str0,\"\\\\.\")## [1] │ I am an actuary<.> What is your profession?"},{"path":"kiến-thức-r-cơ-bản-1.html","id":"kiến-thức-r-cơ-bản-1","chapter":"Chương 4 Kiến thức R cơ bản","heading":"Chương 4 Kiến thức R cơ bản","text":"","code":""},{"path":"kiến-thức-r-nâng-cao.html","id":"kiến-thức-r-nâng-cao","chapter":"Chương 5 Kiến thức R nâng cao","heading":"Chương 5 Kiến thức R nâng cao","text":"Trong cuốn sách này chúng tôi cố gắng tránh nhắc đến các khái niệm toán học phức tạp bởi đối tượng chúng tôi hướng đến là những người làm việc với dữ liệu nhưng không có một nền tảng chuyên sâu về toán học. Tuy nhiên để làm việc được với dữ liệu thì các kiến thức về ma trận nói riêng và kiến thức về đại số tuyến tính nói chung là bắt buộc phải nắm vững. Điều đáng tiếc là tại thời điểm chúng tôi viết cuốn sách này, đa số các chương trình đào tạo dành cho sinh viên các ngành kinh tế đang cắt giảm dần kiến thức về toán học và đặc biệt là kiến thức đại số tuyến tính.","code":""},{"path":"kiến-thức-r-nâng-cao.html","id":"ma-trận","chapter":"Chương 5 Kiến thức R nâng cao","heading":"5.1 Ma trận","text":"Ma trận có ý nghĩa đặc biệt quan trọng trong phân tích dữ liệu bởi đa số các dữ liệu đều được chuyển thành kiểu ma trận để dễ dàng phân tích và tính toán. Cũng giống như véc-tơ, ma trận là một đối tượng dùng để lưu các biến có cùng kiểu. Khác với véc-tơ, ma trận lưu phần tử theo hàng và cột, nghĩa là trong không gian hai chiều trong khi véc-tơ lưu phần tử trong không gian một chiều. Bạn đọc cũng có thể hiểu véc-tơ là một cột trong khi ma trận là tập hợp của các cột có cùng độ dài. Kích thước của một véc-tơ là chiều dài của véc-tơ đó trong khi kích thước của một ma trận là số hàng và số cột của ma trận đó.","code":""},{"path":"kiến-thức-r-nâng-cao.html","id":"khởi-tạo-ma-trận","chapter":"Chương 5 Kiến thức R nâng cao","heading":"5.1.1 Khởi tạo ma trận","text":"Hàm số dùng để tạo ra ma trận trong R là hàm matrix(). Khi tạo ma trận, bạn đọc sẽ luôn luôn phải khởi tạo giá trị cho ma trận đó. Đoạn lệnh sau sẽ khởi tạo một ma trận có tên M, có 3 hàng, 4 cột, và giá trị trong ma trận là các số tự nhiên từ 1 đến 12 được sắp xếp theo thứ tựCác giá trị dùng để khởi tạo cho ma trận là các số từ 1 đến 12 và được điền vào ma trận M theo nguyên tắc từ trên xuống dưới rồi từ trái sang phải, nghĩa làCột thứ nhất sẽ được ưu tiên cho giá trị trước; phần tử hàng thứ nhất của cột thứ nhất sẽ được điền giá trị trước tiên, sau đó đến phần tử ở hàng thứ hai của cột thứ nhất, …Sau khi điền hết cột thứ nhất R sẽ tiếp tục điền vào giá trị ở hàng thứ nhất của cột thứ hai, rồi phần tử hàng hai ở cột thứ hai,… cho đến hết cột thứ hai.Quá trình điền số vào ma trận sẽ tiếp tục như thế sau khi tất cả các phần tử trong ma trận đều có giá trị. Véc-tơ dùng để khởi tạo giá trị cho ma trận có độ dài 12 vừa đúng với số phần tử trong ma trận nên câu lệnh tạo ma trận M ở trên hoạt động bình thường. Trong trường hợp bạn đọc sử dụng véc-tơ có độ dài khác 12 để khởi tạo giá trị cho ma trận, câu lệnh vẫn sẽ chạy nhưng có kèm theo cảnh báo:Bạn đọc có thể thấy rằng:Nếu véc-tơ dùng để khởi tạo giá trị cho ma trận M có độ dài lớn hơn 12, R sẽ dùng 12 giá trị đầu tiên để khởi tạo giá trị cho ma trận.Nếu véc-tơ dùng để khởi tạo giá trị cho ma trận M có độ dài nhỏ hơn 12, R sẽ lặp lại véc-tơ đó cho đến khi có độ dài lớn hơn hoặc bằng 12 rồi sau đó dùng 12 giá trị đầu tiên để khởi tạo giá trị cho ma trận.Khi khởi tạo ma trận, bạn đọc có thể yêu cầu giá trị được khởi tạo theo hàng thay vì theo cột bằng tham số byrow = TRUE trong hàm matrix().Để biết kích cỡ của ma trận, chúng ta sử dụng hàm dim(). Hàm dim() trả lại giá trị là một véc-tơ kiểu số có độ dài là hai, phần tử thứ nhất là số hàng, phần tử thứ hai là số cột của ma trận:Ma trận cũng có thể được khởi tạo bằng cách ghép các véc-tơ hoặc các ma trận khác theo hàng hay theo cột bằng các hàm cbind() hoặc rbind():Hàm cbind() nối các ma trận có cùng số hàng hoặc ma trận với véc-tơ có độ dài bằng số hàng của ma trận.Tương tự, rbind() nối các ma trận có cùng số cột hoặc ma trận với véc-tơ có độ dài bằng số cột của ma trận.Các phép tính toán thông thường trên ma trận cũng có nguyên tắc giống như đối với véc-tơ. Các phép toán như cộng, trừ, nhân, chia, lũy thừa, …, sẽ tác động lên tất cả các phần tử trong ma trận theo thứ tự của các phần tử xuất hiện trên ma trận. Ví dụ khi nhân ma trận M kích thước 3 \\(\\times\\) 4 với một số ta sẽ có kết quả như sau:Bạn đọc có thể thấy rằng kết quả nhận được là một ma trận có kích thước bằng với kích thước của ma trận M và mỗi phần tử bằng phần tử ở vị trí tương ứng của ma trận M nhân với 2.Khi thực hiện phép nhân thông thường ma trận M với một ma trận M1 có cùng kích thước thì kết quả nhận được là một ma trận mà mỗi phần tử bằng tích của 2 phần tử ở vị trí tương ứng của M và M1. R sẽ báo lỗi nếu thực hiện phép nhân thông thường giữa hai ma trận không có cùng kích thước:Phép nhân thông thường cũng có thể được thực hiện giữa ma trận M với một véc-tơ có độ dài nhỏ hơn hoặc bằng số phần tử của M. Trước khi thực hiệp phép nhân, R sẽ chuyển các phần tử trong véc-tơ vào một ma trận có kích thước tương ứng với M sau đó thực hiện phép nhân giống như nhân hai ma trận có cùng kích thước:Khi thực hiện tính toán như trên, R đã tự động lặp lại véc-tơ x cho đến khi số lượng phần tử bằng với số phần tử của M, điền các giá trị này vào một ma trận có kích thước bằng với kích thước của M rồi sau đó thực hiện phép nhân. Thật vậy, chúng ta có thể kiểm tra như sau:","code":"\nM<-matrix(1:12, nrow = 3, ncol = 4) # nrow: số hàng, ncol: số cột\nM # In M##      [,1] [,2] [,3] [,4]\n## [1,]    1    4    7   10\n## [2,]    2    5    8   11\n## [3,]    3    6    9   12\nM<-matrix(1:13, nrow = 3, ncol = 4) # Sẽ có cảnh báo;\nM<-matrix(1:5, nrow = 3, ncol = 4) # Sẽ có cảnh báo;\nM<-matrix(1:12, nrow = 3, ncol = 4, byrow = TRUE)\nM # in M ra của sổ console##      [,1] [,2] [,3] [,4]\n## [1,]    1    2    3    4\n## [2,]    5    6    7    8\n## [3,]    9   10   11   12\ndim(M) # ma trận 3 hàng 4 cột## [1] 3 4\ncbind(M,rep(1,3)) # ghép THEO CỘT, ma trận M (3 hàng, 4 cột) với véc-tơ độ dài 3##      [,1] [,2] [,3] [,4] [,5]\n## [1,]    1    2    3    4    1\n## [2,]    5    6    7    8    1\n## [3,]    9   10   11   12    1\nrbind(M,rep(1,4)) # ghép THEO HÀNG, ma trận M (3 hàng, 4 cột) với véc-tơ độ dài 4##      [,1] [,2] [,3] [,4]\n## [1,]    1    2    3    4\n## [2,]    5    6    7    8\n## [3,]    9   10   11   12\n## [4,]    1    1    1    1\nM<-matrix(1:12, nrow = 3, ncol = 4)\nM * 2 # in M*2 ra của sổ console##      [,1] [,2] [,3] [,4]\n## [1,]    2    8   14   20\n## [2,]    4   10   16   22\n## [3,]    6   12   18   24\nM<-matrix(1:12, nrow = 3, ncol = 4)\nM1<-matrix(rep(c(0,1),6), nrow = 3, ncol = 4)\nM * M1# in M * M1 ra của sổ console##      [,1] [,2] [,3] [,4]\n## [1,]    0    4    0   10\n## [2,]    2    0    8    0\n## [3,]    0    6    0   12\nM<-matrix(1:12, nrow = 3, ncol = 4)\nx<-c(-2,-1,0,1,2) # véc-tơ độ dài 5\nM * x # phép nhân được thực hiện mà không báo lỗi##      [,1] [,2] [,3] [,4]\n## [1,]   -2    4   -7   20\n## [2,]   -2   10    0  -22\n## [3,]    0  -12    9  -12\ny<-rep(x,3)\n# Lặp lại x cho đến khi số phần tử của véc-tơ thu được lớn hơn 12\n\nM1<-matrix(y[1:12],nrow = 3, ncol = 4)\n# Dùng 12 giá trị ban đầu để tạo thành ma trận có cùng kích thước 3*4\n\nM * M1 # Kết quả giống như M * x##      [,1] [,2] [,3] [,4]\n## [1,]   -2    4   -7   20\n## [2,]   -2   10    0  -22\n## [3,]    0  -12    9  -12"},{"path":"kiến-thức-r-nâng-cao.html","id":"lấy-phần-tử-con-và-ma-trận-con-của-ma-trận.","chapter":"Chương 5 Kiến thức R nâng cao","heading":"5.1.2 Lấy phần tử con và ma trận con của ma trận.","text":"Tương tự như với véc-tơ, chúng ta sử dụng dấu ngoặc vuông [] để lấy phần tử con trong ma trận. Khác với véc-tơ, ma trận có chỉ số hàng và chỉ số cột nên chúng ta cần cho biết phần tử được lấy ra ở hàng thứ bao nhiêu và cột thứ bao nhiêu:Chúng ta cũng có thể lấy ra véc-tơ hàng hoặc véc-tơ cột của ma trận bằng cách sauĐể lấy ra một ma trận con của một ma trận, chúng ta cũng tạo véc-tơ chỉ số giống như cách tạo chỉ số với véc-tơ. Thay vì chỉ tạo một véc-tơ chỉ số duy nhất như khi làm với véc-tơ, chúng ta cần tạo một véc-tơ chỉ số theo hàng và một véc-tơ chỉ số theo cột. Bạn đọc có thể tạo chỉ số bằng một véc-tơ kiểu số hoặc véc-tơ kiểu logical hoặc kết hợp cả hai phương pháp này","code":"\nM[2,3] # Phần tử ở hàng thứ hai, cột thứ ba của ma trận M## [1] 8\nM[,3] # Lấy ra véc-tơ cột thứ 3 của ma trận M## [1] 7 8 9\nM[2,] # Lấy ra véc-tơ hàng thứ 2 của ma trận M## [1]  2  5  8 11\nchi_so_hang<-c(TRUE,FALSE,TRUE) # Chỉ số theo hàng kiểu logical\nchi_so_cot<-c(2,4) # Chỉ số cột theo kiểu số\nM[chi_so_hang,chi_so_cot] # Ma trận con của ma trận M##      [,1] [,2]\n## [1,]    4   10\n## [2,]    6   12"},{"path":"kiến-thức-r-nâng-cao.html","id":"các-phép-toán-trên-ma-trận","chapter":"Chương 5 Kiến thức R nâng cao","heading":"5.1.3 Các phép toán trên ma trận","text":"Các phép toán trên ma trận có ý nghĩa đặc biệt quan trọng trong phân tích dữ liệu. Ở các chương tiếp theo bạn đọc sẽ thấy rằng tất cả các tính toán nhằm biến đổi dữ liệu, hoặc ước lượng tham số cho các mô hình trên dữ liệu đều dựa trên các phép tính toán trên ma trận. Chúng tôi sẽ giải thích các phép toán này một cách đơn giản nhất để những bạn đọc không có nền tảng chuyên sâu về toán cũng có thể hiểu được. Tuy nhiên, để có kỹ năng thành thạo trong biến đổi dữ liệu, phân tích dữ liệu, và xây dựng các mô hình trên dữ liệu, chúng tôi khuyên bạn đọc nên tự trang bị cho mình các kiến thức về ma trận nói riêng và đại số tuyến tính nói chung.","code":""},{"path":"kiến-thức-r-nâng-cao.html","id":"phép-chuyển-vị","chapter":"Chương 5 Kiến thức R nâng cao","heading":"5.1.3.1 Phép chuyển vị","text":"Phép toán chuyển vị (transpose) là một phép toán biến đổi một ma trận \\(M\\) kích thước \\(n \\times p\\) thành một ma trận mới, ký hiệu là \\(M^T\\), có kích thước \\(p \\times n\\). Phần tử hàng thứ \\(j\\) và cột thứ \\(\\) của ma trận \\(M^T\\) bằng phần tử ở hàng thứ \\(\\) và cột thứ \\(j\\) của ma trận \\(M\\).\\[\\begin{align}\n\\begin{bmatrix}\nm_{11} & m_{12} & m_{13}\\\\\nm_{21} & m_{22} & m_{23}\n\\end{bmatrix}\n\n\\xrightarrow[\\text{(transpose)}]{\\text{chuyển vị}}\n\n\\begin{bmatrix}\nm_{11} & m_{21} \\\\\nm_{12} & m_{22} \\\\\nm_{13} & m_{23}\n\\end{bmatrix}\n\\tag{5.1}\n\\end{align}\\]Hàm số để thực hiện phép chuyển vị ma trận trong R là hàm t()Bạn đọc có thể thấy rằng nếu thực hiện phép chuyển vị hai lần liên tiếp ta sẽ thu được ma trận ban đầu","code":"\nM<-matrix(1:12, nrow = 3, ncol = 4, byrow = TRUE) # nrow: số hàng, ncol: số cột\nM # in M ra của sổ console##      [,1] [,2] [,3] [,4]\n## [1,]    1    2    3    4\n## [2,]    5    6    7    8\n## [3,]    9   10   11   12\nt(M) # ma trận chuyển vị của ma trận##      [,1] [,2] [,3]\n## [1,]    1    5    9\n## [2,]    2    6   10\n## [3,]    3    7   11\n## [4,]    4    8   12\nt(t(M)) # ma trận chuyển vị của ma trận chuyển vị là ma trận ban đầu##      [,1] [,2] [,3] [,4]\n## [1,]    1    2    3    4\n## [2,]    5    6    7    8\n## [3,]    9   10   11   12"},{"path":"kiến-thức-r-nâng-cao.html","id":"phép-nhân-ma-trận","chapter":"Chương 5 Kiến thức R nâng cao","heading":"5.1.4 Phép nhân ma trận","text":"Phép nhân ma trận (matrix multiplication) của ma trận \\(\\) với ma trận \\(B\\) chỉ thực hiện được nếu số cột của ma trận \\(\\) bằng với số hàng của ma trận \\(B\\). Giả sử rằng \\(\\) có kích thước là \\(n \\times p\\) và \\(B\\) có kích thước là \\(p \\times k\\) thì kết quả của phép nhân ma trận của ma trận \\(\\) với ma trận \\(B\\) là một ma trận \\(M\\) có kích thước \\(n \\times k\\), phần tử ở hàng thứ \\(\\) và cột thứ \\(j\\) của ma trận \\(M\\) là tích vô hướng giữa véc-tơ hàng \\(\\) của ma trận \\(\\) và véc-tơ cột \\(j\\) của ma trận \\(B\\). Nhắc lại với bạn đọc rằng tích vô hướng của hai véc-tơ \\(x\\) và \\(y\\) (phải) có cùng độ dài \\(n\\) được ký hiệu là \\(<x,y>\\) và được tính như sau\\[\\begin{align}\n<x,y> = \\sum\\limits_{= 1}^n \\ x_i y_i\n\\end{align}\\]trong đó \\(x_i\\), \\(y_i\\) lần lượt là phần tử thứ \\(\\) của véc-tơ \\(x\\) và véc-tơ \\(y\\).Để phân biệt phép nhân ma trận với phép nhân thông thường, chúng tôi sử dụng ký hiệu * cho phép nhân ma trận mà chỉ đơn giản ký hiệu phép nhân ma trận giữa ma trận \\(\\) và ma trận \\(B\\) là \\(AB\\). Công thức dưới đây mô tả phép nhân ma trận giữa một ma trận có 2 hàng và 3 cột với một ma trận có 3 hàng và 4 cột để được một ma trận có kích thước là 2 hàng và 4 cột\\[\\begin{align}\n& AB = M \\\\\n\n& \\begin{bmatrix}\na_{11} & a_{12} & a_{13}\\\\\na_{21} & a_{22} & a_{23}\n\\end{bmatrix}\n\n\\% * \\%\n\n\\begin{bmatrix}\nb_{11} & b_{12} & b_{13} & b_{14} \\\\\nb_{21} & b_{22} & b_{23} & b_{24} \\\\\nb_{31} & b_{32} & b_{33} & b_{34}\n\\end{bmatrix}\n\n=\n\n\\begin{bmatrix}\nm_{11} & m_{12} & m_{13} & m_{14} \\\\\nm_{21} & m_{22} & m_{23} & m_{24} \\\\\n\\end{bmatrix} \\\\\n\n& m_{ij} = <[,],B[,j]>\n\\tag{5.2}\n\\end{align}\\]trong đó \\([,]\\) là véc-tơ hàng \\(\\) của ma trận \\(\\) và \\(B[,j]\\) là véc-tơ cột \\(j\\) của ma trận \\(B\\).Toán tử dùng để thực hiện phép nhân ma trận trong R là %*%. Bạn đọc có thể thực hiện phép nhân hai ma trận và B như sauChúng ta có thể kiểm tra giá trị của phần tử ở hàng thứ 2 và cột thứ 3 của ma trận kết quả (số 100) chính là tích vô hướng giữa véc-tơ hàng thứ hai của ma trận và véc-tơ cột thứ ba của ma trận B.\\[\\begin{align}\n<(2,4,6),(7,8,9)> = 2 \\times 7 + 4 \\times 8 + 6 \\times 9 = 100\n\\end{align}\\]Bạn đọc cần phân biệt giữa phép nhân ma trận (ký hiệu %*%) và phép nhân thông thường (ký hiệu *) như đã trình bày ở trên. Để tránh gây nhầm lẫn, chúng tôi luôn sử dụng cụm từ nhân ma trận cho phép nhân %*%.","code":"\nA<-matrix(1:6, nrow = 2, ncol = 3) # ma trận kích thước 2 * 3\nprint(A)##      [,1] [,2] [,3]\n## [1,]    1    3    5\n## [2,]    2    4    6\nB<-matrix(1:12, nrow = 3, ncol = 4) # ma trận kích thước 3 * 4\nprint(B)##      [,1] [,2] [,3] [,4]\n## [1,]    1    4    7   10\n## [2,]    2    5    8   11\n## [3,]    3    6    9   12\nM <- A %*% B # kết quả là ma trận M có kich thước 2 * 4\nprint(M)##      [,1] [,2] [,3] [,4]\n## [1,]   22   49   76  103\n## [2,]   28   64  100  136"},{"path":"kiến-thức-r-nâng-cao.html","id":"ma-trận-đường-chéo-và-ma-trận-đơn-vị","chapter":"Chương 5 Kiến thức R nâng cao","heading":"5.1.5 Ma trận đường chéo và ma trận đơn vị","text":"Các khái niệm và phép toán trên ma trận được trình bày bắt đầu từ phần này sẽ chỉ áp dụng trên ma trận vuông, nghĩa là ma trận có số hàng bằng với số cột. Trong một ma trận vuông, các phần tử nằm trên đường chéo chính là các phần tử có chỉ số hàng bằng với chỉ số cột, các phần tử nằm trên đường chéo phụ là các phần tử có chỉ số hàng cộng với chỉ số cột bằng \\(n+1\\) trong đó \\(n\\) là số hàng (hay cột) của ma trận vuông. Với ma trận vuông M có kích thước \\(n \\times n\\) đường chéo chính của ma trận được ký hiệu là diag(M) xác định như sau\\[\\begin{align}\n& M\n=\n\\begin{bmatrix}\nm_{11} & m_{12} & m_{13} & m_{14} \\\\\nm_{21} & m_{22} & m_{23} & m_{24} \\\\\nm_{31} & m_{32} & m_{33} & m_{34}\\\\\nm_{41} & m_{42} & m_{43} & m_{44}\n\\end{bmatrix} \\\\\n& \\\\\n& diag(M) = (m_{11}, m_{22}, m_{33},  m_{44})\n\\tag{5.3}\n\\end{align}\\]Ma trận có tất cả các phần tử nằm ngoài đường chéo chính bằng 0 được gọi là ma trận đường chéo. Hàm diag() trong R được sử dụng để lấy ra véc-tơ đường chéo chính của một ma trận vuông, hoặc để tạo ra một ma trận đường chéo, hoặc cũng có thể dùng để khai báo một ma trận đường chéo:Ma trận đơn vị kích thước \\(n \\times n\\), thường được ký hiệu \\(I_n\\), là một ma trận đường chéo mà tất cả các phần tử trên đường chéo chính bằng 1. Ma trận đơn vị \\(I_n\\) có tính chất quan trọng là mọi ma trận M kích thước \\(k \\times n\\) khi thực hiện phép nhân ma trận với ma trận \\(I_n\\) sẽ có kết quả đúng bằng ma trận M. Bạn đọc có thể quan sát ví dụ sau:","code":"\nM<-matrix(1:9, nrow = 3, ncol = 3)\ndiag(M) # Lấy ra véc-tơ đường chéo chính của M## [1] 1 5 9\n# Tạo ra ma trận đường chéo có đường chéo chính là (1,10,100)\nM1<-diag(c(1,10,100))\nprint(M1)##      [,1] [,2] [,3]\n## [1,]    1    0    0\n## [2,]    0   10    0\n## [3,]    0    0  100\nIn<-diag(rep(1,4)) # ma trận đơn vị kích thước 4*4\nprint(In)##      [,1] [,2] [,3] [,4]\n## [1,]    1    0    0    0\n## [2,]    0    1    0    0\n## [3,]    0    0    1    0\n## [4,]    0    0    0    1\nM<-matrix(1:20, nrow = 5, ncol = 4) # ma trận vuông 5 * 4\nprint(M %*% In) # kết quả vẫn là ma trận M##      [,1] [,2] [,3] [,4]\n## [1,]    1    6   11   16\n## [2,]    2    7   12   17\n## [3,]    3    8   13   18\n## [4,]    4    9   14   19\n## [5,]    5   10   15   20"},{"path":"kiến-thức-r-nâng-cao.html","id":"định-thức-của-ma-trận","chapter":"Chương 5 Kiến thức R nâng cao","heading":"5.1.6 Định thức của ma trận","text":"Định thức của ma trận là một khái niệm toán học phức tạp. Phần lớn bạn đọc khi làm quen với khái niệm định thức trong môn học đại số tuyến tính sẽ được giới thiệu về công thức tính định thức của một ma trận, hoặc sử dụng định thức của ma trận để thực hiện tính toán như giải hệ phương trình thay vì thực sự hiểu khái niệm định thức được bắt đầu từ đâu. Định thức là một giá trị số thực đặc trưng của một ma trận vuông. Định thức cho biết nhiều tính chất quan trọng của ma trận đó, đồng thời định thức xuất hiện trong rất nhiều tính toán liên quan đến ma trận.Hãy bắt đầu với một ma trận vuông \\(M\\) kích thước \\(2 \\times 2\\) như sau:\n\\[\\begin{align}\nM\n=\n\\begin{bmatrix}\nm_{11} & m_{12}\\\\\nm_{21} & m_{22}\n\\end{bmatrix}\n\\end{align}\\]Định thức của ma trận \\(M\\) được ký hiệu là \\(|M|\\) hoặc \\(det(M)\\) được xác định bởi công thức\n\\[\\begin{align}\ndet(M) = m_{11} \\times m_{22} - m_{12} \\times m_{21}\n\\tag{5.4}\n\\end{align}\\]Định thức của ma trận M có thể biểu diễn dưới dạng diện tích của hình bình hành tạo thành từ 4 điểm có tọa độ \\((0,0)\\), \\((m_{11},m_{12})\\), \\((m_{21},m_{22})\\), và \\((m_{11}+m_{21}, m_{12}+m_{22})\\). Thật vậy, giả sử hai ma trận \\(M\\) và \\(M_1\\) có kích thước \\(2 \\times 2\\); ma trận \\(M_1\\) nhận được bằng cách đổi vị trí 2 dòng của ma trận \\(M\\),\\[\\begin{align}\n& M\n=\n\\begin{bmatrix}\n2 & 1 \\\\\n1 & 3\n\\end{bmatrix}  \\text{  và  } M_1\n=\n\\begin{bmatrix}\n1 & 3 \\\\\n2 & 1\n\\end{bmatrix} \\\\\n& \\\\\n\n& det(M) = 5  \\text{  và  } det(M_1) = -5\n\n\\end{align}\\]Hiểu một cách đơn giản, chúng ta có thể coi định thức của \\(M\\) và \\(M_1\\) qua diện tích của các hình bình hành tạo bởi hai véc-tơ (2,1) và véc-tơ (1,3) giống như hình ??. Sự khác nhau của hai định thức chỉ ở thứ tự véc-tơ: đối với định thức của \\(M\\) thì véc-tơ \\((2,1)\\) được đặt trước trong khi đối với định thức của \\(M_1\\) thì véc-tơ \\((1,3)\\) được tính đến trước.\n(#fig:fgadvr01 )Định thức của ma trận kích thước 2 * 2 có thể được biểu diễn dưới dạng diện tích. Hình bên trái: Định thức là số dương vì véc-tơ hàng thứ nhất (2,1) nằm phía bên phải véc-tơ hàng thứ hai (1,3). Hình bên phải: Định thức là số âm vì véc-tơ hàng thứ nhất (1,3) nằm phía bên trái véc-tơ hàng thứ hai (2,1)\nGiá trị của định thức sẽ cho ta biết thông tin về các véc-tơ tạo nên ma trận:Định thức bằng 0 chỉ xảy ra khi hai véc-tơ (hai mũi tên màu xanh và màu cam xuất phát từ điểm \\((0,0)\\)) trùng nhau hoặc đối đỉnh nhau. Điều này chỉ xảy ra khi véc-tơ thứ nhất bằng véc-tơ thứ hai nhân với một số. Trong trường hợp tổng quát với ma trận vuông kích thước \\(n \\times n\\), định thức bằng 0 khi một véc-tơ nào đó là tổ hợp tuyến tính của các véc-tơ còn lại.Định thức gần bằng 0 nghĩa là góc tạo bởi 2 véc-tơ rất gần 0 hoặc tạo với nhau một góc xấp xỉ 180 độ. Ma trận vuông kích thước \\(n \\times n\\) có định thức xấp xỉ bằng 0 nghĩa là mối liên hệ tuyến tính giữa các véc-tơ của ma trận là rất chặt chẽ.Dấu của định thức cho ta biết vị trí của các véc-tơ. Véc-tơ hàng thứ nhất tương ứng với màu xanh trong khi véc-tơ hàng thứ hai tương ứng với màu cam. Dấu của định thức dương chỉ khi véc-tơ màu xanh nằm phía trên (bên trái) véc-tơ màu cam, và dấu của định thức là âm chỉ khi véc-tơ màu xanh nằm phía dưới (bên phải) véc-tơ màu cam.Với các ma trận vuông kích thước \\(n \\times n\\); \\(n \\geq 3\\) định thức của ma trận được tính bằng cách lựa chọn một dòng (hoặc cột) thứ \\(\\) bất kỳ và sau đó thực hiện phép khai triển\n\\[\\begin{align}\ndet(M) = \\sum\\limits_{j = 1}^n (-1)^{+j} \\times m_{ij} \\times det(M_{-,-j})\n\\tag{5.5}\n\\end{align}\\]\ntrong đó \\(M_{\\{-,-j\\}}\\) mà ma trận vuông kích thước \\((n-1) \\times (n-1)\\) nhận được sau khi bỏ đi hàng thứ \\(\\) và cột thứ \\(j\\) của ma trận \\(M\\).Định thức của ma trận \\(M\\) kích thước \\(3 \\times 3\\) có thể tính toán dựa trên định thức của các ma trận con và lựa chọn hàng \\(=2\\) như sau\\[\\begin{align}\n& M\n=\n\\begin{bmatrix}\nm_{11} & m_{12} & m_{13}  \\\\\nm_{21} & m_{22} & m_{23} \\\\\nm_{31} & m_{32} & m_{33}\n\\end{bmatrix} \\\\\n& \\\\\n& det(M) = - m_{21} \\times \\begin{vmatrix}\nm_{12} & m_{13}  \\\\\nm_{32} & m_{33}\n\\end{vmatrix} + m_{22} \\times \\begin{vmatrix}\nm_{11} & m_{13}  \\\\\nm_{31} & m_{33}\n\\end{vmatrix} - m_{23} \\times \\begin{vmatrix}\nm_{11} & m_{12}  \\\\\nm_{31} & m_{32}\n\\end{vmatrix}\n\\tag{5.6}\n\\end{align}\\]Hàm det() trong R được sử dụng để tính định thức của ma trận.Một vài tính chất quan trọng của định thức:Định thức của một ma trận đường chéo bằng tích các phần tử nằm trên đường chéo chính của ma trận đó. Ma trận đường chéo là một trường hợp đặc biệt của ma trận tam giác. Ma trận tam giác trên là ma trận có tất cả các phần tử nằm phía dưới đường chéo chính nhận giá trị bằng 0. Tương tự, ma trận tam giác dưới là ma trận có tất cả các phần tử nằm phía trên đường chéo chính nhận giá trị bằng 0. Các ma trận tam giác có tính chất như đã phát biểu ở trên: định thức của các ma trận này bằng tích các phần tử nằm trên đường chéo chính.Định thức của ma trận chuyển vị của một ma trận bằng định thức của ma trận đó. Bạn đọc có thể kiểm tra bằng câu lệnh trên R như sau\n\\[\\begin{align}\ndet(M) = det(M^T)\n\\end{align}\\]Định thức của tích hai ma trận bằng tích của các định thức. Lưu ý rằng phép nhân hai ma trận vuông chỉ có ý nghĩa khi đây là hai ma trận vuông có cùng kích thước:\n\\[\\begin{align}\ndet(\\% * \\% B) = det() \\times det(B)\n\\end{align}\\]","code":"\nM<-matrix(c(2,1,1,3),nrow = 2,ncol = 2,byrow = TRUE)\ndet(M)## [1] 5\nM<-matrix(1:16,nrow = 4,ncol = 4)\nM[lower.tri(M)]<-0 # cho các phần tử phía dưới đường chéo chính bằng 0\nprint(M) # ma trận M là ma trận tam giác trên##      [,1] [,2] [,3] [,4]\n## [1,]    1    5    9   13\n## [2,]    0    6   10   14\n## [3,]    0    0   11   15\n## [4,]    0    0    0   16\nprint(c(det(M), prod(diag(M)))) # định thức của M bằng tích các số trên đường chéo chính## [1] 1056 1056\nM<-matrix(rnorm(16),nrow = 4,ncol = 4)\nprint(c(det(M),det(t(M))))## [1] 8.130451 8.130451\nM<-matrix(rnorm(16),nrow = 4,ncol = 4)\nM1<-matrix(rnorm(16),nrow = 4,ncol = 4)\nprint(c(det(M%*%M1),det(M)*det(M1)))## [1] 92.82662 92.82662"},{"path":"kiến-thức-r-nâng-cao.html","id":"ma-trận-nghịch-đảo","chapter":"Chương 5 Kiến thức R nâng cao","heading":"5.1.7 Ma trận nghịch đảo","text":"Ma trận nghịch đảo của một ma trận vuông \\(M\\), thường được ký hiệu \\(M^{-1}\\), là ma trận vuông có cùng kích thước với ma trận \\(M\\) và thỏa mãn tính chất: phép nhân ma trận giữa ma trận \\(M\\) với ma trận nghịch đảo \\(M^{-1}\\) sẽ cho kết quả là một ma trận đơn vị. Không phải ma trận vuông nào cũng có ma trận nghịch đảo; chỉ có các ma trận có định thức khác 0 là có ma trận nghịch đảo. Các ma trận có ma trận nghịch đảo được còn được gọi là các ma trận khả nghịch. Các ma trận khả nghịch luôn có một ma trận nghịch đảo duy nhất.\\[\\begin{align}\nM \\ \\% * \\% \\ M^{-1} = I_n\n\\tag{5.7}\n\\end{align}\\]Hai lần lấy nghịch đảo liên tiếp với một ma trận khả nghịch sẽ quay trở lại ma trận ban đầu, hay nói một cách khác ma trận nghịch đảo của ma trận \\(M^{-1}\\) chính là ma trận \\(M\\)\n\\[\\begin{align}\nM^{-1} \\ \\% * \\% \\ M = I_n\n\\end{align}\\]Phương pháp chung để tính toán ma trận nghịch đảo là dựa trên các ma trận liên hợp (adjugate matrix). Ma trận liên hợp của ma trận \\(M\\) được ký hiệu là \\(adj(M)\\) là ma trận vuông kích thước \\(n \\times n\\) mà phần tử ở hàng thứ \\(\\), cột thứ \\(j\\) được tính bằng\n\\[\\begin{align}\nadj(M)_{ij} = (-1)^{j+} det(M_{\\{-j,-\\}})\n\\tag{5.8}\n\\end{align}\\]Khi tính toán định thức của ma trận \\(M\\), chúng tôi đã sử dụng ký hiệu \\(M_{\\{-j,-\\}}\\) cho ma trận vuông kích thước \\((n-1) \\times (n-1)\\) nhận được sau khi bỏ đi hàng thứ \\(j\\) và cột thứ \\(\\) của ma trận \\(M\\). Bạn đọc lưu ý rằng có sự thay đổi vị trí của \\(\\) và \\(j\\) trong vế phải của phương trình (5.8). Ma trận nghịch đảo \\(M^{-1}\\) được tính từ ma trận liên hợp như sau\n\\[\\begin{align}\nM^{-1} = \\cfrac{1}{det(M)} adj(M)_{ij}\n\\tag{5.9}\n\\end{align}\\]Nhìn chung, để tính toán ma trận nghịch đảo của một ma trận kích thước \\(n \\times n\\), chúng ta sẽ phải tính toán định thức của ma trận ban đầu và định thức của \\(n^2\\) ma trận vuông có kích thước \\((n-1) \\times (n-1)\\). Với các ma trận vuông có kích thước lớn, việc tính toán sử dụng công thức như trên sẽ tốn nhiều thời gian và bộ nhớ. Có nhiều thuật toán để tính xấp xỉ ma trận nghịch đảo của một ma trận. Nội dung của các thuật toán này vượt quá phạm vi của cuốn sách. Bạn đọc có thể sử dụng hàm solve() có sẵn trong để tính toán ma trận nghịch đảo như sauCác tính toán liên quan đến định thức cần nhớĐịnh thức của ma trận sau khi nhân tất cả các phần tử với một số\n\\[\\begin{align}\ndet(\\lambda M) = \\lambda^n \\times det(M)\n\\tag{5.10}\n\\end{align}\\]Định thức của ma trận sau khi nhân tất cả các phần tử với một số\n\\[\\begin{align}\ndet(\\lambda M) = \\lambda^n \\times det(M)\n\\tag{5.10}\n\\end{align}\\]Định thức của ma trận chuyển vị \\(M^{-T}\\) bằng định thức của ma trận \\(M\\)\n\\[\\begin{align}\ndet(M^T) = det(M)\n\\tag{5.11}\n\\end{align}\\]Định thức của ma trận chuyển vị \\(M^{-T}\\) bằng định thức của ma trận \\(M\\)\n\\[\\begin{align}\ndet(M^T) = det(M)\n\\tag{5.11}\n\\end{align}\\]Tích của định thức của ma trận \\(M^{-1}\\) với định thức của ma trận \\(M\\) bằng 1.\n\\[\\begin{align}\ndet(M^{-1}) \\times det(M) = det(I_n) = 1\n\\tag{5.12}\n\\end{align}\\]Tích của định thức của ma trận \\(M^{-1}\\) với định thức của ma trận \\(M\\) bằng 1.\n\\[\\begin{align}\ndet(M^{-1}) \\times det(M) = det(I_n) = 1\n\\tag{5.12}\n\\end{align}\\]","code":"\nset.seed(10)\n\n# Tạo ma trận gồm toàn các số ngẫu nhiên\nM<-matrix(rnorm(16),nrow = 4,ncol = 4)\nM1<-solve(M) # Ma trận M1 là ma trận nghịch đảo của ma trận M\n\n# Kiểm tra:\nM1 %*% M # Tích của M1 với M là ma trận đơn vị##               [,1]         [,2]          [,3]          [,4]\n## [1,]  1.000000e+00 1.371264e-16 -2.640509e-16 -2.563393e-16\n## [2,]  2.593928e-18 1.000000e+00  4.325543e-17  2.018082e-16\n## [3,] -1.526477e-17 2.711603e-17  1.000000e+00  3.759516e-17\n## [4,]  2.101255e-16 1.072900e-16 -6.432270e-17  1.000000e+00\nM %*% M1 # Tích của M với M1 là ma trận đơn vị##               [,1]          [,2]         [,3]          [,4]\n## [1,]  1.000000e+00 -2.732291e-17 2.528256e-17 -1.023377e-16\n## [2,] -8.333103e-17  1.000000e+00 6.305725e-17 -3.129494e-16\n## [3,] -4.952797e-17 -5.792706e-18 1.000000e+00  1.667626e-17\n## [4,]  3.116926e-18  1.904906e-17 8.713393e-17  1.000000e+00"},{"path":"kiến-thức-r-nâng-cao.html","id":"mảng-nhiều-chiều","chapter":"Chương 5 Kiến thức R nâng cao","heading":"5.2 Mảng nhiều chiều","text":"Ma trận lưu phần tử trong hai chiều mà chúng ta gọi là hàng và cột. Đa số dữ liệu kiểu bảng biểu truyền thống đều có thể biểu diễn dưới dạng ma trận. Tuy nhiên có những kiểu dữ liệu mà khi biểu diễn dưới dạng ma trận hai chiều là không dễ dàng và có thể gây nhầm lẫn cho người sử dụng. Có thể kể đến dữ liệu kiểu hình ảnh. Khi bạn đọc lưu một bức ảnh màu lên trên máy tính điện tử, bức ảnh sẽ được số hóa thành một mảng ba chiều, bao gồm có chiều cao, chiều rộng của ảnh và một chiều thứ ba là màu sắc của điểm ảnh. Phức tạp hơn nữa nếu dữ liệu là một đoạn phim, hay một hình động, bạn đọc sẽ cần phải sử dụng thêm chiều thứ tư để mô tả thời gian xuất hiện của mỗi hình ảnh trong đoạn phim.Thông thường thì người làm việc với dữ liệu sẽ đổi các mảng nhiều chiều về mảng hai chiều hoặc một chiều (véc-tơ) để dễ dàng xử lý. Tuy nhiên, bạn đọc cũng cần nằm được các thao tác cơ bản khi làm việc với mảng nhiều chiều để xử lý được các dữ liệu như đề cập ở trên.","code":""},{"path":"kiến-thức-r-nâng-cao.html","id":"khởi-tạo-mảng-nhiều-chiều","chapter":"Chương 5 Kiến thức R nâng cao","heading":"5.2.1 Khởi tạo mảng nhiều chiều","text":"Để tạo mảng nhiều chiều bạn đọc sử dụng hàm array().Bạn đọc có thể thấy R hiển thị mảng ba chiều Ar kích thước \\(2 \\times 3 \\times 4\\) như là sự kết hợp của 4 ma trận kích thước \\(2 \\times 3\\). Tương tự như khi khởi tạo giá trị cho ma trận, số lượng phần tử đưa vào trong mảng phải bằng với số phần tử của mảng, trong trường hợp mảng Ar ở trên là véc-tơ có độ dài 24 tương ứng với \\(2 \\times 3 \\times 4 = 24\\) phần tử của mảng.Để lấy ra các phần tử con (một biến, một ma trận, hay 1 mảng nhiều chiều) từ một nhiều mảng, bạn đọc sử dụng dấu ngoặc vuông [] giống như khi làm với ma trận. Lưu ý rằng khi lấy phần tử con từ một mảng, bạn đọc cần phải sử dụng chỉ số cho tất cả các chiều.Thứ tự các phần tử khi điền vào mảng khi sử dụng hàm trong hàm array() sẽ là ưu tiên ma trận kích thước \\(2 \\times 3\\) tương ứng với chỉ số [,,1] trước, rồi đến ma trận kích thước \\(2 \\times 3\\) tương ứng với chỉ số [,,2], …, và tiếp tục như thế cho đến khi tất cả các phần tử của mảng được gán giá trị.","code":"\nAr<-array(1:24,dim=c(2,3,4)) # mảng 3 chiều\nAr # hiển thị mảng 3 chiều Ar## , , 1\n## \n##      [,1] [,2] [,3]\n## [1,]    1    3    5\n## [2,]    2    4    6\n## \n## , , 2\n## \n##      [,1] [,2] [,3]\n## [1,]    7    9   11\n## [2,]    8   10   12\n## \n## , , 3\n## \n##      [,1] [,2] [,3]\n## [1,]   13   15   17\n## [2,]   14   16   18\n## \n## , , 4\n## \n##      [,1] [,2] [,3]\n## [1,]   19   21   23\n## [2,]   20   22   24\nAr[1,2,1] # phần tử có các chỉ số 1 - 2 - 1## [1] 3\nAr[,,1] # ma trận 2 * 3##      [,1] [,2] [,3]\n## [1,]    1    3    5\n## [2,]    2    4    6\nAr[,c(1,3),c(1,4)] # mảng trận 2 * 2 * 2## , , 1\n## \n##      [,1] [,2]\n## [1,]    1    5\n## [2,]    2    6\n## \n## , , 2\n## \n##      [,1] [,2]\n## [1,]   19   23\n## [2,]   20   24"},{"path":"kiến-thức-r-nâng-cao.html","id":"sử-dụng-mảng-nhiều-chiều-để-biến-đổi-dữ-liệu-kiểu-hình-ảnh","chapter":"Chương 5 Kiến thức R nâng cao","heading":"5.2.2 Sử dụng mảng nhiều chiều để biến đổi dữ liệu kiểu hình ảnh","text":"Để bạn đọc có cái nhìn trực quan hơn về mảng nhiều chiều, chúng ta sẽ thực hiện các phép biến đổi, tính toán trên một dữ liệu cụ thể. Như chúng tôi đã nói ở trên, mảng nhiều chiều là một đối tượng thích hợp dùng để lưu dữ liệu kiểu hình ảnh. Thư viện imager có thể được cài đặt bổ sung vào R có các hàm thích hợp để làm việc với dữ liệu kiểu hình ảnh. Chúng ta sẽ sử dụng một mảng nhiều chiều để lưu một bức ảnh và thực hiện các phép biến đổi bức ảnh đó sử dụng tính toán trên mảng nhiều chiều.Hàm load.image() trong thư viện imager có thể đọc các file hình ảnh có định dạng .png, .jpeg, hoặc .bmp. Bạn đọc có thể đọc một hình ảnh có một trong các định dạng kể trênĐể biết img được đọc bằng hàm load.image() là kiểu đối tượng nào, bạn đọc dùng hàm class()R cho biết đây là một đối tượng kiểu cimg. Kiểu đối tượng này về bản chất là một mảng bốn chiều. Chiều thứ nhất là cho biết chiều rộng của bức ảnh, chiều thứ hai cho biết chiều cao của bức ảnh, chiều thứ ba là chiều thời gian nên luôn bằng 1 đối với dữ liệu kiểu ảnh, và chiều thứ tư bằng 3 nếu bức ảnh là ảnh màu.Đối tượng kiểu cimg cho phép bạn đọc thực hiện các biến đổi, tính toán giống như trên một mảng nhiều chiều mà không cần phải chuyển đổi sang kiểu mảng. Chẳng hạn như để biết bức ảnh được lưu bởi đối tượng tên img có bao nhiêu chiều, chúng ta sử dụng hàm dim() giống như với mảngBức ảnh được lưu bởi đối tượng img ở trên là một bức ảnh màu có chiều rộng 535 và chiều cao 595. Chiều thứ ba bằng 1 nghĩa là đây là một bức ảnh (chiều thứ ba lớn hơn 1 khi đối tượng là hình ảnh động hoặc video). Chiều thứ tư bằng 3 đại diện cho 3 sắc màu: màu đỏ (Red), màu xanh lá cây (Green), và màu xanh da trời (Blue). Bạn đọc có thể hình dung một bức ảnh màu ở trên như là sự kết hợp của ba ma trận cùng kích thước 535 \\(\\times\\) 595, ma trận thứ nhất đại diện cho màu đỏ, ma trận thứ hai đại diện cho màu xanh lá cây và ma trận thứ ba đại diện cho màu xanh da trời. Mỗi giá trị trong ma trận là một số trong khoảng từ 0 đến 1. Giá trị 0 tương ứng với màu đen và giá trị càng gần 1 thì màu sắc của điểm đó càng gần màu mà ma trận đại diện. Để quan sát ma trận tương ứng với mỗi màu, bạn đọc cần gán giá trị của 2 ma trận còn lại bằng 0 trước khi hiển thị.Hàm .cimg() được dùng để đổi một mảng bốn chiều sang kiểu cimg để có thể hiển thị khi sử dụng hàm plot(). Lưu ý rằng hãy luôn sử dụng chiều thứ ba bằng 1 và chiều thứ tư bằng 3 nếu bạn muốn tạo ảnh màu. Các câu lệnh dưới đây tạo ra các bức ảnh mà các giá trị trong các ma trận màu sắc hoàn toàn là các giá trị ngẫu nhiên phân phối đều (uniform) trong khoảng (0,1).Tham số interpolate nhận giá trị bằng FALSE có nghĩa là các điểm ảnh giữ nguyên giá trị. Tham số này có giá trị là mặc định là TRUE. Khi interpolate nhận giá trị bằng mặc định, hình ảnh hiển thị sẽ có sự giao thoa về màu sắc tại viền các điểm ảnh và làm cho ảnh nhìn mượt mà hơn.Về bản chất, xử lý ảnh trên máy tính điện tử chính là xử lý các con số nằm trong mảng nhiều chiều. Chúng tôi sẽ giới thiệu một vài kỹ thuật xử lý đơn giản trên ảnh để bạn đọc có thể hiểu hơn về xử lý mảng nhiều chiều.Trước hết là thao tác cắt ảnh. Cắt ảnh chính là một phép lấy mảng con từ một mảng ban đầu. Thật vậy, trong ví dụ dưới đây chúng tôi cắt bức ảnh được lưu trong đối tượng tên là img thành hai nửa, bức ảnh được chia theo chiều rộng:Tiếp theo, chúng ta sẽ thực hiện tăng hoặc giảm độ sáng của ảnh. Tăng hoặc giảm độ sáng của ảnh tương đương với việc điều chỉnh đồng thời các số trong mảng nhiều chiều gần hơn đến giá trị 1 hoặc gần hơn đến giá trị 0 theo cùng một tỷ lệ. Chúng ta thực hiện như sauMột kỹ thuật xử lý ảnh khác là giảm kích thước của ảnh. Giả sử bạn đọc muốn giảm kích thước ảnh mỗi chiều 50%. Để làm được việc này, mỗi ma trận kích thước \\(n \\times m\\) sẽ được đổi thành ma trận kích thước \\([n/2] \\times [m/2]\\) trong đó \\([n/2]\\) là phần nguyên của số \\(n/2\\). Nguyên tắc chuyển từ ma trận ban đầu sang ma trận có kích thước nhỏ hơn là mỗi ô \\(2 \\times 2\\) của ma trận ban đầu được chuyển thành 1 số (ô) trong ma trận mới. Giá trị mới này thường là giá trị trung bình, hoặc max, min của 4 phần tử của ma trận ban \\(2 \\times 2\\)Bạn đọc có thể thấy rằng giảm kích thước của ảnh cũng là giảm kích thước của các mảng nhiều chiều. Khi kích thước ảnh giảm, khả năng hiển thị của ảnh cũng bị ảnh hưởng. Chúng ta sẽ quay trở lại với dữ liệu kiểu hình ảnh khi thảo luận về Mô hình mạng nơ-ron tích chập.","code":"\n#install.packages(\"imager\")\nlibrary(imager)\nsetwd(\"../KHDL_KTKD Final/Image\")\nimg<-load.image(\"cat.jpg\") # đọc hình ảnh tên \"cat\" vào\nplot(img)\nclass(img)## [1] \"cimg\"         \"imager_array\" \"numeric\"\ndim(img) # mảng bốn chiều: chiều rộng * chiều cao * chiều thời gian * chiều màu sắc## [1] 535 595   1   3\nimg_red<-img\nimg_red[,,1,2:3]<-0 # cho 2 ma trận màu xanh lá và xanh dương bằng 0\n\nimg_green<-img\nimg_green[,,1,c(1,3)]<-0 # cho 2 ma trận màu đỏ và xanh dương bằng 0\n\nimg_blue<-img\nimg_blue[,,1,1:2]<-0 # cho 2 ma trận màu xanh lá và đỏ bằng 0\n\npar(mfrow = c(1,3))\nplot(img_red, main = \"Chỉ còn lại màu đỏ\")\nplot(img_green, main = \"Chỉ còn lại màu xanh lá\")\nplot(img_blue, main = \"Chỉ còn lại màu xanh dương\")\nimg1<-array(runif(5*5*1*3),dim=c(5,5,1,3)) # bức ảnh màu kích thước 5*5\nimg1<-as.cimg(img1)\n\nimg2<-array(runif(1000*1000*1*3),dim=c(1000,1000,1,3)) # bức ảnh màu kích thước 1000*1000\nimg2<-as.cimg(img2)\n\npar(mfrow = c(1,2))\nplot(img1, interpolate = FALSE, main = \"Ảnh nhiễu 5 * 5\")\nplot(img2, interpolate = FALSE, main = \"Ảnh nhiễu 1000 * 1000\")\nn<-dim(img)[1] # Chiều rộng của ảnh\nk<-round(n/2) # Điểm giữa để chia ảnh làm hai nửa\nimg1<-img[1:k,,,] # img1 là nửa bên trái của ảnh\nimg2<-img[(k+1):n,,,] # img2 là nửa bên phải của ảnh\npar(mfrow = c(1,2))\nplot(as.cimg(img1), main = \"Nửa bên trái\")\nplot(as.cimg(img2), main = \"Nửa bên phải\")\nimg1<-img + (1 - img) * 0.3 # img1 là bức ảnh sau khi tăng độ sáng lên 30%\nimg2<-img - img * 0.5 # img2 là bức ảnh sau khi giảm độ sáng đi 50%\npar(mfrow = c(1,3))\nplot(img, rescale = FALSE, main= \"Ảnh ban đầu\")\nplot(as.cimg(img1), rescale = FALSE, main = \"Tăng độ sáng\")\nplot(as.cimg(img2),rescale = FALSE, main = \"Giảm độ sáng\")\ngiamchieu<-function(M,k){\n  n<-dim(M)[1]; m<-dim(M)[2]\n  n1<-round(n/k)-1; m1<-round(m/k)-1\n  M1<-matrix(0,n1,m1)\n  for (i in 1:n1){\n    for (j in 1:m1){\n      i1<-(k*(i-1)+1):(k*i)\n      j1<-(k*(j-1)+1):(k*j)\n      M1[i,j]<-mean(M[i1,j1],na.rm=TRUE)\n    }\n  }\n  return(M1)\n}\n\nn<-dim(img)[1]; m<-dim(img)[2]\nk1<-10; k2<-20\nn1<-round(n/k1)-1; m1<-round(m/k1)-1\nn2<-round(n/k2)-1; m2<-round(m/k2)-1\n\nimg1<-array(0,dim=c(n1,m1,1,3));\nimg2<-array(0,dim=c(n2,m2,1,3));\n\nfor (i in 1:3){\n  img1[,,1,i]<-giamchieu(img[,,1,i],k1)\n  img2[,,1,i]<-giamchieu(img[,,1,i],k2)\n}\n\npar(mfrow = c(1,3))\nplot(img, interpolate = FALSE, main= \"Ảnh ban đầu\")\nplot(as.cimg(img1), interpolate = FALSE, main = \"Giảm kích thước 1:10\")\nplot(as.cimg(img2), interpolate = FALSE, main = \"Giảm kích thước 1:20\")"},{"path":"kiến-thức-r-nâng-cao.html","id":"list-trong-r","chapter":"Chương 5 Kiến thức R nâng cao","heading":"5.3 List trong R","text":"Không giống như véc-tơ, ma trận, hay mảng nhiều chiều, list là một cấu trúc trong R mà có thể chứa nhiều kiểu đối tượng khác nhau bao gồm biến, véc-tơ, ma trận, và cả các list khác. Với những bạn đọc đã học qua Python, list cũng giống như một dictionary. Đối với các bạn đọc đã học qua ngôn ngữ lập trình C++, list tương tự như một struct. list đóng vai trò quan trọng trong R, đặc biệt là trong viết hàm số và lập trình hướng đối tượng.Trong phần này của cuốn sách, chúng ta sẽ tìm hiểu cách tạo ra list và ứng dụng cấu trúc của list để phục vụ công việc phân tích dữ liệu một cách hiệu quả nhất.","code":""},{"path":"kiến-thức-r-nâng-cao.html","id":"khởi-tạo-list-và-chỉ-số-của-list.","chapter":"Chương 5 Kiến thức R nâng cao","heading":"5.3.1 Khởi tạo list và chỉ số của list.","text":"Hàm số để tạo ra một list trong R là hàm list(). Giả sử chúng ta muốn tạo thành một đối tượng có tên Sv1 chứa các thông tin về một sinh viênTên của sinh viên: được lưu trong một biến kiểu chuỗi ký tự.\nTên của sinh viên: được lưu trong một biến kiểu chuỗi ký tự.Ngày sinh của sinh viên: được lưu trong một biến kiểu thời gian.\nNgày sinh của sinh viên: được lưu trong một biến kiểu thời gian.Giới tính của sinh viên: được lưu trong một biến kiểu logic, giá trị TRUE tương ứng với giới tính Nam, và FALSE tương ứng với giới tính nữ.\nGiới tính của sinh viên: được lưu trong một biến kiểu logic, giá trị TRUE tương ứng với giới tính Nam, và FALSE tương ứng với giới tính nữ.Bảng điểm của sinh viên: là một data.frame có 2 cột, 1 cột là tên môn học và một cột là điểm của môn học tương ứng.\nBảng điểm của sinh viên: là một data.frame có 2 cột, 1 cột là tên môn học và một cột là điểm của môn học tương ứng.Chúng ta sử dụng hàm list() để tạo ra một list có tên SV1 như sauBạn đọc có thể thấy SV1 có bốn đối tượng con có tên là Ten, Ngay_sinh, Gioi_tinh, và Bang_diem. Mỗi đối tượng con có một kiểu giá trị khác nhau, riêng đối tượng con Bang_diem là một dữ liệu hay còn được gọi là một data.frame.Để lấy ra một đối tượng con của list bạn đọc sử dụng ký hiệu $. Ví dụ bạn muốn hiển thị bảng điểm của sinh viên có thông tin được lưu trong SV1, hãy sử dụng câu lệnh sau:Để biết tên các đối tượng trong một list bạn đọc sử dụng hàm names():Một cách khác để lấy ra một đối tượng con của list là sử dụng chỉ số của đối tượng. bảng điểm nằm ở vị trí thứ 4 trong list nên bạn đọc sử dụng câu lệnh sauBạn đọc có thể thấy rằng để lấy ra phần tử con, chúng ta cần phải sử dụng hai lần dấu ngoặc vuông [[]]. Nếu chỉ sử dụng một dấu ngoặc vuông [], phần tử được lấy ra sẽ là một list có 1 phần tử và phần tử duy nhất đó là bảng điểm.Để thêm một đối tượng vào list, chúng ta có thể đặt tên trực tiếp cho đối tượng mới và gán giá trị cho đối tượng. Ví dụ như chúng ta muốn thêm thông tin về quê quán của sinh viên vào một đối tượng có tên là \\(que\\_quan\\)Để xóa đi một đối tượng khỏi list, chúng ta gán cho đối tượng đó giá trị bằng NULLNhư chúng ta đã thảo luận trong phần giới thiệu, list là một cấu trúc nhiều lớp, nghĩa là một list có thể chứa các đối tượng có kiểu list. Thật vậy, giả sử chúng ta có list có tên là SV2 chứa các thông tin tương ứng của một sinh viên khácChúng ta có thể tạo một list có tên là DS chứa thông tin của cả 2 sinh viênĐể xem bảng điểm của sinh viên thứ hai, chúng ta cần sử dụng 2 lần ký hiệu $:","code":"\nSV1<-list(Ten = \"Nguyễn Đức Nam\",\n          Ngay_sinh = as.Date(\"2000-06-20\"),\n          Gioi_tinh = TRUE,\n          Bang_diem = data.frame(Mon_hoc = c(\"Giải tích\", \"Đại số\", \"Xác suất\"),\n                                 Diem = c(6.5, 8.5, 7.0)))\nstr(SV1) # xem cấu trúc của list SV1## List of 4\n##  $ Ten      : chr \"Nguyễn Đức Nam\"\n##  $ Ngay_sinh: Date[1:1], format: \"2000-06-20\"\n##  $ Gioi_tinh: logi TRUE\n##  $ Bang_diem:'data.frame':   3 obs. of  2 variables:\n##   ..$ Mon_hoc: chr [1:3] \"Giải tích\" \"Đại số\" \"Xác suất\"\n##   ..$ Diem   : num [1:3] 6.5 8.5 7\nSV1$Bang_diem # Hiển thị bảng điểm##     Mon_hoc Diem\n## 1 Giải tích  6.5\n## 2    Đại số  8.5\n## 3  Xác suất  7.0\nnames(SV1)## [1] \"Ten\"       \"Ngay_sinh\" \"Gioi_tinh\" \"Bang_diem\"\nSV1[[4]] # Sử dụng 2 lần dấu ngoặc vuông##     Mon_hoc Diem\n## 1 Giải tích  6.5\n## 2    Đại số  8.5\n## 3  Xác suất  7.0\nSV1[4] # Là một list có 1 phần tử## $Bang_diem\n##     Mon_hoc Diem\n## 1 Giải tích  6.5\n## 2    Đại số  8.5\n## 3  Xác suất  7.0\nSV1$que_quan<-\"Hà Nội\" # Thêm vào một phần tử có tên que_quan là một biến\nstr(SV1) # list SV1 đã có thêm phần tử thứ năm## List of 5\n##  $ Ten      : chr \"Nguyễn Đức Nam\"\n##  $ Ngay_sinh: Date[1:1], format: \"2000-06-20\"\n##  $ Gioi_tinh: logi TRUE\n##  $ Bang_diem:'data.frame':   3 obs. of  2 variables:\n##   ..$ Mon_hoc: chr [1:3] \"Giải tích\" \"Đại số\" \"Xác suất\"\n##   ..$ Diem   : num [1:3] 6.5 8.5 7\n##  $ que_quan : chr \"Hà Nội\"\nSV1$que_quan<-NULL # xóa phần tử có tên que_quan khỏi SV1\nstr(SV1) # list SV1 chỉ còn 4 phần tử## List of 4\n##  $ Ten      : chr \"Nguyễn Đức Nam\"\n##  $ Ngay_sinh: Date[1:1], format: \"2000-06-20\"\n##  $ Gioi_tinh: logi TRUE\n##  $ Bang_diem:'data.frame':   3 obs. of  2 variables:\n##   ..$ Mon_hoc: chr [1:3] \"Giải tích\" \"Đại số\" \"Xác suất\"\n##   ..$ Diem   : num [1:3] 6.5 8.5 7\nSV2<-list(Ten = \"Nguyễn Thị Loan\",\n          Ngay_sinh = as.Date(\"2000-05-13\"),\n          Gioi_tinh = FALSE,\n          Bang_diem = data.frame(Mon_hoc = c(\"Xác suất\", \"Thống kê\", \"Học máy\",\"AI\"),\n                                 Diem = c(7.0, 9.5, 10.0, 9.0)),\n          Que_quan = \"Hà Nội\")\nDS<-list(SV1 = SV1,SV2 = SV2) # DS là một list có 2 phần tử, mỗi phần tử là 1 list\nstr(DS) # xem cấu trúc của list DS## List of 2\n##  $ SV1:List of 4\n##   ..$ Ten      : chr \"Nguyễn Đức Nam\"\n##   ..$ Ngay_sinh: Date[1:1], format: \"2000-06-20\"\n##   ..$ Gioi_tinh: logi TRUE\n##   ..$ Bang_diem:'data.frame':    3 obs. of  2 variables:\n##   .. ..$ Mon_hoc: chr [1:3] \"Giải tích\" \"Đại số\" \"Xác suất\"\n##   .. ..$ Diem   : num [1:3] 6.5 8.5 7\n##  $ SV2:List of 5\n##   ..$ Ten      : chr \"Nguyễn Thị Loan\"\n##   ..$ Ngay_sinh: Date[1:1], format: \"2000-05-13\"\n##   ..$ Gioi_tinh: logi FALSE\n##   ..$ Bang_diem:'data.frame':    4 obs. of  2 variables:\n##   .. ..$ Mon_hoc: chr [1:4] \"Xác suất\" \"Thống kê\" \"Học máy\" \"AI\"\n##   .. ..$ Diem   : num [1:4] 7 9.5 10 9\n##   ..$ Que_quan : chr \"Hà Nội\"\nDS$SV2$Bang_diem # Xem bảng điểm của sinh viên thứ hai##    Mon_hoc Diem\n## 1 Xác suất  7.0\n## 2 Thống kê  9.5\n## 3  Học máy 10.0\n## 4       AI  9.0"},{"path":"kiến-thức-r-nâng-cao.html","id":"sử-dụng-list-trong-viết-hàm-số","chapter":"Chương 5 Kiến thức R nâng cao","heading":"5.3.2 Sử dụng list trong viết hàm số","text":"Hầu như tất cả các hàm số được xây dựng sẵn trong R đều cho kết quả đầu ra dưới dạng list. Ví dụ, bạn đọc quan sát giá trị đầu ra của hàm có tên là uniroot() như sauHàm uniroot() được sử dụng để tìm nghiệm duy nhất của một hàm số trên một khoảng. Đoạn câu lệnh ở trên sử dụng hàm uniroot() để tìm nghiệm duy nhất của phương trình \\(x^2 - 1/4 = 0\\) trên khoảng \\((0,1)\\). Kết quả của hàm uniroot() là một list có 5 phần tử đều là các biến kiểu số với tên tương ứng là root, f.root, iter, init., và estim.prec. Các hàm số phức tạp hơn sẽ có kết quả đầu ra phức tạp hơn rất nhiều. Bạn đọc cần đọc kỹ hướng dẫn của các hàm để hiểu mỗi đối tượng con của kết quả đầu ra có ý nghĩa như thế nào.Bạn đọc cũng nên sử dụng đối tượng list để làm đầu ra cho các hàm số tự xây dựng. Chúng ta sẽ quay trở lại ví dụ về xây dựng hàm số PV() để tính giá trị hiện tại của một dòng tiền. Đối với một dòng tiền tương lai, ngoài giá trị hiện tại, bạn đọc có thể quan tâm đến các giá trị khác như Macaulay Duration, Modified Duration, Dollar Duration. Ý nghĩa và cách tính các giá trị này ở trong phần phụ lục @ref(#advrappen3).Chúng ta có thể sử dụng hàm summary_CF() để tính toán các đặc trưng của một trái phiếu với các thông số như sau:\nBảng 5.1: Các thông số của một trái phiếu\nChúng ta tạo ra dòng tiền tương lai của trái phiếu với các thông số như trên và sau đó sử dụng hàm summary_CF()Chúng ta chuyển sang một ví dụ khác khi sử dụng list làm đầu ra cho một hàm số tự xây dựng. Khi bạn đọc tìm hiểu về giá trị của một véc-tơ kiểu số, chúng ta thường tính toán các giá trị đặc trưng như giá trị trung bình, giá trị lớn nhất, nhỏ nhất, các phân vị, và muốn xem phân phối các giá trị trong véc-tơ đó như thế nào. Chúng ta có thể tự viết một hàm số để thực hiện việc này với đầu ra là một list:Chúng ta có thể sử dụng hàm summary_vec() để tổng hợp thông tin về lợi suất tính theo ngày của chỉ số FTSE (chỉ số cổ phiếu của 100 công ty có giá trị vốn hóa thị trường lớn nhất niêm yết trên Sở giao dịch chứng khoán London) trong năm 1991 đến năm 1999. Chỉ số này được lưu trong dữ liệu EuStockMarkets có sẵn trong R.Một lợi thế khác của đối tượng kiểu list là có thể đẩy nhanh tốc độ tính toán khi dùng các hàm họ apply(). Chúng ta sẽ thảo luận vấn đề này trong phần tiếp theo của cuốn sách.","code":"\nf<-function(x) x^2 - 1/4\nresult<-uniroot(f,c(0,1))\nclass(result) # Đối tượng result có kiểu list## [1] \"list\"\nstr(result) # Xem cấu trúc của đối tượng result## List of 5\n##  $ root      : num 0.5\n##  $ f.root    : num -2.85e-05\n##  $ iter      : int 6\n##  $ init.it   : int NA\n##  $ estim.prec: num 6.1e-05\nsummaryCF<-function(i,CF){\n  n<-length(CF)\n  PV<-sum(CF/((1+i)^(1:n)))\n  Mac_D<-sum(CF*(1:n)/((1+i)^(1:n)))/PV\n  Mod_D<-Mac_D/(1+i)\n  Dollar_D<-PV*Mod_D*0.01\n  ket_qua<-list(PV = PV, Mac_D = Mac_D, Mod_D = Mod_D,\n               Dollar_D = Dollar_D)\n  return(ket_qua)\n}\n# Nhập liệu\nF<-10 # Mệnh giá trái phiếu, đơn vị tỷ đồng\nT<-12 # 12 năm cho đến ngày đáo hạn\nc<-9.25/100 # Lãi suất coupon\ni<-5/100 # Lãi suất dùng để chiết khấu\nCF<-c(rep(c*F,(T-2)),c*F+F) # Dòng tiền tương lai của trái phiếu\nsummaryCF(i,CF)## $PV\n## [1] 13.53023\n## \n## $Mac_D\n## [1] 7.884908\n## \n## $Mod_D\n## [1] 7.509436\n## \n## $Dollar_D\n## [1] 1.016044\nsummary_vec<-function(x){\n  do_dai<-length(x) # Độ dài của véc-tơ\n  ty_le_na<-paste(round(sum(is.na(x))/do_dai*100,2),\"%\")\n  # % giá trị không quan sát được\n  gioi_han<-c(min(x,na.rm=TRUE),max(x,na.rm=TRUE))\n  trung_binh<-mean(x,na.rm=TRUE)\n  do_lech_chuan<-sd(x,na.rm=TRUE)\n  phan_vi<-quantile(x,c(0.01,0.1,0.25,0.5,0.75,0.9,0.99),na.rm=TRUE)\n  do_thi<-ggplot(data=data.frame(x=x), aes(x=x))+\n    geom_histogram(color = \"#640514\",\n                   fill = \"white\",alpha = 0.3)+\n    xlab(\"\")+ylab(\"\")+theme_minimal()\n  result<-list(do_dai = do_dai, ty_le_na = ty_le_na, gioi_han = gioi_han,\n               trung_binh = trung_binh, do_lech_chuan = do_lech_chuan,\n               phan_vi = phan_vi, do_thi = do_thi)\n  return(result)\n}\nchi_so<-EuStockMarkets[,4] # lấy chỉ số FTSE ra từ cột thứ 4 của EuStockMarkets\nn<-length(chi_so) # độ dài của chuỗi chỉ số chứng khoán\nloi_suat<-log(chi_so[2:n]/chi_so[1:(n-1)]) # lợi suất của chỉ số\nsummary_vec(loi_suat)## $do_dai\n## [1] 1859\n## \n## $ty_le_na\n## [1] \"0 %\"\n## \n## $gioi_han\n## [1] -0.04139903  0.05439552\n## \n## $trung_binh\n## [1] 0.0004319851\n## \n## $do_lech_chuan\n## [1] 0.007957728\n## \n## $phan_vi\n##            1%           10%           25%           50%           75% \n## -2.060655e-02 -9.139666e-03 -4.318778e-03  8.021069e-05  5.253592e-03 \n##           90%           99% \n##  9.714781e-03  1.931723e-02 \n## \n## $do_thi"},{"path":"kiến-thức-r-nâng-cao.html","id":"các-hàm-họ-apply","chapter":"Chương 5 Kiến thức R nâng cao","heading":"5.4 Các hàm họ apply()","text":"Nhóm hàm apply() là nhóm hàm có sẵn trong R cho phép bạn đọc thực hiện lặp đi lặp lại một hàm số trên nhiều đối tượng. Về cơ bản nhóm hàm này hoạt động giống như một vòng lặp nhưng câu lệnh viết bằng nhóm hàm này sẽ chạy nhanh hơn và đơn giản hơn viết vòng lặp rất nhiểu.Các hàm mà chúng tôi sẽ giới thiệu đến bạn đọc trong phần này bao gồm apply(), lapply() và sapply(). Còn nhiều hàm khác thuộc nhóm hàm này như vapply(), tapply(), mapply(), …, nhưng về nguyên tắc hoạt động của các hàm này là tương tự và chỉ khác ở chỗ chúng áp dụng trên các loại đối tượng khác nhau nên bạn đọc có thể tự tìm hiểu mà không gặp khó khăn nào.","code":""},{"path":"kiến-thức-r-nâng-cao.html","id":"hàm-apply","chapter":"Chương 5 Kiến thức R nâng cao","heading":"5.4.1 Hàm apply()","text":"Cho một véc-tơ \\(x\\) kiểu số và một hàm \\(f\\), chẳng hạn như \\(f(x) = x^2\\). Khi bạn đọc viết f(x), R sẽ hiểu rằng bạn đang thực hiện hàm số \\(f\\) cho từng phần tử của véc-tơ \\(x\\) và sẽ trả lại giá trị là một véc-tơ mà từng phần tử tương ứng là bình phương của các phần tử trong \\(x\\). Việc thực hiện hàm \\(f\\) trên véc-tơ \\(x\\) diễn ra một cách đồng thời và hiệu quả hơn với việc viết một vòng lặp để tính hàm \\(f\\) trên từng phần tử của \\(x\\).Điều gì xảy ra khi x không phải là một véc-tơ đồng các phần tử con của x không phải là một biến, chẳng hạn nhưx là một ma trận và bạn muốn tính toán một hàm \\(f\\) trên các phần tử con của x là một véc-tơ hàng hoặc một véc-tơ cột.\nx là một ma trận và bạn muốn tính toán một hàm \\(f\\) trên các phần tử con của x là một véc-tơ hàng hoặc một véc-tơ cột.x là một dữ liệu và bạn muốn thực hiện một hàm \\(f\\) trên tất cả các cột dữ liệu.\nx là một dữ liệu và bạn muốn thực hiện một hàm \\(f\\) trên tất cả các cột dữ liệu.x là một list và bạn muốn thực hiện một hàm \\(f\\) trên tất cả các đối tượng con của x.\nx là một list và bạn muốn thực hiện một hàm \\(f\\) trên tất cả các đối tượng con của x.Các hàm thuộc họ apply() sẽ giúp bạn đọc thực hiện tác tính toán như vậy. Cách viết hàm apply() như sau:trong đó x là một ma trận, một mảng nhiều chiều, hoặc một dữ liệu; tham số MARGIN là một số, hoặc véc-tơ chỉ số cho biết hàm sẽ áp dụng trên chiều (hoặc các chiều) nào, và FUN là hàm số mà bạn muốn thực hiện. Ví dụ như bạn đọc muốn tính giá trị trung bình của mỗi cột của một ma trận M, hãy sử dụng câu lệnh như sauDo ma trận M có 5 cột nên giá trị trả lại là một véc-tơ kiểu số có độ dài bằng 5. Véc-tơ này chứa giá trị là trung bình của các cột thứ 1, 2, 3, 4, và 5 của ma trận M.Về nguyên tắc đối tượng sử dụng trong hàm apply() là ma trận hoặc mảng nhiều chiều. Bạn đọc cũng có thể sử dụng hàm apply() trên đối tượng là dữ liệu kiểu bảng. Khi đối tượng của hàm apply() có từ 3 chiều trở lên, giá trị của tham số MARGIN có thể là một số hoặc một véc-tơ. Thật vậy,Giá trị trả lại sẽ là một véc-tơ có độ dài là 2, phần tử thứ nhất là giá trị trung bình của các phần tử thuộc ma trận kích thước \\(5 \\times 2\\) thứ nhất, nghĩa là ma trận Ar[,,1], và phần tử thứ hai là giá trị trung bình của các phần tử thuộc ma trận kích thước \\(5 \\times 2\\) thứ hai (ma trận Ar[,,2]). Chúng ta có thể kiểm tra kết quả như sau:Chúng ta có thể áp dụng đồng thời hàm mean() theo chiều thứ 2 và chiều thứ 3 trên mảng \\(Ar\\) như sauKết quả thu được sẽ là một ma trận kích thước \\(2 \\times 2\\) mà các phần tử sẽ tương ứng với giá trị trung bình:Phần tử ở vị trí [1,1] của ma trận kết quả là giá trị trung bình của véc-tơ Ar[,1,1]Phần tử ở vị trí [1,1] của ma trận kết quả là giá trị trung bình của véc-tơ Ar[,1,1]Phần tử ở vị trí [1,2] của ma trận kết quả là giá trị trung bình của véc-tơ Ar[,1,2]Phần tử ở vị trí [1,2] của ma trận kết quả là giá trị trung bình của véc-tơ Ar[,1,2]Phần tử ở vị trí [2,1] của ma trận kết quả là giá trị trung bình của véc-tơ Ar[,2,1]Phần tử ở vị trí [2,1] của ma trận kết quả là giá trị trung bình của véc-tơ Ar[,2,1]Phần tử ở vị trí [2,2] của ma trận kết quả là giá trị trung bình của véc-tơ Ar[,2,2]Phần tử ở vị trí [2,2] của ma trận kết quả là giá trị trung bình của véc-tơ Ar[,2,2]Chúng ta có thể sánh giá trị trung bình của các véc-tơ với ma trận kết quả của hàm apply():Hàm số sử dụng với tham số FUN có thể là hàm số có sẵn trong R, hoặc trong các thư viện cài đặt bổ sung, hoặc cũng có thể là một hàm số mà bạn đọc tự xây dựng. Khi các câu lệnh của hàm số tự xây dựng ngắn gọn, bạn đọc có thể định nghĩa hàm số đó bên trong hàm apply(). Giá trị đầu ra của hàm số được tự định nghĩa cũng có thể là một véc-tơ, thậm chí là một ma trận hay là một mảng nhiều chiều, thậm chí là một list. Nếu kết quả đầu ra của hàm sử dụng trong apply() là một ma trận hoặc mảng nhiều chiều, R sẽ chuyển ma trận hoặc mảng nhiều chiều về dạng véc-tơ. Trong trường hợp đầu ra của hàm tự định nghĩa là một list, giá trị đầu ra sẽ là một ma trận hoặc mảng nhiều chiều mà mỗi phần tử con là một list. Dưới đây là một ví dụ mà giá trị đầu ra là một véc-tơ ba chiều.Kết quả nhận được sẽ là một ma trận kích thước \\(3 \\times 5\\). Cột thứ nhất của ma trận kết quả nhận giá trị (1, 10.5, 20) tương ứng với các giá trị min, mean, và max của véc-tơ M[,1].Bạn đọc cũng có thể tự xây dựng hàm số trên môi trường chung sau đó gọi tên hàm số này trong hàm apply(). Nếu hàm số tự xây dựng là hàm số có tham số khác ngoài x, bạn đọc cần phải khai báo giá trị cho tham số đó trong môi trường cục bộ của hàm apply():Khi tham số của hàm số mà bạn đọc muốn áp dụng trên ma trận là không cố định mà thay đổi theo một chiều của x thì không nên khai báo giá trị của tham số theo dạng véc-tơ trong hàm apply(). Thật vậy, giả sử bạn đọc muốn tính các giá trị phân vị ở mức xác suất 10% và 90% lần lượt của véc-tơ hàng thứ nhất và véc-tơ hàng thứ hai của một ma trận M kích thước \\(2 \\times 10\\). Hàm số quantile(x, probs = p) là hàm số có sẵn trong R được sử dụng để tính giá trị phân vị tại mức xác suất \\(p\\) của véc-tơ số x. Hãy quan sát kết quả của hàm apply() khi sử dụng tham số probs của hàm quantile() dưới dạng véc-tơ:Bạn đọc có thể thấy rằng kết quả của hàm apply() khi tham số probs là một véc-tơ là một ma trận, trong đó cột thứ nhất là giá trị phân vị tại các mức xác suất 10% và 90% của véc-tơ M[1,] và cột thứ hai giá trị phân vị tại các mức xác suất 10% và 90% của véc-tơ M[2,]. Giá trị chúng ta mong muốn lấy ra chính là các số nằm trên đường chéo chính của ma trận kết quả. Sẽ không có khó khăn gì nếu số lượng hàng của ma trận M nhỏ. Bạn đọc sẽ gặp vấn đề khi số lượng véc-tơ được áp dụng là lớn bởi kích thước của ma trận kết quả sẽ tăng lên theo cấp số nhân. Thật vậy, nếu M có \\(n\\) hàng và hàm số được áp dụng có 1 tham số, ma trận kết quả sẽ có kích thước sẽ là \\(n \\times n\\) nếu bạn đọc sử dụng trực tiếp hàm apply(). Chẳng hạn như bạn muốn tính giá trị phân vị ở các mức xác suất 10%, 30%, 50%, 70%, và 90% của lần lượt các véc-tơ hàng thứ 1, 2, 3, 4, và 5 của một ma trận M kích thước \\(5 \\times 10\\).Điều gì xảy ra khi ma trận M có \\(10^4\\) véc-tơ hàng? Ma trận kết quả sẽ có kích thước là \\(10^4 \\times 10^4\\). Khi ma trận M có \\(10^5\\) véc-tơ hàng? Ma trận kết quả sẽ có kích thước là \\(10^5 \\times 10^5\\) và R sẽ báo lỗi vì bộ nhớ không đủ để lưu một ma trận có kích thước như vậy.Một cách đơn giản để tiết kiệm thời gian và bộ nhớ khi áp dụng hàm số có tham số là hãy thêm tham số vào như là một phần của ma trận M và điều chỉnh lại hàm quantile() trước khi sử dụng hàm apply()","code":"\nx<-1:5; f<-function(x) x^2\nf(x) # f được áp dụng trên từng phần tử của x## [1]  1  4  9 16 25\napply(x, MARGIN, FUN, ...)\nM<-matrix(1:100,20,5) # Ma trận kích thước 20 * 5\napply(M, MARGIN = 2, FUN = mean)## [1] 10.5 30.5 50.5 70.5 90.5\n# MARGIN = 2 nghĩa là tính theo cột (1 theo hàng)\nAr<-array(1:20,dim=c(5,2,2)) # Mảng kích thước 5 * 2 * 2\napply(Ar, MARGIN = 3, FUN = mean)## [1]  5.5 15.5\n# MARGIN = 3 nghĩa là áp dụng hàm mean theo chiều thứ 3\nmean(Ar[,,1]) # bằng phần tử thứ nhất khi dùng apply## [1] 5.5\nmean(Ar[,,2]) # bằng phần tử thứ hai khi dùng apply## [1] 15.5\napply(Ar, MARGIN = c(2,3), mean) # MARGIN = c(2,3) nghĩa là áp dụng hàm mean theo chiều thứ 2 và 3##      [,1] [,2]\n## [1,]    3   13\n## [2,]    8   18\nmean(Ar[,1,1]) # bằng phần tử ở vị trí [1,1] của ma trận kết quả## [1] 3\nmean(Ar[,1,2]) # bằng phần tử ở vị trí [1,2] của ma trận kết quả## [1] 13\nmean(Ar[,2,1]) # bằng phần tử ở vị trí [1,2] của ma trận kết quả## [1] 8\nmean(Ar[,2,2]) # bằng phần tử ở vị trí [1,2] của ma trận kết quả## [1] 18\napply(M, 2, function(x) c(min(x),mean(x),max(x)))##      [,1] [,2] [,3] [,4]  [,5]\n## [1,]  1.0 21.0 41.0 61.0  81.0\n## [2,] 10.5 30.5 50.5 70.5  90.5\n## [3,] 20.0 40.0 60.0 80.0 100.0\nM[,1]##  [1]  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20\nc(min(M[,1]), mean(M[,1]), max(M[,1]))## [1]  1.0 10.5 20.0\nmy_range<-function(x, a) (max(x^a) - min(x^a)) # hàm số có tham số khác là a\napply(M, 2, my_range, a = 2)## [1]  399 1159 1919 2679 3439\n# Cần khai báo giá trị tham số a của my_range trong hàm apply\nM<-matrix(1:20,2,10) # ma trận 2 * 10\napply(M, 1, quantile, probs = c(0.1,0.9))##     [,1] [,2]\n## 10%  2.8  3.8\n## 90% 17.2 18.2\nquantile(M[1,],0.1) # giá trị mong muốn## 10% \n## 2.8\nquantile(M[2,],0.9) # giá trị mong muốn##  90% \n## 18.2\nM<-matrix(1:50,5,10) # ma trận 5 * 10\napply(M, 1, quantile, probs = c(0.1,0.3,0.5,0.7,0.9)) # ma trận kết quả kích thước 5 * 5##     [,1] [,2] [,3] [,4] [,5]\n## 10%  5.5  6.5  7.5  8.5  9.5\n## 30% 14.5 15.5 16.5 17.5 18.5\n## 50% 23.5 24.5 25.5 26.5 27.5\n## 70% 32.5 33.5 34.5 35.5 36.5\n## 90% 41.5 42.5 43.5 44.5 45.5\ndiag(apply(M, 1, quantile, probs = c(0.1,0.3,0.5,0.7,0.9))) # lấy ra đường chéo chính## [1]  5.5 15.5 25.5 35.5 45.5\nM1<-cbind(M,c(0.1,0.3,0.5,0.7,0.9)) # Thêm tham số vào cột cuối của ma trận M\nmy_quantile<-function(x){\n  # Định nghĩa lại hàm quantile có tham số p là giá trị cuối của véc-tơ\n  n<-length(x)\n  quantile(x[1:(n-1)], x[n])\n}\napply(M1, 1, my_quantile)## [1]  5.5 15.5 25.5 35.5 45.5"},{"path":"kiến-thức-r-nâng-cao.html","id":"hàm-lapply-và-sapply.","chapter":"Chương 5 Kiến thức R nâng cao","heading":"5.4.2 Hàm lapply() và sapply().","text":"Cơ chế hoạt động của lapply() tương tự như apply() và chỉ khác ở đối tượng áp dụng và cấu trúc của kết quả đầu ra. lapply() thường áp dụng trên các đối tượng kiểu list hoặc các kiểu đối tượng mà có thể sử dụng ký hiệu $ để gọi phần tử con như data.frame và tibble. Chúng ta sẽ thảo luận về các đối tượng này ở phần sau của cuốn sách. Khi bạn đọc sử dụng hàm lapply(), bạn không cần phải sử dụng tham số MARGIN bởi vì lapply() sẽ luôn luôn hiểu các đối tượng được tác động đến là tất cả các đối tượng con của list.Kết quả của lapply() là một list có số lượng phần tử và tên các phần tử con giống với véc-tơ x. Trong trường hợp áp dụng hàm mean(), mỗi giá trị nằm trong list kết quả là giá trị trung bình của phần tử có tên tương ứng nằm trong list ban đầu. hàm mean() không thể sử dụng với đối tượng là một list nên phần tử x4 của kết quả có giá trị NA.Hàm sapply() có cơ chế hoạt động hoàn toàn tương tự như lapply() và chỉ khác ở chỗ kết quả đầu ra là dưới dạng véc-tơ, ma trận, hoặc mảng. Thật vậy, vẫn với đối tượng x kiểu list ở trên, chúng ta sử dụng sapply() thay vì lapply() sẽ cho kết quả dưới dạng véc-tơ thay vì dưới dạng listCác hàm lapply() và sapply() thường xuyên được sử dụng khi làm việc với dữ liệu vì R lưu dữ liệu dưới dạng các data.frame hoặc tibbles. Khi sử dụng các hàm lapply() và sapply() với dữ liệu, các đối tượng được tác động đến sẽ luôn luôn là các véc-tơ cột. Hãy quan sát ví dụ dưới đây khi sử dụng sapply() để tính tỷ lệ giá trị không quan sát được của mỗi véc-tơ cột của một dữ liệu có tên là gapminder nằm trong thư viện dslabs.","code":"\nx <- list(x1 = 1:10,\n          x2 = c(TRUE,FALSE,TRUE,TRUE),\n          x3 = matrix(1:6,2,3),\n          x4 = list(x41 = c(1,2), x42 = c(3,4)) )\nlapply(x, mean) # áp dụng hàm mean trên tất cả các phần tử con của x## $x1\n## [1] 5.5\n## \n## $x2\n## [1] 0.75\n## \n## $x3\n## [1] 3.5\n## \n## $x4\n## [1] NA\nsapply(x, mean) # áp dụng hàm mean trên tất cả các phần tử con của x##   x1   x2   x3   x4 \n## 5.50 0.75 3.50   NA\ns1<-sapply(gapminder, function(x) sum(is.na(x))/length(x))\n# s1 là tỷ lệ không quan sát được của mỗi cột trong dữ liệu\ns1<-sort(s1) # sắp xếp s1 theo thứ tự tăng dần\nprint(s1) # hiển thị s1##          country             year  life_expectancy        continent \n##       0.00000000       0.00000000       0.00000000       0.00000000 \n##           region       population        fertility infant_mortality \n##       0.00000000       0.01754386       0.01773352       0.13779042 \n##              gdp \n##       0.28183973\nbarplot(s1,border = \"#640514\", col = \"white\",\n        ylab = \"Tỷ lệ\",\n        xlab = \"Tên biến/cột\",\n        main = \"Tỷ lệ giá trị không quan sát được của các cột dữ liệu Gapminder\")"},{"path":"kiến-thức-r-nâng-cao.html","id":"phụ-lục-1","chapter":"Chương 5 Kiến thức R nâng cao","heading":"5.5 Phụ lục","text":"","code":""},{"path":"kiến-thức-r-nâng-cao.html","id":"advrappen1","chapter":"Chương 5 Kiến thức R nâng cao","heading":"5.5.1 Kiến thức nâng cao liên quan đến ma trận","text":"Các kiến thức liên quan đến ma trận trong phần này đòi hỏi bạn đọc cần có kiến thức nâng cao hơn. Nếu bạn đọc cảm thấy không cần thiết có thể bỏ qua.","code":""},{"path":"kiến-thức-r-nâng-cao.html","id":"giá-trị-riêng-và-véc-tơ-riêng-của-ma-trận","chapter":"Chương 5 Kiến thức R nâng cao","heading":"5.5.1.1 Giá trị riêng và véc-tơ riêng của ma trận","text":"Cho là một ma trận kích thước \\(n \\times n\\). Nếu \\(\\lambda\\) và véc-tơ \\(\\textbf{v}\\) thỏa mãn\n\\[\\begin{align}\n\\ \\textbf{v} = \\lambda \\ \\textbf{v}\n\\end{align}\\]\nthì \\(\\lambda\\) là một giá trị riêng và \\(\\textbf{v}\\) là một véc-tơ riêng của ma trận vuông . Giá trị riêng \\(\\lambda\\) là nghiệm của phương trình\n\\[\\begin{align}\ndet(- \\lambda I_n) = 0\n\\end{align}\\]\ntrong đó \\(det\\) là định thức của ma trận và \\(I_n\\) là ma trận đơn vị kích thước \\(n \\times n\\). Vế bên trái của phương trình ở trên là một đa thức bậc \\(n\\) và giá trị riêng \\(\\lambda\\) là nghiệm của đa thức đó. Véc-tơ \\(\\textbf{v}\\) là véc-tơ riêng tương ứng với giá trị riêng \\(\\lambda\\).Nếu ma trận \\(\\) là một ma trận đối xứng thì tất cả các giá trị riêng của ma trận đều là số thực và ma trận đó có \\(n\\) véc-tơ riêng tương ứng với \\(n\\) giá trị riêng. Giả sử \\(\\lambda_1\\), \\(\\lambda_2\\), \\(\\cdots\\), \\(\\lambda_n\\) là \\(n\\) giá trị riêng của ma trận . Các véc-tơ riêng tương ứng là \\(\\textbf{v}_1\\), \\(\\textbf{v}_2\\), \\(\\cdots\\), \\(\\textbf{v}_n\\) và chúng ta giả sử rằng các véc-tơ \\(\\textbf{v}_i\\) đều đã được chuẩn hóa sao cho\n\\[\\begin{align}\n& \\textbf{v}_i^T \\textbf{v}_j = 0 \\ \\ \\forall \\neq j, 1 \\leq ,j \\leq n \\\\\n& \\textbf{v}_i^T \\textbf{v}_i = 1 \\ \\ \\forall , 1 \\leq \\leq n\n\\end{align}\\]\nKhi đó, ma trận có thể được khai triển dưới dạng\n\\[\\begin{align}\n= V \\Gamma V^T\n\\end{align}\\]\ntrong đó V là ma trận có các cột là các véc-tơ riêng của ma trận và \\(\\Gamma\\) là ma trận đường chéo có các phần tử nằm trên đường chéo chính là các giá trị riêng của ma trận \n\\[\\begin{align}\n& V[,] = \\textbf{v}_i \\forall , 1 \\leq \\leq n \\\\\n& \\Gamma = diag(\\lambda_1, \\lambda_2, \\cdots, \\lambda_n)\n\\end{align}\\]\nKhai triển trên của ma trận vuông được gọi là spectral decomposition.Hàm eigen() được sử dụng để tính toán các giá trị riêng và véc-tơ riêng của ma trận. Bạn đọc quan sát ví dụ dưới đây: hàm eigen() được sử dụng để tính toán các giá trị riêng và véc-tơ riêng của ma trận tương quan của ba biến medv, lstat, và rm (ma trận \\(3 \\times 3\\)) trong dữ liệu Boston của thư viện MASS.","code":"\ndat<-dplyr::select(Boston, medv, lstat, rm) # Lựa chọn 3 cột\nM<-cor(dat) # Tính ma trận tương quan\nresult<-eigen(M) # Hàm eigen tính giá trị riêng và véc-tơ riêng\nresult$values # Giá trị riêng## [1] 2.3658220 0.3900962 0.2440818\nresult$vectors # Véc-tơ riêng theo cột##            [,1]       [,2]       [,3]\n## [1,]  0.5959303 -0.1286943  0.7926568\n## [2,] -0.5741638  0.6218127  0.5326207\n## [3,]  0.5614294  0.7725197 -0.2966654"},{"path":"kiến-thức-r-nâng-cao.html","id":"khai-triển-trực-giao-của-ma-trận","chapter":"Chương 5 Kiến thức R nâng cao","heading":"5.5.1.2 Khai triển trực giao của ma trận","text":"Mọi ma trận kích thước \\(m \\times n\\) đều có thể viết dưới dạng\n\\[\\begin{align}\n= Q U\n\\end{align}\\]\ntrong đó \\(Q\\) là một ma trận trực giao kích thước, nghĩa là \\(Q^T Q = \\) và \\(U\\) là một ma trận đường tam giác trên. Nếu \\(m \\geq n\\) thì có hai cách để chọn kích thước cho các ma trậnCách thứ nhất: \\(Q\\) là một ma trận kích thước \\(m \\times n\\) và \\(R\\) là một ma trận kích thước \\(n \\times n\\).Cách thứ nhất: \\(Q\\) là một ma trận kích thước \\(m \\times n\\) và \\(R\\) là một ma trận kích thước \\(n \\times n\\).Cách thứ hai: \\(Q\\) là một ma trận kích thước \\(m \\times m\\) và \\(R\\) là một ma trận kích thước \\(n \\times n\\).Cách thứ hai: \\(Q\\) là một ma trận kích thước \\(m \\times m\\) và \\(R\\) là một ma trận kích thước \\(n \\times n\\).","code":""},{"path":"kiến-thức-r-nâng-cao.html","id":"ma-trận-xác-định-dương","chapter":"Chương 5 Kiến thức R nâng cao","heading":"5.5.1.3 Ma trận xác định dương","text":"Ma trận đối xứng được gọi là xác định dương khi và chỉ khi\n\\[\\begin{align}\n\\textbf{x}^T \\ \\ \\textbf{x} > 0\n\\end{align}\\]\nvới mọi véc-tơ \\(\\textbf{x}\\) khác véc-tơ \\(\\textbf{0}\\). Định nghĩa này của ma trận xác định dương rất khó được xác thực, đó có các định nghĩa tương đương khác sẽ có ý nghĩa thực tiễn hơn. Chẳng hạn như nếu một ma trận đối xứng có tất cả các giá trị riêng là số dương thì đó là một ma trận xác định dương.Tương tự như khai niệm ma trận xác định dương, ta có các khái niệmMa trận xác định không âm là ma trận đối xứng thỏa mãn \\(\\textbf{x}^T \\ \\ \\textbf{x} \\geq 0\\) với mọi véc-tơ \\(\\textbf{x}\\) khác véc-tơ \\(\\textbf{0}\\). Ma trận xác định không âm là ma trận có tất cả các giá trị riêng không âm.Ma trận xác định không âm là ma trận đối xứng thỏa mãn \\(\\textbf{x}^T \\ \\ \\textbf{x} \\geq 0\\) với mọi véc-tơ \\(\\textbf{x}\\) khác véc-tơ \\(\\textbf{0}\\). Ma trận xác định không âm là ma trận có tất cả các giá trị riêng không âm.Ma trận xác định âm là ma trận đối xứng thỏa mãn \\(\\textbf{x}^T \\ \\ \\textbf{x} < 0\\) với mọi véc-tơ \\(\\textbf{x}\\) khác véc-tơ \\(\\textbf{0}\\). Ma trận xác định âm là ma trận có tất cả các giá trị riêng là số âm.Ma trận xác định âm là ma trận đối xứng thỏa mãn \\(\\textbf{x}^T \\ \\ \\textbf{x} < 0\\) với mọi véc-tơ \\(\\textbf{x}\\) khác véc-tơ \\(\\textbf{0}\\). Ma trận xác định âm là ma trận có tất cả các giá trị riêng là số âm.Ma trận xác định không dương là ma trận đối xứng thỏa mãn \\(\\textbf{x}^T \\ \\ \\textbf{x} \\leq 0\\) với mọi véc-tơ \\(\\textbf{x}\\) khác véc-tơ \\(\\textbf{0}\\). Ma trận xác định không dương là ma trận có tất cả các giá trị riêng không dương.Ma trận xác định không dương là ma trận đối xứng thỏa mãn \\(\\textbf{x}^T \\ \\ \\textbf{x} \\leq 0\\) với mọi véc-tơ \\(\\textbf{x}\\) khác véc-tơ \\(\\textbf{0}\\). Ma trận xác định không dương là ma trận có tất cả các giá trị riêng không dương.Ma trận xác định dương kích thước \\(n \\times n\\) có thể được viết dưới dạng\n\\[\\begin{align}\n= L D L^T\n\\end{align}\\]\nvới \\(L\\) là ma trận tam giác dưới có tất cả các phần tử nằm trên đường chéo chính bằng 1, ma trận \\(D\\) là ma trận đường chéo có tất cả các phần tử nằm trên đường chéo chính là số dương.\\(D\\) là ma trận có tất cả các phần tử nằm trên đường chéo chính là số dương nên \\(D\\) có thể được viết dưới dạng\n\\[\\begin{align}\nD = \\hat{D} \\ \\hat{D}\n\\end{align}\\]\ntrong đó \\(\\hat{D}\\) là ma trận đường chéo có phần tử thứ \\(\\) trên đường chéo chính bằng căn bậc 2 của phần \\(D_{,}\\).Khi đó, ma trận \\(\\) có thể được viết dưới dạng\n\\[\\begin{align}\n= L D L^T = L \\hat{D} \\ \\hat{D} L^T = \\hat{L} \\ \\hat{L}^T\n\\end{align}\\]\ntrong đó \\(\\hat{L}\\) là ma trận tam giác dưới. Khai triển này thường được gọi là Cholesky decomposion của một ma trận xác định dương.","code":""},{"path":"kiến-thức-r-nâng-cao.html","id":"advrappen2","chapter":"Chương 5 Kiến thức R nâng cao","heading":"5.5.2 Gradient, Hessian và Jacobian","text":"Trong tính toán và tối ưu hàm nhiều biến, grandient, hessian và jacobian là các khái niệm vô cùng quan trọng.","code":""},{"path":"kiến-thức-r-nâng-cao.html","id":"định-nghĩa","chapter":"Chương 5 Kiến thức R nâng cao","heading":"5.5.2.1 Định nghĩa","text":"Cho hàm số \\(f: \\mathbb{R}^n \\rightarrow \\mathbb{R}\\)\n\\[\\begin{align}\nf(\\textbf{x}) = f(x_1, x_2, \\cdots, x_n)\n\\end{align}\\]Véc-tơ của các đạo hàm bậc nhất của hàm \\(f\\) theo các thành phần của véc-tơ \\(\\textbf{x}\\) được gọi là \\(gradien\\) của hàm số \\(f\\) tại điểm \\(\\textbf{x}\\). Gradient của hàm \\(f\\) được ký hiệu như sau\\[\\begin{align}\n\\nabla f(\\textbf{x}) = \\left(\\cfrac{\\partial f(\\textbf{x})}{\\partial x_1}, \\cfrac{\\partial f(\\textbf{x})}{\\partial x_2}, \\cdots, \\cfrac{\\partial f(\\textbf{x})}{\\partial x_n}\\right)\n\\end{align}\\]Ma trận chứa đạo hàm bậc hai của hàm \\(f\\) theo các thành phần của véc-tơ \\(\\textbf{x}\\), mà phần tử ở hàng thứ \\(\\) cột \\(j\\) là đạo hàm của hàm \\(f\\) lần lượt theo \\(x_i\\), \\(x_j\\), được gọi là ma trận \\(hessian\\) của hàm \\(f\\) tại điểm \\(\\textbf{x}\\). Ma trận hessian thường được ký hiệu là \\(\\nabla^2 f(\\textbf{x})\\). Phần tử ở hàng thứ \\(\\) và cột \\(j\\) của ma trận hessian được xác định như sau\n\\[\\begin{align}\n\\left[\\nabla^2 f(\\textbf{x})\\right]_{,j} = \\cfrac{\\partial^2 f(\\textbf{x})}{\\partial x_i \\partial x_j}\n\\end{align}\\]Lưu ý rằng hessian là ma trận đối xứng bởi vì\n\\[\\begin{align}\n\\cfrac{\\partial^2 f(\\textbf{x})}{\\partial x_i \\partial x_j} = \\cfrac{\\partial^2 f(\\textbf{x})}{\\partial x_j \\partial x_i}\n\\end{align}\\]Khái niệm Jacobian được dùng thay thế cho gradient khi hàm số \\(f\\) nhận giá trị trong không gian \\(\\mathbb{R}^m\\), với\n\\[\\begin{align}\nf(\\textbf{x}) = f(x_1, x_2, \\cdots, x_n) =\n\\begin{bmatrix}\nf_1(x_1, x_2, \\cdots, x_n) \\\\\nf_2(x_1, x_2, \\cdots, x_n) \\\\\n\\cdots \\\\\nf_m(x_1, x_2, \\cdots, x_n) \\\\\n\\end{bmatrix}\n\\end{align}\\]\nta có ma trận Jacobian của hàm \\(f\\) tại điểm \\(x\\) là ma trận kích thước \\(n \\times m\\), ký hiệu \\([\\nabla f(\\textbf{x})]^T\\) trong đó cột thứ \\(j\\) của ma trận Jacobian là gradient của hàm \\(f_j\\) tại điểm \\(\\textbf{x}\\). Trong nhiều trường hợp, khái niệm \\(grandient\\) và \\(Jacobian\\) có thể được sử dụng thay thế cho nhau.Ví dụ, với \\(\\textbf{x}\\) là véc-tơ có độ dài \\(n\\). Nếu \\(\\textbf{}\\) là véc-tơ độ dài \\(n\\) và \\(\\) là ma trận kích thước \\(m \\times n\\) thì chúng ta có\n\\[\\begin{align}\n\\nabla \\textbf{}^T \\textbf{x}) = \\textbf{} \\\\\n\\nabla \\textbf{x} = ^T\n\\end{align}\\]","code":""},{"path":"kiến-thức-r-nâng-cao.html","id":"gradient-và-hessian-của-dạng-toàn-phương","chapter":"Chương 5 Kiến thức R nâng cao","heading":"5.5.2.2 Gradient và hessian của dạng toàn phương","text":"Cho\n\\[\\begin{align}\nf(\\textbf{x}) = \\cfrac{1}{2} \\ \\textbf{x}^T \\textbf{x} + \\textbf{}^T \\textbf{x}\n\\end{align}\\]\nvới \\(\\) là một ma trận đối xứng và \\(\\textbf{}\\) là véc-tơ độ dài \\(n\\) thì ta có\n\\[\\begin{align}\n& \\nabla f(\\textbf{x}) = \\textbf{x} + \\textbf{} \\\\\n& \\nabla^2 f(\\textbf{x}) = \n\\end{align}\\]","code":""},{"path":"kiến-thức-r-nâng-cao.html","id":"gradient-và-hessian-của-tích-các-hàm-số","chapter":"Chương 5 Kiến thức R nâng cao","heading":"5.5.2.3 Gradient và hessian của tích các hàm số","text":"Cho hàm \\(f\\) là tích của hai hàm \\(g\\) và \\(h\\) đều là các hàm số có đạo hàm có tập xác định trên tập \\(\\mathbb{R}^n\\) và nhận giá trị trên tập các số thực\n\\[\\begin{align}\nf(\\textbf{x}) = h(\\textbf{x}) g(\\textbf{x})\n\\end{align}\\]Gradient và hessian của hàm số \\(f\\) được tính toán thông qua gradient và hessian của các hàm \\(g\\) và \\(h\\) như sau\n\\[\\begin{align}\n& \\nabla f(\\textbf{x}) = \\nabla h(\\textbf{x}) \\ g(\\textbf{x}) + h(\\textbf{x}) \\  \\nabla g(\\textbf{x}) \\\\\n& \\nabla^2 f(\\textbf{x}) = \\left[\\nabla^2 h(\\textbf{x})\\right] g(\\textbf{x}) + \\left[\\nabla^2 g(\\textbf{x})\\right] h(\\textbf{x}) + \\nabla h(\\textbf{x}) \\nabla g(\\textbf{x})^T + \\nabla g(\\textbf{x}) \\nabla h(\\textbf{x})^T\n\\end{align}\\]","code":""},{"path":"kiến-thức-r-nâng-cao.html","id":"quy-tắc-chain-rule","chapter":"Chương 5 Kiến thức R nâng cao","heading":"5.5.2.4 Quy tắc Chain rule","text":"chain rule là quy tắc tính toán đạo hàm của một hàm số trên một hàm số khác. Cho \\(g\\) là hàm số \\(\\mathbb{R}^m \\rightarrow \\mathbb{R}\\) và với mỗi \\(\\), \\(1 \\leq \\leq m\\), ta có hàm \\(f_i\\) nhận có tập xác định trên \\(\\mathbb{R}^n\\) và nhận giá trị trên tập các số thực:\n\\[\\begin{align}\nf_i(\\textbf{x}) = f_i(x_1, x_2, \\cdots, x_n)\n\\end{align}\\]Hàm số \\(f\\) được xác định bởi\n\\[\\begin{align}\nf(\\textbf{x}) = g(f_1(\\textbf{x}), f_2(\\textbf{x}), \\cdots, f_m(\\textbf{x}))\n\\end{align}\\]\nsẽ có gradient được xác định theo chain rule như sau\n\\[\\begin{align}\n\\nabla f(\\textbf{x}) = \\left[ \\nabla f_1, \\nabla f_2, \\cdots, \\nabla f_m \\right] \\cdot \\nabla g(f_1(\\textbf{x}), f_2(\\textbf{x}), \\cdots, f_m(\\textbf{x}))\n\\end{align}\\]Trong công thức tính gradient ở trên, thành phần thứ nhất là một ma trận kích thước \\(n \\times m\\) trong đó cột thứ \\(j\\) là gradient của hàm \\(f_j\\) theo \\(\\textbf{x}\\), thành phần thứ hai là gradient (véc-tơ) của hàm \\(g\\) tính tại điểm \\(\\left(f_1(\\textbf{x}), f_2(\\textbf{x}), \\cdots, f_m(\\textbf{x})\\right)\\).","code":""},{"path":"kiến-thức-r-nâng-cao.html","id":"sgd","chapter":"Chương 5 Kiến thức R nâng cao","heading":"5.5.3 Thuật toán gradient descent","text":"Gradient descent là một phương pháp giải số các bài toán tối ưu không bị ràng buộc. Đây là thuật toán sử dụng vòng lặp để tìm kiếm điểm cực tiểu địa phương của một hàm nhiều biến khả vi. Ý tưởng của thuật toán là từ một điểm ban đầu chúng ta sẽ thực hiện các vòng lặp để thay đổi giá trị đó theo hướng ngược lại với độ dốc (gradient) của hàm số tính tại điểm hiện tại. Gradient descent là phương pháp chủ yếu trong quá trình ước lượng tham số của các mô hình học máy phức tạp.Giả sử hàm số chúng ta muốn tối thiểu hóa là hàm \\(f\\) và điểm khởi đầu là điểm \\(x_0\\), chúng ta sẽ di chuyển đến điểm \\(x_1\\) được tính toán bởi công thức sau\n\\[\\begin{align}\nx_1 = x_0 - \\lambda \\cdot \\nabla f(x_0)\n\\end{align}\\]Một cách tổng quát, chúng ta có thể viết công thức để di chuyển từ điểm \\(x_n\\) đến \\(x_{n+1}\\) như sau\n\\[\\begin{align}\nx_{n+1} = x_n - \\lambda \\cdot \\nabla f(x_n)\n\\end{align}\\]\nvới \\(\\lambda\\) là một số dương nhỏ. Thuật toán gradient descent sẽ dừng lại cho đến khi đạt được tiêu chí là hàm \\(f\\) không còn giảm một cách đáng kể sau mỗi lần thay đổi \\(x\\). Một vài lưu ý khi thực hiện thuật toán này làThuật toán gradient descent luôn hiểu với mục tiêu là tìm cực tiểu của hàm \\(f\\), đó trong bài toán tìm cực đại, chúng ta áp dụng gradient descent trên hàm \\(-f\\).Thuật toán gradient descent luôn hiểu với mục tiêu là tìm cực tiểu của hàm \\(f\\), đó trong bài toán tìm cực đại, chúng ta áp dụng gradient descent trên hàm \\(-f\\).Gradient descent chỉ hội tụ đến một điểm cực tiểu địa phương chứ không hội tụ đến điểm cực tiểu tổng thể, đó lựa chọn điểm \\(x_0\\) bạn đầu là khá quan trọng nếu hàm \\(f\\) có một (vài) điểm cực tiểu địa phương khác với điểm cực tiểu tổng thể.Gradient descent chỉ hội tụ đến một điểm cực tiểu địa phương chứ không hội tụ đến điểm cực tiểu tổng thể, đó lựa chọn điểm \\(x_0\\) bạn đầu là khá quan trọng nếu hàm \\(f\\) có một (vài) điểm cực tiểu địa phương khác với điểm cực tiểu tổng thể.Lựa chọn giá trị của \\(\\lambda\\) cũng là rất quan trọng. Nếu \\(\\lambda\\) quá nhỏ, thuật toán sẽ hội tụ chậm, nghĩa là cần rất nhiều bước để tìm được điểm tối ưu. Nếu \\(\\lambda\\) lớn, thuật toán sẽ không tìm được điểm cực tiểu. Lưu ý rằng, tại mỗi bước lựa chọn giá trị của \\(\\lambda\\) không nhất thiết phải là hằng số. Kinh nghiệm lựa chọn \\(\\lambda\\) là cho \\(\\lambda\\) giảm dần sau một số bước.Lựa chọn giá trị của \\(\\lambda\\) cũng là rất quan trọng. Nếu \\(\\lambda\\) quá nhỏ, thuật toán sẽ hội tụ chậm, nghĩa là cần rất nhiều bước để tìm được điểm tối ưu. Nếu \\(\\lambda\\) lớn, thuật toán sẽ không tìm được điểm cực tiểu. Lưu ý rằng, tại mỗi bước lựa chọn giá trị của \\(\\lambda\\) không nhất thiết phải là hằng số. Kinh nghiệm lựa chọn \\(\\lambda\\) là cho \\(\\lambda\\) giảm dần sau một số bước.Ví dụ 1: Hàm số \\(f(x) = (x-2)^2\\) có điểm cực tiểu là 2. Chúng ta có thể kiểm tra sự hội tụ của thuật toán gradient descent như sau: có thể dễ dàng thấy rằng đạo hàm của hàm \\(f\\) là \\(2 \\times (x-2)\\); giá trị \\(x_0\\) được lựa chọn ngẫu nhiên trong khoảng từ -100 đến 100; \\(\\lambda\\) được cho cố định là 0.1; thuật toán sẽ dừng lại khi giá trị hàm \\(f\\) tại 2 bước kế tiếp nhau nhỏ hơn \\(10^{-8}\\).Bạn đọc có thể thấy rằng sau khoảng 20 đến 30 vòng lặp thuật toán đã hội tụ. Hình 5.1 mô tả quá trình hội tụ của 50 điểm ngẫu nhiên trong hình vuông \\([-50,50] \\times [-50,50]\\) hội tụ về điểm tối ưu \\((x_1 = 5, x_2 = 10)\\) của hàm \\(f(x_1, x_2) = (x_1 - 5)^2 + (x_2 - 10)^2\\).\nHình 5.1: Quá trình hội tụ các điểm ngẫu nhiên đến điểm cực tiểu của hàm f sử dụng thuật toán gradient descent\n","code":"\nlambda<-0.1\nvong_lap_toi_da = 40\nsai_so_nho_nhat<-10^(-8)\n\n# Hàm số cần tìm cực tiểu\nf<-function(x) (x-2)^2\n\n# Gradient tại điểm x\ngrad.f<-function(x) 2*(x-2)\n\n# Cho x0 bat ky\nset.seed(20)\nv_x0<-runif(20,-100,100)\n\n# dat: du lieu luu ket qua\ndat<-data.frame(so_vong_lap = 1:vong_lap_toi_da)\n\nfor (j in 1:length(v_x0)){\n  x0<-v_x0[j]\n  # Bước 1\n  x<-rep(0,vong_lap_toi_da)\n  x[1]<- x0 - lambda * grad.f(x0)\n  sai_so<-abs(f(x[i])-f(x0))\n\n  # Từ bước 2 trở đi\n  i <- 2\n  while (i< vong_lap_toi_da & sai_so > sai_so_nho_nhat){\n    x[i]<-x[i-1] - lambda * grad.f(x[i-1])\n    sai_so = abs(f(x[i])-f(x[i-1]))\n    i<-i+1\n  }\n  x[i:vong_lap_toi_da]<-x[i-1]\n  dat<-mutate(dat,x)\n  names(dat)[j+1]<-paste(\"Vong_lap\",j)\n}\n\ndat%>%gather(\"loop\", \"value\", -so_vong_lap)%>%\n  ggplot(aes(so_vong_lap, value, col = loop))+\n  geom_line(alpha = 0.5)+theme_minimal()+\n  ylab(\"Giá trị hàm f\")+xlab(\"Số vòng lặp\")+\n  geom_hline(yintercept = 2, color = \"red\")+\n  theme(legend.position = \"none\")"},{"path":"kiến-thức-r-nâng-cao.html","id":"advrappen3","chapter":"Chương 5 Kiến thức R nâng cao","heading":"5.5.4 Các giá trị đặc trưng của một dòng tiền tương lai","text":"","code":""},{"path":"kiến-thức-r-nâng-cao.html","id":"giá-trị-hiện-tại-và-tỷ-suất-sinh-lời-nội-bộ","chapter":"Chương 5 Kiến thức R nâng cao","heading":"5.5.4.1 Giá trị hiện tại và tỷ suất sinh lời nội bộ","text":"Một dòng tiền \\(CF_{t_1}\\), \\(CF_{t_2}\\), \\(\\cdots\\), \\(CF_{t_n}\\) xảy ra tại các thời điểm \\(t_k\\) với \\(1 \\leq k \\leq n\\). Giá trị hiện tại của dòng tiền với lãi suất \\(\\) được tính theo phương pháp lãi gộp như sau:\n\\[\\begin{align}\nPV = \\sum\\limits_{k=1}^n \\cfrac{CF_{t_k}}{(1+)^{t_k}}\n\\end{align}\\]\nGiá trị hiện tại của một dòng tiền là một thước đo cho giá trị của tài sản tạo ra dòng tiền đó. Nhìn chung, tài sản nào có giá trị hiện tại lớn hơn thì có giá trị cao hơn.Tỷ suất sinh lời nội bộ (Internal rate return hay IRR) là mức lãi suất \\(i_0\\) mà tính theo mức lãi suất này giá trị hiện tại của một dòng tiền bằng 0. Tỷ suất sinh lời nội bộ không tồn tại nếu dòng tiền tương lai của một tài sản chỉ có giá trị dương (hoặc âm).","code":""},{"path":"kiến-thức-r-nâng-cao.html","id":"durations","chapter":"Chương 5 Kiến thức R nâng cao","heading":"5.5.4.2 Durations","text":"Duration không phải là thước đo thời gian từ lúc bắt đầu đến lúc đáo hạn của dòng tiền mà là một thước đo cho sự nhạy cảm của giá trị hiện tại của dòng tiền theo sự thay đổi của lãi suất.Macaulay Duration là thước đo, tính bằng đơn vị thời gian, là giá trị trung bình có trọng số của khoảng thời gian tính từ lúc bắt đầu đến thời điểm mà các dòng tiền xuất hiện. Macaulay Duration được ký hiệu là \\(Mac\\_D\\) được tính bằng công thức sau:\n\\[\\begin{align}\nMac_D = w_1 \\times t_1 + w_2 \\times t_2 + \\cdots + w_n \\times t_n\n\\end{align}\\]\nvới tỷ trọng \\(w_k\\) được tính bằng tỷ trọng của giá trị hiện tại của \\(CF_{t_k}\\) trong giá trị hiện tại của toàn bộ dòng tiền\n\\[\\begin{align}\nw_k = \\cfrac{1}{PV} \\times \\cfrac{CF_{t_k}}{(1+)^{t_k}}\n\\end{align}\\]Macaulay Duration là thước đo, tính bằng đơn vị thời gian, là giá trị trung bình có trọng số của khoảng thời gian tính từ lúc bắt đầu đến thời điểm mà các dòng tiền xuất hiện. Macaulay Duration được ký hiệu là \\(Mac\\_D\\) được tính bằng công thức sau:\n\\[\\begin{align}\nMac_D = w_1 \\times t_1 + w_2 \\times t_2 + \\cdots + w_n \\times t_n\n\\end{align}\\]\nvới tỷ trọng \\(w_k\\) được tính bằng tỷ trọng của giá trị hiện tại của \\(CF_{t_k}\\) trong giá trị hiện tại của toàn bộ dòng tiền\n\\[\\begin{align}\nw_k = \\cfrac{1}{PV} \\times \\cfrac{CF_{t_k}}{(1+)^{t_k}}\n\\end{align}\\]Modified duration đo lường sự thay đổi (theo %) sự thay đổi của giá trị hiện tại của dòng tiền khi có sự thay đổi nhỏ trong lãi suất:\n\\[\\begin{align}\nMod_D & = - \\lim\\limits_{\\rightarrow 0} \\cfrac{1}{PV} \\ \\cfrac{\\Delta PV}{\\Delta } \\\\\n& = \\cfrac{1}{PV} \\ \\sum\\limits_{k=1}^n \\cfrac{t_k CF_{t_k}}{(1+)^{t_k+1}} \\\\\n& = \\cfrac{1}{1+} \\ \\cfrac{1}{PV} \\ \\cfrac{t_k CF_{t_k}}{(1+)^{t_k}}\n\\end{align}\\]\nBạn đọc có thể thấy rằng mặc dù ý nghĩa khác nhau, nhưng \\(Mod_D\\) có thể được tính từ \\(Mac_D\\) như sau\n\\[\\begin{align}\nMod_D = \\cfrac{1}{1+} \\times Mac_D\n\\end{align}\\]Modified duration đo lường sự thay đổi (theo %) sự thay đổi của giá trị hiện tại của dòng tiền khi có sự thay đổi nhỏ trong lãi suất:\n\\[\\begin{align}\nMod_D & = - \\lim\\limits_{\\rightarrow 0} \\cfrac{1}{PV} \\ \\cfrac{\\Delta PV}{\\Delta } \\\\\n& = \\cfrac{1}{PV} \\ \\sum\\limits_{k=1}^n \\cfrac{t_k CF_{t_k}}{(1+)^{t_k+1}} \\\\\n& = \\cfrac{1}{1+} \\ \\cfrac{1}{PV} \\ \\cfrac{t_k CF_{t_k}}{(1+)^{t_k}}\n\\end{align}\\]\nBạn đọc có thể thấy rằng mặc dù ý nghĩa khác nhau, nhưng \\(Mod_D\\) có thể được tính từ \\(Mac_D\\) như sau\n\\[\\begin{align}\nMod_D = \\cfrac{1}{1+} \\times Mac_D\n\\end{align}\\]Dollar duration là khoản tăng lên (theo số tiền tuyệt đối) của giá trị hiện tại của một dòng tiền khi lãi suất giảm 1% trong lãi suất\n\\[\\begin{align}\nDollar_D = Mod_D \\times PV \\times 0.01\n\\end{align}\\]Dollar duration là khoản tăng lên (theo số tiền tuyệt đối) của giá trị hiện tại của một dòng tiền khi lãi suất giảm 1% trong lãi suất\n\\[\\begin{align}\nDollar_D = Mod_D \\times PV \\times 0.01\n\\end{align}\\]","code":""},{"path":"kiến-thức-r-nâng-cao.html","id":"bài-tập","chapter":"Chương 5 Kiến thức R nâng cao","heading":"5.6 Bài tập","text":"\\(\\textbf{Câu hỏi }\\) Bạn cố gắng xấp xỉ hàm \\(sin(x)\\) trên khoảng \\([0,2*\\pi]\\) bằng một đa thức bậc 5\n\\[\\begin{align}\nP(x) = a_0 + a_1 \\cdot x + a_2 \\cdot x^2 + a_3 \\cdot x^3 + a_4 \\cdot x^4 + a_5 \\cdot x^5\n\\end{align}\\]\nbằng cách tìm các hệ số \\(\\textbf{} = (a_0, a_1, a_2, a_3, a_4, a_5)\\) sao cho hàm số \\(g(\\textbf{})\\) được xác định bằng\n\\[\\begin{align}\ng(\\textbf{}) = \\int\\limits_0^{2 \\pi} (P(x) - sin(x))^2 \\ dx\n\\end{align}\\]\nđạt giá trị nhỏ nhất. Hãy sử dụng thuật toán gradient descent để tìm \\(\\textbf{}\\).","code":"## \n## Attaching package: 'dplyr'## The following object is masked from 'package:pryr':\n## \n##     where## The following object is masked from 'package:gridExtra':\n## \n##     combine## The following object is masked from 'package:kableExtra':\n## \n##     group_rows## The following objects are masked from 'package:stats':\n## \n##     filter, lag## The following objects are masked from 'package:base':\n## \n##     intersect, setdiff, setequal, union## \n## Attaching package: 'lubridate'## The following objects are masked from 'package:base':\n## \n##     date, intersect, setdiff, union"},{"path":"nhập-dữ-liệu-vào-r.html","id":"nhập-dữ-liệu-vào-r","chapter":"Chương 6 Nhập dữ liệu vào R","heading":"Chương 6 Nhập dữ liệu vào R","text":"","code":""},{"path":"nhập-dữ-liệu-vào-r.html","id":"giới-thiệu-chung","chapter":"Chương 6 Nhập dữ liệu vào R","heading":"6.1 Giới thiệu chung","text":"Trong phần này của cuốn sách, bạn đọc sẽ được tìm hiểu về các kỹ thuật phân tích dữ liệu, bao gồm tiền xử lý dữ liệu, sắp xếp dữ liệu và trực quan hóa dữ liệu.Tiền xử lý dữ liệu bao gồm tất cả các kỹ thuật biến đổi dữ liệu thô thành định dạng để có thể thực hiện phân tích. Dữ liệu thô bao gồm dữ liệu nhận được từ người khác hoặc dữ liệu mà người phân tích tự tìm kiếm từ các nguồn khác nhau.Sắp xếp và tổ chức dữ liệu bao gồm các bước biến đổi, chuyển hóa dữ liệu thành định dạng để có thể trực quan hóa, thực hiện phân tích tính toán và xây dựng mô hình trên dữ liệu.Trực quan hóa dữ liệu là một nghệ thuật biến đổi dữ liệu vốn được hiển thị dưới dạng các con số, chuỗi ký tự, thành các biểu đồ, đồ thị, hình ảnh; sử dụng màu sắc, hình dạng, khoảng cách để mô tả giúp con người dễ dàng nhận thức và hiểu về dữ liệu. Trực quan hóa dữ liệu còn có thể giúp người phân tích dữ liệu và người tiếp nhận dữ liệu dễ dàng tìm ra những giá trị ẩn chứa trong dữ liệu.Trước khi có thể bắt đầu bước tiền xử lý và phân tích dữ liệu, bước đầu tiên là nhập dữ liệu vào R. Trong một vài trường hợp, nhập dữ liệu cần nhập chỉ đơn giản là một bảng Excel có sẵn. Trong một số trường hợp khác, quá trình nhập dữ liệu có thể phức tạp hơn, chẳng hạn như từ một (vài) phần nào đó trong một hoặc nhiều bảng Excel, hoặc từ một cơ sở dữ liệu được lưu trữ trong các máy tính server, hoặc đôi khi cần viết các vòng lặp để lấy dữ liệu từ các trình duyệt web. Đây là chủ đề mà chúng ta sẽ thảo luận trong chương đầu của phần phân tích dữ liệu.","code":""},{"path":"nhập-dữ-liệu-vào-r.html","id":"đối-tượng-lưu-dữ-liệu-trong-r","chapter":"Chương 6 Nhập dữ liệu vào R","heading":"6.2 Đối tượng lưu dữ liệu trong R","text":"Hai kiểu đối tượng thường được dùng để lưu dữ liệu trong R là data.frame và tibble. Chúng ta sẽ thảo luận về data.frame trước vì đây là kiểu lưu dữ liệu phổ biến và xuất hiện trước. Đối tượng kiểu tibble, với một vài ưu điểm hơn data.frame, sẽ được thảo luận trong phần tiếp theo.","code":""},{"path":"nhập-dữ-liệu-vào-r.html","id":"data.frame","chapter":"Chương 6 Nhập dữ liệu vào R","heading":"6.2.1 Data.frame","text":"Data.frame là đối tượng phổ biến nhất để lưu trữ dữ liệu trên môi trường làm việc của R. Hiểu một cách đơn giản, một data.frame giống như một bảng excel mà mỗi cột tương ứng với một véc-tơ và mỗi dòng tương ứng với một quan sát. Ngay khi cài đặt R, đã có nhiều đối tượng là dữ liệu kiểu data.frame đã được lưu trữ trong R và đã sẵn sàng được sử dụng. Bạn đọc sử dụng câu lệnh data() để biết trên môi trường đang sử dụng có những dữ liệu nào,Sau khi thực thi câu lệnh, bạn đọc có thể thấy trên cửa sổ script xuất hiện một cửa sổ mới với danh sách tất cả các dữ liệu sẵn có trong R và những dữ liệu có trong các thư viện đang được gọi lên trên môi trường đang làm việc. Để biết trong một thư viện cụ thể có những dữ liệu nào, bạn đọc có thể sử dụng lệnh data() kèm với tùy chọn package như sau:Trong danh sách dữ liệu của thư viện dslabs, bạn đọc có thể thấy rất nhiều đối tượng kiểu data.frame. Một trong số đó có một dữ liệu có tên là murders. Bạn đọc có thể kiểm tra kiểu của đối tượng này bằng hàm class():Thông thường để có hiểu biết ban đầu về một đối tượng kiểu data.frame, chúng ta nên bắt đầu bằng đọc mô tả về dữ liệu (nếu có):Có một nhóm các câu lệnh thường được sử dụng để có cái nhìn tổng thể về cấu trúc của dữ liệu. Các câu lệnh này được liệt kê ở dưới:Hàm head() cho kết quả là các dòng đầu tiên của dữ liệu và giúp bạn đọc cái nhìn trực quan về tên các cột hoặc kiểu dữ liệu trong các cột. Tuy nhiên hàm head() không hiệu quả khi dữ liệu có nhiều cột. Hàm head() có tham số n đi kèm cho biết số lượng dòng dữ liệu bạn đọc muốn hiển thị. n nhận giá trị mặc định bằng 6.Hàm View() hiển thị về dữ liệu một cách trực quan và dễ nhìn nhất. Sau khi thực thi, kết quả của hàm View() sẽ xuất hiện trên cửa sổ Script. Ngoài ý nghĩa hiển thị dữ liệu, kết quả của hàm View() còn là một bảng tương tác mà bạn đọc có thể thực hiện thao tác như sắp xếp dữ liệu trong các cột. Hàm View() có hạn chế khi dữ liệu có quá nhiều dòng hoặc nhiều cột và thời gian hiển thị lâu hơn với head().Hàm str() là cách hiển thị dữ liệu một cách tổng quát và hiệu quả hơn với head(). Sau thi thực thi hàm str() trên dữ liệu murders, bạn đọc có thể có cái nhìn tổng thể về dữ liệu này:\nmurders là một data.frame có 5 cột (còn gọi là các variables) và 51 dòng (còn gọi là các observations).\nTên của 5 cột lần lượt là state, abb, region, population, và total.\nHàm str() cho chúng ta thấy thấy được kiểu dữ liệu của từng cột: cột state và cột abb chứa dữ liệu kiểu character; cột region chứa dữ liệu kiểu factor, các cột population, và total chứa dữ liệu kiểu numeric.\nmurders là một data.frame có 5 cột (còn gọi là các variables) và 51 dòng (còn gọi là các observations).Tên của 5 cột lần lượt là state, abb, region, population, và total.Hàm str() cho chúng ta thấy thấy được kiểu dữ liệu của từng cột: cột state và cột abb chứa dữ liệu kiểu character; cột region chứa dữ liệu kiểu factor, các cột population, và total chứa dữ liệu kiểu numeric.Hàm glimpse() là một hàm số trong thư viện dplyr cũng thường được sử dụng để tìm hiểu về dữ liệu. Kết quả của hàm glimpse() tương tự như hàm str().Một hàm số khác cũng thường được sử dụng để người phân tích có cái nhìn tổng quan về dữ liệu là hàm summary(). Hàm số này có thể được áp dụng trên các đối tượng kiểu khác nhau, không nhất thiết phải là kiểu data.frame. Khi sử dụng trên data.frame, hàm summary() cho chúng ta nhiều thông tin hữu ích về dữ liệu. Thật vậy, bạn đọc có thể quan sát kết quả của hàm summary() trên dữ liệu murders:Có thể thấy rằng hàm summary() cho biết thông chi tiết hơn với str() về giá trị trong các cột dữ liệu:Ngoài kiểu dữ liệu của từng biến, summary() còn cung cấp các giá trị thống kê cho phép người phân tích dữ liệu có hình dung ban đầu về phân phối xác suất của các biến liên tục và tần suất xuất hiện của các giá trị trong biến rời rạc.Trong trường hợp cột dữ liệu có giá trị không quan sát được, hàm summary() cũng sẽ cho biết có bao nhiêu giá trị không quan sát được trong mỗi cột.Bạn đọc hãy sử dụng các hàm liệt kê ở trên để tìm hiểu về dữ liệu có tên gapminder trong thư viện dslabsXét về cấu trúc, có thể hiểu một data.frame là một trường hợp đặc biệt của đối tượng kiểu list trong R. Các phần tử con của một data.frame là các véc-tơ (cột) dữ liệu. Để lấy ra một cột dữ liệu của một data.frame chúng ta sử dụng ký tự $ để kết nối tên data.frame và tên cột dữ liệu. Ví dụ, để lấy giá trị của cột có tên là population từ dữ liệu murders, chúng ta viết câu lệnh như sau:Hoặc lấy ra cột có tên là region:Kiểu dữ liệu của cột region là kiểu factor. Về bản chất, một véc-tơ kiểu factor là một véc-tơ kiểu chuỗi ký tự nhưng được lưu trong R theo một cách hiệu quả hơn, tiết kiệm bộ nhớ, và thuận lợi cho người sử dụng khi phân tích dữ liệu. Trước hết, mỗi giá trị trong véc-tơ chuỗi ký tự sẽ được cho tương ứng với một số tự nhiên, bắt đầu từ 1 đến số lượng chuỗi ký tự khác nhau trong véc-tơ đó. Thay vì lưu chính xác giá trị của các chuỗi ký tự, véc-tơ kiểu factor lưu dữ liệu dưới dạng các số tự nhiên. Mỗi khi cần hiển thị giá trị của một chuỗi ký tự, véc-tơ kiểu factor sẽ tìm kiếm số tự nhiên tương ứng với giá trị của chuỗi ký tự đó. Cách lưu dữ liệu như vậy hiệu quả hơn về mặt bộ nhớ đặc biệt là khi trong véc-tơ chuỗi ký tự có nhiều giá trị bị lặp lại.Các hàm số summary() và table() có thể được sử dụng để tổng hợp thông tin về một véc-tơ kiểu factor:Từ kết quả của hàm table() trên véc-tơ region có thể thấy rằng:Biến region có 4 giá trị riêng biệt và quy tắc cho tương ứng mỗi giá trị đến các số tự nhiên là: Northeast:1; South:2; North Central:3, và West:4.Tần suất xuất hiện của các giá trị trong véc-tơ region\nNortheast xuất hiện 9 lần;\nSouth xuất hiện 17 lần;\nNorth Central xuất hiện có 12 lần;\nWest xuất hiện 13 lần.\nNortheast xuất hiện 9 lần;South xuất hiện 17 lần;North Central xuất hiện có 12 lần;West xuất hiện 13 lần.Trong hầu hết các trường hợp, dữ liệu cần xử lý và phân tích là dữ liệu được nhập từ các nguồn bên ngoài. Dữ liệu sau khi được nhập vào R sẽ được lưu dưới dạng data.frame để dễ dàng xử lý. Trong một vài trường hợp, bạn đọc sẽ cần tự tạo data.frame trực tiếp từ các câu lệnh R. Hàm số để tạo một data.frame là data.frame(). Các véc-tơ khai báo cho giá trị của các cột cần có độ dài bằng nhau nếu không R sẽ báo lỗi. Ví dụ, để tạo một data.frame có tên df với các cột lần lượt là id, names, grades, và result, chúng ta viết câu lệnh như sau:Khi lưu dữ liệu kiểu data.frame sẽ có một số nhược điểm khiến cho việc lấy dữ liệu từ nguồn bên ngoài vào bị hạn chế, chẳng hạn như tên cột dữ liệu bị tự động thay đổi nếu không phù hợp, hoặc kiểu dữ liệu bị tự động thay đổi. Để khắc phục các nhược điểm này, một kiểu đối tượng mới được phát triển để lưu dữ liệu trên môi trường làm việc của R, đó là kiểu tibble. Trong phần tiếp theo chúng ta sẽ thảo luận về đối tượng lưu dữ liệu này.","code":"\ndata()\nlibrary(dslabs) # Gọi thư viện dslabs\ndata(package = \"dslabs\") # Liệt kê những data trong dslabs\nclass(murders) # Trả lại kết quả là một data frame## [1] \"data.frame\"\n? murders # Cửa sổ help sẽ hiển thị mô tả về murders\nView(murders) # Hiển thị data.frame dưới dạng bảng\nhead(murders,k = 5) # Hiển thị k dòng đầu tiên của data.frame\nstr(murders) # Hiển thị cấu trúc của data.frame\nglimpse(murders) # Hiển thị cấu trúc của data.frame\nsummary(murders)##     state               abb                      region     population      \n##  Length:51          Length:51          Northeast    : 9   Min.   :  563626  \n##  Class :character   Class :character   South        :17   1st Qu.: 1696962  \n##  Mode  :character   Mode  :character   North Central:12   Median : 4339367  \n##                                        West         :13   Mean   : 6075769  \n##                                                           3rd Qu.: 6636084  \n##                                                           Max.   :37253956  \n##      total       \n##  Min.   :   2.0  \n##  1st Qu.:  24.5  \n##  Median :  97.0  \n##  Mean   : 184.4  \n##  3rd Qu.: 268.0  \n##  Max.   :1257.0\nmurders$population # in ra màn hình cột population của data.frame murders\nmurders$region # in ra màn hình cột region của data.frame murders\nsummary(murders$region) # Tổng hợp thông tin của vec-tơ dạng factor##     Northeast         South North Central          West \n##             9            17            12            13\ntable(murders$region) # cho kết quả tương tự như summary## \n##     Northeast         South North Central          West \n##             9            17            12            13\ndf<-data.frame( # Hàm data.frame() dùng để tạo data.frame tên df\n      id = paste(\"SV\",1:5), # Cột có tên là ID nhận giá trị \"SV1\",...,\"SV5\"\n      names = c(\"You\", \"Me\", \"Him\", \"Her\", \"John\"), # Cột names\n      grades = c(5.5, 1.5, 10.0, 9.0, 7.6), # Cột grades\n      result = c(TRUE, FALSE,TRUE, TRUE, TRUE)) # Cột result\ndf # hiển thị data.frame có tên df##     id names grades result\n## 1 SV 1   You    5.5   TRUE\n## 2 SV 2    Me    1.5  FALSE\n## 3 SV 3   Him   10.0   TRUE\n## 4 SV 4   Her    9.0   TRUE\n## 5 SV 5  John    7.6   TRUE"},{"path":"nhập-dữ-liệu-vào-r.html","id":"tibble","chapter":"Chương 6 Nhập dữ liệu vào R","heading":"6.2.2 Tibble","text":"Về cơ bản một tibble là cũng có thể hiểu là một data.frame với một vài điều chỉnh để giúp việc lấy dữ liệu từ nguồn bên ngoài vào và thực hiện phân tích trở nên dễ dàng hơn. Theo kinh nghiệm của chúng tôi, ở mức độ phân tích dữ liệu thông thường, sự khác khác nhau giữa tibble và data.frame là không đáng kể. Sự khác nhau cơ bản giữu hai đối tượng này có thể liệt kê như sau:Thứ nhất, khi một tibble ra màn hình sẽ chỉ có 10 dòng đầu được hiển thị và số lượng cột được sẽ luôn khớp với kích thước cửa sổ Console hiện tại. Việc này giúp cho cửa sổ Console không bị tràn dòng giống như khi chúng ta data.frame có kích thước lớn. Ngoài ra, khi hiển thị trên cửa sổ Console, thông tin về kiểu dữ liệu của cột cũng sẽ xuất hiện ngay dưới tên cột. Thư viện để làm việc trên đối tượng tibble là thư viện có cùng tên. Bạn đọc có thể sử dụng hàm as_tibble() để đổi một data.frame sang kiểu tibble và dùng hàm .data.frame() để thực hiện phép đổi ngược lại. Bạn đọc có thể thực hiện một tibble trực tiếp lên màn hình Console mà không gặp phải vấn đề về tràn dòng như ví dụ dưới đây:Thứ hai, khi lấy dữ liệu từ nguồn ngoài, tibble không đổi tên cột dù tên cột không phải là kiểu tên cho phép trong R. Đồng thời, khi tạo một tibble, bạn đọc có thể đặt tên biến (cột) là một tên không được phép sử dụng với tên biến thông thường trong R.Thứ ba, khi dữ liệu từ nguồn bên ngoài được lưu vào một tibble, kiểu dữ liệu sẽ không thay đổi.Hàm tibble() được sử dụng để tạo ra một tibble trong R. Trong ví dụ dưới đây, chúng tôi tạo ra một dữ liệu có 3 cột với tên biến đều không được sử dụng làm tên biến trong R, tuy nhiên R vẫn không báo lỗi và dữ liệu được tạo ra có các cột có tên không thay đổi:Nếu thay thế hàm tibble() trong đoạn câu lệnh trên bằng hàm data.frame() thì data.frame được tạo thành sẽ tự động thay đổi tên cộtBạn đọc có thể thấy rằng dữ liệu có tên df khi được lưu dưới dạng data.frame có tên các cột đã tự động thay đổi. Việc tự động thay đổi tên cột sẽ khiến cho người phân tích dữ liệu gặp khó khăn khi kiểm soát tên biến mỗi khi lấy dữ liệu từ nguồn ngoài, đặc biệt đối với những dữ liệu lớn và phức tạp.Những điểm khác nhau giữa tibble và data.frame sẽ được tiếp tục đề cập ở các phần tiếp theo của chương khi chúng tôi thảo luận về các hàm dùng để nhập dữ liệu vào R.","code":"\nlibrary(tibble)\ntrump_tweets # in một data.frame sẽ bị tràn dòng\nas_tibble(trump_tweets) # Hiển thị 1 tibble hiệu quả hơn.\ntib<-tibble( # hàm tibble dùng để tạo tibble\n  \":D\" = c(1,2,3), # có thể dùng tên cột là \":D\"\n  \":p\" = c(\"X\",\"Y\",\"Z\"), # có thể dùng tên cột là \":p\"\n  \"1\" = 2 # có thể dùng tên cột là \"1\"\n)\ntib## # A tibble: 3 × 3\n##    `:D` `:p`    `1`\n##   <dbl> <chr> <dbl>\n## 1     1 X         2\n## 2     2 Y         2\n## 3     3 Z         2\ndf<-data.frame( # tạo data.frame thay vì tibble\n  \":D\" = c(1,2,3), # data.frame sẽ đổi tên cột cho phù hợp\n  \":p\" = c(\"X\",\"Y\",\"Z\"), # data.frame sẽ đổi tên cột cho phù hợp\n  \"1\" = 2 # data.frame sẽ đổi tên cột cho phù hợp\n)\ndf # hãy quan sát xem tên cột của df thay đổi như thế nào##   X.D X.p X1\n## 1   1   X  2\n## 2   2   Y  2\n## 3   3   Z  2"},{"path":"nhập-dữ-liệu-vào-r.html","id":"nhập-dữ-liệu-bằng-hàm-có-sẵn","chapter":"Chương 6 Nhập dữ liệu vào R","heading":"6.3 Nhập dữ liệu bằng hàm có sẵn","text":"Dữ liệu từ các nguồn ngoài tồn tại ở nhiều định dạng khác nhau được lưu và trích xuất từ các phần mềm/hệ thống khác nhau. Một trong những điểm mạnh của phần mềm R là khả năng đọc được hầu hết các định dạng khác nhau của dữ liệu. Ngay trong các thư viện sẵn có khi chúng ta cài đặt R, đã có một danh sách các hàm cho phép chúng ta đọc dữ liệu từ nhiều định dạng. Một số hàm điển hình và định dạng dữ liệu tương ứng được liệt kê trong các Bảng ??Để đọc dữ liệu từ nguồn ngoài bằng một trong các hàm được liệt kê trong Bảng ??, cấu trúc câu lệnh sẽ như sau:trong đó TenHamSo là tên của hàm đọc dữ liệu; Duong_dan là đường dẫn đến folder chứa dữ liệu; và Ten_file là tên file chứa dữ liệu bao gồm cả phần định dạng.Khi lấy dữ liệu từ các nguồn bên ngoài vào bằng các câu lệnh có sẵn, tên của các cột dữ liệu có thể bị thay đổi một số tên cột không thể được dùng để đặt tên của data.frame. đó, bạn đọc hãy luôn kiểm tra lại tên các cột dữ liệu sau khi đọc. Hàm names() cho biết tên các cột của một data.frame.Để đổi tên của các biến trong data.frame có tên df ở trên, bạn đọc sử dụng hàm cần gán names() để gọi tên các biến sau đó gán giá trị của hàm này bằng một véc-tơ kiểu chuỗi ký tự chứa tên các cột. Hãy đảm bảo rằng độ dài của vec-tơ chứa tên cột bằng số cột của df nếu không sẽ có cảnh báo từ R.Thực hành:Bạn đọc hãy tìm trên máy tính của mình các file dữ liệu có định dạng như trong Bảng ?? sau đó hãy sử dụng hàm số tương ứng để đọc dữ liệu vào R.Mặc dù R đã hỗ trợ việc đọc dữ liệu từ hầu hết các định dạng khác nhau, tuy nhiên khi dữ liệu ngày càng lớn và phức tạp thì các hàm đọc dữ liệu sẵn có sẽ không còn đáp ứng được nhu cầu. Chính vì thế có các thư viện được thiết kế riêng cho việc đọc dữ liệu. Trong các phần tiếp theo chúng ta sẽ thảo luận về các thư viện như vậy và các ưu thế khi sử dụng các thư viện này với việc sử dụng các hàm sẵn có.","code":"\nTenHamSo(Duong_dan/Ten_file)\ndf<-read.csv(header = TRUE,\n              text = \"@1,@2\n                      1,2\n                      3,4\") # sử dụng read.csv để đọc đoạn text\nnames(df) # hiển thị tên của các cột## [1] \"X.1\" \"X.2\"\nnames(df)<-c(\"c1\",\"c2\") # đổi tên 2 cột của data.frame df\ndf # in data.frame df##   c1 c2\n## 1  1  2\n## 2  3  4"},{"path":"nhập-dữ-liệu-vào-r.html","id":"nhập-dữ-liệu-bằng-thư-viện-readr.","chapter":"Chương 6 Nhập dữ liệu vào R","heading":"6.4 Nhập dữ liệu bằng thư viện readr.","text":"Thư viện readr là một (1) trong tám (8) thư viện được tích hợp sẵn trong thư viện tổng hợp tidyverse chuyên dành cho việc phân tích dữ liệu. Thư viện readr có các câu lệnh để đọc dữ liệu tương tự như các câu lệnh sẵn có trong R nhưng đặc biệt hiệu quả hơn về tốc độ (thời gian) đọc dữ liệu. Chẳng hạn như hàm số dùng để đọc các file định dạng .csv trong thư viện readr là hàm read_csv() thường được dùng thay thế cho hàm có sẵn read.csv() khi chúng ta cần đọc các file có dung lượng lớn.Chúng ta thực hiện một ví dụ như sau để sánh thời gian đọc dữ liệu của hàm read_csv() và hàm read.csv(): chúng ta sẽ tạo hai file dữ liệu bao gồm test1.csv và test2.csv là các file chứa các số được sinh ngẫu nhiên. Dữ liệu test1.csv có \\(10^2\\) hàng và \\(10^4\\) cột, trong khi dữ liệu test2.csv có \\(10^2\\) hàng và \\(10^5\\) cột. Chúng ta sẽ dùng hàm sẵn có và hàm read_csv() cùng đọc hai dữ liệu này sau đó sánh thời gian đọc dữ liệu. Các câu lệnh dưới đây dùng để tạo và lưu các dữ liệu:Bạn đọc có thể kiểm tra kích thước của các file test1.csv và test2.csv trên máy tính để thấy rằng dung lượng của các file lần lượt là 18 Mega byte và 180 Mega byte. Chúng ta sẽ kiểm tra thời gian mà các hàm read.csv() và read_csv() nhập dữ liệu đối với dữ liệu test1.csv trước:Đối với dữ liệu test1.csv thì thời gian nhập dữ liệu của read_csv() có nhanh hơn nhưng không có sự khác biệt đáng kể. Tuy nhiên sự khác biệt sẽ rõ ràng khi nhập dữ liệu test2.csv. Bạn đọc cân nhắc khi dùng hàm read.csv() đọc dữ liệu bởi thời gian nhập dữ liệu với những file có dung lượng hơn 100 Mega bytes có thể lên đến hơn 20 phút.Trên máy tính của chúng tôi, hàm read_csv() sẽ mất khoảng 2 phút để đọc dữ liệu test2.csv, nghĩa là thời gian có thể nhanh hơn đến 10 lần. Điều này đặc biệt quan trọng mỗi khi chúng ta cần đọc các dữ liệu có kích thước lớn. Một lưu ý khác đối với read_csv() là dữ liệu sẽ được lưu dưới dạng một tibble, nghĩa là dữ liệu sẽ không bị thay đổi tên biến và kiểu giá trị của biến.Ngoài hàm read_csv(), thư viện readr còn có các hàm số khác để đọc các kiểu định dạng file khác nhau. Danh sách các hàm thường hay dùng được liệt kê trong bảng ??Ngoài việc các hàm trong thư viện readr luôn lưu dữ liệu vào một tibble, một số lưu ý khác khi bạn đọc sử dụng các hàm số này là:Thứ nhất: các hàm số đọc dữ liệu của readr luôn hiểu hàng đầu tiên của dữ liệu là tên của các biến. đó, nếu thực sự hàng đầu tiên không phải là tên của biên, bạn đọc cần sử dụng tham số col_names và gán giá trị cho tham số này bằng FALSE. Thật vậy, hãy quan sát sự khác nhau giữa việc có sử dụng và không sử dụng col_names = FALSE trong ví dụ dưới đây:Bạn đọc có thể thấy rằng khi không sử dụng col_names = FALSE như trong đoạn câu lệnh thứ nhất, hàm read_csv() sẽ hiểu hàng đầu tiên, tương ứng với các số 1, 2, và 3, là tên các cột và dữ liệu chỉ được tính bắt đầu từ hàng thứ hai. Điều này giải thích tại sao kết quả nhận được là một tibble chỉ có 1 quan sát. Trong đoạn câu lệnh thứ hai, Khi chúng ta sử dụng col_names = FALSE, hàm read_csv() sẽ tự động đặt tên các cột là X1, X2, X3 và kết quả nhận được là một tibble có 2 quan sát.Khi dữ liệu được trích xuất từ nhiều nguồn khác nhau, trong một số trường hợp các dòng đầu tiên của dữ liệu là các đoạn văn bản được viết để mô tả về dữ liệu. đó khi sử dụng các hàm đọc dữ liệu, bạn đọc cần có thể sử dụng tham số skip = k để loại bỏ k dòng đầu tiên của file dữ liệu. Ví dụ, dữ liệu dưới đây có hai dòng đầu tiên là các đoạn văn bản không thuộc về dữ liệu, chúng ta loại bỏ đi hai dòng đó trong câu lệnh đọc dữ liệu như sau:Bạn đọc cũng có thể gán một véc-tơ chuỗi ký tự cho tham số col_names tạo tên cho các biến ngay khi gọi hàm read_csv(). Tuy nhiên để tránh sự phức tạp, chúng tôi khuyên bạn đọc hãy đặt tên cho các cột bằng các gọi hàm names() như chúng tôi đã đề cập ở trên.Cách sử dụng các hàm khác ngoài read_csv() được liệt kê trong Bảng ?? hoàn toàn tương tự và bạn đọc có thể tham khảo trong hướng dẫn của thư viện readr.Bạn đọc hãy thử kiểm tra xem các câu lệnh đọc dữ liệu dưới đây có vấn đề gì không và nếu có thể, bạn đọc hãy thử lựa chọn hàm hoặc thêm tham số phù hợp để đọc dữ liệu.","code":"\nx<-matrix(rnorm(10^6),10^2,10^4) # Ma trận 100 hàng, 10^4 cột\nwrite.csv(x,\"test1.csv\") # Ma tran thanh file .csv\nx<-matrix(rnorm(10^7),10^2,10^5) # Ma trận 100 hàng, 10^5 cột\nwrite.csv(x,\"test2.csv\") # Ma tran thanh file .csv\nstart<-proc.time() # lưu lại thời điểm trước khi chạy read.csv\ndat<-read.csv(\"test1.csv\") # dùng hàm read.csv để load dữ liệu\nproc.time() - start # tính thời gian hàm read.csv chạy\n\nstart<-proc.time() # lưu lại thời điểm trước khi chạy read_csv\ndat<-read_csv(\"test1.csv\") # dùng hàm read_csv để load dữ liệu\nproc.time() - start # tính thời gian hàm read_csv chạy\nstart<-proc.time() # lưu lại thời điểm trước khi chạy read.csv\ndat<-read.csv(\"test2.csv\") # !!! THỜI GIAN CHẠY CÓ THỂ LÊN ĐẾN 20-25 phút\nproc.time() - start # tính thời gian hàm read.csv chạy\n\nstart<-proc.time() # lưu lại thời điểm trước khi chạy read_csv\ndat<-read_csv(\"test2.csv\") # dùng hàm read_csv để load dữ liệu\nproc.time() - start # tính thời gian hàm read_csv chạy\nlibrary(readr)\n# Kết quả sẽ là một Tibble 1 hàng và 3 cột\nread_csv(\"1,2,3\n         4,5,6\") # Tên các cột là \"1\", \"2\", và \"3\"## # A tibble: 1 × 3\n##     `1`   `2`   `3`\n##   <dbl> <dbl> <dbl>\n## 1     4     5     6\n# Kết quả sẽ là một Tibble 2 hàng và 3 cột\nread_csv(\"1,2,3\n         4,5,6\", col_names = FALSE)## # A tibble: 2 × 3\n##      X1    X2    X3\n##   <dbl> <dbl> <dbl>\n## 1     1     2     3\n## 2     4     5     6\n# readr tự động đặt tên các cột X1, X2, X3\n# Kết quả sẽ là một Tibble 2 hàng và 3 cột\nread_csv(\"Trường Công nghệ\n        Khoa toán Kinh tế\n         1,2,3\n         4,5,6\", col_names = FALSE, skip = 2) # readr sẽ không đọc 2 dòng đầu## Rows: 2 Columns: 3\n## ── Column specification ────────────────────────────────────────────────────────\n## Delimiter: \",\"\n## dbl (3): X1, X2, X3\n## \n## ℹ Use `spec()` to retrieve the full column specification for this data.\n## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.## # A tibble: 2 × 3\n##      X1    X2    X3\n##   <dbl> <dbl> <dbl>\n## 1     1     2     3\n## 2     4     5     6\nread_csv(\"x,y\\n1,2,3\\n4,5,6\") # \\n thay cho xuống dòng\nread_csv(\"x,y,z\\n1,2\\n1,2,3,4\")\nread_csv(\"x,y\\n\\\",1,\\n,a,b\",col_names = FALSE)\nread_csv(\"x;y\\n1;2\\nx;y\") # Thử hàm số khác\nread_csv(\"x|y\\n1|2\") # Thử hàm số khác"},{"path":"nhập-dữ-liệu-vào-r.html","id":"tương-tác-giữa-r-với-microsoft-excel","chapter":"Chương 6 Nhập dữ liệu vào R","heading":"6.5 Tương tác giữa R với Microsoft Excel","text":"Speardsheet trong Microsoft Excel là công cụ rất phổ biến trong các môi trường làm việc liên quan đến lĩnh vực kinh tế và kinh doanh. Ngoài chức năng tính toán, phân tích, vẽ đồ thị rất phong phú và trực quan, Speardsheet cũng là một công cụ có thể dùng để lưu trữ dữ liệu khi cần. Thực tế là người phân tích dữ liệu trong môi trường kinh doanh sẽ thường xuyên nhận được dữ liệu từ các phòng ban, đơn vị khác trong các file có định dạng của Microsoft Excel như .xls, .xlsx, .xlsb, hoặc .xlsm.Bạn đọc có thể sử dụng trực tiếp Speardsheet để thực hiện những phân tích và tính toán đơn giản. Tuy nhiên để thực hiện các yêu cầu phức tạp hơn, chẳng hạn như khi làm việc trên các bảng có kích thước lớn, khi làm việc với nhiều bảng cùng lúc, hoặc khi thực hiện xây dựng các mô hình phức tạp trên dữ liệu, bạn cần phải sử dụng các công cụ phân tích dữ liệu cao cấp hơn, như R là một ví dụ.Ngoài hỗ trợ việc đọc dữ liệu từ các file được lưu dưới các định dạng của Microsoft Excel, R còn có thể truy cập và làm việc trực tiếp trên các Speardsheet mà không cần sử dụng Microsoft Excel. Các thư viện bổ sung mà chúng thôi thường sử dụng để có thể thực thi câu lệnh từ R, đọc dữ liệu và kết nối với Microsoft Excel là các thư viện readxl và openxlsx. Thư viện readxl bao gồm các hàm đọc dữ liệu từ nhiều định dạng khác nhau từ Excel trong khi thư viện openxlsx có tính năng cho phép sử dụng các câu lệnh của R để điều khiển, tính toán, và định dạng các Spreadsheet mà không cần mở Microsoft Excel. Cách sử dụng các hàm cơ bản trong hai thư viện này được trình bày trong các phần dưới đây.","code":""},{"path":"nhập-dữ-liệu-vào-r.html","id":"đọc-dữ-liệu-được-lưu-bằng-microsoft-excel","chapter":"Chương 6 Nhập dữ liệu vào R","heading":"6.5.1 Đọc dữ liệu được lưu bằng Microsoft Excel","text":"Hàm read_excel() được sử dụng để đọc các file được lưu bằng Microsoft Excel. Hàm số sẽ hỗ trợ đọc các file có định dạng .xlsx, .xlsm, và .xls. Hai biến thể khác của hàm read_excel() là read_xlsx() và read_xls() được sử dụng khi chúng ta biết chính xác định dạng của file cần đọc. Các tham số thường được sử dụng trong các hàm này được liệt kê như sau:Tham số sheet cho biết tên của sheet của trong file mà bạn muốn đọc dữ liệu từ đó. Tham số này có ý nghĩa khi file chúng ta cần lấy dữ liệu có nhiều sheet. Nếu không sử dụng tham số này trong hàm read_excel(), giá trị mặc định sẽ là sheet thứ nhất của file.Tham số range được sử dụng nếu bạn đọc chỉ muốn lấy dữ liệu từ một phần chứ không phải toàn bộ sheet. Chẳng hạn như khi khai báo range = ‘A1:E100’, hàm read_excel() sẽ chỉ lấy dữ liệu từ cell A1 đến cell E100 của sheet tương ứng.Tham số col_names nhận một trong hai giá trị là TRUE hoặc FALSE. Giá trị TRUE cho biết có sử dụng hàng đầu tiên làm tên các biến, trong khi giá trị FALSE cho biết không lấy hàng đầu tiên làm tên cột.Tham số skip cho biết số lượng dòng sẽ bỏ qua trước khi bắt đầu đọc dữ liệu. Nếu bạn sử dụng đồng thời hai tham số range và skip thì hàm read_excel() sẽ ưu tiên tham số range và bỏ qua tham số skipNgoài ra hàm read_excel() còn có các tham số khác có thể hữu ích trong nhiều trường hợp như col_types hay n_max. Bạn đọc cần đọc mô tả của hàm số read_excel() để hiểu chính xác hơn về các tham số này.Tóm lại, để đọc dữ liệu được lưu bằng Microsoft Excel, bạn đọc cần lựa chọn hàm tương ứng với định dạng của file, sau đó sử dụng các tham số được liệt kê ở trên để điều chỉnh việc đọc dữ liệu. Các câu lệnh dưới đây được sử dụng để đọc dữ liệu nằm trong range được giới hạn từ cell A1 đến cell E100, trong sheet có tên ‘ws1’, và file có tên ‘wb1.xlsx’Thực hành: bạn đọc hãy tìm các file dữ liệu có định dạng khác nhau được lưu bằng Microsoft Excel trên máy tính của mình và sử dụng các hàm tương ứng trong thư viện readxl để đọc dữ liệu.","code":"\nsetwd(path) # Đặt đường dẫn đến folder chứa file\ndat<-read_xlsx(\"wb1.xlsx\",\n               sheet = \"ws1\",\n               range = \"A1:E100\",\n               col_names = TRUE)"},{"path":"nhập-dữ-liệu-vào-r.html","id":"tương-tác-r-với-spreadsheet","chapter":"Chương 6 Nhập dữ liệu vào R","heading":"6.5.2 Tương tác R với Spreadsheet","text":"Ngoài việc lấy dữ liệu từ các file được lưu bằng Microsoft Excel, bạn đọc cũng có thể sử dụng R để thực hiện tính toán trên các spreardsheet và lưu lại kết quả dưới định dạng của Microsoft Excel mà không cần phải mở file trực tiếp . Các hàm số: loadWorkbook(), addWorksheet(), writeData(), và saveWorkbook() trong thư viện openxlsx có thể giúp chúng ta thực hiện được các yêu cầu này.Hàm loadWorkbook() đượpc sử dụng để mở một excel workbook và lưu thành một đối tượng kiểu workbook trên R. Câu lệnh dưới đây dùng để lấy thông tin từ file có tên là ‘mau bd.xlsx’ trong đường dẫn tương ứng và sau đó lưu thông tin vào một đối tượng kiểu workbook với tên wb1:Hàm str() có thể sử dụng để xem cấu trúc của đối tượng wb1. Bạn đọc cần lưu ý là các workbook lớn có thể khiến cho kết quả của hàm str() khi hiển thị trở nên khó hiểu.Bạn đọc có thể hiểu đối tượng kiểu workbook sẽ hoạt động giống như một đối tượng kiểu list mà mỗi list con tương ứng với một sheet của workbook đó. Bạn đọc có thể sử dụng hàm names() để biết wb1 có những đối tượng con nào:Hàm addWorksheet() được sử dụng để thêm một worksheet vào trong một đối tượng workbook. Cấu trúc câu lệnh của hàm addWworksheet() khá đơn giản và các tham số được sử dụng chủ yếu là để thiết kế định dạng cho worksheet mới. Ví dụ, câu lệnh sau được sử dụng để thêm vào workbook wb1 một worksheet có tên là Sheet 2Hàm writeData() có thể được sử dụng để thay đổi và sửa thông tin trên đối tượng kiểu workbook. Các tham số thường được sử dụng với hàm writeData() bao gồm có:Tham số startCol: chỉ số của cột trên cùng của worksheet mà chúng ta bắt đầu ghi thông tin lên. Nếu không sử dụng tham số này, chỉ số cột đầu tiên sẽ luôn là 1, nghĩa là cột của workssheet mà chúng ta muốn thay đổi thông tin.Tham số statRow: chỉ số của hàng trên worksheet mà chúng ta bắt đầu ghi thông tin lên. Nếu không sử dụng tham số này, chỉ số hàng đầu tiên sẽ luôn là hàng 1 của workssheet mà chúng ta muốn thay đổi thông tin.Tham số colNames: nhận giá trị bằng TRUE hoặc FALSE. Giá trị colNames = TRUE cho biết có ghi nhận thông tin tên cột của dữ liệu (hoặc véc-tơ) vào hàng đầu tiên. Giá trị colNames = FALSE nghĩa là không ghi nhận tên cột của dữ liệu hay véc-tơ vào hàng đầu tiên.Tham số rowNames: : nhận giá trị bằng TRUE hoặc FALSE. Giá trị TRUE cho biết có ghi nhận thông tin tên hàng của dữ liệu (hoặc véc-tơ) vào cột đầu tiên của worksheet. Giá trị FALSE nghĩa là không ghi nhận tên hàng của dữ liệu hay véc-tơ vào hàng đầu tiên của worksheet.Đoạn câu lệnh dưới đây sẽ lưu thông tin của một đối tượng kiểu data.frame có tên là women là một data.frame có sẵn vào worksheet có tên Sheet 2 của wb1. Dữ liệu sẽ được ghi vào bắt đầu từ hàng thứ nhất và cột thứ nhất, với tên của các biến trong dữ liệu ở trên hàng thứ nhất:Hàm số saveWorkbook() được sử dụng để lưu một đối tượng workbook thành một excel workbook có định dạng phù hợp:Bạn đọc có thể sử dụng Microsoft Excel để mở Workbook có tên ‘mau bd1.xlsx’ và xem thông tin về dữ liệu women ở trong worksheet có tên là “Sheet 2” của Workbook này. Trong hàm saveWorkbook(), tham số overwrite nhận giá trị TRUE có nghĩa là nếu trong đường dẫn tương ứng đã có file có tên trùng với tên Workbook mới thì sẽ lưu Workbook mới thay thế cho Workbook cũ có trùng tên.","code":"\nwb1<-loadWorkbook(\"../KHDL_KTKD Final/Dataset/mau bd.xlsx\")\nstr(wb1)\nclass(wb1)## [1] \"Workbook\"\n## attr(,\"package\")\n## [1] \"openxlsx\"\nnames(wb1)## [1] \"Sheet\"  \"Sheet1\"\naddWorksheet(wb1, \"Sheet 2\")\nwriteData(wb1, \"Sheet 2\", women,\n          startCol = 1, startRow = 1,\n          colNames = TRUE, rowNames = FALSE)\nsaveWorkbook(wb1, \"mau bd1.xlsx\", overwrite = TRUE)"},{"path":"nhập-dữ-liệu-vào-r.html","id":"kết-nối-r-với-cơ-sở-dữ-liệu","chapter":"Chương 6 Nhập dữ liệu vào R","heading":"6.6 Kết nối R với cơ sở dữ liệu","text":"Có thể sử dụng R như một công cụ để kết nối vào các cơ sở dữ liệu (database) và viết các câu lệnh truy vấn từ R vào các cơ sở dữ liệu. Để thực hiện được việc này, trước tiên, bạn đọc cần phải cài đặt một Open Database Connectivity (ODBC), được gọi là kết nối cơ sở dữ liệu mở. Kết nối này giúp cho hệ điều hành máy tính mà chúng ta cài đặt phần mềm R tương thích với hệ quản lý cơ sở dữ liệu mà chúng ta cần sử dụng. Chúng tôi sử dụng hệ điều hành Windows và hệ quản trị cơ sở dữ liệu MySQL nên chúng tôi sẽ lựa chọn ODBC cho Windows và MySQL. Bạn đọc tham khảo các ODBC cho MySQL với các hệ điều hành khác (Linux, MacOS) tại địa chỉhttps://dev.mysql.com/downloads/connector/odbc/Tại thời điểm nhóm tác giả viết cuốn sách này, ODBC cho hệ điều hành Windows đang là phiên bản 8.0. Sau khi cài đặt ODBC lên hệ điều hành, bạn đọc đã có thể sử dụng R để truy cập vào một cơ sở dữ liệu và thực hiện các câu lệnh truy vấn dữ liệu trên cơ sở dữ liệu đó trên R với sự trợ giúp của thư viện DBI. Sau khi cài đặt thư viện DBI, bạn đọc cần tạo một kết nối giữa R và cơ sở dữ liệu bằng hàm dbConnect()Trong câu lệnh ở trên,Biến con là biến lưu kết nối mà chúng ta sẽ gọi ra mỗi khi truy cập vào cơ sở dữ liệu.Giá trị gán cho tham số Server của hàm dbConnect() là ten_server là địa chỉ của máy tính lưu trữ cơ sở dữ liệu. Nếu cơ sở dữ liệu nằm trên máy tính cá nhân, ten_server thường được gán bằng ‘localhost’. Nếu cơ sở dữ liệu được lưu trên một máy tính server, ten_server được gán bằng địa chỉ của máy tính server đó.Tham số Database cần được gán cho giá trị là tên của cơ sở dữ liệu.Các tham số UID và PWD lần lượt là tên người sử dụng và mật mã được cấp để truy cập vào cơ sở dữ liệu.Sau khi đã tạo được kết nối, bạn đọc có thể thực hiện các câu lệnh truy vấn dữ liệu từ R với hàm DBI::dbGetQuery(). Quy tắc viết câu lệnh truy vấn từ R như sautrong đó Cau_lenh_truy_van là câu lệnh truy vấn được lưu dưới dạng biến chuỗi ký tự. Ví dụ, bạn đọc muốn lấy ra thông tin của tất cả những khách hàng có ngày sinh là ngày 01 tháng 01 năm 2000 từ một bảng có tên Life_Insured từ một cơ sở dữ liệu có tên tktdb, bạn đọc viết câu lệnh như sauCác hệ quản trị cơ sở dữ liệu cho phép tìm kiếm, truy vấn, và sắp xếp dữ liệu hiệu quả và tiết kiệm thời gian hơn với các phần mềm phân tích dữ liệu như R hay Python. đó, nếu cần thiết có các phép tìm kiếm, lọc, sắp xếp dữ liệu trước khi phân tích bạn đọc nên thực hiện các phép biến đổi trên các câu lệnh SQL trước khi nhập liệu vào R.","code":"\nlibrary(DBI)\ncon <- dbConnect(odbc::odbc(), .connection_string = \"Driver={MySQL ODBC 8.0 Unicode Driver};\",\n    Server = \"ten_serve\", Database = \"db_name\", UID = \"ID\", PWD = \"password\")\nDBI::dbGetQuery(\"Cau_lenh_truy_van\")\nsql <- \"select * from tktdb.Life_Insured\n      where DOB = '2000-01-01'\" # Viết chính xác câu lệnh truy vấn trên MySQL\ndf <- DBI::dbGetQuery(sql) # df sẽ lưu kết quả của câu lệnh truy vấn"},{"path":"tiền-xử-lý-dữ-liệu.html","id":"tiền-xử-lý-dữ-liệu","chapter":"Chương 7 Tiền xử lý dữ liệu","heading":"Chương 7 Tiền xử lý dữ liệu","text":"","code":""},{"path":"tiền-xử-lý-dữ-liệu.html","id":"tiền-xử-lý-dữ-liệu-là-gì","chapter":"Chương 7 Tiền xử lý dữ liệu","heading":"7.1 Tiền xử lý dữ liệu là gì?","text":"Tiền xử lý dữ liệu là một công việc đòi hỏi sự tỉ mỉ cẩn thận và là một trong những bước quan trọng nhất trong một quy trình làm việc trên dữ liệu. Tiền xử lý dữ liệu là tập hợp tất cả các bước kỹ thuật nhằm đảm bảo cho dữ liệu bạn sử dụng phân tích hoặc xây dựng mô hình được đảm bảo về định dạng, giá trị, và ý nghĩa. Hiểu một cách đơn giản, tiền xử lý dữ liệu là quá trình biến dữ liệu thô thành dữ liệu có thể sử dụng được để phân tích và đưa ra kết quả.Khi làm việc với dữ liệu, thực tế là đến hơn 50% các trường hợp bạn đọc sẽ nhận được những dữ liệu ở dạng thô chưa qua tiền xử lý. Thông thường thì đối với những dữ liệu được nhập và xuất ra qua một hệ thống được phát triển đầy đủ, công việc tiền xử lý chỉ cần một vài bước cơ bản để đi đến kết quả. Tuy nhiên, trong trường hợp dữ liệu bạn nhận được là dữ liệu được nhập một cách thủ công, qua tay nhiều người nhập, hoặc là dữ liệu thu thập tự động từ các website, thì đây thực sự sẽ là một vấn đề lớn. Tiền xử lý dữ liệu trong tình huống như vậy có thể chiếm từ 80% đến 90% thời gian công việc của bạn!Các vấn đề thường gặp phải khi làm việc với một dữ liệu thô thường xuất phát từ hai nguyên nhân:Thứ nhất là dữ liệu sai định dạng, nghĩa là trong cùng một cột dữ liệu có các biến kiểu khác nhau hoặc kiểu của biến không đúng như quy ước.Thứ nhất là dữ liệu sai định dạng, nghĩa là trong cùng một cột dữ liệu có các biến kiểu khác nhau hoặc kiểu của biến không đúng như quy ước.Thứ hai là dữ liệu chứa giá trị không quan sát được hoặc chứa các giá trị ngoại lai. Các giá trị ngoại lai thường được gọi là các outliers.Thứ hai là dữ liệu chứa giá trị không quan sát được hoặc chứa các giá trị ngoại lai. Các giá trị ngoại lai thường được gọi là các outliers.Hãy quan sát một ví dụ như sau: bạn nhận được dữ liệu về 3 ứng cử viên từ bộ phận nhân sự và bạn muốn xem xét độ tuổi trung bình của những ứng cử viên và tỷ lệ Nam/Nữ trong danh sách ứng tuyểnĐây là một dữ liệu không thể sử dụng để phân tích bởi vì giá trị trong các cột ngày sinh là không đúng định dạng ngày tháng và có các giá trị không quan sát được ở cột giới tính. Nếu sử dụng dữ liệu này để phân tích mà bỏ qua việc tiền xử lý dữ liệu thì kết quả sẽ sai lệch hoàn toàn với bản chất của dữ liệu:Giả sử bộ phận nhân sự muốn biết độ tuổi trung bình của các ứng cử viên. Không thể trả lời câu hỏi này nếu bỏ qua việc tiền xử lý vì cột ngày sinh đang có dạng chuỗi ký tự và định dạng ngày tháng là không thống nhất.Bộ phận nhân sự muốn biết tỷ lệ Nam/Nữ tham gia ứng tuyển. Cũng không thể trả lời câu hỏi này vì có nhiều giá trị không quan sát được trong cột giới tính. Nếu bỏ qua những giá trị không có quan sát, tỷ lệ giới tính Nam là 100%. Nhưng con số này rõ ràng không chính xác!Tiền xử lý dữ liệu không chỉ bao gồm các công cụ kỹ thuật mà còn yêu cầu cả kiến thức phổ thông và kiến thức chuyên môn nghiệp vụ của người xử lý dữ liệu. Khi có vấn đề gây khó hiểu về dữ liệu nhận được, điều trước hết cần làm đó là liên hệ với người chủ dữ liệu để kiểm tra lại thông tin. Khi việc này là không thể thực hiện được, người xử lý dữ liệu sẽ phải đưa ra các phán đoán về dữ liệu đó dựa trên hiểu biết của mình.Giả sử không thể có thêm thông tin nào từ nơi cung cấp dữ liệu, chúng ta cần phải đưa ra phán đoán với dữ liệu kể trên. Trước hết, với cột ngày sinh của các nhân viên:Giá trị 01/02/98 có khả năng cao là ngày 01 tháng 02 năm 1998 quy ước phổ biến ở Việt Nam là viết theo thứ tự ngày -> tháng -> năm.Giá trị 12/17/1999 có khả năng cao là ngày 17 tháng 12 năm 1999. Khi gặp các trường hợp này nhiều khả năng người nhập dữ liệu sử dụng format ngày tháng của Microsoft Excel.Giá trị 1-1-1992 có khả năng cao là ngày 01 tháng 01 năm 1992.Như vậy với mỗi giá trị trong cột ngày sinh, bạn đọc cần một phép biến đổi khác nhau để đưa dữ liệu về đúng với định dạng. Chúng ta sẽ sử dụng hàm .Date() với tham số format để chỉnh định dạng của các biến ngày tháng và lưu vào một véc-tơ có tên là DOB như sauVéc-tơ DOB được tính toán trong các câu lệnh ở trên chứa giá trị ngày sinh kiểu dạng ngày tháng đúng định dạng của các ứng cử viên và bạn đọc có thể sử dụng làm đầu vào để tính tuổi của các ứng cử viên.Đối với cột giới tính của nhân viên:Giới tính của ứng cử viên có tên Trần Văn Cường là không quan sát được tuy nhiên theo tên của ứng cử viên thì nhiều khả năng đây là Nam.Giới tính của ứng cử viên Lê Thị Loan là không quan sát được tuy nhiên theo tên của ứng cử viên thì nhiều khả năng đây là Nữ.Sau những bước xử lý như trên, chúng ta đã có một dữ liệu được định dạng chính xác như ở dưới. Đây là một dữ liệu đã được làm sạch và sẵn sàng để trả lời cho các câu hỏi như độ tuổi trung bình hay tỷ lệ Nam/Nữ của các ứng cử viên.Các bước xử lý dữ liệu như trên mặc dù đơn giản nhưng lại là điển hình của tiền xử lý dữ liệu. Dữ liệu chúng ta nhận được sẽ ít khi được định dạng chuẩn và sẵn sàng cho mục đích phân tích. Để xử lý những giá trị sai định dạng, hoặc thay thế cho các giá trị không quan sát, hoặc loại bỏ đi các giá trị ngoại lai, người làm dữ liệu phải sử dụng kiến thức phổ thông, kiến thức nghiệp vụ để làm sạch và đưa ra những dự đoán tốt nhất có thể.","code":"\nDOB<-rep(as.Date(\"1900-01-01\"),3)\nDOB[1]<-as.Date(\"01/02/98\", format = \"%d/%m/%y\")\nDOB[2]<-as.Date(\"12/17/1999\", format = \"%m/%d/%Y\")\nDOB[3]<-as.Date(\"1-1-1992\", format = \"%d-%m-%Y\")"},{"path":"tiền-xử-lý-dữ-liệu.html","id":"định-dạng-cột-dữ-liệu-sử-dụng-thư-viện-readr","chapter":"Chương 7 Tiền xử lý dữ liệu","heading":"7.2 Định dạng cột dữ liệu sử dụng thư viện readr","text":"","code":""},{"path":"tiền-xử-lý-dữ-liệu.html","id":"quy-tắc-định-dạng-tự-động-của-readr","chapter":"Chương 7 Tiền xử lý dữ liệu","heading":"7.2.1 Quy tắc định dạng tự động của readr","text":"Trong phần trước, chúng tôi đã giới thiệu đến bạn đọc các hàm số trong thư viện readr dùng để đọc dữ liệu từ nguồn ngoài. Bên cạnh hỗ trợ đọc dữ liệu, readr còn có các hàm số để hỗ trợ tiền xử lý dữ liệu. Trước hết, mỗi khi đọc dữ liệu từ các nguồn ngoài vào R, thư viện readr có các quy tắc chung về định dạng kiểu dữ liệu cho các biến. Các quy tắc chung đó được tổng hợp trong Bảng ??. Để đảm bảo thời gian đọc dữ liệu không bị kéo dài, mỗi khi đọc dữ liệu readr sử dụng 1000 hàng dữ đầu tiên của dữ liệu để dự đoán kiểu dữ liệu của từng biến bằng:Lưu ý rằng các giá trị không quan sát được không ảnh hưởng đến việc readr dự đoán kiểu dữ liệu của một cột, nghĩa là các hàm đọc dữ liệu sẽ bỏ qua các giá trị NA trong 1000 hàng đầu tiên để đưa ra phán đoán về kiểu dữ liệu của biến. Từ Bảng ?? có thể thấy rằng trong nhiều trường hợp, nhất là khi dữ liệu không được định dạng trước khi đưa vào R, readr sẽ lưu dữ liệu dưới dạng chuỗi ký tự.Khi đọc dữ liệu kiểu số, sự khác nhau trong cách sử dụng dấu thập phân là ‘.’ và ‘,’ có thể làm cho giá trị của biến kiểu số thay đổi về giá trị thật. Nếu bạn đọc không sử dụng thêm tham số trong hàm đọc dữ liệu, read_csv() luôn mặc định dấu thập phân là ‘.’ và trong khi hàm read_csv2() mặc định dấu thập phân là ‘,’. Hãy quan sát ví dụ dưới đây khi sử dụng hàm read_csv2() để đọc một dữ liệu kiểu số được định dạng không thống nhất:Lưu ý rằng hàm read_csv2() là hàm dùng để đọc dữ liệu mà các cột được phân tách bằng dấu ; và cho đầu ra là một tibble giống như các hàm đọc dữ liệu khác từ readr. Từ kết quả ở trên, có thể đưa ra nhận xét như sau:Trong cột C1 có các số đặc biệt như 1e-10 hay Inf và read_csv2() vẫn hiểu đây là các kiểu số thông thường.Trong cột C2 chứa các giá trị Trong cột 2.2 và Trong cột 3,2 và khi được đọc bằng hàm read_csv2() đã cho kết quả là véc-tơ kiểu số có hai phần tử lần lượt là 22 và 3.2. Điều này có nghĩa là read_csv2() bỏ qua dấu ‘.’ trong 2.2 và cho kết quả đầu ra là 22, trong khi giá trị 3,2 có dấu ‘,’ được hiểu là số thập phân.Tương tự, trong cột thứ ba (C3) giá trị 1.0 được hiểu là giá trị 10, trong khi 1.000 được hiểu là giá trị 1 vì ‘,’ được hiểu là dấu của số thập phân.Hàm read_csv2() không phân tích được kiểu dữ liệu trong các cột C4 và C5 nên kiểu dữ liệu của hai cột này trong tibble kết quả là kiểu chuỗi ký tự.Chúng ta tiếp tục xem xét quy tắc tự động định dạng véc-tơ kiểu logical qua ví dụ dưới đây:Như chúng tôi đã đề cập ở trên, hàm read_csv() được sử dụng để đọc dữ liệu từ nguồn ngoài có các cột ngăn cách nhau bằng dấu ‘,’. Có thể thấy rằng các giá trị tương ứng với biến logic được liệt kê trong Bảng ?? đều được hàm read_csv() hiểu đúng là kiểu logical.Chúng ta sẽ thảo luận về biến kiểu ngày tháng khi được định dạng tự động bằng readr. Như chúng tôi đã trình bày trong Bảng ??, readr chỉ có thể tự động định dạng đúng véc-tơ kiểu ngày tháng nếu giá trị từ nguồn bên ngoài vào được viết theo định dạng “yyyy-mm-dd” hoặc “yyyy/mm/dd”. Bạn đọc hãy quan sát ví dụ dưới đây:Từ kết quả có thể phân tích cách thức thư viện readr tự động định dạng dữ liệu kiểu ngày tháng như sau:Giá trị trong các cột được viết theo một trong hai kiểu định dạng là “yyyy-mm-dd” hoặc “yyyy/mm/dd” đều được hiểu là kiểu ngày tháng. Có thể thấy rằng các cột C1 và C2 đều được định nghĩa đúng kiểu ngày tháng.Cột C3 mặc dù giá trị hàng thứ hai bị viết ngược giá trị ngày với giá trị tháng nhưng readr vẫn ghi nhận đúng cột này là kiểu ngày tháng và ghi nhận đúng giá trị.Cột C4 và cột C5 không được hiểu là kiểu ngày tháng dữ liệu không được viết theo một trong hai định dạng trong Bảng ??Để tìm hiểu chi tiết hơn cách thư viện readr tự động định dạng kiểu giá trị của dữ liệu đọc từ nguồn bên ngoài, bạn đọc tham khảo hướng dẫn của hàm guess_parse(). Đây là hàm được mặc định sử dụng trong các hàm đọc dữ liệu với mục đích dự đoán kiểu giá trị của các biến.Khi thư viện readr không thể phân tích được định dạng của các biến, kiểu biến mặc định sẽ là kiểu chuỗi ký tự. Trong các phần tiếp theo, chúng ta sẽ thảo luận về các hàm được sử dụng để chuyển đổi các véc-tơ kiểu chuỗi ký tự như vậy thành kiểu dữ liệu đúng của véc-tơ đó.","code":"\nfile<-\"C1;C2;C3;C4;C5\n       1e-10;2.2;1.0;TRUE; 1.0.0.0\n       Inf;3,2;1,000.0;1;10%\"\n# Dữ liệu có 5 cột và 2 hàng\nread_csv2(file)## # A tibble: 2 × 5\n##        C1    C2    C3 C4    C5     \n##     <dbl> <dbl> <dbl> <chr> <chr>  \n## 1   1e-10  22      10 TRUE  1.0.0.0\n## 2 Inf       3.2     1 1     10%\nfile<-\"C1,C2,C3,C4,C5,C6\n        TRUE,t,True,false,F,true\n        F,F,FALSE,T,f,True\"\nread_csv(file)## # A tibble: 2 × 6\n##   C1    C2    C3    C4    C5    C6   \n##   <lgl> <lgl> <lgl> <lgl> <lgl> <lgl>\n## 1 TRUE  TRUE  TRUE  FALSE FALSE TRUE \n## 2 FALSE FALSE FALSE TRUE  FALSE TRUE\nfile<-\"C1,C2,C3,C4,C5\n        2020/01/12,2020-01-12,2020/01/12,2020/1/1,2020|1|1\n        2021-12-31,2021/12/31,2021/31/12,2021-12-31,2021-12-31\"\nread_csv(file)## # A tibble: 2 × 5\n##   C1         C2         C3         C4         C5        \n##   <date>     <date>     <date>     <chr>      <chr>     \n## 1 2020-01-12 2020-01-12 2020-01-12 2020/1/1   2020|1|1  \n## 2 2021-12-31 2021-12-31 NA         2021-12-31 2021-12-31"},{"path":"tiền-xử-lý-dữ-liệu.html","id":"định-dạng-véc-tơ-bằng-các-hàm-parse_","chapter":"Chương 7 Tiền xử lý dữ liệu","heading":"7.2.2 Định dạng véc-tơ bằng các hàm parse_()","text":"Với các cột dữ liệu mà không thể xác định được kiểu biến, thư viện readr sẽ lưu dưới dạng véc-tơ kiểu chuỗi ký tự. Để đưa biến về đúng với giá trị thật, bạn đọc cần định dạng lại các cột cho đúng với mong muốn. Các hàm parse_() trong thư viện readr hỗ trợ bạn đọc thực hiện các yêu cầu như vậy. Nhóm hàm parse_() có đầu vào là một véc-tơ kiểu chuỗi ký tự và đầu ra sẽ là kiểu dữ liệu mà bạn đọc mong muốn, bao gồm dữ liệu kiểu số, kiểu logic, kiểu thời gian, và kiểu chuỗi ký tự. Với mỗi định dạng khác nhau, nhóm hàm parse_() sẽ có hàm số tương ứng và có các tham số phù hợp để đáp ứng được yêu cầu về định dạng.","code":""},{"path":"tiền-xử-lý-dữ-liệu.html","id":"định-dạng-véc-tơ-kiểu-logic","chapter":"Chương 7 Tiền xử lý dữ liệu","heading":"7.2.2.1 Định dạng véc-tơ kiểu logic","text":"Định dạng lại một véc-tơ kiểu chuỗi ký tự thành kiểu logical là đơn giản nhất bởi các giá trị có thể nhận của biến logic chỉ bao gồm TRUE hoặc FALSE. Hàm số sử dụng trong trường hợp này là parse_logical(). Bạn đọc hãy quan sát ví dụ sau:Bạn đọc có thể thấy rằng tất cả các giá trị nằm trong véc-tơ x, ngoại trừ các giá trị ‘.’, ‘@’, ‘2’, đều được chuyển sang đúng với định dạng của biến logic. Có thể thấy rằng parse_logical() tự động đổi giá trị \"1\" thành TRUE và giá trị \"0\" thành FALSE. Tham số na trong hàm parse_logical() được sử dụng để khai báo các giá trị mà người phân tích dữ liệu cho rằng tương đương với giá trị không quan sát được khi chuyển đổi định dạng sang kiểu logic. Trong câu lệnh ở trên, sử dụng hay không sử dụng tham số na không làm thay đổi véc-tơ kết quả đầu ra. Tuy nhiên, trong một số trường hợp, giá trị NA lại được lưu bằng một chuỗi ký tự có ý nghĩa trong chuyển đổi định đạng, chẳng hạn như khi người lưu trữ dữ liệu ngầm định ký tự ‘0’ tương đương với không quan sát được. Ngoài ra, việc sử dụng tham số na sẽ ảnh hưởng đến kết quả của hàm problem() được sử dụng để liệt kê các giá trị mà không thể chuyển đổi sang dạng logic.Khi véc-tơ x có kích thước lớn, các phần tử không thể đổi sang kiểu logic sẽ được lưu vào một tibble để người phân tích dữ liệu có thể dễ dàng truy cập. Bạn đọc hãy quan sát ví dụ sau:. Bạn đọc sử dụng hàm problems() để xem danh sách các các giá trị này:Trong kết quả của hàm problem(), cột row cho biết vị trí của các phần tử trong véc-tơ x1 không thể đổi sang biến kiểu logic. Giá trị thực của các phần tử này nằm trong cột actual. Bạn đọc có thể quan sát các giá trị trong cột actual để tìm hiểu nguyên nhân tại sao hàm parse_logical() không thể hoạt động trên các giá trị này.","code":"\nx<-c(\"TRUE\",\"True\",\"1\",\"0\",\"2\",\".\",\"@\",\n     \"FALSE\",\"false\",\"f\",\"F\",\"T\",\"t\",\"true\",\"false\")\nparse_logical(x, na = c(\".\", \"@\"))##  [1]  TRUE  TRUE  TRUE FALSE    NA    NA    NA FALSE FALSE FALSE FALSE  TRUE\n## [13]  TRUE  TRUE FALSE\n## attr(,\"problems\")\n## # A tibble: 1 × 4\n##     row   col expected           actual\n##   <int> <int> <chr>              <chr> \n## 1     5    NA 1/0/T/F/TRUE/FALSE 2\nx1<-sample(x, 10^3, replace = TRUE)\ny<-parse_logical(x1)\nproblems(y)## # A tibble: 198 × 4\n##      row   col expected           actual\n##    <int> <int> <chr>              <chr> \n##  1     1    NA 1/0/T/F/TRUE/FALSE 2     \n##  2     8    NA 1/0/T/F/TRUE/FALSE 2     \n##  3    19    NA 1/0/T/F/TRUE/FALSE 2     \n##  4    22    NA 1/0/T/F/TRUE/FALSE @     \n##  5    34    NA 1/0/T/F/TRUE/FALSE .     \n##  6    36    NA 1/0/T/F/TRUE/FALSE 2     \n##  7    37    NA 1/0/T/F/TRUE/FALSE @     \n##  8    38    NA 1/0/T/F/TRUE/FALSE 2     \n##  9    40    NA 1/0/T/F/TRUE/FALSE @     \n## 10    43    NA 1/0/T/F/TRUE/FALSE 2     \n## # ℹ 188 more rows"},{"path":"tiền-xử-lý-dữ-liệu.html","id":"định-dạng-véc-tơ-kiểu-số","chapter":"Chương 7 Tiền xử lý dữ liệu","heading":"7.2.2.2 Định dạng véc-tơ kiểu số","text":"Các nguyên nhân thường dẫn đến việc các hàm đọc dữ liệu trong thư viện readr không thể tự động định dạng một véc-tơ có kiểu số là:Cách đánh số thập phân của các số trong véc-tơ. Chẳng hạn như tại Việt Nam số thập phân được sử dụng là dấu phẩy (,) trong khi R hiểu số thập phân là dấu chấm (.). Một số quốc gia khác trên thế giới như Pháp cũng sử dụng dấu thập phân là dấu phẩy.Cách viết các số sử dụng cùng với các ký tự chấm hoặc phẩy để người đọc dễ dàng đọc số đó. Chẳng hạn như tại Việt Nam, chúng ta viết số 1 tỷ với dấu chấm phân tách các số không như sau: 1.000.000.000. Tại Thụy Sỹ cách phân tách số lại được viết theo cách khác; số 1 tỷ được viết thành 1’000’000’000. Khi gặp các trường hợp này, chúng ta cần cung cấp cho R định dạng đúng của các số đó.Khi các con số đi kèm theo đơn vị, chẳng hạn như đi kèm với ký hiệu tiền tệ: “100.000 đồng”, “100.000 vnd”, hoặc đi kèm với ký hiệu % như 50%, các hàm đọc dữ liệu của thư viện readr cũng sẽ không thể tự động chuyển đổi các giá trị này sang kiểu số nếu không có gợi ý thích hợp.Bạn đọc có thể sử dụng parse_double() hoặc parse_number() khi gặp phải các vấn đề ở trên. Chẳng hạn như khi gặp vấn đề về dấu phẩy (,) đối với dấu thập phân, nghĩa là dữ liệu từ nguồn bên ngoài viết số thập phân sử dụng dấu phẩy (,), bạn đọc sử dụng parse_number() với tham số locale = locale(decimal_mark = ‘,’) để hàm đọc dữ liệu hiểu rằng phần thập phân được ngăn cách với phần nguyên bằng dấu phẩy (,) và có thể đổi định dạng véc-tơ kiểu chuỗi ký tự sang véc-tơ kiểu số thành công:Khi gặp phải vấn đề về việc sử dụng các ký tự không đúng định dạng để phân tách các giá trị đơn vị hàng nghìn, hàng triệu, hàng tỷ; chúng ta sử tham số grouping_mark trong hàm locate(). Ví dụ dưới đây sử dụng đồng thời hai tham số decimal_mark và grouping_mark của hàm locate() để biến đổi cách viết số thập phân theo kiểu viết của Việt Nam sang kiểu số và giữ đúng giá trị :Khi gặp phải chuỗi ký tự chứa biến kiểu số đi kèm với đơn vị tiền tệ, hoặc đơn vị %, hàm parse_number() vẫn cho phép chuyển đổi chuỗi ký tự sang kiểu số mà không cần sử dụng thêm tham số nào cả. Bạn đọc hãy quan sát ví dụ dưới đây:Bạn đọc cần thận trọng khi véc-tơ kiểu số có % ở phía sau. Hàm parse_number() loại bỏ ký tự % theo sau và giữa nguyên giá trị số đó. Áp dụng parse_number() trên giá trị 2.000,5 % cho kết quả là số 2000.5 chứ không phải là 20.005. Để có giá trị đúng kiểu số, chúng ta cần chia kết quả cho 100.","code":"\nx<-c(\"0,5\",\"1,5\") # Dấu thập phân là dấu phẩy\nparse_number(x, locale = locale(decimal_mark = \",\"))## [1] 0.5 1.5\nx<-c(\"1.000,5\",\"1.000.000,5\")\n# véc-tơ chứa các số\n# 1000,5: một nghìn phẩy năm\n# 1000000,5: một triệu phẩy năm\n# dấu thập phân là dấu \",\"\n# phân tách hàng nghìn, hàng triệu là dấu .\nparse_number(x, locale = locale(decimal_mark = \",\",\n                                grouping_mark = \".\"))## [1]    1000.5 1000000.5\nx<-c(\"1.000,5 đồng\",\"1.000.000,5 vnd\", \"2.000,5 %\")\n# số kiểu Việt Nam\n# có đơn vị tiền phía sau\n# có ký hiệu % phía sau\nparse_number(x, locale = locale(decimal_mark = \",\",\n                                grouping_mark = \".\"))## [1]    1000.5 1000000.5    2000.5"},{"path":"tiền-xử-lý-dữ-liệu.html","id":"định-dạng-véc-tơ-kiểu-thời-gian","chapter":"Chương 7 Tiền xử lý dữ liệu","heading":"7.2.2.3 Định dạng véc-tơ kiểu thời gian","text":"Hàm số parse_datetime() có thể sử dụng để chuyển đổi các véc-tơ kiểu chuỗi ký tự sang véc-tơ kiểu thời gian và véc-tơ kiểu ngày tháng.Hai tham số của hàm parse_datetime() mà bạn đọc cần lưu ý là tham số na và tham số format.Tham số na là một véc-tơ chứa các giá trị mà bạn đọc mặc định là các giá trị không quan sát được. Trong véc-tơ x ở trên, giá trị 01/01/1900 được gán cho tham số na mặc dù giá trị này là ngày tháng có ý nghĩa. Điều này khiến cho giá trị thứ ba trong véc-tơ kết quả có giá trị là NA. Nếu không sử dụng tham số na, giá trị thứ ba của véc-tơ kết quả sẽ là ngày 01 tháng 01 năm 1990. Đối với một vài hệ thống lưu dữ liệu, có thể xảy ra trường hợp các giá trị không được ghi nhận nhưng vẫn được gán một giá trị mặc định nào đó. Các giá trị mặc định này nếu giữ nguyên giá trị sẽ làm sai lệch phân tích. Giả sử với véc-tơ ở trên, nếu chúng ta biết giá trị mặc định gán cho các giá trị không quan sát được của hệ thống là 01/01/1900, việc gán giá trị này cho tham số na là cần thiết.Tham số na là một véc-tơ chứa các giá trị mà bạn đọc mặc định là các giá trị không quan sát được. Trong véc-tơ x ở trên, giá trị 01/01/1900 được gán cho tham số na mặc dù giá trị này là ngày tháng có ý nghĩa. Điều này khiến cho giá trị thứ ba trong véc-tơ kết quả có giá trị là NA. Nếu không sử dụng tham số na, giá trị thứ ba của véc-tơ kết quả sẽ là ngày 01 tháng 01 năm 1990. Đối với một vài hệ thống lưu dữ liệu, có thể xảy ra trường hợp các giá trị không được ghi nhận nhưng vẫn được gán một giá trị mặc định nào đó. Các giá trị mặc định này nếu giữ nguyên giá trị sẽ làm sai lệch phân tích. Giả sử với véc-tơ ở trên, nếu chúng ta biết giá trị mặc định gán cho các giá trị không quan sát được của hệ thống là 01/01/1900, việc gán giá trị này cho tham số na là cần thiết.Tham số format sử dụng trong hàm parse_datetime() là gợi ý cho R về định dạng của biến kiểu ngày tháng. Khi gán giá trị cho tham số format, bạn đọc cần lưu ý:\nMỗi thành phần của biến thời gian (ngày, tháng, năm, giờ, phút, giây,…) được định nghĩa bắt đầu bằng % và theo sau 1 chữ cái, chẳng hạn như bạn đọc sử dụng %Y khi muốn gợi ý rằng biến kiểu thời gian nằm trong chuỗi ký tự được sử dụng 4 chữ số để chỉ định giá trị năm. –>\nCác ký tự không liên quan đến các thành phần của thời gian, ngoại trừ các khoảng trắng phía trước và sau biến thời gian, cần phải được khai báo chính xác. Hãy quan sát ví dụ dưới đây\nTham số format sử dụng trong hàm parse_datetime() là gợi ý cho R về định dạng của biến kiểu ngày tháng. Khi gán giá trị cho tham số format, bạn đọc cần lưu ý:Mỗi thành phần của biến thời gian (ngày, tháng, năm, giờ, phút, giây,…) được định nghĩa bắt đầu bằng % và theo sau 1 chữ cái, chẳng hạn như bạn đọc sử dụng %Y khi muốn gợi ý rằng biến kiểu thời gian nằm trong chuỗi ký tự được sử dụng 4 chữ số để chỉ định giá trị năm. –>Các ký tự không liên quan đến các thành phần của thời gian, ngoại trừ các khoảng trắng phía trước và sau biến thời gian, cần phải được khai báo chính xác. Hãy quan sát ví dụ dưới đâyTừ ví dụ trên bạn đọc có thể thấy rằngCần khai báo chính xác các ký tự nằm giữa các biến thời gian. Ký tự @ nằm giữa các giá trị ngày, tháng, năm; phân tách giữa ngày tháng với giờ, phút, giây là ký tự %; phân tách giữa các thành phần của thời gian trong ngày là ký tự #. Tất cả đều cần phải được khai báo chính xác trong tham số format. Điều này giải thích tại sao hai giá trị đầu trong véc-tơ x được chuyển đổi sang dạng biến thời gian. Giá trị thứ ba trong véc-tơ x gặp vấn đề vì phân tách giữa các thành phần của thời gian trong ngày sử dụng dấu :.Các khoảng trắng nằm trước và sau các chuỗi ký tự được bỏ qua và không ảnh hưởng đến kết quả. Các giá trị thứ nhất và thứ hai trong véc-tơ x có khoảng trắng phía trước và phía sau nhưng hàm parse_datetime() bỏ qua các khoảng trắng đo khi chuyển đổi ký tự sang ngày tháng.Để biết một cách chính xác cách gán giá trị cho tham số format, bạn đọc nên tham khảo hướng dẫn sử dụng hàm parse_datetime(). Chúng tôi tóm tắt cách định dạng các thành phần của một biến thời gian trong bảng ??Lưu ý rằng khi bạn đọc sử dụng %y để định nghĩa cho giá trị năm, các ký tự từ 00 đến 69 sẽ được chuyển thành năm 2000 đến năm 2069. Trong khi đó, các ký tự từ 70 đến 99 sẽ được chuyển thành năm 1970 đến 1999. Ngoài ra, thành phần tháng của biến thời gian trong nhiều dữ liệu thường được viết dưới dạng chuỗi ký tự thay vì sử dụng số. đó bạn đọc cần các lựa chọn %b hoặc %B để gợi ý cho R. Hãy quan sát ví dụ sau:Khi ngày tháng được viết bằng chuỗi ký tự viết tắt, bằng chữ hoa hoặc chữ thường, như sep hay JAN, gợi ý cần sử dụng là %b. Điều này giải thích tại sao kết quả của hàm parse_datetime() sử dụng %b cho kết quả đúng định dạng ngày tháng đối với ba giá trị đầu của véc-tơ, và cho kết quả là NA với phân tử thứ tư tháng của phần tử này được viết đầy đủ là april. Ngược lại, hàm parse_datetime() thứ hai cho kết quả ba giá trị đầu của véc-tơ là NA và phần tử thứ tư là giá trị ngày tháng được định dạng đúng là chúng ta sử dụng tham số format với gợi ý cho cách viết tháng là %B.","code":"\nx<-c(\"1/2/2023\", \"23/10/2023 \", \"01/01/1900\")\nparse_datetime(x, format = \"%d/%m/%Y\",\n               na = c(\"01/01/1900\"))## [1] \"2023-02-01 UTC\" \"2023-10-23 UTC\" NA\nx<-c(\" 1@2@2023-23#25#01  \", \"  23@10@2023-01#06#59 \", \"01@01@2023-00:00:00\")\nparse_datetime(x, format = \"%d@%m@%Y-%H#%M#%S\")## [1] \"2023-02-01 23:25:01 UTC\" \"2023-10-23 01:06:59 UTC\"\n## [3] NA\n# Gợi ý cho R là ngày, tháng, năm cách nhau bởi @\n# và giờ phút, giây cách nhau bởi #\nx<-c(\"sep 21, 23 \", \"  JAN 1, 69 \", \"Dec 25, 70\", \"april 3, 99\")\nparse_datetime(x, format = \"%b %d, %y\")## [1] \"2023-09-21 UTC\" \"1969-01-01 UTC\" \"1970-12-25 UTC\" NA\nparse_datetime(x, format = \"%B %d, %y\")## [1] NA               NA               NA               \"1999-04-03 UTC\""},{"path":"tiền-xử-lý-dữ-liệu.html","id":"định-dạng-cột-kiểu-chuỗi-ký-tự","chapter":"Chương 7 Tiền xử lý dữ liệu","heading":"7.2.3 Định dạng cột kiểu chuỗi ký tự","text":"Khi bạn đọc dùng thư viện readr để đọc dữ liệu từ nguồn bên ngoài, các biến trong dữ liệu không phân tích được định dạng sẽ lưu dưới dạng các véc-tơ chuỗi ký tự. Vậy tại sao cần định dạng lại các véc-tơ đó thành véc-tơ kiểu chuỗi ký tự? Nghe có vẻ vô lý nhưng đây lại là vấn đề phức tạp nhất trong định dạng cột dữ liệu! Để hiểu vấn đề này bạn đọc cần tìm hiểu một chút về cách máy tính điện tử lưu và mở một chuỗi ký tự.Giả sử một người muốn gửi một dữ liệu chứa ký tự “” đến một người nhận dữ liệu khác. Sau khi viết ký tự “” lên một phần mềm soạn thảo văn bản, người gửi dữ liệu sẽ cần lưu ký tự đó lên máy tính của mình. Tất nhiên máy tính điện tử sẽ không thể ghi nhớ chữ “” một cách tượng hình mà sẽ mã hóa, hay thuật ngữ chuyên ngành gọi là encode, chữ “” thành một đoạn mã nhị phân bao gồm 0 và 1 mà máy tính điện tử có thể lưu được. Khi dữ liệu được gửi sang một máy tính khác, đoạn mã bao gồm các chữ số 0 và 1 đó sẽ được gửi đi. Khi máy tính điện tử khác mở dữ liệu, đoạn mã nhị phân sẽ được giải mã, hay thuật ngữ chuyên ngành gọi là decode, để hiển thị. Sẽ không có vấn đề gì xảy ra nếu quy tắc mã hóa và giải mã được thống nhất và chữ “” sẽ được hiển thị chính xác trên máy tính thứ hai.Thực tế là trước khi có bộ mã hóa và quy tắc mã hóa chung được công nhận rộng rãi như Unicode và UTF-8, rất khó để có sự thống nhất quy tắc mã hóa ký tự. May mắn là đến thời điểm chúng tôi viết cuốn sách này đa số các hệ điều hành, hệ soạn thảo văn bản, đều sử dụng bảng mã Unicode và bộ mã hóa UTF-8. Giải thích chi tiết về bộ mã hóa hay quy tắc mã hóa là rất phức tạp và vượt quá nội dung của cuốn sách này. Chúng tôi chỉ cần bạn đọc hiểu về Unicode và UTF-8 như sau:Unicode là một bảng mã chuẩn được công nhận rộng rãi cho biết quy tắc cho tương ứng hầu hết các ký tự từ đơn giản đến phức tạp, kể cả các ngôn ngữ sử dụng ký tự tượng hình phức tạp như chữ Hán của tiếng Trung Quốc, tiếng Nhật, chữ Nôm của tiếng Việt, với một số nằm giữa số 0 đến số 10FFFF khi viết theo hệ 16. Một số khi viết trong hệ 16 có thể sử dụng, bao gồm 0, 1, …, 9, , B, C, D, E, F, để biểu diễn, đó số các ký tự mà bảng mã Unicode có thể đưa vào là \\(16^4 + 16^5 = 1.114.112\\) ký tự, bao gồm \\(16^5\\) số từ 0 đến FFFFF và \\(16^4\\) số từ 100000 đến 10FFFF. Ví dụ, bạn đọc có thể dễ dàng tìm thấy được qua các công cụ tìm kiếm rằng ký tự “” có mã Unicode là “0041” và “” có mã Unicode là “0061”.UTF-8 là quy tắc lưu các số viết trong hệ 16 của bảng mã Unicode thành các chuỗi nhị phân 0 và 1 để máy tính có thể nhận biết được. Số 8 ở đây có nghĩa là 8-bits hay là một byte - là 8 giá trị 0 và 1 đứng liền nhau. Một ký tự bất kỳ trong bảng mã Unicode đều có thể được mã hóa thành 1, 2, 3 hoặc nhiều byte theo quy tắc mã hóa UTF-8. Chữ “” với mã Unicode “0041” sẽ được lưu trong máy tính điện tử dưới dạng một byte là “01000001”, hay “” có mã Unicode “0061” được máy tính điện tử lưu bằng 1 byte có giá trị “01100001”.Quay trở lại vấn đề định dạng lại dữ liệu kiểu chuỗi ký tự, sẽ không có vấn đề xảy ra nếu người nhập liệu sử dụng bộ mã hóa UTF-8 bởi readr luôn sử dụng UTF-8 để giải mã. Trong thực tế thì vẫn còn một số hệ thống, hoặc hệ soạn thảo văn bản sử dụng cách mã hóa khác với UTF-8. Điều này làm cho dữ liệu khi được nhập vào R sẽ hiển thị không đúng như mong muốn. Ví dụ, khi đọc một dữ liệu từ nguồn ngoài vào bằng read_csv() và cho kết quả như sauCột của dữ liệu đã không được lưu bằng bộ mã hóa UTF-8 nên thư viện readr không hiển thị được các chuỗi ký tự có ý nghĩa. Để định dạng lại cột dữ liệu, bạn đọc sử dụng hàm parse_character() với tham số encoding. Không dễ để biết được hay dự đoán dữ liệu đã được mã hóa bằng bộ mã hóa nào. Thư viện readr cung cấp hàm guess_encoding() hỗ trợ bạn đọc dự đoán một biến kiểu chuỗi ký tự đã được mã hóa bẳng bộ mã hóa nào. Tuy nhiên trải nghiệm của chúng tôi với hàm số này là không tốt! Lời khuyên của chúng tôi là bạn đọc khi có thể hãy tìm hiểu nguồn gốc của dữ liệu: dữ liệu được sinh ra từ đâu, hoặc từ hệ thống nào,…, để đưa ra phán đoán. Nếu không thể tìm kiếm nguồn gốc của dữ liệu, giải pháp duy nhất là thử giải mã đoạn văn bản bằng một số bộ mã hóa thường gặp cho đến khi gặp được kết quả mong muốn! Trong trường hợp dữ liệu ở trên nguồn là tiếng Việt nên chúng ta có thể thử các bộ mã hóa như Latin1 hay Latin2. Cách sử dụng hàm parse_character() để giải mã các chuỗi ký tự như sau:Kết quả khi sử dụng bộ mã Latin2 đã cho một vài giá trị có ý nghĩa, chúng ta tiếp tục thử với Latin1:May mắn là cột dữ liệu đều đã có thể đọc được với người Việt. Chúng ta có thể suy đoán đây là một dữ liệu về giá của các loại quả, đó cột B của dữ liệu cần được định dạng lại kiểu số. Bạn đọc có thể sử dụng parse_numbder() như đã trình bày ở trên. Dữ liệu sau khi được định dạng lại các cột đã dễ hiểu hơn rất nhiều:","code":"\nx<-read_csv(\"../KHDL_KTKD Final/Dataset/Book1.csv\")\nx## # A tibble: 5 × 2\n##   A              B         \n##   <chr>          <chr>     \n## 1 \"l\\xea\"        20.000 vnd\n## 2 \"t\\xe1o\"       35.000 vnd\n## 3 \"qu\\xfdt\"      30.000 vnd\n## 4 \"c\\xe0 t\\xedm\" 5.500 vnd \n## 5 \"m\\xedt\"       10.000 vnd\nparse_character(x$A, locale = locale(encoding = \"Latin2\"))## [1] \"lę\"     \"táo\"    \"quýt\"   \"cŕ tím\" \"mít\"\nparse_character(x$A, locale = locale(encoding = \"Latin1\"))## [1] \"lê\"     \"táo\"    \"quýt\"   \"cà tím\" \"mít\"\ntibble(Name = parse_character(x$A, locale = locale(encoding = \"Latin1\")),\n      Price = parse_number(x$B, locale = locale(grouping_mark = \".\")))## # A tibble: 5 × 2\n##   Name   Price\n##   <chr>  <dbl>\n## 1 lê     20000\n## 2 táo    35000\n## 3 quýt   30000\n## 4 cà tím  5500\n## 5 mít    10000"},{"path":"tiền-xử-lý-dữ-liệu.html","id":"giá-trị-không-quan-sát-được","chapter":"Chương 7 Tiền xử lý dữ liệu","heading":"7.3 Giá trị không quan sát được","text":"Giá trị không quan sát được là các giá trị NA xuất hiện trong dữ liệu khi nhập vào R. Có nhiều lý khác nhau dẫn đến việc dữ liệu không quan sát được. Chẳng hạn như thông tin người làm dữ liệu cung cấp không đầy đủ, hoặc người cung cấp dữ liệu từ chối chia sẻ thông tin, hoặc hệ thống quản lý dữ liệu bị lỗi, hoặc cũng có thể người quản lý dữ liệu chủ động xóa dữ liệu vì lý bảo mật. Giá trị không quan sát được ngoài các giá trị NA xuất hiện trong dữ liệu còn có thể là các giá trị không phù hợp với kiểu dữ liệu hoặc miền giá trị của cột dữ liệu. Đối với một vài hệ thống, khi dữ liệu được xuất ra giá trị không quan sát được vẫn được ghi nhận bằng một giá trị nào đó. Bạn đọc cần cẩn trọng khi làm việc với những dữ liệu như vậy.","code":""},{"path":"tiền-xử-lý-dữ-liệu.html","id":"xác-định-giá-trị-không-quan-sát-được","chapter":"Chương 7 Tiền xử lý dữ liệu","heading":"7.3.1 Xác định giá trị không quan sát được","text":"Khi không được xử lý một cách thích hợp, giá trị không quan sát được sẽ làm sai lệch kết quả của các phân tích về dữ liệu, khiến người ra quyết định dựa trên dữ liệu mắc phải sai lầm. Ví dụ, một yêu cầu về phân tích độ tuổi và giới tính của sinh viên được gửi kèm với dữ liệu như sau:Trong dữ liệu ở trên, mặc dù chỉ có một giá trị là NA trong cột Gender, nhưng nếu quan sát kỹ trong dữ liệu ở trên, bạn đọc sẽ nhận ra rằng:Trong cột Name, giá trị 12345 không thể là tên của một sinh viên, đó đây cũng là một giá trị không quan sát được.Trong cột Age, giá trị ở hàng thứ hai là kiểu chuỗi ký tự Nhập sai ngày sinh không phù hợp với giá trị trong dữ liệu. Thứ hai, tuổi của một sinh viên không thể là số âm, nên giá trị -1 ở hàng thứ ba cũng không phù hợp với miền giá trị của cột này. Như vậy, cột Age có hai giá trị không quan sát được.Cột Gender: có giá trị là ký tự N không rõ là thể hiện cho giới tính Nam hay Nữ, giá trị này cũng là không quan sát được.Cột MSV: giả sử từ thông tin bên ngoài, bạn đọc biết rằng mã sinh viên phải là một đoạn ký tự có độ dài là 8, bắt đầu bằng đoạn ký tự “MSV” và theo sau là 5 chữ số. Như vậy giá trị MS34 ở hàng thứ tư cũng là một giá trị không quan sát được trong cột MSV.Để xác định dữ liệu có giá trị ngoại lai hay không cần sử dụng các kiến thức tổng hợp, kiến thức chuyên môn và kiến thức về xác suất - thống kê toán:Cột Height có giá trị chiều cao ở hàng thứ nhất là 1.76 cm. Giá trị đo lường này quá nhỏ để làm chiều cao của một người bình thường. Nhiều khả năng khi đo chiều cao của sinh viên, người nhập dữ liệu đã ghi lại theo đơn vị mét.Cột Weight có giá trị cân nặng của hàng thứ tư là 150 kg. Mặc dù dữ liệu có rất ít quan sát để đưa ra kết luận phân phối xác suất của cân nặng của sinh viên, tuy nhiên với kiến thức thực tế chúng ta có thể kết luận rằng 150 kg là một cân nặng lớn bất thường với các giá trị cân nặng còn lại. Đây nhiều khả năng là một giá trị ngoại lai.Để xác định các giá trị không quan sát được và giá trị ngoại lai tùy thuộc vào từng dữ liệu cụ thể và kiến thức tổng hợp và kiến thức chuyên môn của người xử lý dữ liệu và nằm ngoài phạm vi thảo luận của cuốn sách này. Dữ liệu ở trên chỉ là một dữ liệu nhỏ và không yêu cầu kiến thức chuyên môn hoặc các kiến thức về xác suất thống kê nên việc xác định các giá trị không quan sát được và giá trị ngoại lai là không quá khó khăn.Nguyên tắc xử lý giá trị không quan sát được là luôn cố gắng tìm cách thay thế các giá trị đó bằng một giá trị dự đoán dựa trên kinh nghiệm và kiến thức của người phân tích. Nếu việc tìm kiếm giá trị thay thế là không thể thực hiện được, hoặc không mang lại giá trị cho phân tích dữ liệu, thì giải pháp mới là xóa quan sát hoặc biến có chứa NA.Đối với dữ liệu trong ví dụ ở trên, chúng ta biến đổi các giá trị không quan sát được thành NA bằng các câu lệnh như sau:Đối với các giá trị ngoại lai về chiều cao, chúng ta sẽ đổi giá trị bị ghi nhận sai đơn vị về đúng đơn vị. Với giá trị cân nặng 150 kg, dữ liệu nhỏ, nên các phân tích thống kê sẽ không có ý nghĩa. Có hai cách để xử lý giá trị ngoại lai, đó là giữ nguyên giá trị ban đầu hoặc thay thế giá trị này bằng giá trị lớn nhất của những giá trị thông thường.Dữ liệu sau khi xử lý giá trị ngoại lai và định nghĩa lại các giá trị không quan sát được sẽ có dạng như sau:Với những dữ liệu nhỏ thì hiển thị trực tiếp dữ liệu cũng cho phép người phân tích xác định vị trí của giá trị không quan sát được trong từng biến. Khi dữ liệu có kích thước lớn thì hiển thị dữ liệu không phải là cách xác định vị trí của NA hiệu quả. Hàm số .na() thường được sử dụng để xác định vị trí của giá trị không quan sát được trong các trường họp như vậy. Ngoài ra, hàm summary() cũng có thể được sử dụng cùng với .na() để quản lý giá trị các giá trị không quan sát được:Hàm summary() cho kết quả là các giá trị thống kê mô tả của các biến trong dữ liệu, bao gồm cả số lượng giá trị không quan sát được của các biến dạng số và dạng factor. Bạn đọc có thể thấy rằng sử dụng hàm số summary() trên dữ liệu summary() cho chúng ta biết trong mỗi cột Age và Gender có hai giá trị không quan sát được, trong khi trong các cột Height và Weight không có giá trị không quan sát được. Hạn chế của hàm summary() là không cho chúng ta biết số lượng giá trị không quan sát được trong các biến kiểu chuỗi ký tự.Hàm số .na() thường được sử dụng để xác định vị trí của giá trị không quan sát được trong dữ liệu. Hàm .na() có thể áp dụng trên một véc-tơ, một ma trận, một dữ liệu, hay một mảng nhiều chiều và trả lại kết quả tương ứng là một véc-tơ, một ma trận, hay một mảng nhiều chiều chứa các biến kiểu logical có kích thước bằng với kích thước dữ liệu đầu vào, đồng thời tại vị trí tương ứng của giá trị không quan sát được sẽ có giá trị TRUE, và có giá trị FALSE tại các vị trí còn lại. Ví dụ, chúng ta áp dụng hàm .na() trên dữ liệu df sẽ cho kết quả là một ma trận kiểu logical kích thước \\(4 \\times 6\\) tương ứng với 4 hàng và 6 cột của dữ liệu:Bạn đọc có thể nhận thấy các vị trí nhận giá trị TRUE trong ma trận kết quả tương ứng với giá trị không quan sát được trong các cột MSV, Name, Age và Gender của dữ liệu df.Khi dữ liệu lớn thì việc hiển thị trực tiếp kết quả của hàm .na() là không hiệu quả. Chúng ta cần kết hợp .na() với các hàm số khác hoặc với các kỹ thuật trực quan hóa để đánh giá được tỷ lệ không quan sát được của các biến trong dữ liệu. Thật vậy, khi thực hiện phân tích trên dữ liệu có tên gapminder là một dữ liệu trong thư viện dslabs có kích thước \\(\\text{10545 (dòng) } \\times \\text{ 9 (cột) }\\), chúng ta có thể kết hợp .na() với đồ thị dạng cột để mô tả tỷ lệ số giá trị không quan sát được trong mỗi cột như sau:\nHình 7.1: Mô tả tỷ lệ giá trị không quan sát được của các biến trong dữ liệu gapminder bằng đồ thị dạng cột\nCó thể dễ dàng nhận thấy rằng biến gdp có tỷ lệ giá trị không quan sát được cao nhất, lên đến khoảng 28%, sau đó là biến infant_mortality với tỷ lệ giá trị không quan sát được khoảng 14%. Các biến population và fertility có tỷ lệ giá trị không quan sát được là khoảng 2%. Các biến còn lại gần như không có giá trị không quan sát được.Chúng ta có thể sử dụng các kỹ thuật trực quan hóa dữ liệu để mô tả tỷ lệ giá trị không quan sát được của các biến một cách chi tiết hơn. các hàm số phục vụ cho trực quan hóa dữ liệu được giới thiệu trong phần sau của cuốn sách nên chúng tôi không giới thiệu chi tiết ở phần này. Chúng tôi sẽ mô tả cách xác định giá trị không quan sát được bằng một hàm số Visual_Na() mà chúng tôi tự phát triển. Hàm số Visual_Na() có đầu vào là một dữ liệu và tên một biến rời rạc. Kết quả của hàm Visual_Na() sẽ cho bạn đọc thông tin về tỷ lệ giá trị không quan sát được của từng phần khi chia dữ liệu thành các nhóm nhỏ theo từng giá trị của biến rời rạc. Ví dụ, bạn đọc muốn tìm hiểu về tỷ lệ giá trị không quan sát được của các biến trong dữ liệu gapminder theo từ năm, nghĩa là theo biến year, bạn đọc chỉ cần thực thi câu lệnh của hàm Visual_Na() như dưới đây sau đó hàm số thêm tham số cho hàm số này:\nHình 7.2: Tỷ lệ giá trị không quan sát được của các biến trong dữ liệu gapminder theo năm bắt đầu từ 1960 đến 2016\nHình 7.2 cho biết chi tiết hơn về tỷ lệ giá trị NA xuất hiện trong các biến với hình 7.1. Có thể thấy rằng biến gdp có tỷ lệ NA cao nhất là trong giai đoạn 2012 đến 2016 tỷ lệ giá trị NA là gần 100%. Biến infant_mortality ngoài năm 2016 có tỷ lệ giá trị NA là 100% còn có tỷ lệ giá trị không quan sát được khá cao trong các năm trước năm 1980. Hai biến population và fertility có tỷ lệ giá trị NA là 100% vào 2016, trong khi các năm khác tỷ lệ giá trị NA là bằng 0%.","code":"## # A tibble: 4 × 6\n##   MSV      Name            Age                Gender `Height (cm)` `Weight (kg)`\n##   <chr>    <chr>           <chr>              <chr>          <dbl>         <dbl>\n## 1 MSV00001 12345           30                 Nam             1.76            68\n## 2 MSV43241 Nguyễn Văn An   Nhập sai ngày sinh N             169               72\n## 3 MSV65432 Lê Thị Loan     -1                 Nữ            155               48\n## 4 MSV34    Trần Mạnh Cường 15                 <NA>          175              150\ndf$MSV[(nchar(df$MSV)!=8)]<-NA # mã sinh viên không có 8 ký tự là không quan sát được\ndf$Name[df$Name==\"12345\"]<-NA\ndf$Age<-parse_number(df$Age, na = c(\"-1\")) # tuổi có giá trị (-1) là không quan sát được\ndf$Gender[df$Gender == \"N\"]<-NA\ndf$Gender<-as.factor(df$Gender)\ndf$`Height (cm)`[1]<-df$`Height (cm)`[1] * 100 # đổi đơn vị đo từ mét sang cm\ndf## # A tibble: 4 × 6\n##   MSV      Name              Age Gender `Height (cm)` `Weight (kg)`\n##   <chr>    <chr>           <dbl> <fct>          <dbl>         <dbl>\n## 1 MSV00001 <NA>               30 Nam              176            68\n## 2 MSV43241 Nguyễn Văn An      NA <NA>             169            72\n## 3 MSV65432 Lê Thị Loan        NA Nữ               155            48\n## 4 <NA>     Trần Mạnh Cường    15 <NA>             175           150\nsummary(df)##      MSV                Name                Age         Gender   Height (cm)   \n##  Length:4           Length:4           Min.   :15.00   Nam :1   Min.   :155.0  \n##  Class :character   Class :character   1st Qu.:18.75   Nữ  :1   1st Qu.:165.5  \n##  Mode  :character   Mode  :character   Median :22.50   NA's:2   Median :172.0  \n##                                        Mean   :22.50            Mean   :168.8  \n##                                        3rd Qu.:26.25            3rd Qu.:175.2  \n##                                        Max.   :30.00            Max.   :176.0  \n##                                        NA's   :2                               \n##   Weight (kg)   \n##  Min.   : 48.0  \n##  1st Qu.: 63.0  \n##  Median : 70.0  \n##  Mean   : 84.5  \n##  3rd Qu.: 91.5  \n##  Max.   :150.0  \n## \nis.na(df)##        MSV  Name   Age Gender Height (cm) Weight (kg)\n## [1,] FALSE  TRUE FALSE  FALSE       FALSE       FALSE\n## [2,] FALSE FALSE  TRUE   TRUE       FALSE       FALSE\n## [3,] FALSE FALSE  TRUE  FALSE       FALSE       FALSE\n## [4,]  TRUE FALSE FALSE   TRUE       FALSE       FALSE\n# Véc-tơ y chứa tỷ lệ giá trị NA trong mỗi cột\ny<-sapply(gapminder,\n          function(x) sum(is.na(x))/length(x))\n\n# Dùng đồ thị dạng cột để mô tả tỷ lệ NA\ndf<-data.frame(variable = names(y), NA_rate = y, row.names = NULL)\ndf%>%ggplot(aes(y = variable, x = NA_rate))+\n  geom_bar(stat = \"identity\",alpha = 0.5, color = \"darkblue\", fill = \"#640514\")+\n  theme_minimal()+scale_x_continuous(labels = scales::percent)+\n  ylab(\"\")+\n  xlab(\"Tỷ lệ NA\")\nVisual_Na<-function(df,variable){\n  # Tìm chỉ số của biến variable\n  ind<-names(df)==variable\n\n  # Tính tỷ lệ NA của từng biến theo từng nhóm\n  # Nhóm đươc xác định theo giá trị của variable\n  df1<-df%>%group_by(df[,ind])%>%\n    group_modify(~summarize(.x, across(everything(), function(x) sum(is.na(x))/length(x) ))) %>%\n    as.data.frame()%>%gather(variables,na_rate,-1)\n\n  # Đặt lại tên biến nhóm theo\n  names(df1)[1]<-\"variable\"\n\n  # Biểu diễn đồ thị\n  p<-df1%>%ggplot(aes(x = variable, y = variables, fill = na_rate))+\n    geom_tile(color = \"grey30\", height = 1, width = 1)+\n    scale_fill_gradient(low=\"white\", high = \"#640514\",\n                        labels = scales::label_percent())+\n    theme_minimal()+ylab(\"\")+xlab(\"\")\n  return(p)\n}\nVisual_Na(gapminder,\"year\")"},{"path":"tiền-xử-lý-dữ-liệu.html","id":"xử-lý-giá-trị-không-quan-sát-được.","chapter":"Chương 7 Tiền xử lý dữ liệu","heading":"7.3.2 Xử lý giá trị không quan sát được.","text":"Khi dữ liệu có giá trị không quan sát được, cách tiếp cận hợp lý nhất là tìm các giá trị thay thế cho các giá trị này dựa trên kinh nghiệm và suy đoán của người xử lý dữ liệu. Chỉ khi việc tìm kiếm dữ liệu thay thế là không thể thực hiện được, hoặc không đem lại giá trị cho các bước phân tích tiếp theo, chúng ta mới áp dụng cách đơn giản hơn là xóa các quan sát hoặc xóa các biến chứa các giá trị đó. Sự xuất hiện của giá trị không quan sát được trong dữ liệu thường được chia làm hai trường hợp, đó là dữ liệu không quan sát được xuất hiện một cách không ngẫu nhiên và dữ liệu không quan sát được xuất hiện một cách ngẫu nhiên.Giá trị không quan sát được xuất hiện một cách không ngẫu nhiên là trường hợp mà có một số quan sát (hoặc biến) có tỷ lệ giá trị NA rất cao, trong khi tất cả các quan sát (hoặc biến) còn lại có tỷ lệ NA nhỏ. Nguyên nhân dẫn đến việc giá trị không quan sát được xuất hiện một cách không ngẫu nhiên thường đến từ nguyên nhân chủ quan, khi người cung cấp dữ liệu không muốn cung cấp thông tin, hoặc lỗi của hệ thống cung cấp dữ liệu. Khi gặp vấn đề như vậy thì xóa quan sát (hoặc biến) có tỷ lệ giá trị NA cao là giải pháp hợp lý.Ví dụ, chúng ta có dữ liệu về thông tin sinh viên của một lớp như trong Bảng ??Có thể thấy rằng, quan sát tương ứng với mã sinh viên MSV33789 ngoài thông tin về mã sinh viên và tên sinh viên, các thông tin khác đều không quan sát được. Ngoài sinh viên này, các sinh viên còn lại đều có đầy đủ thông tin. Trong trường hợp này phương pháp xử lý hiệu quả nhất là xóa sinh viên có mã MSV33789 khỏi dữ liệu trước khi tiến hành phân tích.Dữ liệu không quan sát được xuất hiện một cách không ngẫu nhiên có thể tập trung ở một số biến như ví dụ dưới đâyCó thể thấy rằng trong Bảng ?? có biến GPA có đa số các giá trị là không quan sát được. Ngoài ra, không thể đưa ra phán đoán cho các giá trị của biến này dựa trên các biến còn lại. Nếu không có thêm thông tin, mọi phân tích liên quan đến giá trị của biến GPA sẽ không có ý nghĩa, đó cách xử lý tốt nhất là xóa biến này ra khỏi dữ liệu.Có nhiều cách để xóa quan sát và biến khỏi dữ liệu, hai cách phổ biến thường được sử dụng là:Để xóa các quan sát có chứa giá trị không quan sát được ra khỏi dữ liệu, bạn đọc có thể sử dụng hàm drop_na() của thư viện tidyr. Cấu trúc câu lệnh của drop_na() như sau:Để xóa một biến khỏi dữ liệu, bạn đọc có thể coi dữ liệu như là một list và gán giá trị của biến đó bằng giá trị NULL:Dữ liệu không quan sát được xuất hiện một cách ngẫu nhiên là trường hợp mà các giá trị không quan sát được nằm rải rác ở các cột và các quan sát không theo một quy tắc nào. Khi gặp trường hợp, này nếu xóa đi các quan sát hoặc biến có chứa giá trị NA thì tỷ lệ dữ liệu bị xóa đi sẽ là rất đáng kể. Thật vậy, chúng tôi sẽ sử dụng dữ liệu cự thể để để minh họa cho vấn đề này. Dữ liệu được sử dụng là mpg của thư viện ggplot2. Đây là một data.frame có kích thước 234 (quan sát) \\(\\times\\) 11 (biến) mô tả mức độ tiêu hao nhiên liệu của các loại xe ô tô thương mại đang bán trên thị trường trong hai năm 1999 và 2008. Dữ liệu không có giá trị NA nhưng chúng tôi sẽ thêm các giá trị không quan sát được vào dữ liệu một các ngẫu nhiên. Sau đó dữ liệu chính xác sẽ được sử dụng để minh họa và đánh giá phương pháp xử lý giá trị không quan sát được trong phần sau.Bạn đọc sử dụng đoạn câu lệnh dưới đây để thêm giá trị không quan sát được vào trong dữ liệu một cách ngẫu nhiên. Dữ liệu mới sau khi thêm NA vào sẽ được gọi tên là na.mpg để phân biệt với dữ liệu ban đầu.Chúng ta thấy rằng có 8 trên tổng số 11 biến có giá trị NA, mỗi cột có 5 giá trị NA xuất hiện một cách ngẫu nhiên trên tổng số 234 giá trị (tỷ lệ khoảng 2%). Tuy nhiên số quan sát có chứa NA lại lớn hơn 2% rất nhiều. Nếu sử dụng hàm drop_na() của thư viện tidyr để xóa các quan sát có giá trị không quan sát được ra khỏi dữ liệu, chúng ta có thể tính được tỷ lệ dữ liệu còn giữ lại là bao nhiêu như sau:Có thể thấy nếu 2% dữ liệu không quan sát được xuất hiện ngẫn nhiên ở mỗi biến thì tỷ lệ dữ liệu còn lại là khoảng 85% nếu chúng ta áp dụng phương pháp xóa các quan sát có chứa NA, nghĩa là 15% dữ liệu đã bị xóa. Chúng tôi thử tăng tỷ lệ giá trị không quan sát được trên mỗi cột lên thành 3%, 5%, 10%, 20%, 30% và quan sát tỷ lệ dữ liệu còn lại sau khi xóa và cho kết quả như Bảng ??Bảng ?? cho thấy nếu xử lý giá trị không quan sát được bằng cách xóa quan sát thì tỷ lệ dữ liệu còn lại giảm đi rất nhanh. Khi tỷ lệ NA xuất hiện một cách ngẫu nhiên trong mỗi biến từ 5% trở lên chúng ta phải xóa đi hơn 35% số quan sát. Tỷ lệ dữ liệu xóa như vậy sẽ ảnh hưởng lớn đến kết quả của phân tích dữ liệu. Rõ ràng đây không phải là một giải pháp hiệu quả khi giá trị NA xuất hiện một cách ngẫu nhiên.Phương pháp xử lý giá trị không quan sát được thường được áp dụng trong trường hợp này là thay thế giá trị không quan sát được bằng các giá trị thích hợp. Cách tiếp cận đơn giản nhất đó là giả thiết các biến chứa giá trị không quan sát được độc lập với các biến còn lại và sử dụng các giá trị đặc trưng của biến đó để thay thế cho giá trị không quan sát được. Cách tiếp cận phức tạp hơn nhưng cũng cho hiệu quả cao hơn là cân nhắc mối liên hệ giữa các biến trong dữ liệu và xây dựng các thuật toán để dự đoán giá trị thích hợp thay thế cho các giá trị không quan sát được. Mỗi phương pháp đều có ưu nhược điểm riêng và chúng tôi thường thử cả hai hướng tiếp cận sau đó đánh giá hiệu quả của kết quả phân tích. Các phương pháp thay thế giá trị không quan sát được bằng một giá trị thích hợp được trình bày trong các phần tiếp theo.","code":"\n# Dữ liệu có tên là dat\ndat<-drop_na(dat) # Xóa các quan sát có giá trị NA ra khỏi dữ liệu\ndat$ten_cot<-NULL # Xóa cột có tên là ten_cot ra khoi du lieu\n# Tạo dữ liệu mới giống như dữ liệu mpg\nna.mpg<-mpg\n\n# Định dạng các cột kiểu biến rời rạc thành kiểu factor\nchiso<- !(names(na.mpg) %in% c(\"displ\", \"cty\", \"hwy\"))\nna.mpg[,chiso]<-lapply(na.mpg[,chiso], as.factor)%>%\n  as.data.frame()\n\n# Viết hàm số để thêm giá trị NA vào một véc-tơ\n## Hàm số thêm vào véc-tơ x các giá trị NA một cách ngẫu nhiên\n## Tỷ lệ giá trị NA được thêm vào là na.rate\nrd.add<-function(x, na.rate){\n  n<-length(x)\n  k<-round(n*na.rate)\n  ind<-sample(1:n,k,replace=FALSE)\n  x[ind]<-NA\n  return(x)\n}\n\n# Thêm giá trị NA vào các cột NGOẠI TRỪ ba cột\n## Cột nhà sản xuất: manufacturer\n## Cột loại xe: model\n## Cột năm sản xuất\n## tỷ lệ thêm NA một cách ngẫu nhiên vào các cột là 2%\nchiso<- !(names(na.mpg) %in% c(\"manufacturer\", \"model\", \"year\"))\nset.seed(12)\nna.mpg[,chiso]<-as.data.frame(lapply(na.mpg[,chiso],\n                                     rd.add,\n                                      na.rate = 0.02))\n\n# Xem mỗi cột có bao nhiêu giá trị NA\nsapply(na.mpg, f<-function(x) sum(is.na(x)))## manufacturer        model        displ         year          cyl        trans \n##            0            0            5            0            5            5 \n##          drv          cty          hwy           fl        class \n##            5            5            5            5            5\nnrow(drop_na(na.mpg))/nrow(na.mpg)## [1] 0.8461538"},{"path":"tiền-xử-lý-dữ-liệu.html","id":"thay-thế-giá-trị-không-quan-sát-được-bằng-các-đại-lượng-đặc-trưng-của-biến","chapter":"Chương 7 Tiền xử lý dữ liệu","heading":"7.3.2.1 Thay thế giá trị không quan sát được bằng các đại lượng đặc trưng của biến","text":"Phương pháp thay thế này dựa trên giả thiết rằng biến chứa giá trị không quan sát được không có mối liên hệ đến các biến còn lại, và chúng ta sẽ sử dụng một trong các giá trị đặc trưng của các giá trị quan sát được của cột đó như trung bình (mean), trung vị (median), hoặc mode để thay thế cho các giá trị không quan sát được.Giá trị trung bình thường được sử dụng để thay thế cho các giá trị không quan sát được cho véc-tơ kiểu số liên tục và phân phối của các giá trị không có đuôi dài và không có giá trị bất thường.Giá trị trung vị, là giá trị tại ngưỡng xác suất 50%, thường được sử dụng để thay thế cho các giá trị không quan sát được trong véc-tơ kiểu số liên tục và véc-tơ có đuôi dài. Giá trị trung vị có ưu điểm là ít bị ảnh hưởng bởi các giá trị ngoại lai và không bị thay đổi sau các bước biến đổi dữ liệu bằng các hàm đơn điệu.Giá trị mode, là giá trị mà hàm mật độ có xác suất cao nhất, có thể dùng cho cả véc-tơ kiểu số liên tục hoặc véc-tơ kiểu biến rời rạc. Trong trường hợp véc-tơ kiểu số liên tục, bạn đọc cần phải ước lượng hàm mật độ nên giá trị mode sẽ còn phụ thuộc vào phương pháp tiếp cận của người phân tích.Để thay thế giá trị không quan sát được bằng một giá trị khác, bạn đọc có thể sử dụng một trong các hàm có sẵn như sau:Hàm số na_if() của thư viện dplyrHàm số replace_na() của thư viện tidyrCác hàm số người dùng tự định nghĩaĐể đơn giản hóa, chúng ta giả sử rằng sẽ luôn luôn sử dụng giá trị thay thế là giá trị trung vị khi đối với véc-tơ kiểu số liên tục và giá trị mode đối với véc-tơ kiểu biến rời rạc.Giá trị thực tế của các biến kiểu số liên tục, bao gồm các biến có tên là displ, hwy, và cty, và giá trị được dùng để thay thế bằng giá trị trung vị được tổng kết lại trong Bảng ??Giá trị thực tế của các biến rời rạc và giá trị dùng để thay thế bằng giá trị mode được tổng kết trong Bảng ??.Không dễ dàng để đưa ra kết luận là thay thế các giá trị không quan sát được của véc-tơ kiểu số liên tục bằng giá trị trung vị như trong Bảng ?? là hiệu quả hay không. Chúng ta chỉ có thể thấy rằng giá trị thay thế là giá trị trung vị nên luôn nằm giữa các giá trị thực và khoảng cách từ giá trị thay thế đến các giá trị thực không quá lớn. Về lý thuyết, khi thay thế giá trị không quan sát được bằng giá trị trung vị, chúng ta đang cố gắng đưa ra các dự đoán cho giá trị không quan sát được sao cho giá trị trung bình của sai số tuyệt đối, còn gọi là Mean Absoluted Error hay MAE là nhỏ nhất. Trong trường hợp chúng ta sử dụng giá trung bình để thay thế cho các giá trị không quan sát được, chúng ta tối thiểu hóa trung bình của bình phương sai số dự đoán, còn gọi là Mean Squared Error hay MSE.Thay thế các giá trị không quan sát được trong các véc-tơ kiểu biến rời rạc bằng giá trị mode là nguyên tắc làm giảm thiểu tối đa xác suất dự đoán sai đối với giá trị không quan sát được. Tuy nhiên, giá trị thay thế cho giá trị rời rạc trong bảng ?? chỉ cho một vài lần dự đoán đúng cho mỗi biến. Nguyên nhân là giá trị mode trong các biến rời rạc không chiếm ưu thế với các giá trị khác. Chẳng hạn như biến drv bị dự đoán sai 4 trên 5 kết quả thực sự biến này có đến 2 giá trị mode.","code":"\nmy_mode<-function(x){ # Tự định nghĩa hàm mode\n  names(which.max(table(x)))\n}\nmy_fillna_1<-function(x){ # Tự định nghĩa cách thay thế giá trị NA\n  if(is.numeric(x)){\n    # Nếu x là biến liên tục thì dùng median\n    x[is.na(x)]<-median(x,na.rm=TRUE)\n  } else {\n    # Nếu x là biến rời rạc thì dùng mode\n    x[is.na(x)]<-my_mode(x)\n  }\n  return(x)\n}\n# Áp dụng hàm my_fill_na1 vò dữ liệu na.m\nmpg_1<-lapply(na.mpg, my_fillna_1)%>%as.data.frame()"},{"path":"tiền-xử-lý-dữ-liệu.html","id":"thay-thế-giá-trị-không-quan-sát-được-bằng-một-mẫu-ngẫu-nhiên.","chapter":"Chương 7 Tiền xử lý dữ liệu","heading":"7.3.2.2 Thay thế giá trị không quan sát được bằng một mẫu ngẫu nhiên.","text":"Phương pháp thay thế giá trị NA bằng mẫu ngẫu nhiên vẫn giữ nguyên giả thiết rằng biến chứa giá trị không quan sát được không có mối liên hệ đến các cột còn lại. Cácgiá trị được sử dụng để thay thế cho các giá trị không quan sát được là một mẫu được lấy ngẫu nhiên từ phân phối xác suất của các giá trị quan sát được. Hàm sample() là hàm số được sử dụng để lấy mẫu ngẫu nhiên từ một dữ liệu cho trước. Để lấy ra k giá trị ngẫu nhiên từ một véc-tơ x, chúng ta sử dụng câu lệnh như sau:Tham số replace trong hàm sample() nhận giá trị bằng TRUE có ý nghĩa là giá trị ngẫu nhiên được lấy ra từ véc-tơ x có thể được lấy lặp lại. Chúng ta định nghĩa hàm fill_na_2() dùng để thay thế giá trị không quan sát được trong một véc-tơ x bằng phương pháp lấy mẫu ngẫu nhiên như sauGiá trị thực tế của các biến kiểu số liên tục và giá trị dùng để thay thế bằng phương pháp lấy mẫu ngẫu nhiên được tổng kết trong Bảng ??Giá trị thực tế của các biến rời rạc và giá trị dùng để thay thế bằng phương pháp lấy mẫu ngẫu nhiên được tổng kết trong Bảng ??Hiệu quả của phương pháp lấy mẫu ngẫu nhiên với phương pháp sử dụng các giá trị trung vị hoặc mode là không rõ ràng. Phương pháp thay thế giá trị không quan sát được bằng một mẫu ngẫu nhiên chỉ cho hiệu quả khi dữ liệu đủ lớn và phân phối xác suất của các biến quan sát được ổn định. Nhược điểm lớn nhất của phương pháp này đó là giá trị được sử dụng để thay thế được sinh ngẫu nhiên nên có khả năng sẽ làm cho dữ liệu bị sai lệch, đồng thời mỗi lần thực hiện phương pháp sẽ cho các kết quả khác nhau tùy theo hàm sinh ngẫu nhiên.","code":"\nsample(x, size = k, replace = TRUE)\nmy_fillna_2<-function(x){ # Hàm thay thế giá trị NA, phương pháp thứ 2\n  ind<-is.na(x) # véc-tơ kiểu logic, nhận giá trị TRUE tại các vị trí NA\n  k<-sum(ind)\n  x[ind]<-sample(x[!ind],k,replace = TRUE)\n  return(x)\n}\nset.seed(12)\nmpg_1<-lapply(na.mpg, my_fillna_2)%>%as.data.frame()"},{"path":"tiền-xử-lý-dữ-liệu.html","id":"replaceNAbymodel","chapter":"Chương 7 Tiền xử lý dữ liệu","heading":"7.3.2.3 Thay thế giá trị không quan sát được bằng mô hình dự đoán","text":"Giả thiết rằng cột chứa giá trị không quan sát được không có mối liên hệ đến các cột còn lại là một giả thiết không thực tế. Bởi vì các biến trong dữ liệu thực tế luôn luôn có mối liên hệ với nhau dù ít hay nhiều. Nói theo nguyên lý của xác suất - thống kê thì các biến trong dữ liệu thực tế thường hiếm khi độc lập với nhau. Làm thế nào để biết hai biến dữ liệu bất kỳ là độc lập hay phụ thuộc ? Đây là một câu hỏi không dễ. Bạn đọc cần có kiến thức về xác suất và thống kê toán để có được câu trả lời chính xác. Có rất nhiều lý thuyết khác nhau nghiên cứu về sự phụ thuộc giữa các biến và đa số các lý thuyết đó vượt quá phạm vi của cuốn sách này. Chúng tôi chỉ trình bày các phương pháp được công nhận rộng rãi và có mức độ phức tạp vừa đủ để bạn đọc không cần có nền tảng nâng cao về toán học, xác suất, thống kê có thể hiểu được. Nhìn chung, để đưa ra kết luận hai cột dữ liệu có độc lập hay không, bạn đọc có thể sử dụng các kiểm định như sau:Kiểm định Khi-bình phương khi cả hai biến đều là biến rời rạc.Kiểm định hệ số tương quan Person, hoặc hệ số tương quan Spearman, hoặc hệ số tương quan Kendall khi cả hai biến đều là biến liên tục.Sử dụng phân tích phương sai (hay còn gọi là anova test) trong trường hợp một biến là rời rạc và một biến là liên tục.Chi tiết của các kiểm định này được trình bày ở phần Phụ lục 7.5.1. Chúng ta sẽ sử dụng các phương pháp này để kiểm tra mối liên hệ giữa các biến trong dữ liệu na.mpg.Để thực hiện kiểm định Khi-bình phương trong R, chúng ta sử dụng hàm chisq.test(). Ví dụ, để kiểm ra hai biến rời rạc là year và drv có mối liên hệ hay không, chúng ta thực hiện như sau:Giá trị p-value bằng 43% nghĩa là xác suất bác bỏ giả thiết hai biến year và drv độc lập là 1 - 43% = 57%. Thông thường, mức xác suất bác bỏ giả thiết độc lập thường được chọn ở mức 95% hoặc 99%. xác suất bác bỏ giả thiết độc lập là thấp nên trong trường hợp này có thể đưa ra kết luận rằng hai biến year và drv là không có mối liên hệ.Tương tự, để kiểm ra hai biến drv và cyl có mối liên hệ hay không, chúng ta cũng thực hiện kiểm định Khi-bình phươngTrong trường hợp này, xác suất bác bỏ giả thiết độc lập là xấp xỉ 100% nên chúng ta có thể đưa ra kết luận rằng hai biến drv và cyl là có mối liên hệ.Để kiểm định hệ số tương quan giữa hai biến liên tục chúng ta sử dụng hàm cor.test(). Tham số method nhận giá trị pearson, kendall, hoặc spearman tương ứng với kiểm định hệ số tương quan Pearson, hệ số tương quan Kendall, hoặc hệ số tương quan Spearman. Chúng ta kiểm định sự độc lập giữa hai biến displ và hwy như sauKiểm định đối với ba hệ số tương quan đều cho xác suất bác bỏ giả thiết độc lập là xấp xỉ 100%. Nói một cách khác có thể khẳng định rằng hai biến displ và hwy là có sự phụ thuộc.Sau cùng, để kiểm định sự phụ thuộc giữa một biến rời rạc và một biến liên tục, chúng ta sử dụng phân tích phương sai. Hàm số để thực hiện phân tích phương sai trong R là hàm aov(). Chúng ta kiểm định sự phụ thuộc giữa biến liên tục hwy và biến rời rạc cyl như sau:Xác suất bác bỏ giả thiết giá trị trung bình của biến hwy bằng nhau theo các nhóm của biến cyl là xấp xỉ 100% hay nói một cách khác hwy và cyl là có mối liên hệ.Để xem xét một cách tổng thể mối liên hệ giữa các biến trong dữ liệu na.mpg, bạn đọc có thể sử dụng kiểm định phù hợp với từng cặp biến và lưu xác suất bác bỏ giả thiết độc lập vào một ma trận. Hàm số ind_check() được chúng tôi tự xây dựng với tham số đầu vào là một dữ liệu, dưới dạng một tibble hoặc một data.frame, cho đầu ra là một ma trận cho biết xác suất bác bỏ giả thiết độc lập của từng cặp biến như thế nào.Ma trận thể hiện xác suất bác bỏ giả thiết độc lập giữa từng cặp biến trong dữ liệu na.mpg ở trong Hình 7.3\nHình 7.3: Ma trận mức xác suất bác bỏ giả thuyết độc lập giữa các cặp biến trong dữ liệu na.mpg\nCó thể thấy rằng ngoại trừ biến year ít có mối liên hệ đến các biến khác, còn lại đa số các biến là có mối liên hệ với nhau. Điều này được thể hiện qua xác suất bác bỏ giả thiết độc lập giữa các biến trong ma trận của hình đều xấp xỉ 100%. Khi xây dựng mô hình trên dữ liệu, sự xuất hiện của các biến ít có mối liên hệ đến các biến khác sẽ khiến mô hình bị nhiễu và làm giảm chất lượng dự đoán. đó, chúng tôi sẽ loại bỏ biến year khi dự đoán giá trị không quan sát được của các biến khác.Phương pháp để xây dựng mô hình dự đoán cho các giá trị không quan sát được là thuật toán rừng ngẫu nhiên. Đây là một thuật toán mở rộng của mô hình dạng cây quyết định sẽ được trình bày trong chương ??. Còn quá sớm để nói về mô hình này, bạn đọc cần hiểu rằng chúng ta sẽ dựa vào các giá trị quan sát được để xây dựng mô hình, hay tổng quát hơn là xây dựng một hàm số với giá trị của hàm là biến có chứa giá trị NA và biến số là các biến là các giá trị quan sát được. Thư viện missForest hỗ trợ chúng ta thực hiện việc này. Bạn đọc có thể cài thư viện sau đó sử dụng hàm missForest(). Quá trình thay thế giá trị không quan sát được của dữ liệu na.mpg bằng cách dự đoán dựa trên thuật toán rừng ngẫu nhiên được thực hiện mà chỉ cần một dòng lệnh:Sau khi có giá trị dự đoán cho các giá trị không quan sát được, chúng ta sẽ hiển thị giá trị thực của các biến kiểu số và các giá trị thay thế hàm missForest() như trong Bảng ??Tương tự, giá trị thực tế tại các vị trí không quan sát được của các biến rời rạc và giá trị dự đoán bằng thuật toán rừng ngẫu nhiên ở trong Bảng ??Có thể nhận thấy rằngĐối với các biến kiểu số, giá trị dùng để thay thế cho các giá trị không quan sát được không khác nhiều với giá trị thực tế. Phương pháp thay thế giá trị NA bằng cách sử dụng thuật toán rừng ngẫu nhiên là hiệu quả hơn hẳn với hai phương pháp trước đó.Đối với các biến kiểu rời rạc, ngoại trừ biến trans và biến fl, các biến còn lại đều được dự đoán chính xác 100% bằng thuật toán rừng ngẫu nhiên. Hiệu quả của việc sử dụng thuật toán rừng ngẫu nhiên rõ ràng là vượt trội hơn hai phương pháp còn lại.Nhìn chung, xây dựng mô hình để dự đoán giá trị không quan sát được của một biến dựa trên các biến khác là phương pháp cho hiệu quả tốt hơn, đặc biệt đối với dữ liệu có các biến có mối liên hệ chặt chẽ với nhau. Điểm bất lợi duy nhất của phương pháp này là sự phức tạp trong kỹ thuật xây dựng mô hình. Bạn đọc cần có các hiểu biết cơ bản về xây dựng mô hình, trong trường hợp này là mô hình cây quyết định, và các kỹ thuật thống kê hiện đại như kỹ thuật lấy mẫu lặp, để hiểu được nguyên tắc dự đoán giá trị không quan sát được. Tất nhiên, thực thi hàm missForest() của thư viện cùng tên không cần bạn phải có các kiến thức này. Để hiểu được chính xác cách xây dựng mô hình, bạn đọc tham khảo chương ??.","code":"\nchisq.test(na.mpg$year,na.mpg$drv)## \n##  Pearson's Chi-squared test\n## \n## data:  na.mpg$year and na.mpg$drv\n## X-squared = 1.689, df = 2, p-value = 0.4298\nchisq.test(na.mpg$drv, na.mpg$cyl)## \n##  Pearson's Chi-squared test\n## \n## data:  na.mpg$drv and na.mpg$cyl\n## X-squared = 90.288, df = 6, p-value < 2.2e-16\ncor.test(na.mpg$displ, na.mpg$hwy, method = \"pearson\")## \n##  Pearson's product-moment correlation\n## \n## data:  na.mpg$displ and na.mpg$hwy\n## t = -17.743, df = 223, p-value < 2.2e-16\n## alternative hypothesis: true correlation is not equal to 0\n## 95 percent confidence interval:\n##  -0.8143893 -0.7048317\n## sample estimates:\n##       cor \n## -0.765092\ncor.test(na.mpg$displ, na.mpg$hwy, method = \"kendall\")## \n##  Kendall's rank correlation tau\n## \n## data:  na.mpg$displ and na.mpg$hwy\n## z = -13.857, p-value < 2.2e-16\n## alternative hypothesis: true tau is not equal to 0\n## sample estimates:\n##        tau \n## -0.6534741\ncor.test(na.mpg$displ, na.mpg$hwy, method = \"spearman\")## \n##  Spearman's rank correlation rho\n## \n## data:  na.mpg$displ and na.mpg$hwy\n## S = 3467012, p-value < 2.2e-16\n## alternative hypothesis: true rho is not equal to 0\n## sample estimates:\n##        rho \n## -0.8262809\nsummary(aov(hwy~cyl,data=na.mpg))##              Df Sum Sq Mean Sq F value Pr(>F)    \n## cyl           3   4479  1492.9   101.6 <2e-16 ***\n## Residuals   220   3233    14.7                   \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## 10 observations deleted due to missingness\n### Hàm số ind_check()\nind_check<-function(dat){\n  dat<-as.data.frame(dat)\n  dat.name<-names(dat)\n  p<-dim(dat)[2]\n  M<-matrix(0,p,p)\n  for (i in 1:(p-1)){\n    for (j in (i+1):p){\n      x<-dat[,i]\n      y<-dat[,j]\n      if(is.character(x)|is.character(y)){\n        return(NA)\n      } else {\n        if (is.numeric(x)){\n          if (is.numeric(y)){\n            test<-cor.test(x, y, method = \"spearman\")\n            M[i,j]<-1 - test$p.value\n          } else {\n            test<-summary(aov(x ~ y))\n            M[i,j]<-1 - test[[1]][[5]][1]\n          }\n        } else {\n          if (is.numeric(y)){\n            test<-summary(aov(y ~ x))\n            M[i,j]<-1 - test[[1]][[5]][1]\n          } else {\n            test<-chisq.test(x,y)\n            M[i,j]<-1 - test$p.value\n          }\n        }\n      }\n      M[j,i]<-M[i,j]\n    }\n  }\n  colnames(M)<-dat.name\n  rownames(M)<-dat.name\n  diag(M)<-1\n  return((round(M,3)))\n}\nlibrary(missForest)\nna.mpg<-as.data.frame(na.mpg)\n\n### Thời gian chạy mất khoảng 1-2 phút\nmodel<-missForest(select(na.mpg,-year), maxiter = 200, ntree = 100)\nmpg_1<-model$ximp # Dữ liệu mpg_1 là dữ liệu sau khi thay thế NA"},{"path":"tiền-xử-lý-dữ-liệu.html","id":"xử-lý-giá-trị-ngoại-lai","chapter":"Chương 7 Tiền xử lý dữ liệu","heading":"7.4 Xử lý giá trị ngoại lai","text":"","code":""},{"path":"tiền-xử-lý-dữ-liệu.html","id":"ảnh-hưởng-của-giá-trị-ngoại-lai-lên-kết-quả-phân-tích","chapter":"Chương 7 Tiền xử lý dữ liệu","heading":"7.4.1 Ảnh hưởng của giá trị ngoại lai lên kết quả phân tích","text":"Giá trị ngoại lai hay còn được gọi là giá trị bất thường là một, hoặc một số điểm dữ liệu có giá trị sai khác đáng kể với đa số các quan sát khác. Một giá trị ngoại lai xuất hiện trong dữ liệu có thể là lỗi trong quản lý dữ liệu, sai số trong đo lường hoặc cũng có thể bản chất phân phối của dữ liệu. Tùy theo nguồn gốc của giá trị ngoại lai mà chúng ta có cách xử lý dữ liệu khác nhau. Khi không được xử lý thích hợp, các giá trị ngoại lai có thể làm sai lệch kết luận của các phân tích dựa trên dữ liệu.Giá trị ngoại lai được hiểu là những điểm dữ liệu khác xa tập hợp các điểm còn lại. Không có một định nghĩa chính xác nào cho khái niệm khác xa các giá trị còn lại. đó, tùy theo bản chất của dữ liệu, và tùy theo quan điểm của người phân tích dữ liệu, mà một (hay một số) giá trị có khả năng là giá trị ngoại lai hay không. Giá trị ngoại lai thường chỉ được nhắc đến với các dữ liệu có số quan sát đủ lớn để đưa ra kết luận có ý nghĩa thống kê.\nHình 7.4: Giá trị ngoại lai xuất hiện trong dữ liệu có ít (10) và có nhiều (100) quan sát. Hình bên trái: Các điểm và B nằm cách xa 8 điểm còn lại nhưng dữ liệu chưa đủ lớn để đưa ra kết luận. Hình bên phải: điểm và B nằm cách xa 98 điểm còn lại, có thể kết luận và B là các điểm ngoại lai.\nHình 7.4 mô tả trực quan các điểm và B có khả năng là giá trị ngoại lai trong hai trường hợp là có ít quan sát và có nhiều quan sát. Khi dữ liệu có 10 quan sát như hình bên trái, có 8 quan sát nằm gần nhau, điểm B nằm xa hơn tập hợp các điểm còn lại một chút, còn điểm nằm cách xa hơn. Khi gặp dữ liệu như vậy, chúng ta có thể khá chắc chắn với kết luận rằng điểm là giá trị ngoại lai vì điểm này nằm cách rất xa các điểm còn lại. Tuy nhiên, kết luận điểm B có phải ngoại lai hay không thì còn tùy thuộc vào cách tiếp cận của người phân tích dữ liệu. Hình bên phải với dữ liệu có 100 quan sát. Các điểm nằm gần nhau định hình khá rõ miền giá trị của trung tâm của dữ liệu. Chúng ta có thể kết luận một cách khá chắc chắn rằng điểm là một giá trị ngoại lai. Điểm B mặc dù nằm khá xa trung tâm của dữ liệu, và cũng có thể kết luận khá chắc chắn rằng đây là điểm ngoại lai.Nguồn gốc của giá trị ngoại lai là có thể đến từ nhiều nguyên nhân khác nhau, bao gồm cả nguyên nhân khách quan hoặc nguyên nhân chủ quan. Các nguyên nhân khách quan có thể nguồn sinh dữ liệu, hay hệ thống quản lý dữ liệu gặp sự cố, lỗi trong quá trình truyền hoặc sao chép dữ liệu. Nguyên nhân chủ quan bao gồm có các hành vi gian lận, lỗi nhập và sao chép dữ liệu của con người, hoặc các giá trị được cố tình đưa vào trong dữ liệu với mục đích lấy phản hồi từ người dùng dữ liệu.Nếu không xử lý giá trị ngoại lai, kết quả phân tích sẽ bị sai lệch đáng kể. Và dữ liệu có kích thước càng nhỏ thì ảnh hưởng của giá trị ngoại lai lại càng lớn. Trong ví dụ trong đồ thị bên trái của Hình 7.4, giả sử chúng ta cần phân tích sự tác động của biến X lên biến Y bằng một mối quan hệ tuyến tính. Chúng ta xây dựng mô hình tuyến tính trong ba trường hợpTrường hợp 1: Giữ nguyên 10 quan sát và xây dựng mô hình mô tả mối liên hệ tuyến tính.Trường hợp 2: Loại bỏ điểm trước khi xây dựng mô hình.Trường hợp 3: Loại bỏ điểm và điểm B trước khi xây dựng mô hình.Các đường tuyến tính mô tả mối liên hệ giữa biến X và Y được mô tả trong Hình 7.5\nHình 7.5: Xây dựng mô hình trên dữ liệu có chứa và không chứa giá trị ngoại lai. Hình bên trái: bao gồm cả hai điểm và B trong xây dựng mô hình. Hình ở giữa: loại điểm và giữ lại điểm B trong xây dựng mô hình. Hình bên phải: loại bỏ cả điểm và điểm B trước khi xây dựng mô hình.\nKhi giữ nguyên 10 điểm dữ liệu để xây dựng mối quan hệ tuyến tính, đường thẳng mô tả mối quan hệ giữa X và Y là nằm trong đồ thị phía bên trái của Hình 7.5. Đường thẳng này có hệ số góc dương, nghĩa là một đường dốc lên khi đi từ trái sang phải. Điều này có nghĩa là biến X có tác động cùng chiều lên biến Y, nghĩa là khi X tăng hoặc giảm thì nhiều khả năng Y cũng sẽ tăng hoặc giảm.Khi giữ nguyên 10 điểm dữ liệu để xây dựng mối quan hệ tuyến tính, đường thẳng mô tả mối quan hệ giữa X và Y là nằm trong đồ thị phía bên trái của Hình 7.5. Đường thẳng này có hệ số góc dương, nghĩa là một đường dốc lên khi đi từ trái sang phải. Điều này có nghĩa là biến X có tác động cùng chiều lên biến Y, nghĩa là khi X tăng hoặc giảm thì nhiều khả năng Y cũng sẽ tăng hoặc giảm.Sau khi loại bỏ điểm và tính toán lại, đường thẳng mô tả mối quan hệ tuyến tính giữa X và Y ở là đường thẳng trong đồ thị ở giữa của Hình 7.5. Đường thẳng gần như nằm ngang, cho thấy X không có tác động lên biến Y. Nghĩa là X có tăng hay giảm cũng không làm thay đổi Y.Sau khi loại bỏ điểm và tính toán lại, đường thẳng mô tả mối quan hệ tuyến tính giữa X và Y ở là đường thẳng trong đồ thị ở giữa của Hình 7.5. Đường thẳng gần như nằm ngang, cho thấy X không có tác động lên biến Y. Nghĩa là X có tăng hay giảm cũng không làm thay đổi Y.Sau cùng, trong đồ thị phía bên phải của Hình 7.5, sau khi loại bỏ các điểm X và điểm B, đường thẳng mô tả mối quan hệ tuyến tính giữa X và Y là đường dốc xuống, nghĩa là mối tác động của X lên Y là ngược chiều.Sau cùng, trong đồ thị phía bên phải của Hình 7.5, sau khi loại bỏ các điểm X và điểm B, đường thẳng mô tả mối quan hệ tuyến tính giữa X và Y là đường dốc xuống, nghĩa là mối tác động của X lên Y là ngược chiều.Bạn đọc có thể thấy rằng kết luận từ kết quả ước lượng mô hình thay đổi hoàn toàn khi chúng ta có các lựa chọn khác nhau về việc có loại bỏ hay không các giá trị được cho là ngoại lai ra khỏi dữ liệu. Sự tác động của X lên Y từ thuận chiều trong đồ thị bên trái đến không có mối liên hệ trong đồ thị ở giữa, và sau cùng là sự tác động ngược chiều của X lên Y trong đồ thị bên phải. Điều này cho thấy việc xác định và xử lý giá trị ngoại lai là vô cùng quan trọng trước khi xây dựng mô hình.Trong phần tiếp theo chúng ta sẽ thảo luận về các phương pháp dùng để xác định các giá trị ngoại lai trong dữ liệu.","code":""},{"path":"tiền-xử-lý-dữ-liệu.html","id":"phương-pháp-phát-hiện-giá-trị-ngoại-lai","chapter":"Chương 7 Tiền xử lý dữ liệu","heading":"7.4.2 Phương pháp phát hiện giá trị ngoại lai","text":"Không có một định nghĩa chính xác như thế nào là giá trị ngoại lai, chính vì thế không có phương pháp chung để phát hiện giá trị ngoại lai. Với mỗi dữ liệu, với mỗi cách nhìn nhận giá trị ngoại lại khác nhau, mà có phương pháp tiếp cận cụ thể để xác định các giá trị đó. Trong phần này, chúng tôi chỉ trình bày các phương pháp chung được chấp nhận rộng rãi. Đây là các phương pháp đơn giản, dễ hiểu và có thể thực hiện được mà không cần bổ sung thêm kiến thức. Các phương pháp phức tạp hơn, đòi hỏi kiến thức nâng cao về dữ liệu như phân nhóm, phân cụm, sẽ được thảo luận trong chương sách học máy không có giám sát.","code":""},{"path":"tiền-xử-lý-dữ-liệu.html","id":"phát-hiện-giá-trị-ngoại-lai-trong-một-véc-tơ","chapter":"Chương 7 Tiền xử lý dữ liệu","heading":"7.4.2.1 Phát hiện giá trị ngoại lai trong một véc-tơ","text":"Để xác định một giá trị là giá trị ngoại lai hay không thường bao gồm hai bước, bước thứ nhất là sử dụng các phương pháp xác suất thống kê để xác định các giá trị có khả năng cao là ngoại lai, sau đó bước thứ hai là sử dụng kiến thức chuyên môn hoặc hỏi ý kiến chuyên gia để khẳng định lại kết quả từ bước thứ nhất.Nếu véc-tơ là một véc-tơ kiểu chuỗi ký tự mà không phải kiểu factor thì không có quy tắc rõ ràng nào để xác định giá trị ngoại lai. Trước hết, việc một biến kiểu chuỗi ký tự có phải là một giá trị ngoại lai hay không phụ thuộc vào bản chất của véc-tơ chuỗi ký tự. Việc này hoàn toàn phụ thuộc vào hiểu biết của người phân tích dữ liệu với véc-tơ đó. Ví dụ, véc-tơ chuỗi ký tự là tên của người bằng tiếng Việt, một giá trị ngoại lai có thể là một tên người với nhiều hơn 6 hay 7 từ, bởi vì theo kiến thức chung, tên người bình thường bao gồm 2,3 hoặc 4 từ. Ngoài ra, một chuỗi ký tự có thể là ngoại lai nếu chuỗi ký tự có độ dài bất thường, có chứa nhiều ký tự bất thường, hay một chuỗi ký tự không có ý nghĩa trong một véc-tơ bao gồm các chuỗi ký tự có ý nghĩa. Nói một cách khác một giá trị có phải là ngoại lai hay không hoàn toàn phụ thuộc vào cách tiếp cận của người phân tích dữ liệu. Các phương pháp xử lý dữ liệu kiểu chuỗi ký tự hiện nay có khả năng biến đổi một chuỗi ký tự thành một véc-tơ kiểu số. Việc xác định chuỗi ký tự có phải là một giá trị bất thường hay không sẽ liên quan đến việc xác định một véc-tơ kiểu số có phải là một véc-tơ có giá trị bất thường trong một tập hợp các véc-tơ. Các kỹ thuật này vượt quá phạm vi của cuốn sách nên chúng tôi không đề cập ở đây.\nHình 7.6: Tần suất xuất hiện của các loại đồ uống được bán tại một siêu thị. Loại đồ uống có tần suất xuất hiện thấp có khả năng là giá trị ngoại lai\nKhi gặp trường hợp như đồ thị trong Hình 7.6, có khả năng đồ uống có tên Collagen là giá trị ngoại lai. Chúng ta chưa thể khẳng định bởi vì nếu siêu thị thực sự có bán loại đồ uống này và việc sản phẩm không được khách hàng ưa chuộng, thì sản phẩm xuất hiện với tần xuất thấp là bình thường. Tuy nhiên cũng có thể tên sản phẩm xuất hiện trong danh sách bán hàng dù siêu thị bán cũng có thể là lỗi gặp phải trong quản lý hệ thống bán hàng, hoặc người bán hàng đã ghi nhận tên Collagen cho một đồ uống khác.Đối với véc-tơ kiểu số, các giá trị có khả năng là ngoại lai thường là các giá trị nằm ở đuôi của phân phối xác suất. Các giá trị nằm ở đuôi là các giá trị nằm cách xa các giá trị trung bình, hoặc trung vị, về phía bên phải hoặc bên trái. Để biết một véc-tơ kiểu số có giá trị ngoại lai hay không, bạn đọc nên sử dụng đồ thị Boxplot. Các điểm nằm phía dưới điểm nhỏ nhất (\\(Q_0\\)) và nằm phía trên điểm lớn nhất (\\(Q_4\\)) của đồ thị boxplot có nhiều khả năng là các giá trị ngoại lai. Điểm nhỏ nhất và điểm lớn nhất của đồ thị Boxplot được xác định dựa trên mức tứ phân vị thứ nhất (\\(Q_1\\)) và mức tứ phân vị thứ ba (\\(Q_3\\)):\n\\[\\begin{align}\n&\\text{Inter Quartile Range (IQR)} = Q_3 - Q_1 \\\\\n&\\text{Điểm nhỏ nhất } (Q_0) = Q_1 - 1.5 \\times IQR \\\\\n&\\text{Điểm lớn nhất } (Q_4) = Q_3 + 1.5 \\times IQR\n\\end{align}\\]Các giá trị trong véc-tơ kiểu số nằm ngoài khoảng \\((Q_0, Q_4)\\) có nhiều khả năng là giá trị ngoại lai. Giá trị càng nhỏ hơn \\(Q_0\\) và càng cao hơn \\(Q_4\\) thì khả năng là giá trị ngoại lai lại càng cao.Chúng tôi sẽ lấy một ví dụ mô tả việc sử dụng đồ thị Boxplot để phát hiện giá trị ngoại lai trên một dữ liệu thực tế. Hình 7.7 mô tả phân phối xác suất của véc-tơ chứa khối lượng giao dịch, tính bằng triệu cổ phiếu/ngày, của cổ phiếu tập đoàn FLC. Cổ phiếu được niêm yết trên sàn giao dịch chứng khoán Thành phố Hồ Chí Minh từ ngày 6 tháng 10 năm 2011 đến ngày 8 tháng 9 năm 2022. Dữ liệu có 2719 quan sát.\nHình 7.7: Sử dụng đồ thị boxplot để mô tả lịch sử khối lượng giao dịch cổ phiếu FLC. Các giá trị có khả năng là giá trị ngoại lai nằm trên điểm Q4 của phân phối xác suất.\nChúng ta có thể thấy trên đồ thị Boxplot không có điểm nằm dưới \\(\\text{Q_0}\\) trong khi có 8 quan sát có giá trị lớn hơn \\(\\text{Q_4}\\). Và các giá trị này có khả năng là các giá trị ngoại lai. Có 3 quan sát với giá trị lớn hơn 100 triệu, nghĩa là có ba ngày mà có hơn 100 triệu cổ phiếu FLC được giao dịch. Nếu có một chút kinh nghiệm về giao dịch thị trường chứng khoán Việt Nam, bạn đọc có thể kiểm chứng được đây là số lượng cổ phiếu giao dịch lớn bất thường.Ba phiên giao dịch có khối lượng giao dịch lớn hơn 100 triệu cổ phiếu là các phiên giao dịch ngày 10 tháng 1 năm 2022, ngày 11 tháng 1 năm 2022 và phiên giao dịch ngày 1 tháng 4 năm 2022. Thực tế cho thấy đây là ba phiên giao dịch mà cổ phiếu FLC đã bị thao túng giá và dẫn đến việc cố phiếu FLC bị cấm giao dịch trên sàn giao dịch HOSE kể từ tháng 09 năm 2022.Từ khoảng tháng 10 năm 2021 giá cổ phiếu FCL bắt đầu tăng nhanh. Đến đầu tháng 01 năm 2022, giá cổ phiếu đã tăng lên gấp 2 lần. Ngày 10 và ngày 11 tháng 01 năm 2022, các cổ đông chính của FLC bán ra khối lượng rất lớn các cổ phiếu mà không đăng ký với Ủy ban chứng khoán theo quy định. Sau hai phiên giao dịch này giá cổ phiếu FLC giảm mạnh về đến mức trước đó vài tháng.Ngày 31 tháng 03 năm 2022 các thông tin giả mạo về nhu cầu mua cổ phiếu FLC với khối lượng lớn được đưa ra sau nhiều ngày giá cổ phiếu FLC giảm hết biên độ làm cho nhu cầu mua FLC trong ngày 01 tháng 04 năm 2022 cao đột biến.Việc thao túng giá và đưa thông tin giả mạo khiến cho số lượng cố phiếu FLC tăng lên đột biến đã bị các cơ quan chức năng phát hiện và đưa ra lệnh cấm giao dịch với cổ phiếu này. Đây là ví dụ điển hình về dữ liệu có giá trị ngoại lai có nguyên nhân chủ quan từ con người.Ngoài đồ thị Boxplot, bạn đọc có thể sử dụng các đồ thị mô tả phân phối của biến liên tục như đồ thị histogram hay đồ thị density để xác định giá trị ngoại lai trong véc-tơ kiểu số. Ví dụ, Hình 7.8 mô tả phân phối của chiều cao của 245 nam giới là nhân viên của một công ty. Đơn vị đo chiều cao là cm.\nHình 7.8: Kết hợp Boxplot và Histogram để xác định giá trị ngoại lai trong dữ liệu. Hình bên trái: đồ thị boxplot cho có điểm nằm phía dưới điểm Q0. Hình bên phải: đồ thị histogram cho thấy có giá trị bất thường nằm ở đuôi bên trái\nCả hai đồ thị Boxplot và Histogram trong Hình 7.8 đều cho thấy trong dữ liệu có các giá trị là chiều cao của nam giới xấp xỉ giá trị 0 và nhiều khả năng đây là các giá trị ngoại lai. Đồ thị Histogram còn cho thấy có nhiều hơn 1 giá trị có giá trị như vậy. Lọc các giá trị đó ra khỏi véc-tơ chúng ta sẽ thu được 5 giá trị là 1,52; 1,74; 1,70; 1,62; và 1,80. Đây không thể là chiều cao của nam giới đo bằng đơn vị cm. Có nhiều khả năng là khi ghi lại chiều cao của các nhân viên này, người nhập dữ liệu đã sử dụng đơn vị là mét thay vì cm. Chúng ta có thể sửa các giá trị ngoại lai này bằng cách đổi từ đơn vị mét sang cm. Phân phối xác suất của chiều cao sau khi sửa lại dữ liệu được mô tả như hình dưới đây:\nHình 7.9: Kết hợp Boxplot và Histogram để xác định giá trị ngoại lai trong dữ liệu. Hình bên trái: đồ thị Boxplot sau khi điều chỉnh lại chiều cao từ đơn vị mét sang cm. Hình bên phải: đồ thị Histogram sau khi điều chỉnh lại chiều cao từ đơn vị mét sang cm\nVéc-tơ kiểu số là đơn vị đo lường hay đơn vị tiền tệ rất thường xuyên gặp vấn đề như kể trên. Ngay khi gặp giá trị ngoại lai trong véc-tơ kiểu số như trên bạn đọc hãy nghĩ đến sai đơn vị đo lường là nguyên nhân đầu tiên.Ngoài việc sử dụng các tứ phân vị để phát hiện giá trị ngoại lai, một phương pháp định lượng khác cũng thường được đề cập đến trong nhiều tài liệu là sử dụng \\(\\text{Z-Score}\\). Đây là thước đo được tính bằng khoảng cách từ 1 điểm đến giá trị trung bình của dữ liệu sau đó chia cho độ lệch chuẩn của dữ liệu\n\\[\\begin{align}\n\\text{Z-Score}(x_i) = \\cfrac{|x_i - \\bar{x}|}{\\sigma(x)}\n\\end{align}\\]\nvới \\(x_i\\) là giá trị thứ \\(\\) trong véc-tơ \\(x\\), \\(\\bar{x}\\) là giá trị trung bình của véc-tơ \\(x\\), và \\(\\sigma(x)\\) là độ lệch chuẩn của các số trong véc-tơ \\(x\\). Z-Score được tính toán dựa trên giả thiết là dữ liệu có phân phối chuẩn, đó các điểm dữ liệu có Z-Score lớn, thường được lấy ngưỡng lớn hơn 3, được coi là các giá trị ngoại lai. Chẳng hạn như khi vẽ Z-Score của tất cả các điểm dữ liệu trong dữ liệu về chiều cao của nhân viên trong ví dụ được mô tả trong Hình 7.8, chúng ta sẽ có giá trị Z-Score của chiều cao của tất cả các nhân viên như trong Hình 7.10\nHình 7.10: Giá trị Z-Score chiều cao của tất cả các nhân viên. Các quan sát có Z-Score lớn hơn 3 có nhiều khả năng là giá trị ngoại lai\nCác điểm có Z-score lớn hơn ngưỡng 3 trong Hình 7.10 là các điểm bị ghi nhận sai đơn vị đo lường từ cm sang mét và có Z-Score lên đến hơn 6. Trong trường hợp này Z-Score cũng là phương pháp định lượng hiệu quả để xác định giá trị ngoại lai. Tuy nhiên, Z-Score có điểm bất lợi là giá trị này được tính toán dựa trên giá trị trung bình và độ lệch tiêu chuẩn của dữ liệu trong khi chính các giá trị đó lại bị tác động rất mạnh bởi các giá trị ngoại lai. Một cách để giảm thiểu tác động của giá trị ngoại lai đến tính toán \\(\\text{Z-Score}(x_i)\\) là không tính đến \\(x_i\\) khi tính toán trung bình \\(\\bar{x}\\) và \\(\\sigma(x)\\).Đa số các phương pháp xác định giá trị ngoại lai ở trên đều dựa trên giả thiết là véc-tơ dữ liệu có phân phối chuẩn. Tuy nhiên, không phải lúc nào phân phối chuẩn cũng phù hợp với véc-tơ kiểu số. Dữ liệu về bồi thường bảo hiểm là một điển hình của dữ liệu không có phân phối chuẩn. Hình 7.11 mô tả số liệu về tiền bồi thường bảo hiểm sức khỏe của hơn 1.000 khách hàng tại một công ty bảo hiểm:\nHình 7.11: Phân phối xác suất của tiền bồi thường bảo hiểm y tế tại của hơn 1.000 khách hàng tại một công ty bảo hiểm. Hình bên trái: đồ thị histogram cho thấy phân phối của số tiền bồi thường không phải là phân phối chuẩn. Hình bên phải: có nhiều điểm có Z-Score lớn hơn ngưỡng 3\nNhiều điểm dữ liệu được xác định là ngoại lai mặc dù thực tế thì đây vẫn là các giá trị thông thường. Nguyên nhân là phân phối của số tiền bảo hiểm y tế không phải là phân phối chuẩn. Trong trường hợp này, tính toán Z-score trên dữ liệu ban đầu sẽ không cho kết quả chính xác. Khi gặp dữ liệu không có phân phối chuẩn, trước hết cần biến đổi dữ liệu về phân phối chuẩn hoặc biến đổi về gần phân phối chuẩn nhất có thể trước khi thực hiện tính Z-Score. Phép biến đổi về dữ liệu phân phối chuẩn thường được sử dụng nhất là biến đổi Box-Cox được trình bày trong Phụ lục 7.5.2.\nHình 7.12: Dữ liệu bồi thường bảo hiểm sau khi đã được biến đổi về gần với phân phối chuẩn. Hình bên trái: đồ thị Histogram cho thấy phân phối của các điểm dữ liệu đã gần với phân phối chuẩn hơn với dữ liệu ban đầu. Hình bên phải: các điểm có nhiều khả năng là giá trị ngoại lai là các giá trị có Z-score trên ngưỡng 3\nHình 7.12 mô tả dữ liệu bồi thường bảo hiểm sau khi đã được biến đổi về phân phối chuẩn. Có thể thấy rằng sau phép biến đổi, các giá trị có Z-Score lớn hơn ngưỡng 3 đã giảm đi rất nhiều. Các điểm này nhiều khả năng là những khoản tiền bồi thường lớn bất thường và cần được kiểm tra lại. Ngoài biến đổi Box-Cox, một phương pháp khác để biến đổi véc-tơ về phân phối chuẩn là sử dụng hàm ngược của hàm phân phối chuẩn. Phương pháp này được trình bày trong Phụ lục ??.Trong thực tế trong rất nhiều trường hợp, chúng ta chỉ quan sát trên các biến dữ liệu riêng lẻ thì không thể xác định được giá trị ngoại lai. Giống như điểm B trong đồ thị bên phải của Hình 7.4, nếu chúng ta chỉ quan sát vị trí của điểm này trên trục \\(\\overrightarrow{Ox}\\) hoặc trục \\(\\overrightarrow{Oy}\\) một cách riêng biệt thì không thể xác định được đây là giá trị ngoại lai. Điều này có nghĩa là một quan sát có thể không phải là ngoại lai nếu như chỉ quan sát trên từng biến dữ liệu, nhưng lại là giá trị ngoại lai khi quan sát đồng thời các thành phần của quan sát đó. Các kỹ thuật xác định giá trị ngoại lai trong không gian nhiều chiều sẽ được thảo luận trong phần tiếp theo.","code":""},{"path":"tiền-xử-lý-dữ-liệu.html","id":"giá-trị-ngoại-lai-trong-không-gian-nhiều-chiều","chapter":"Chương 7 Tiền xử lý dữ liệu","heading":"7.4.2.2 Giá trị ngoại lai trong không gian nhiều chiều","text":"Xác định giá trị ngoại lai trong không gian nhiều chiều phức tạp hơn trong không gian một chiều. Thật vậy, trong không gian một chiều, chúng ta cần xác định những số nào là giá trị ngoại lai của một véc-tơ. Trong khi trong không gian nhiều chiều, chúng ta cần phải xác định các quan sát nào là giá trị ngoại lai trong một dữ liệu. Ngoài việc xem xét giá trị trong từng biến dữ liệu, chúng ta cần phải xem xét cả mối liên hệ giữa các biến.Các phương pháp để xác định giá giá trị ngoại lai trong không gian nhiều chiều về cơ bản vẫn dựa trên nguyên tắc cơ bản áp dụng trong không gian một chiều, đó là các quan sát càng xa điểm trung tâm của dữ liệu thì quan sát đó càng có khả năng cao là giá trị ngoại lai. Khái niệm xa hay gần trong một không gian nhiều chiều luôn gắn liền với một khái niệm về khoảng cách. Khoảng cách thường được sử dụng nhiều nhất trong không gian nhiều chiều là khoảng cách Euclid. Tuy nhiên khoảng cách Euclid có nhược điểm là không tính đến mối liên hệ giữa các biến dữ liệu. Khoảng cách thường được sử dụng hơn để xác định giá trị ngoại lai là khoảng cách Mahalanobis.Cho \\(\\textbf{x}_i = x_{i1}, x_{i2}, \\cdots, x_{ip}\\) là quan sát thứ \\(\\) và \\(\\boldsymbol{\\mu} = \\mu_{1}, \\mu_{2}, \\cdots, \\mu_{p}\\) là véc-tơ giá trị trung bình của các véc-tơ cột. Khoảng cách Euclid và khoảng cách Mahalanobis từ điểm \\(\\textbf{x}_i\\) đến \\(\\boldsymbol{\\mu}\\) được định nghĩa như sau:\n\\[\\begin{align}\nD^{Euc}(\\textbf{x}_i,\\boldsymbol{\\mu}) & = \\sqrt{(\\textbf{x}_i - \\boldsymbol{\\mu})^T (\\textbf{x}_i - \\boldsymbol{\\mu}}) \\\\\nD^{Mah}(\\textbf{x}_i,\\boldsymbol{\\mu})& = \\sqrt{(\\textbf{x}_i - \\boldsymbol{\\mu})^T \\ \\Sigma^{-1} \\  (\\textbf{x}_i - \\boldsymbol{\\mu})} \\\\\n\\end{align}\\]\ntrong đó \\(D^{Euc}(\\textbf{x}_i,\\boldsymbol{\\mu})\\) và \\(D^{Mah}(\\textbf{x}_i,\\boldsymbol{\\mu})\\) lần lượt là khoảng cách Euclid và khoảng cách Mahalanobis từ quan sát \\(\\textbf{x}_i\\) đến điểm trung bình \\(\\boldsymbol{\\mu}\\). Trong công thức tính khoảng cách Mahalanobis, \\(\\Sigma^{-1}\\) là ma trận nghịch đảo của ma trận hiệp phương sai của các biến trong dữ liệu. Có thể thấy rằng khoảng cách Euclid là trường hợp riêng của khoảng cách Mahalanobis khi các cột dữ liệu có phương sai bằng 1 và đôi một độc lập với nhau.Bạn đọc có thể tự viết các hàm số tính khoảng cách Euclid và hàm số tính khoảng cách Mahalanobis giữa 2 véc-tơ bất kỳ. Ví dụ, trong các câu lệnh dưới đây, các hàm có tên là Dis.Euc() và hàm Dis.Mah() được định nghĩa như sauChúng ta quay lại ví dụ về dữ liệu bao gồm 10 quan sát với hai giá trị ngoại lai là điểm và điểm B trong Hình 7.4. Chúng ta tính toán khoảng cách Euclid của mỗi điểm đến trung tâm của dữ liệu và sắp xếp các điểm theo thứ tự khoảng cách Euclid đến điểm trung tâm giảm dần. Kết quả được cho trong Bảng @ref(tag:tbptdl010)Có thế thấy rằng khi chỉ có 10 quan sát, khoảng cách Euclid có thể sử dụng để phát hiện được giá trị ngoại lai là điểm và điểm B vì hai điểm này có khoảng cách đến trung tâm xa hơn với các điểm còn lại. Khoảng cách Mahalanobis cho kết quả là điểm là giá trị ngoại lai, trong khi điểm B lại không cho kết quả rõ ràng.Sử dụng khoảng cách Euclid có thể gặp vấn đề khi số lượng quan sát nhiều hơn và mối liên hệ giữa X và Y rõ ràng hơn. Thật vậy, chúng ta thực hiện tính toán các khoảng cách Euclid và Mahalanobis từ 100 điểm trong đồ thị bên phải của Hình 7.4 đến trung tâm của dữ liệu đó, sau đó sắp xếp các điểm theo thứ tự khoảng cách Euclid giảm dần giống như khi có 10 quan sát. 10 điểm có khoảng cách Euclid đến trung tâm lớn nhất được liệt kê trong Bảng ??.Bạn đọc có thể thấy khoảng cách Euclid không cho kết quả tốt như khoảng cách Malahanobis khi dữ liệu nhiều quan sát và mối liên hệ giữa các biến là rõ ràng hơn.\n* Khi đo bằng khoảng cách Euclid, điểm vẫn là điểm xa trung tâm dữ liệu nhất. Tuy nhiên khoảng cách từ điểm B đến trung tâm dữ liệu là nhỏ hơn một số điểm khác, mặc dù các điểm đó không phải là các giá trị ngoại lai.\n* Khi tính bằng khoảng cách Mahalanobis, chúng ta có thể thấy rằng điểm là điểm có khoảng cách xa nhất, sau đó đến điểm B với khoảng cách Malahanobis là 4.675. Các điểm khác đều có khoảng cách Mahalanobis nhỏ hơn 2.5. Như vậy khoảng cách Mahalanobis xác định giá trị ngoại lai tốt hơn khoảng cách Euclid trong trường hợp này.Các kỹ thuật phát hiện giá trị ngoại lai phức tạp hơn dựa trên nguyên lý phân nhóm và phân cụm sẽ được trình bày trong chương học máy không có giám sát. Nguyên tắc xác định một quan sát ngoại lai là phân chia dữ liệu thành các cụm sao cho các quan sát trong cùng một cụm có tính chất tương tự nhau. Các quan sát không nằm trong cụm nào, hoặc trong các cụm có rất ít quan sát, là các điểm dữ liệu có nhiều khả năng là giá trị ngoại lai.","code":"\nDis.Euc<-function(x,y) sum((x-y)^2)^0.5\nDis.Mah<-function(x,y,Sigma) (t(x-y)%*% solve(Sigma) %*%(x-y))^0.5"},{"path":"tiền-xử-lý-dữ-liệu.html","id":"xử-lý-giá-trị-ngoại-lai.","chapter":"Chương 7 Tiền xử lý dữ liệu","heading":"7.4.3 Xử lý giá trị ngoại lai.","text":"Có nhiều phương pháp để xử lý giá trị ngoại lai trong dữ liệu. Tùy thuộc vào tình huống và dữ liệu cụ thể, phương pháp nào cũng có thể đúng hoặc sai. Điều quan trọng là bạn đọc phải phân tích các tình huống có thể liên quan đến giá trị ngoại lai. Đôi khi việc phân tích các giá trị ngoại lai này còn giúp bạn có những hiểu biết hơn về dữ liệu và tối ưu công việc phân tích của bạn.Phương pháp đơn giản nhất và nhưng kém hiệu quả nhất là loại bỏ các quan sát, hoặc biến có chứa giá trị ngoại lai. Phương pháp này chỉ có ý nghĩa khi bạn có số lượng quan sát đủ lớn và các giá trị bị coi là ngoại lai không có có ý nghĩa trong xác định phân phối xác suất của từng biến.Phương pháp thứ hai là thay thế giá trị ngoại lai bằng một giá trị khác: bạn đọc có thể thay thế giá trị ngoại lai bằng giá trị có ý nghĩa hơn, chẳng hạn như thay thế các giá trị nhỏ hơn giá trị \\(Q_0\\) bằng chính \\(Q_0\\) và thay thế các giá trị lớn hơn \\(Q_4\\) bằng chính \\(Q_4\\). Một cách tiếp cận khác cũng có thể sử dụng là thay thế giá trị ngoại lai bằng giá trị trung bình, trung vị, hoặc mode của phân phối. Đây là phương pháp đơn giản, dễ sử dụng và có thể cho hiệu quả tốt hơn với phương pháp xóa quan sát.Phương pháp sau cùng, và cũng là phương pháp đòi hỏi kỹ thuật phức tạp nhất, là coi giá trị ngoại lai như một giá trị không quan sát được, sau đó xây dựng mô hình để dự đoán cho giá trị ngoại lai. Các phương pháp thay thế giá trị ngoại lai bằng giá trị dự đoán dựa trên các mô hình tương tự như các phương pháp xử lý dữ liệu không quan sát được. Bạn đọc có thể tham khảo phần 7.3.2.3 của cuốn sách.","code":""},{"path":"tiền-xử-lý-dữ-liệu.html","id":"phụ-lục-2","chapter":"Chương 7 Tiền xử lý dữ liệu","heading":"7.5 Phụ lục","text":"","code":""},{"path":"tiền-xử-lý-dữ-liệu.html","id":"appenptdl01","chapter":"Chương 7 Tiền xử lý dữ liệu","heading":"7.5.1 Kiểm định sự độc lập của hai biến","text":"","code":""},{"path":"tiền-xử-lý-dữ-liệu.html","id":"pearsons-chi-squared-tests","chapter":"Chương 7 Tiền xử lý dữ liệu","heading":"7.5.1.1 Pearson’s Chi-squared tests","text":"","code":""},{"path":"tiền-xử-lý-dữ-liệu.html","id":"appenptdl02","chapter":"Chương 7 Tiền xử lý dữ liệu","heading":"7.5.2 Box-Cox transformation","text":"Biến đổi Box-Cox là một phương pháp biến đổi để đưa một véc-tơ có phân phối khác với phân phối chuẩn thành một véc-tơ có phân phối gần với phân phối chuẩn. Phép biến đổi Box-Cox chỉ có duy nhất một tham số \\(\\lambda\\).\n\\[\\begin{align}\nx(\\lambda) = \\begin{cases}\n\\cfrac{y^\\lambda-1}{\\lambda} \\text{ nếu } \\lambda \\neq 0 \\\\\nlog(y) \\text{ nếu } \\lambda = 0\n\\end{cases}\n\\end{align}\\]giá trị \\(\\lambda\\) thường được lựa chọn trong khoảng (-5,5) sao cho khoảng cách giữa dữ liệu sau khi biến đổi đến phân phối chuẩn là nhỏ nhất. Khoảng cách giữa hai phân phối được đo bằng khoảng cách Kolmogorov-Smirnov. Dữ liệu nào có khoảng cách Kolmogorov-Smirnov đến phân phối chuẩn nhỏ nhất là dữ liệu có phân phối gần với phân phối chuẩn nhất. Hình (@fig::fgoutlier11) mô tả quá trình tìm tham số \\(\\lambda\\) của biến đổi Box-Cox sao cho dữ liệu về bồi thường bảo hiểm y tế được mô tả trong Hình … được biến đổi về gần phân phối chuẩn.\nHình 7.13: Lựa chọn tham số để thực hiện biến đổi Box-Cox. Giá trị tham số lambda tối thiểu hóa khoảng cách của phân phối của dữ liệu đến phân phối chuẩn được lựa chọn\nTham số \\(\\lambda\\) tối thiểu hóa khoảng cách từ phân phối của dữ liệu đến phân phối chuẩn là 0.21, chúng ta có phân phối của dữ liệu sau khi biến đổi trong Hình 7.14\nHình 7.14: Biến đổi số tiền bồi thường thành phân phối chuẩn bằng cách sử dụng biến đổi Box-Cox với tham số 0.21\n","code":""},{"path":"tiền-xử-lý-dữ-liệu.html","id":"phương-pháp-chuẩn-hóa-véc-tơ-bằng-hàm-ngược","chapter":"Chương 7 Tiền xử lý dữ liệu","heading":"7.5.3 Phương pháp chuẩn hóa véc-tơ bằng hàm ngược","text":"Phương pháp biến đổi này dựa trên hai kết quả cơ bản như sau:Nếu \\(X\\) là một biến ngẫu nhiên liên tục có hàm phân phối \\(F\\) thì biến ngẫu nhiên \\(F(X)\\) sẽ có phân phối \\(\\text{Uniform}(0,1)\\)Nếu \\(X\\) là một biến ngẫu nhiên liên tục có hàm phân phối \\(F\\) thì biến ngẫu nhiên \\(F(X)\\) sẽ có phân phối \\(\\text{Uniform}(0,1)\\)Nếu \\(U\\) là một biến ngẫu nhiên phân phối \\(\\text{Uniform}(0,1)\\) thì biến ngẫu nhiên \\(\\Phi^{-1}(U)\\) sẽ có phân phối chuẩn với trung bình bằng 0 và phương sai bằng 1, trong đó \\(\\Phi^{-1}\\) là hàm ngược của hàm phân phối của biến ngẫu nhiên phân phối chuẩn với trung bình bằng 0 và phương sai bằng 1.Nếu \\(U\\) là một biến ngẫu nhiên phân phối \\(\\text{Uniform}(0,1)\\) thì biến ngẫu nhiên \\(\\Phi^{-1}(U)\\) sẽ có phân phối chuẩn với trung bình bằng 0 và phương sai bằng 1, trong đó \\(\\Phi^{-1}\\) là hàm ngược của hàm phân phối của biến ngẫu nhiên phân phối chuẩn với trung bình bằng 0 và phương sai bằng 1.Cả hai kết quả này đều có thể được chứng minh bằng kiến thức xác suất cơ bản. Thứ nhất, \\(F(X)\\) có phân phối \\(\\text{Uniform}(0,1)\\) vì \\(F(X)\\) nhận giá trị trên \\([0,1]\\), đồng thời\n\\[\\begin{align}\n\\mathbb{P}\\left(F(X) < x\\right) &= \\mathbb{P}\\left(X < F^{-1}(x)\\right) \\\\\n& = F(F^{-1}(x)) \\\\\n& = x\n\\end{align}\\]\nlà hàm phân phối xác suất của biến ngẫu nhiên \\(\\text{Uniform}(0,1)\\)Thứ hai, biến ngẫu nhiên \\(\\Phi^{-1}(U)\\) có phân phối chuẩn với trung bình bằng 0 và phương sai bằng 1 vì\n\\[\\begin{align}\n\\mathbb{P}\\left(\\Phi^{-1}(U) < x\\right) & = \\mathbb{P}\\left(U < \\Phi(x)\\right) \\\\\n& = \\Phi(x)\n\\end{align}\\]Như vậy, biến ngẫu nhiên \\(X\\) bất kỳ có thể được biến đổi thành biến ngẫu nhiên phân phối chuẩn với trung bình bằng 0 và phương sai bằng 1 bằng phép biến đổi \\(N = \\Phi^{-1}(F(X))\\) với \\(F\\) là hàm phân phối của \\(X\\).Khó khăn lớn nhất trong phép biến đổi này là tìm ra phân phối \\(F\\) của biến ngẫu nhiên \\(X\\) vì chúng ta chỉ có một véc-tơ quan sát được của \\(X\\). Việc này đòi hỏi các kiến thức liên quan đến thống kê toán.Quay trở lại với ví dụ về dữ liệu về số tiền bồi thường bảo hiểm trong hình 7.11, chúng ta có thể sử dụng phân phối \\(\\text{Pareto}(\\alpha,\\beta)\\) cho số tiền bảo hiểm. Các tham số của phân phối \\(\\text{Pareto}\\) ước lượng cho dữ liệu là \\(\\alpha = 2.17\\), và \\(\\beta = 3.34\\). Chúng ta biến đổi số tiền bồi thường thành phân phối chuẩn như sau:\n\\[\\begin{align}\n& N = \\Phi^{-1}(F(X))\n\\end{align}\\]\nvới \\(F(x)\\) được xác định bởi\n\\[\\begin{align}\nF(x) = 1 - \\left(\\cfrac{\\beta}{x+\\beta} \\right)^\\alpha\n\\end{align}\\]Hình 7.15 mô tả dữ liệu trước và sau khi biến đổi\nHình 7.15: Biến đổi số tiền bồi thường có phân phối Pareto thành phân phối chuẩn bằng cách sử dụng hàm ngược\nCó thể thấy dữ liệu sau khi biến đổi đã gần với phân phối chuẩn hơn với dữ liệu gốc.","code":""},{"path":"biến-đổi-và-sắp-xếp-dữ-liệu.html","id":"biến-đổi-và-sắp-xếp-dữ-liệu","chapter":"Chương 8 Biến đổi và sắp xếp dữ liệu","heading":"Chương 8 Biến đổi và sắp xếp dữ liệu","text":"","code":""},{"path":"biến-đổi-và-sắp-xếp-dữ-liệu.html","id":"biến-đổi-dữ-liệu-bằng-thư-viện-dplyr","chapter":"Chương 8 Biến đổi và sắp xếp dữ liệu","heading":"8.1 Biến đổi dữ liệu bằng thư viện dplyr","text":"Dữ liệu trước khi được đưa vào các bước phân tích, trực quan hóa, hoặc xây dựng mô hình, thường không có cấu trúc đúng như mong muốn. Thông thường, chúng ta sẽ cần thay đổi tên các biến, tạo thêm một số biến mới, sắp xếp lại các quan sát theo một trật tự, lọc dữ liệu theo một số tiêu chí, hoặc tổng hợp và nhóm dữ liệu vào các nhóm để dễ dàng hơn cho các bước tiếp theo. Các bước biến đổi dữ liệu như vậy sẽ được trình bày chi tiết trong phần này của cuốn sách. Đa số các hàm số được sử dụng để thực hiện các phép biến đổi dữ liệu như vậy sẽ nằm trong thư viện dplyr và thư viện tidyr. Đây là hai thư viện con của thư viện tổng hợp tidyverse. Để sử dụng hai thư viện này, bạn đọc có thể gọi thư viện tidyverse hoặc gọi tên hai thư viện dplyr và thư viện tidyr lên cửa sổ làm việc. Bạn đọc cũng cần lưu ý rằng trong thư viện dplyr có một số hàm trùng tên với các hàm có sẵn trong R, chẳng hạn như hàm filter(), hàm select(), hoặc hàm lag(). Bạn đọc cần kiểm tra thứ tự ưu tiên của các thư viện khi sử dụng các hàm kể trên, hoặc gọi tên thư viện đi kèm với tên hàm để tránh gặp lỗi khi thực thi các câu lệnh.Dữ liệu để minh họa cho các phép biến đổi trong chương này là dữ liệu gapminder nằm trong thư viện dslabs. Đây là dữ liệu mô tả sức khỏe và thu nhập của người dân thuộc tất cả các quốc gia trên thế giới được quan sát từ năm 1960 đến năm 2016. Chúng tôi lựa chọn dữ liệu này vì đây là dữ liệu dễ hiểu với đa số bạn đọc và có nhiều ý tưởng để phân tích. Mặc dù dữ liệu gapminder đã được đề cập trong phần tiền xử lý dữ liệu nhưng chúng tôi khuyên bạn đọc nên tham khảo mô tả về các biến trong dữ liệu một lần nữa trước khi đi đến các phần tiếp theo của cuốn sách.Để tránh việc hiển thị dữ liệu bị tràn dòng, chúng ta sẽ đổi dữ liệu về dạng tibble trước khi thực thi các hàm sốTrong các phần tiếp theo, chúng tôi sẽ lần lượt giới thiệu các hàm quan trọng trong thư viện dplyr dùng để thực hiện các biến đổi dữ liệu và các tham số quan trọng của các hàm số đó. Chúng tôi cũng sẽ giới thiệu với bạn đọc về cách kết hợp các hàm số đó với nhau thành một câu lệnh duy nhất bằng cách sử dụng toán tử pipe (%>%).","code":"\nmytib<-as.tibble(gapminder) # mytib là dữ liệu kiểu tibble"},{"path":"biến-đổi-và-sắp-xếp-dữ-liệu.html","id":"thêm-biến-bằng-hàm-mutate","chapter":"Chương 8 Biến đổi và sắp xếp dữ liệu","heading":"8.1.1 Thêm biến bằng hàm mutate()","text":"Khi muốn tính toán giá trị cho một biến mới dựa trên các biến hiện có và thêm biến đó vào dữ liệu, bạn đọc sử dụng hàm mutate(). Hàm mutate() luôn thêm biến vào vị trí phía sau biến cuối cùng theo thứ tự từ trái sang phải trong dữ liệu hiện có. Khi bạn muốn thêm biến vào một vị trí cụ thể trong dữ liệu, bạn có thể sử dụng các tham số sau trong hàm mutate().Tham số .sử dụng để gán giá trị cho tên biến hiện có sẽ nằm phía trước biến mà bạn đọc muốn thêm vào.Tham số .sử dụng để gán giá trị cho tên biến hiện có sẽ nằm phía sau biến mà bạn đọc muốn thêm vào.Ví dụ, chúng ta muốn thêm biến thu nhập bình quân đầu người của mỗi quốc gia, được tính bằng thu nhập quốc nội (biến gdp) chia cho dân số của quốc gia đó (biến population), chúng ta sử dụng mutate() như sau:Câu lệnh trên có ý nghĩa là thêm biến có tên là gdp_per_capita, được tính bằng tổng thu nhập quốc nội (gdp) chia cho dân số (population) của quốc gia đó. chúng ta không chỉ định vị trí cụ thể cho biến gdp_per_capita, biến này sẽ nằm ở vị trí cuối cùng theo thứ tự từ trái sang phải trong dữ liệu mới.Nếu bạn đọc muốn biến mới được thêm vào ngay sau biến infant_mortality, hãy sử dụng tham số .trong hàm mutate() như sau:Tương tự như tham số ., chúng ta sử dụng tham số .trong hàm mutate() để cho biết biến mới thêm vào sẽ nằm phía trước biến mà chúng ta chỉ định. Để thêm biến thu nhập bình quân đầu người vào phía trước biến infant_mortality, chúng ta sử dụng tham số .như sauMột hàm số khác được sử dụng với mục đích tương tự là thêm biến mới vào dữ liệu giống như hàm mutate() là hàm transmute(). Hàm transmute() khác hàm mutate() ở chỗ là trong dữ liệu mới được tạo thành chỉ bao gồm các biến mới được khai báo trong hàm số đó. Ví dụ, chúng ta có thể thêm biến thu nhập bình quân đầu người của các quốc gia vào dữ liệu ban đầu bằng hàm transmute():Có thể thấy rằng trong dữ liệu mới được tạo thành, chỉ có một biến duy nhất là biến thu nhập bình quân đầu người (gdp_per_capita) còn các biến khác đã không được giữ lại.","code":"\nmutate(mytib, gdp_per_capita = gdp/population) # Thêm cột có tên là gdp_per_capita## # A tibble: 10,545 × 10\n##    country   year infant_mortality life_expectancy fertility population      gdp\n##    <fct>    <int>            <dbl>           <dbl>     <dbl>      <dbl>    <dbl>\n##  1 Albania   1960            115.             62.9      6.19    1636054 NA      \n##  2 Algeria   1960            148.             47.5      7.65   11124892  1.38e10\n##  3 Angola    1960            208              36.0      7.32    5270844 NA      \n##  4 Antigua…  1960             NA              63.0      4.43      54681 NA      \n##  5 Argenti…  1960             59.9            65.4      3.11   20619075  1.08e11\n##  6 Armenia   1960             NA              66.9      4.55    1867396 NA      \n##  7 Aruba     1960             NA              65.7      4.82      54208 NA      \n##  8 Austral…  1960             20.3            70.9      3.45   10292328  9.67e10\n##  9 Austria   1960             37.3            68.8      2.7     7065525  5.24e10\n## 10 Azerbai…  1960             NA              61.3      5.57    3897889 NA      \n## # ℹ 10,535 more rows\n## # ℹ 3 more variables: continent <fct>, region <fct>, gdp_per_capita <dbl>\nmutate(mytib, gdp_per_capita = gdp/population, .after = infant_mortality)## # A tibble: 10,545 × 10\n##    country        year infant_mortality gdp_per_capita life_expectancy fertility\n##    <fct>         <int>            <dbl>          <dbl>           <dbl>     <dbl>\n##  1 Albania        1960            115.             NA             62.9      6.19\n##  2 Algeria        1960            148.           1243.            47.5      7.65\n##  3 Angola         1960            208              NA             36.0      7.32\n##  4 Antigua and …  1960             NA              NA             63.0      4.43\n##  5 Argentina      1960             59.9          5254.            65.4      3.11\n##  6 Armenia        1960             NA              NA             66.9      4.55\n##  7 Aruba          1960             NA              NA             65.7      4.82\n##  8 Australia      1960             20.3          9393.            70.9      3.45\n##  9 Austria        1960             37.3          7415.            68.8      2.7 \n## 10 Azerbaijan     1960             NA              NA             61.3      5.57\n## # ℹ 10,535 more rows\n## # ℹ 4 more variables: population <dbl>, gdp <dbl>, continent <fct>,\n## #   region <fct>\nmutate(mytib, gdp_per_capita = gdp/population, .before = infant_mortality)## # A tibble: 10,545 × 10\n##    country        year gdp_per_capita infant_mortality life_expectancy fertility\n##    <fct>         <int>          <dbl>            <dbl>           <dbl>     <dbl>\n##  1 Albania        1960            NA             115.             62.9      6.19\n##  2 Algeria        1960          1243.            148.             47.5      7.65\n##  3 Angola         1960            NA             208              36.0      7.32\n##  4 Antigua and …  1960            NA              NA              63.0      4.43\n##  5 Argentina      1960          5254.             59.9            65.4      3.11\n##  6 Armenia        1960            NA              NA              66.9      4.55\n##  7 Aruba          1960            NA              NA              65.7      4.82\n##  8 Australia      1960          9393.             20.3            70.9      3.45\n##  9 Austria        1960          7415.             37.3            68.8      2.7 \n## 10 Azerbaijan     1960            NA              NA              61.3      5.57\n## # ℹ 10,535 more rows\n## # ℹ 4 more variables: population <dbl>, gdp <dbl>, continent <fct>,\n## #   region <fct>\ntransmute(mytib, gdp_per_capita = gdp/population)## # A tibble: 10,545 × 1\n##    gdp_per_capita\n##             <dbl>\n##  1            NA \n##  2          1243.\n##  3            NA \n##  4            NA \n##  5          5254.\n##  6            NA \n##  7            NA \n##  8          9393.\n##  9          7415.\n## 10            NA \n## # ℹ 10,535 more rows"},{"path":"biến-đổi-và-sắp-xếp-dữ-liệu.html","id":"lựa-chọn-biến-bằng-hàm-select","chapter":"Chương 8 Biến đổi và sắp xếp dữ liệu","heading":"8.1.2 Lựa chọn biến bằng hàm select()","text":"Khi dữ liệu có quá nhiều biến trong khi chúng ta chỉ muốn sử dụng một số biến nhất định trong việc phân tích, chúng ta sử dụng hàm select() để lựa chọn các biến cần sử dụng trong các bước tiếp theo. Để biết chính xác tên các biến trong dữ liệu hiện có, chúng ta sử dụng hàm names():Lưu ý duy nhất khi sử dụng hàm select() là cần gọi đúng tên các biến mà chúng ta muốn lựa chọn. Ví dụ, trong số 9 biến hiện có trong mytib, chúng ta chỉ sử dụng ba biến có tên là year, gdp, và population, chúng ta sử dụng hàm select() như sau:Khi sử dụng hàm select() để lựa chọn biến, chúng ta cũng có thể thực hiện đổi tên biến trong cùng một câu lệnh. Ví dụ, bạn đọc muốn tên các biến mới tương ứng với các biến year, gdp, và population lần lượt là Year, Gdp và Population, chúng ta viết câu lệnh với hàm select() như sau:Dữ liệu mytib chỉ có 9 biến nên việc hiển thị và gọi đúng tên biến không gặp vấn đề. Tuy nhiên, khi dữ liệu có quá nhiều biến đồng thời tên các biến dài và khó nhớ thì việc gọi tên chính xác tất cả các biến trở nên khó khăn và câu lệnh select() sẽ trở nên phức tạp. Để hỗ trợ cho người dùng khi gặp những khó khăn như vậy, thư viện dplyr có các phương pháp lựa chọn biến mà không cần gọi chính xác tên biến. Ví dụ, chúng ta có thể sử dụng dấu : đứng giữa hai tên biến cụ thể để lựa chọn tất cả các biến nằm giữa hai biến mà chúng ta chỉ định.Câu lệnh trên có ý nghĩa là lựa chọn từ dữ liệu mytib biến year và tất cả các biến nằm giữa biến gdp và biến population.Ngoài ra, hàm select() còn cho phép bạn đọc lựa chọn các biến mà chúng ta không nhớ chính xác tên bằng cách sử dụng một trong các tham số được liệt kê dưới đây:Tham số starts_with() được sử dụng để lựa chọn các biến có tên bắt đầu bằng một chuỗi ký tự nào đó. Chúng ta chỉ cần viết đoạn ký tự đó trong hàm starts_with() đặt trong hàm select(). Ví dụ, chúng ta không nhớ chính xác tên của biến tỷ lệ tử vong của trẻ sơ sinh trong dữ liệu gapminder; chúng ta chỉ chắc chắn rằng tên biến được bắt đầu bằng infant. Tham số starts_with() được sử dụng trong trường hợp này như sau:Tương tự như tham số starts_with(), các tham số ends_with() hoặc contains() được sử dụng để lựa chọn các biến có tên kết thúc bằng một chuỗi ký tự hoặc có tên chứa một chuỗi ký tự nào đó.Tương tự như tham số starts_with(), các tham số ends_with() hoặc contains() được sử dụng để lựa chọn các biến có tên kết thúc bằng một chuỗi ký tự hoặc có tên chứa một chuỗi ký tự nào đó.Tham số matches() có thể được sử dụng trong trường hợp tổng quát hơn contains()khi các biến được lựa chọn có tên được biểu diễn qua các biểu thức chính quy.Tham số matches() có thể được sử dụng trong trường hợp tổng quát hơn contains()khi các biến được lựa chọn có tên được biểu diễn qua các biểu thức chính quy.Khi số lượng biến được lựa chọn nhiều hơn số biến không được lựa chọn, hoặc khi chúng ta muốn loại một số biến không sử dụng ra khỏi dữ liệu, hàm select() cũng cho phép chúng ta thực thi yêu cầu loại bỏ biến bằng cách thêm dấu - vào trước tên các biến mà chúng ta muốn loại ra khỏi dữ liệu. Ví dụ, khi chúng ta muốn lựa chọn tất cả các biến, ngoại trừ các biến có tên bắt đầu bằng chuỗi ký tự gdp, hàm select() được sử dụng như sau:","code":"\nnames(mytib) # Cho biết tên các biến trong mytib## [1] \"country\"          \"year\"             \"infant_mortality\" \"life_expectancy\" \n## [5] \"fertility\"        \"population\"       \"gdp\"              \"continent\"       \n## [9] \"region\"\nselect(mytib, year, gdp, population) # lựa chọn các cột year, gdp, population## # A tibble: 10,545 × 3\n##     year          gdp population\n##    <int>        <dbl>      <dbl>\n##  1  1960           NA    1636054\n##  2  1960  13828152297   11124892\n##  3  1960           NA    5270844\n##  4  1960           NA      54681\n##  5  1960 108322326649   20619075\n##  6  1960           NA    1867396\n##  7  1960           NA      54208\n##  8  1960  96677859364   10292328\n##  9  1960  52392699681    7065525\n## 10  1960           NA    3897889\n## # ℹ 10,535 more rows\nselect(mytib, Year = year, Gdp =  gdp,  Population = population)## # A tibble: 10,545 × 3\n##     Year          Gdp Population\n##    <int>        <dbl>      <dbl>\n##  1  1960           NA    1636054\n##  2  1960  13828152297   11124892\n##  3  1960           NA    5270844\n##  4  1960           NA      54681\n##  5  1960 108322326649   20619075\n##  6  1960           NA    1867396\n##  7  1960           NA      54208\n##  8  1960  96677859364   10292328\n##  9  1960  52392699681    7065525\n## 10  1960           NA    3897889\n## # ℹ 10,535 more rows\nselect(mytib, year, gdp:population)## # A tibble: 10,545 × 3\n##     year          gdp population\n##    <int>        <dbl>      <dbl>\n##  1  1960           NA    1636054\n##  2  1960  13828152297   11124892\n##  3  1960           NA    5270844\n##  4  1960           NA      54681\n##  5  1960 108322326649   20619075\n##  6  1960           NA    1867396\n##  7  1960           NA      54208\n##  8  1960  96677859364   10292328\n##  9  1960  52392699681    7065525\n## 10  1960           NA    3897889\n## # ℹ 10,535 more rows\n# Lựa chọn tất cả các biến có tên bắt đầu bằng infant\nselect(mytib, starts_with(\"infant\"))## # A tibble: 10,545 × 1\n##    infant_mortality\n##               <dbl>\n##  1            115. \n##  2            148. \n##  3            208  \n##  4             NA  \n##  5             59.9\n##  6             NA  \n##  7             NA  \n##  8             20.3\n##  9             37.3\n## 10             NA  \n## # ℹ 10,535 more rows\nmytib1<-mutate(mytib, gdp_per_capita = gdp/population)\n\n# Lựa chọn các biến có tên chứa \"gdp\"\nselect(mytib1, contains(\"gdp\")) ## # A tibble: 10,545 × 2\n##             gdp gdp_per_capita\n##           <dbl>          <dbl>\n##  1           NA            NA \n##  2  13828152297          1243.\n##  3           NA            NA \n##  4           NA            NA \n##  5 108322326649          5254.\n##  6           NA            NA \n##  7           NA            NA \n##  8  96677859364          9393.\n##  9  52392699681          7415.\n## 10           NA            NA \n## # ℹ 10,535 more rows\nselect(mytib1, - starts_with(\"gdp\"))## # A tibble: 10,545 × 8\n##    country  year infant_mortality life_expectancy fertility population continent\n##    <fct>   <int>            <dbl>           <dbl>     <dbl>      <dbl> <fct>    \n##  1 Albania  1960            115.             62.9      6.19    1636054 Europe   \n##  2 Algeria  1960            148.             47.5      7.65   11124892 Africa   \n##  3 Angola   1960            208              36.0      7.32    5270844 Africa   \n##  4 Antigu…  1960             NA              63.0      4.43      54681 Americas \n##  5 Argent…  1960             59.9            65.4      3.11   20619075 Americas \n##  6 Armenia  1960             NA              66.9      4.55    1867396 Asia     \n##  7 Aruba    1960             NA              65.7      4.82      54208 Americas \n##  8 Austra…  1960             20.3            70.9      3.45   10292328 Oceania  \n##  9 Austria  1960             37.3            68.8      2.7     7065525 Europe   \n## 10 Azerba…  1960             NA              61.3      5.57    3897889 Asia     \n## # ℹ 10,535 more rows\n## # ℹ 1 more variable: region <fct>"},{"path":"biến-đổi-và-sắp-xếp-dữ-liệu.html","id":"lọc-dữ-liệu-bằng-hàm-filter","chapter":"Chương 8 Biến đổi và sắp xếp dữ liệu","heading":"8.1.3 Lọc dữ liệu bằng hàm filter()","text":"Hàm filter() trong thư viện dplyr cho phép chúng ta lọc ra các quan sát có giá trị của một hoặc một vài biến thỏa mãn các điều kiện nào đó. Như chúng tôi đã đề cập ở trên, có một số thư viện khác trong R sử dụng hàm filter() với mục đích khác và chúng ta có thể không chắc chắn về thứ tự ưu tiên của các thư viện đang sẵn sàng trên môi trường làm việc hiện tại. Để đảm bảo gọi đúng hàm filter() trong thư viện dplyr, chúng ta định nghĩa lại hàm filter() như sauCách hoạt động của hàm filter() khá đơn giản. Ví dụ như chúng ta muốn lấy ra dữ liệu của năm 2010 từ dữ liệu gapminder, hàm filter() được sử dụng để lọc ra giá trị 2010 trong biến year như sau:Sau khi thực thi câu lệnh như trên, một tibble mới được tạo thành bao gồm các quan sát với giá trị cột year bằng 2010. Lưu ý rằng nếu chúng ta muốn lưu lại giá trị sau mỗi lần thực hiện biến đổi dữ liệu, hãy gán kết quả vào một dữ liệu mới.Hàm filter() có thể thực hiện việc lọc dữ liệu trên nhiều biến trong cùng một câu lệnh. Ví dụ, chúng ta muốn lọc ra các quan sát của năm 2010 của các quốc gia thuộc lục địa Châu Âu, có thể sử dụng hai phép sánh trong hàm filter() cùng lúc:","code":"\nfilter<-function(...) dplyr::filter(...)\nfilter(mytib, year == 2010) # Chỉ lọc ra các quan sát có year bằng 2010## # A tibble: 185 × 9\n##    country   year infant_mortality life_expectancy fertility population      gdp\n##    <fct>    <int>            <dbl>           <dbl>     <dbl>      <dbl>    <dbl>\n##  1 Albania   2010             14.8            77.2      1.74    2901883  6.14e 9\n##  2 Algeria   2010             23.5            76        2.82   36036159  7.92e10\n##  3 Angola    2010            110.             57.6      6.22   21219954  2.61e10\n##  4 Antigua…  2010              7.7            75.8      2.13      87233  8.37e 8\n##  5 Argenti…  2010             13              75.8      2.22   41222875  4.34e11\n##  6 Armenia   2010             16.1            73        1.55    2963496  4.10e 9\n##  7 Aruba     2010             NA              75.1      1.7      101597 NA      \n##  8 Austral…  2010              4.1            82        1.89   22162863  5.63e11\n##  9 Austria   2010              3.6            80.5      1.44    8391986  2.24e11\n## 10 Azerbai…  2010             33.9            70.1      1.97    9099893  2.12e10\n## # ℹ 175 more rows\n## # ℹ 2 more variables: continent <fct>, region <fct>\nfilter(mytib, year == 2010, continent == \"Europe\")## # A tibble: 39 × 9\n##    country    year infant_mortality life_expectancy fertility population     gdp\n##    <fct>     <int>            <dbl>           <dbl>     <dbl>      <dbl>   <dbl>\n##  1 Albania    2010             14.8            77.2      1.74    2901883 6.14e 9\n##  2 Austria    2010              3.6            80.5      1.44    8391986 2.24e11\n##  3 Belarus    2010              4.7            70.2      1.46    9492122 2.60e10\n##  4 Belgium    2010              3.6            80.1      1.84   10929978 2.67e11\n##  5 Bosnia a…  2010              6.4            77.9      1.24    3835258 8.21e 9\n##  6 Bulgaria   2010             11.2            73.7      1.49    7407297 1.92e10\n##  7 Croatia    2010              4.6            76.7      1.47    4316425 2.80e10\n##  8 Czech Re…  2010              3.4            77.5      1.5    10506617 8.21e10\n##  9 Denmark    2010              3.3            79.4      1.88    5550959 1.69e11\n## 10 Estonia    2010              3.6            76.4      1.63    1332089 8.01e 9\n## # ℹ 29 more rows\n## # ℹ 2 more variables: continent <fct>, region <fct>"},{"path":"biến-đổi-và-sắp-xếp-dữ-liệu.html","id":"sắp-xếp-dữ-liệu-bằng-hàm-arrange","chapter":"Chương 8 Biến đổi và sắp xếp dữ liệu","heading":"8.1.4 Sắp xếp dữ liệu bằng hàm arrange()","text":"Hàm arrange() tạo thành dữ liệu mới thu được bằng cách sắp xếp các quan sát của dữ liệu ban đầu theo thứ tự tăng dần hoặc giảm dần dựa trên giá trị của một hoặc nhiều biến. Nguyên tắc sắp xếp cũng tương tự như quy tắc sắp xếp của hàm sort khi sắp xếp trên véc-tơ, nghĩa là hàm bạn đọc có thể sắp xếp dữ liệu không chỉ theo biến kiểu numeric hay kiểu thời gian, mà còn có thể sắp xếp theo biến kiểu chuỗi ký tự. Lưu ý rằng khi véc-tơ kiểu chuỗi ký tự được định nghĩa dưới dạng factor thì thứ tự tăng dần sẽ được hiểu theo nghĩa là các level của biến kiểu factor tăng dần.Chúng ta sắp xếp dữ liệu gapmider theo thứ tự tăng dần cuả biến year như sau:Để sắp xếp dữ liệu theo thứ tự giảm dần của một biến kiểu số hoặc kiểu thời gian, bạn đọc chỉ cần thêm dấu - vào phía trước tên biến. Tuy nhiên, R sẽ báo lỗi nếu chúng ta thêm dấu - vào trước tên một biến kiểu chuỗi ký tự. Để không xảy ra lỗi này khi sắp xếp dữ liệu theo thứ tự giảm dần của một biến kiểu chuỗi ký tự, chúng ta sử dụng hàm desc() cho tên biến:Hàm desc() có thể sử dụng trên tất cả các kiểu biến khi chúng ta muốn sắp xếp dữ liệu theo thứ tự giá trị trong biến giảm dần. Thêm vào đó, việc sắp xếp dữ liệu sử dụng hàm arrange() có thể được thực hiện dựa trên nhiều biến cùng lúc. Ví dụ, để sắp xếp dữ liệu gapminder theo thứ tự tăng dần theo năm (year), theo Châu lục (continent), và theo vùng (region), bạn đọc viết câu lệnh như sau:Lưu ý rằng thứ tự sắp xếp cũng có thể là tăng theo một biến và giảm theo các biến khác:Khi trong biến dữ liệu sử dụng để sắp xếp có chứa giá trị không quan sát được thì các giá trị này luôn được sắp xếp xuống phía dưới của dữ liệu, bất kể chúng ta sắp xếp dữ liệu theo thứ tự tăng dần hay giảm dần. Thật vậy, cột gdp của dữ liệu gapminder có tỷ lệ giá trị NA khá cao. Khi quan sát phần đuôi của dữ liệu sau khi được sắp xếp theo biến gdp, chúng ta sẽ luôn thấy các giá trị NA được sắp xếp xuống phía dưới:Bạn đọc có thể thấy rằng phía đuôi của kết quả từ phép sắp xếp dữ liệu không thay đổi dù chúng ta có thực hiện sắp xếp theo thứ tự biến gdp tăng dần hay giảm dần. Nguyên nhân là hàm sắp xếp luôn chuyển các giá trị không quan sát được xuống đuôi của kết quả.","code":"\narrange(mytib, year)## # A tibble: 10,545 × 9\n##    country   year infant_mortality life_expectancy fertility population      gdp\n##    <fct>    <int>            <dbl>           <dbl>     <dbl>      <dbl>    <dbl>\n##  1 Albania   1960            115.             62.9      6.19    1636054 NA      \n##  2 Algeria   1960            148.             47.5      7.65   11124892  1.38e10\n##  3 Angola    1960            208              36.0      7.32    5270844 NA      \n##  4 Antigua…  1960             NA              63.0      4.43      54681 NA      \n##  5 Argenti…  1960             59.9            65.4      3.11   20619075  1.08e11\n##  6 Armenia   1960             NA              66.9      4.55    1867396 NA      \n##  7 Aruba     1960             NA              65.7      4.82      54208 NA      \n##  8 Austral…  1960             20.3            70.9      3.45   10292328  9.67e10\n##  9 Austria   1960             37.3            68.8      2.7     7065525  5.24e10\n## 10 Azerbai…  1960             NA              61.3      5.57    3897889 NA      \n## # ℹ 10,535 more rows\n## # ℹ 2 more variables: continent <fct>, region <fct>\n# Sắp xếp dữ liệu theo thứ tự giảm dần theo biến region\narrange(mytib, desc(region))## # A tibble: 10,545 × 9\n##    country   year infant_mortality life_expectancy fertility population      gdp\n##    <fct>    <int>            <dbl>           <dbl>     <dbl>      <dbl>    <dbl>\n##  1 Austria   1960             37.3            68.8      2.7     7065525  5.24e10\n##  2 Belgium   1960             29.5            69.6      2.6     9140563  6.82e10\n##  3 France    1960             23.7            70.5      2.77   45865699  3.50e11\n##  4 Germany   1960             34              69.3      2.41   73179665 NA      \n##  5 Luxembo…  1960             33              69.0      2.35     314586  4.30e 9\n##  6 Netherl…  1960             16.4            73.4      3.12   11418652  9.84e10\n##  7 Switzer…  1960             21.6            71.5      2.52    5296120 NA      \n##  8 Austria   1961             35              69.7      2.79    7105654  5.53e10\n##  9 Belgium   1961             28.1            70.5      2.63    9200393  7.16e10\n## 10 France    1961             22.4            71.1      2.8    46471083  3.69e11\n## # ℹ 10,535 more rows\n## # ℹ 2 more variables: continent <fct>, region <fct>\n# Sắp xếp dữ liệu theo thứ tự tăng dần theo 3 biến\narrange(mytib, year, continent, region)## # A tibble: 10,545 × 9\n##    country    year infant_mortality life_expectancy fertility population     gdp\n##    <fct>     <int>            <dbl>           <dbl>     <dbl>      <dbl>   <dbl>\n##  1 Burundi    1960            145.             40.6      6.95    2786740  3.41e8\n##  2 Comoros    1960            200              44.0      6.79     188732 NA     \n##  3 Djibouti   1960             NA              45.8      6.46      83636 NA     \n##  4 Eritrea    1960             NA              39.0      6.9     1407631 NA     \n##  5 Ethiopia   1960            162              37.7      6.88   22151218 NA     \n##  6 Kenya      1960            119.             47.4      7.95    8105440  2.12e9\n##  7 Madagasc…  1960            112              42.0      7.3     5099371  2.09e9\n##  8 Malawi     1960            218.             38.5      6.91    3618604  3.48e8\n##  9 Mauritius  1960             67.8            58.7      6.17     660023 NA     \n## 10 Mozambiq…  1960            183              38.2      6.6     7493278 NA     \n## # ℹ 10,535 more rows\n## # ℹ 2 more variables: continent <fct>, region <fct>\n# Sắp xếp tăng dần theo năm, giảm dần theo continent, region, và gdp\narrange(mytib, year, desc(continent), desc(region), -gdp) ## # A tibble: 10,545 × 9\n##    country    year infant_mortality life_expectancy fertility population     gdp\n##    <fct>     <int>            <dbl>           <dbl>     <dbl>      <dbl>   <dbl>\n##  1 French P…  1960              NA             56.3      5.66      78083 NA     \n##  2 Samoa      1960              92             51.4      7.65     108645 NA     \n##  3 Tonga      1960              NA             61.2      7.36      61600 NA     \n##  4 Kiribati   1960              NA             45.8      6.95      41234 NA     \n##  5 Micrones…  1960              NA             56.8      6.93      44539 NA     \n##  6 Papua Ne…  1960             135.            38.6      6.28    1966957  8.37e8\n##  7 Fiji       1960              54             55.7      6.46     393383  4.37e8\n##  8 New Cale…  1960              NA             56.4      5.22      78058 NA     \n##  9 Solomon …  1960             132.            50.6      6.39     117869 NA     \n## 10 Vanuatu    1960             107.            46.0      7.2       63701 NA     \n## # ℹ 10,535 more rows\n## # ℹ 2 more variables: continent <fct>, region <fct>\ntail(arrange(mytib, gdp))## # A tibble: 6 × 9\n##   country       year infant_mortality life_expectancy fertility population   gdp\n##   <fct>        <int>            <dbl>           <dbl>     <dbl>      <dbl> <dbl>\n## 1 Venezuela     2016               NA            74.8        NA         NA    NA\n## 2 West Bank a…  2016               NA            74.7        NA         NA    NA\n## 3 Vietnam       2016               NA            75.6        NA         NA    NA\n## 4 Yemen         2016               NA            64.9        NA         NA    NA\n## 5 Zambia        2016               NA            57.1        NA         NA    NA\n## 6 Zimbabwe      2016               NA            61.7        NA         NA    NA\n## # ℹ 2 more variables: continent <fct>, region <fct>\ntail(arrange(mytib, desc(gdp)))## # A tibble: 6 × 9\n##   country       year infant_mortality life_expectancy fertility population   gdp\n##   <fct>        <int>            <dbl>           <dbl>     <dbl>      <dbl> <dbl>\n## 1 Venezuela     2016               NA            74.8        NA         NA    NA\n## 2 West Bank a…  2016               NA            74.7        NA         NA    NA\n## 3 Vietnam       2016               NA            75.6        NA         NA    NA\n## 4 Yemen         2016               NA            64.9        NA         NA    NA\n## 5 Zambia        2016               NA            57.1        NA         NA    NA\n## 6 Zimbabwe      2016               NA            61.7        NA         NA    NA\n## # ℹ 2 more variables: continent <fct>, region <fct>"},{"path":"biến-đổi-và-sắp-xếp-dữ-liệu.html","id":"kết-hợp-các-phép-biến-đổi-bằng-toán-tử-pipe","chapter":"Chương 8 Biến đổi và sắp xếp dữ liệu","heading":"8.1.5 Kết hợp các phép biến đổi bằng toán tử pipe (%>%)","text":"Trước khi giới thiệu về các hàm số khác sử dụng để biến đổi dữ liệu trong thư viện dplyr, chúng tôi giới thiệu đến bạn đọc một công cụ hiệu quả để kết nối các phép biến đổi, đó là toán tử pipe (%>%). Toán tử pipe có thể sử dụng khi chúng ta thực hiện một chuỗi các phép biến đổi trên một dữ liệu mà không cần phải lặp lại việc gọi tên dữ liệu đó. Thuật ngữ toán tử pipe được mượn từ toán học khi nói đến việc sử dụng các hàm số nối tiếp nhau. Sử dụng toán tử pipe trong biến đổi dữ liệu giúp cho việc kết nối các câu lệnh gọi các hàm số trở nên đơn giản và không bị nhầm lẫn.Thật vậy, khi chúng ta muốn thực hiện một phân tích trên dữ liệu gapminder để trả lời câu hỏi: Ba quốc gia có thu nhập bình quân đầu người cao nhất năm 2000 là ba quốc gia nào?. Để trả lời câu hỏi này, bạn đọc sẽ cần thực hiện các phép biến đổi dữ liệu theo thứ tự như sau:Thứ nhất: tính toán thêm cột thu nhập bình quân đầu người, sử dụng hàm mutate().Thứ hai: lọc dữ liệu theo năm, chỉ lấy dữ liệu của năm 2000, sử dụng hàm filter().Thứ ba: lựa chọn cột tên quốc gia (biến country) và biến thu nhập bình quân đầu người vừa tính toán, sử dụng hàm select().Thứ tư: sắp xếp dữ liệu theo cột thu nhập bình quân đầu người, thứ tự sắp xếp là giảm dần, sử dụng hàm arrange().Thứ năm: lấy ra ba hàng đầu tiên của dữ liệu sau khi sắp xếp, sử dụng hàm head().Nếu viết các câu lệnh một cách thông thường, sau mỗi bước ở trên, bạn đọc sẽ phải lưu kết quả và gọi lại kết quả vào bước kế tiếp như sau:Khi viết các câu lệnh như trên sẽ gặp khó khăn và dễ bị nhầm lần chúng ta phải liên tục lưu kết quả của từng câu lệnh và gọi lại kết quả đó trong câu lệnh tiếp theo. Để tránh gặp phải vấn đề này, toán tử pipe, ký hiệu %>%, có thể giúp chúng ta kết nối các câu lệnh lại với nhau trong một câu lệnh duy nhất. Cùng với một yêu cầu như ở trên, cách viết các câu lệnh biến đổi dữ liệu sử dụng toán tử pipe như sau:Bạn đọc có thể thấy rằng kết quả thu được từ hai đoạn câu lệnh là hoàn toàn giống nhau. Cách viết câu lệnh sử dụng toán tử pipe có ưu điểm là rõ ràng, ngắn gọn, và không gây nhầm lẫn. Để bạn đọc làm quen với cách viết câu lệnh sử dụng %>%, từ phần này của cuốn sách, mọi phép biến đổi trên dữ liệu đều được ưu tiên sử dụng cách viết này.Chúng ta sẽ tiếp tục làm quen với các hàm số sử dụng để biến đổi dữ liệu của thư viện dplyr trong các phần tiếp theo.","code":"\n# Bước thứ nhất\nmytib1<-mutate(mytib,gdp_per_capita = gdp/population)\n\n# Bước thứ hai\nmytib1<-filter(mytib1, year == 2010) \n\n# Bước thứ ba\nmytib1<-select(mytib1, country, gdp_per_capita) \n\n# Bước thứ tư\nmytib1<-arrange(mytib1, desc(gdp_per_capita)) \n\n# Bước thứ năm\nhead(mytib1,3) ## # A tibble: 3 × 2\n##   country    gdp_per_capita\n##   <fct>               <dbl>\n## 1 Luxembourg         52210.\n## 2 Japan              40013.\n## 3 Norway             39954.\nmytib%>%\n  \n  # Bước thứ nhất\n  mutate(gdp_per_capita = gdp/population)%>%\n  \n  # Bước thứ hai\n  filter(year == 2010)%>%\n  \n  # Bước thứ ba\n  select(country, gdp_per_capita)%>%\n  \n  # Bước thứ tư\n  arrange(desc(gdp_per_capita)) %>%\n  \n  # Bước thứ năm\n  head(3)## # A tibble: 3 × 2\n##   country    gdp_per_capita\n##   <fct>               <dbl>\n## 1 Luxembourg         52210.\n## 2 Japan              40013.\n## 3 Norway             39954."},{"path":"biến-đổi-và-sắp-xếp-dữ-liệu.html","id":"tổng-hợp-dữ-liệu-bằng-summarise-và-group_by","chapter":"Chương 8 Biến đổi và sắp xếp dữ liệu","heading":"8.1.6 Tổng hợp dữ liệu bằng summarise() và group_by()","text":"Để biết thông tin về thu nhập quốc nội của một quốc gia, chúng ta có thể sử dụng hàm filter() để lọc ra một quan sát và không cần thực hiện thêm tính toán. Câu lệnh dưới đây cho biết thu nhập bình quân đầu người của Việt Nam năm 2000.Khi muốn sánh thu nhập bình quân đầu người của Việt Nam với thu nhập bình quân đầu người của vùng Đông Nam Á trong năm 2000, chúng ta cần lọc dữ liệu theo vùng South-Eastern Asia và năm 2000, loại bỏ các quan sát có giá trị NA, sau đó tính tổng giá trị của biến gdp chia cho tổng giá trị của biến population. Các phép tính toán được thực hiện trên nhiều quan sát cùng một lúc như vậy, với mục tiêu trả lại kết quả là một giá trị đặc trưng của các biến như tổng, giá trị trung bình, trung vị, phương sai,…, được gọi là các phép toán tổng hợp dữ liệu.Hàm summarise() trong thư viện dplyr là một công cụ hiệu quả để thực hiện các phép tính toán tổng hợp dữ liệu như vậy. Để thực hiện phép việc tính thu nhập bình quân đầu người của các quốc gia Đông Nam Á trong năm 2000, chúng ta viết câu lệnh với summarise() như sau:Bạn đọc có thể thấy trong dữ liệu kết quả từ hàm summarise() có bốn biến được tạo thành cũng là bốn biến được khai báo trong hàm số đó. Cách tính toán giá trị của các biến cũng được khai báo ngay bên trong hàm summarise(). Thu nhập bình quân đầu người của các quốc gia Đông Nam Á trong năm 2000 là $ 1257 trong khi thu nhập bình quân đầu người của Việt Nam trong năm 2000 chỉ là $388.Bên trong hàm summarise() chúng ta có thể sử dụng bất kỳ hàm số nào có thể thực thi trên các véc-tơ để tổng hợp giá trị của véc-tơ đó. Ngoài hàm sum() thực hiện để tính tổng, các hàm thông dụng khác có thể sử dụng như:Hàm length() cho biết có bao nhiêu giá trị trong véc-tơ.Hàm mean() cho biết giá trị trung bình của véc-tơ kiểu số.Hàm var() hàm sd() cho biết phương sai và độ lệch chuẩn của véc-tơ kiểu số.Hàm min(), max(), quantile() cho biết các giá trị nhỏ nhất, lớn nhất, các giá trị quantile của véc-tơ kiểu số.Tất cả các hàm có đầu vào là véc-tơ mà bạn đọc tự định nghĩa.Hàm summarise() kết hợp với hàm filter() có thể được sử dụng để tính toán thu nhập bình quân đầu người của các vùng, các Châu lục giống như cách chúng ta tính toán với vùng Đông Nam Á. Tuy nhiên, có thể thấy rằng, để có được giá trị là thu nhập bình quân đầu người của nhiều vùng, hoặc tất cả các châu lục, chúng ta cần thực hiện nhiều lần các câu lệnh như trên và tập hợp kết quả lại trong một dữ liệu duy nhất.Một phương pháp hiệu quả hơn để thực hiện việc này đó là sử dụng hàm group_by() thay thế cho hàm filter(). Hàm số này cho phép chúng ta thực hiện các tính toán theo nhóm được định nghĩa theo các giá trị của một hoặc một vài biến rời rạc. Ví dụ để tính thu nhập bình quân đầu người của tất cả các vùng trên thế giới trong năm 2000 sử dụng dữ liệu gapminder, chúng ta sẽ dùng hàm group_by() để nhóm dữ liệu theo biến gapminder này, sau đó gọi hàm summarise() để tổng hợp dữ liệu cho từng nhóm. Câu lệnh được viết như sau:Có thể thấy trong dữ liệu kết quả từ hàm summarise() có 22 quan sát tương ứng với 22 giá trị khác nhau của region và có 4 biến. Biến đầu tiên là biến region chứa tên 22 vùng; ba biến còn lại được khai báo và tính toán trong hàm summarise(). Quan trọng là khi được gọi bên trong hàm summarise(), các hàm số thực hiện tính toán véc-tơ như hàm sum() chỉ thực thi trên véc-tơ tương ứng với từng vùng, chứ không tính toán trên toàn bộ các quan sát. Bạn đọc có thể thấy trong kết quả các vùng có thu nhập bình quân đầu người ở mức cao hơn 20 nghìn USD là vùng Australia và New Zealand trong khi có những vùng chỉ có thu nhập bình quân đầu người khoảng 271 USD là vùng Đông Phi.Hàm group_by() được sử dụng với một hoặc một vài biến rời rạc và các hàm số được gọi sau hàm số này đều được tính toán theo nhóm thay vì tính toán theo từng quan sát riêng lẻ. Nếu bạn đọc muốn giữ nguyên mỗi quan sát tương ứng với một quốc gia và tính toán thu nhập bình quân đầu người theo vùng tương ứng với quốc gia đó, chúng ta sử dụng hàm transmute() thay thế cho summarise()Dữ liệu mới được tạo thành có 178 quan sát, mỗi mỗi quan sát tương ứng với một quốc gia trong năm 2000. Dữ liệu có 6 biến, trong đó biến region được gọi trong hàm group_by(), năm biến còn lại được khai báo và tính toán trong hàm transmute(). Khi tính toán 5 biến, các biến giữ nguyên giá trị khi chúng ta gán giá trị cho chính biến đó. Với các biến được tính toán dựa trên nhiều quan sát, như biến region_gdp_per_capita trong câu lệnh ở trên, chúng ta sử dụng các hàm số tính toán trên véc-tơ. Nếu bạn đọc muốn giữ nguyên các biến của dữ liệu ban đầu, có thể sử dụng mutate() thay thế cho transmute().Hàm group_by() có thể được sử dụng trên nhiều biến rời rạc cùng một lúc, hoặc có thể được gọi nhiều lần trong một câu lệnh nếu chúng ta muốn nhóm dữ liệu theo các cách khác nhau. Ví dụ, chúng ta muốn thêm vào dữ liệu của năm 2000 các cột chứa giá trị là thu nhập bình quân đầu người theo vùng, theo châu lục, và trên toàn thế giới, chúng ta nhóm dữ liệu lần lượt theo các biến region, continent, và nhóm tất cả dữ liệu, để tính toán trung bình theo nhóm:Bạn đọc có thể thấy trong dữ liệu được tạo thành có các biến region_gdp_per_capita, continent_gdp_per_capita, và global_gdp_per_capita là các biến được tính toán theo các nhóm được quy định bằng các hàm group_by(). Sau khi gọi hàm group_by() và thực hiện tính toán, chúng tôi đã sử dụng hàm ungroup() để trả lại dữ liệu về trạng thái ban đầu. Dữ liệu kết quả đã cho chúng ta một cái nhìn tổng quan hơn về thu nhập bình quân đầu người của tất cả các quốc gia, các vùng lãnh thổ, và các châu lục trên toàn thế giới trong năm 2000. Quan sát đầu tiên tương ứng với Australia, là một quốc gia phát triển, có thu nhập bình quân đầu người khoảng 21 nghìn USD, cao hơn một chút với vùng Australia New Zealand, cao hơn với thu nhập bình quân của Châu đại dương là khoảng 15.5 nghìn USD, và gấp khoảng 4 lần thu nhập bình quân đầu người trên thế giới.Có thể sử dụng nhiều biến rời rạc cùng một lúc trong hàm group_by(). Ví dụ, bạn đọc quan tâm đến sự thay đổi của thu nhập bình quân đầu người của các Châu lục trên thế giới trong các năm 1990, 2000, và 2010. Để thực hiện được tính toán, chúng ta cần nhóm dữ liệu đồng thời theo các biến continent và year như sau:Dữ liệu mới được tạo thành có ba biến, trong đó hai biến đầu tiên được gọi trong hàm group_by() và biến thứ ba có tên continent_gdp_per_capita được tính toán trong hàm summarise().","code":"\nmytib %>% \n  filter(country == \"Vietnam\", year == 2000) %>%\n  mutate(gdp_per_capita = gdp/population) %>%\n  select(country, gdp_per_capita)## # A tibble: 1 × 2\n##   country gdp_per_capita\n##   <fct>            <dbl>\n## 1 Vietnam           388.\nmytib %>% \n  filter(region == \"South-Eastern Asia\", year == 2000) %>%\n  drop_na() %>%\n  summarise(region = \"South-Eastern Asia\",\n            region_gdp = sum(gdp),\n            region_population = sum(population),\n            region_gdp_per_capita = region_gdp/region_population\n            )## # A tibble: 1 × 4\n##   region               region_gdp region_population region_gdp_per_capita\n##   <chr>                     <dbl>             <dbl>                 <dbl>\n## 1 South-Eastern Asia 601360044405         478509017                 1257.\nmytib%>% \n  # Lọc dữ liệu theo năm 2000\n  filter(year == 2000) %>% \n  \n  # Lấy ra 3 cột region, gdp, và population \n  select(region, gdp, population) %>% drop_na() %>%\n  \n  # Nhóm dữ liệu theo biến region\n  group_by(region) %>% \n  \n  # Tổng hợp dữ liệu theo các nhóm\n  summarise(region_gdp = sum(gdp),\n            region_population = sum(population),\n            region_gdp_per_capita = region_gdp/region_population\n            )## # A tibble: 22 × 4\n##    region                    region_gdp region_population region_gdp_per_capita\n##    <fct>                          <dbl>             <dbl>                 <dbl>\n##  1 Australia and New Zealand    4.68e11          22965485                20400.\n##  2 Caribbean                    1.50e11          36996369                 4062.\n##  3 Central America              6.53e11         138780471                 4707.\n##  4 Central Asia                 3.72e10          55117412                  675.\n##  5 Eastern Africa               6.62e10         244407086                  271.\n##  6 Eastern Asia                 6.64e12        1451508364                 4574.\n##  7 Eastern Europe               6.60e11         303788505                 2173.\n##  8 Melanesia                    8.59e 9           6992665                 1229.\n##  9 Micronesia                   3.02e 8            191836                 1573.\n## 10 Middle Africa                3.54e10          95976097                  369.\n## # ℹ 12 more rows\nmytib%>% \n  # Lọc dữ liệu theo năm 2000\n  filter(year == 2000) %>% \n  \n  # Loại bỏ các quan sát có NA\n  drop_na() %>%\n  \n  # Nhóm dữ liệu theo biến region\n  group_by(region) %>% \n  \n  # Tạo thành các cột mới\n  transmute(country = country,\n            gdp = gdp,\n            population = population,\n            gdp_per_capita = gdp/population,\n            region_gdp_per_capita = sum(gdp)/sum(population)\n            )## # A tibble: 178 × 6\n## # Groups:   region [22]\n##    region        country     gdp population gdp_per_capita region_gdp_per_capita\n##    <fct>         <fct>     <dbl>      <dbl>          <dbl>                 <dbl>\n##  1 Southern Eur… Albania 3.69e 9    3121965          1181.                13740.\n##  2 Northern Afr… Algeria 5.48e10   31183658          1757.                 1512.\n##  3 Middle Africa Angola  9.13e 9   15058638           606.                  369.\n##  4 Caribbean     Antigu… 8.03e 8      77648         10335.                 2619.\n##  5 South America Argent… 2.84e11   37057453          7669.                 3806.\n##  6 Western Asia  Armenia 1.91e 9    3076098           621.                 4713.\n##  7 Australia an… Austra… 4.17e11   19107251         21818.                20400.\n##  8 Western Euro… Austria 1.92e11    8050884         23857.                23445.\n##  9 Western Asia  Azerba… 5.27e 9    8117742           650.                 4713.\n## 10 Caribbean     Bahamas 6.33e 9     297891         21241.                 2619.\n## # ℹ 168 more rows\nmytib%>% \n  # Lọc dữ liệu theo năm 2000\n  filter(year == 2000) %>% \n  \n  # Lấy ra 5 cột, và loại bỏ quan sát có NA, thêm biến gdp_per_capita\n  select(country, continent, region, gdp, population) %>%\n  drop_na() %>%\n  mutate(gdp_per_capita = gdp/population) %>%\n  \n  # Nhóm dữ liệu theo biến region và thêm cột region_gdp_per_capita\n  group_by(region) %>%  \n  mutate(region_gdp_per_capita = sum(gdp)/sum(population)) %>% \n  ungroup() %>% # Bỏ nhóm theo reigon\n  \n  # Nhóm dữ liệu theo biến continent và thêm cột continent_gdp_per_capita\n  group_by(continent) %>%  \n  mutate(continent_gdp_per_capita = sum(gdp)/sum(population)) %>% \n  ungroup() %>% # Bỏ nhóm theo continent\n  \n  # Nhóm dữ liệu lại thành một nhóm duy nhất và thêm cột global_gdp_per_capita\n  group_by() %>%  \n  mutate(global_gdp_per_capita = sum(gdp)/sum(population)) %>% \n  ungroup() %>% # Bỏ nhóm\n  \n  # Sắp xếp lại theo biến continent\n  select(-gdp,-population) %>%\n  arrange(desc(continent)) ## # A tibble: 185 × 7\n##    country               continent region   gdp_per_capita region_gdp_per_capita\n##    <fct>                 <fct>     <fct>             <dbl>                 <dbl>\n##  1 Australia             Oceania   Austral…         21818.                20400.\n##  2 Fiji                  Oceania   Melanes…          2076.                 1229.\n##  3 French Polynesia      Oceania   Polynes…         14530.                 7615.\n##  4 Kiribati              Oceania   Microne…           808.                 1573.\n##  5 Micronesia, Fed. Sts. Oceania   Microne…          2175.                 1573.\n##  6 New Caledonia         Oceania   Melanes…         12773.                 1229.\n##  7 New Zealand           Oceania   Austral…         13374.                20400.\n##  8 Papua New Guinea      Oceania   Melanes…           655.                 1229.\n##  9 Samoa                 Oceania   Polynes…          1407.                 7615.\n## 10 Solomon Islands       Oceania   Melanes…          1055.                 1229.\n## # ℹ 175 more rows\n## # ℹ 2 more variables: continent_gdp_per_capita <dbl>,\n## #   global_gdp_per_capita <dbl>\nmytib%>% \n  # Lọc dữ liệu theo các năm 1990, 2000, 2010 và loại bỏ NA\n  filter(year %in% c(1990, 2000, 2010)) %>% \n  drop_na() %>%\n  \n  # Lấy ra 4 biến \n  select(continent, year, population, gdp) %>%\n  \n  # Nhóm dữ liệu theo biến region và biến year\n  group_by(continent, year) %>%  \n  \n  # Tạo thành dữ liệu mới, mỗi quan sát là 1 Châu lục trong 1 năm\n  summarise(continent_gdp_per_capita = sum(gdp)/sum(population))## # A tibble: 15 × 3\n## # Groups:   continent [5]\n##    continent  year continent_gdp_per_capita\n##    <fct>     <int>                    <dbl>\n##  1 Africa     1990                     699.\n##  2 Africa     2000                     738.\n##  3 Africa     2010                     867.\n##  4 Americas   1990                   12740.\n##  5 Americas   2000                   15201.\n##  6 Americas   2010                   16302.\n##  7 Asia       1990                    2074.\n##  8 Asia       2000                    2418.\n##  9 Asia       2010                    3215.\n## 10 Europe     1990                   10695.\n## 11 Europe     2000                   12742.\n## 12 Europe     2010                   14643.\n## 13 Oceania    1990                   13027.\n## 14 Oceania    2000                   15726.\n## 15 Oceania    2010                   17942."},{"path":"biến-đổi-và-sắp-xếp-dữ-liệu.html","id":"sắp-xếp-dữ-liệu-bằng-thư-viện-tidyr","chapter":"Chương 8 Biến đổi và sắp xếp dữ liệu","heading":"8.2 Sắp xếp dữ liệu bằng thư viện tidyr","text":"Dữ liệu trước khi được đưa vào bước trực quan hóa hoặc xây dựng mô hình cần phải được sắp xếp và trình bày một cách nhất quán. Hầu hết các dữ liệu có sẵn trong R, hoặc trong thư viện của R, nhìn chung đều đã được sắp xếp hoàn chỉnh nên chúng ta không gặp phải vấn đề này trong các ví dụ ở trên. Tuy nhiên, dữ liệu lấy từ các nguồn bên ngoài thường gặp phải các vấn đề về sự thiếu nhất quán. Hãy quan sát các ví dụ sau, các dữ liệu cùng thể hiện thông tin về tổng thu nhập quốc dân và dân số của các quốc gia trên thế giới từ năm 2000 đến 2010 nhưng lại được trình bày khác nhau:Tất cả dữ liệu kể trên đều trình bày chung một nguồn thông tin nhưng chỉ có mytib1 là được sắp xếp một cách nhất quán. Trong khi các dữ liệu khác chưa sẵn sàng để thực hiện các bước tiếp theo. Thật vậy:Dữ liệu lưu trong mytib2 có biến gdp_per_capita trình bày dưới dạng chuỗi ký tự, với số tổng thu nhập quốc dân và dân số được lưu trong cùng một cột và cách nhau bởi dấu /. Không thể thực hiện các phân tích hay tính toán với cột dữ liệu như vậy.Dữ liệu lưu trong mytib3 có biến type cho biết số tương ứng trong cột value là giá trị của tổng thu nhập quốc dân hay dân số của nước đó. Giá trị trong cột value không nhất quán vì vừa có thể là số tiền, vừa là số người.Dữ liệu lưu trong mytib4 cũng là một điển hình cho dữ liệu chưa được sắp xếp một cách nhất quán. Giá trị của biến year không được lưu dưới dạng véc-tơ cột mà được lưu dưới dạng tên các cột nên rất khó khăn trong thực hiện tính toán.Một dữ liệu được sắp xếp nhất quán cần phải đảm bảo hai yêu cầu:Thứ nhất: mỗi quan sát là một dòng dữ liệu.Thứ hai: mỗi biến nằm ở một cột. Một biến mô tả một thuộc tính, một tính chất của quan sát tương ứng. Giá trị trong cột phải đồng nhất và đúng định dạng.Lý chính khiến dữ liệu không được sắp xếp nhất quán là dữ liệu thường được tổ chức để thuận lợi cho một số mục tiêu khác với mục tiêu phân tích, chẳng hạn như mục tiêu nhập dữ liệu hoặc mục tiêu để hiển thị trực quan hơn với người quan sát dữ liệu.Quá trình sắp xếp dữ liệu là quá trình biến đổi, chuyển hóa dữ liệu từ các định dạng như dữ liệu trong mytib2, mytib3, hoặc mytib4 về dữ liệu có định dạng như mytib1. Nhìn chung, sắp xếp dữ liệu là công việc tương đối đơn giản với các quy trình khác. Trong phần này, bạn đọc chỉ cần nắm vững nguyên tắc của hai phép biến đổi là kéo dài dữ liệu bằng hàm gather() và mở rộng dữ liệu bằng hàm spread(). Trong các phiên bản mới của thư viện tidyr), các hàm gather() và spread() đã được bổ sung thêm các tham số mới và có thể được gọi bằng tên mới là pivot_longer() và pivot_wider(). Tuy nhiên, các nguyên tắc hoạt động của các hàm này là tương đồng nhau và không có sự khác biệt đáng kể.","code":"\nmytib1## # A tibble: 2,035 × 4\n##    country              year           gdp population\n##    <fct>               <int>         <dbl>      <dbl>\n##  1 Albania              2000   3686649387     3121965\n##  2 Algeria              2000  54790058957    31183658\n##  3 Angola               2000   9129180361    15058638\n##  4 Antigua and Barbuda  2000    802526701.      77648\n##  5 Argentina            2000 284203745280    37057453\n##  6 Armenia              2000   1911563665     3076098\n##  7 Aruba                2000   1858659293       90858\n##  8 Australia            2000 416887521196    19107251\n##  9 Austria              2000 192070749954     8050884\n## 10 Azerbaijan           2000   5272617196     8117742\n## # ℹ 2,025 more rows\nmytib2## # A tibble: 2,035 × 3\n##    country              year gdp_per_capita       \n##    <fct>               <int> <chr>                \n##  1 Albania              2000 3686649387/3121965   \n##  2 Algeria              2000 54790058957/31183658 \n##  3 Angola               2000 9129180361/15058638  \n##  4 Antigua and Barbuda  2000 802526700.6/77648    \n##  5 Argentina            2000 284203745280/37057453\n##  6 Armenia              2000 1911563665/3076098   \n##  7 Aruba                2000 1858659293/90858     \n##  8 Australia            2000 416887521196/19107251\n##  9 Austria              2000 192070749954/8050884 \n## 10 Azerbaijan           2000 5272617196/8117742   \n## # ℹ 2,025 more rows\nmytib3## # A tibble: 4,070 × 4\n##    country              year type               value\n##    <fct>               <int> <chr>              <dbl>\n##  1 Albania              2000 gdp          3686649387 \n##  2 Albania              2000 population      3121965 \n##  3 Algeria              2000 gdp         54790058957 \n##  4 Algeria              2000 population     31183658 \n##  5 Angola               2000 gdp          9129180361 \n##  6 Angola               2000 population     15058638 \n##  7 Antigua and Barbuda  2000 gdp           802526701.\n##  8 Antigua and Barbuda  2000 population        77648 \n##  9 Argentina            2000 gdp        284203745280 \n## 10 Argentina            2000 population     37057453 \n## # ℹ 4,060 more rows\nmytib4## # A tibble: 370 × 13\n##    country type   `2000`  `2001`  `2002`  `2003`  `2004`  `2005`  `2006`  `2007`\n##    <fct>   <chr>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>\n##  1 Albania gdp   3.69e 9 3.94e 9 4.06e 9 4.29e 9 4.54e 9 4.79e 9 5.03e 9 5.33e 9\n##  2 Albania popu… 3.12e 6 3.12e 6 3.12e 6 3.12e 6 3.10e 6 3.08e 6 3.05e 6 3.01e 6\n##  3 Algeria gdp   5.48e10 5.62e10 5.89e10 6.29e10 6.62e10 6.96e10 7.10e10 7.31e10\n##  4 Algeria popu… 3.12e 7 3.16e 7 3.20e 7 3.24e 7 3.28e 7 3.33e 7 3.37e 7 3.43e 7\n##  5 Angola  gdp   9.13e 9 9.42e 9 1.08e10 1.11e10 1.24e10 1.46e10 1.77e10 2.17e10\n##  6 Angola  popu… 1.51e 7 1.56e 7 1.61e 7 1.67e 7 1.73e 7 1.79e 7 1.85e 7 1.92e 7\n##  7 Antigu… gdp   8.03e 8 8.20e 8 8.41e 8 8.84e 8 9.46e 8 9.85e 8 1.12e 9 1.01e 9\n##  8 Antigu… popu… 7.76e 4 7.90e 4 8.00e 4 8.09e 4 8.17e 4 8.26e 4 8.35e 4 8.44e 4\n##  9 Argent… gdp   2.84e11 2.72e11 2.42e11 2.63e11 2.87e11 3.14e11 3.40e11 3.70e11\n## 10 Argent… popu… 3.71e 7 3.75e 7 3.79e 7 3.83e 7 3.87e 7 3.91e 7 3.96e 7 4.00e 7\n## # ℹ 360 more rows\n## # ℹ 3 more variables: `2008` <dbl>, `2009` <dbl>, `2010` <dbl>"},{"path":"biến-đổi-và-sắp-xếp-dữ-liệu.html","id":"kéo-dài-dữ-liệu-bằng-gather","chapter":"Chương 8 Biến đổi và sắp xếp dữ liệu","heading":"8.2.1 Kéo dài dữ liệu bằng gather()","text":"Dữ liệu được lưu dưới dạng của mytib4 có thông tin về năm quan sát được lưu dưới dạng cột. Đây là trường hợp rất hay gặp phải khi lấy dữ liệu từ các nguồn bên ngoài vào. Các dữ liệu này được lưu dưới dạng các ma trận với ít dòng và nhiều cột để thuận lợi cho người sử dụng dữ liệu có quan sát trực quan. Để đưa dữ liệu như mytib4 về định dạng có thể phân tích được, mà mỗi dòng là một quốc gia, được quan sát trong 1 năm, chúng ta cần thêm vào dữ liệu một cột có tên year. Giá trị của biến year là tên tất cả các cột, từ cột 2000 đến cột 2010. Thêm cột year và kéo dài dữ liệu mytib4, chúng ta sử dụng hàm gather() như sau:Bạn đọc có thể thấy rằng kết quả thu được đã có thêm cột year cho biết thông tin mỗi quốc gia được quan sát trong năm bao nhiêu và cột gdp_population cho biết giá trị của tổng thu nhập quốc dân hoặc dân số của quốc gia đó. Qua cách sử dụng hàm gather(), có thể thấy rằngTham số key trong hàm gather() cho biết tên của cột trong dữ liệu mới được tạo thành. Biến này sẽ chứa giá trị là tên các cột của dữ liệu ban đầu được tập hợp lại. Tham số value cho biết tên cột trong dữ liệu mới được tạo thành. Biến này chứa tất cả giá trị trong các cột được tập hợp của dữ liệu ban đầu. Trong câu lệnh ở trên, biến year chứa giá trị là các năm là tên của các cột trong dữ liệu mytib4 trong khi biến value chứa tất cả các giá trị là dân số hoặc thu nhập quốc nội.Sau khi khai báo hai tham số key và value, bạn đọc cần khai báo chính xác thông tin các biến không bị tác động bởi hàm gather(). Trong ví dụ ở trên, tất các các giá trị chúng ta muốn tập hợp lại giá trị nằm trong các cột bắt đầu từ cột có tên 2000, tương ứng với các quan sát trong năm 2000, đến cột có tên 2010, tương ứng với các quan sát của năm 2010. Khai báo -country và -type trong hàm gather() có ý nghĩa là các biến được tập hợp thông tin lại thành một cột duy nhất là tất cả các cột của mytib4, ngoại trừ hai cột country và type.Trong các phiên bản mới của thư viện tidyr, hàm pivot_longer() thường được sử dụng thay thế cho gather(). Ưu điểm của pivot_longer() là câu lệnh dễ hiểu hơn và cho phép chúng ta quản lý tên cột bằng các hàm sử dụng cùng với các biểu thức chính quy, hay regular expression. Ví dụ, chúng ta có thể sử dụng pivot_longer() để tập hợp thông tin của tất cả các cột có tên cột bắt đầu bằng ký tự ‘2’ như sauBạn đọc có thể thấy rằng các tham số names_to và values_to trong hàm pivot_longer() được sử dụng tương tự như tham số key và value trong hàm gather().Hàm pivot_longer() cho phép kéo dài các bảng mà giá trị của nhiều biến được tích hợp trong tên của các cột. Ví dụ, dữ liệu trong mytib5 dưới đây lưu điểm của hai sinh viên theo hai kỳ học của các năm 2023 và năm 2024:Bạn đọc có thể thấy rằng tên các cột có bao gồm hai thông tin là năm học 2023 hoặc 2024 và thông tin về kỳ học S1 hoặc S2. Chúng ta có thể sử dụng pivot_longer() để tạo thành dữ liệu mới, với hai biến mới tương ứng với hai thông tin: year tương ứng với năm học và semester tương ứng với kỳ học của hai sinh viên:Trong các trường hợp phức tạp hơn, trong tên biến của dữ liệu ban đầu vừa chứa tên biến trong dữ liệu mới vừa chứa giá trị của biến trong dữ liệu mới như dữ liệu trong mytib6 dưới đây:Bạn đọc có thể thấy rằng dữ liệu chứa thông tin về 8 người, giữ các chức chức vụ lớp trưởng và bí thư tại 4 lớp với thông tin tương ứng với từng người là tên và điểm. Các cột dữ liệu Ten_LopTruong và Ten_BiThu vừa chứa tên của một biến là tên của người, vừa chứa giá trị của biến chức vụ của người đó. Tương tự, các cột dữ liệu 'Diem_LopTruong' và 'Diem_BiThu' vừa chứa tên của biến là điểm của bạn sinh viên tương ứng, vừa chứa giá trị của biến là chức vụ của người đó. Dữ liệu có thể được sắp xếp lại bằng pivot_longer() như sau:Trong câu lệnh ở trên, chúng tôi sử dụng tham số names_to với phần tử đầu tiên là .value thay vì sử dụng tham số values_to như trên. Mục đích là để khai báo rằng thành phần thứ nhất trong tên của các cột là tên biến, bao gồm các biến Ten và biến Diem. Phần tử thứ hai của tham số names_to là Chuc_vu tương ứng với biến Chuc_vu trong dữ liệu mới. Chúng ta sẽ tiếp tục thực hành với hàm gather() và pivot_longer() trong phần thực hành của chương.","code":"\nmytib4%>%gather(key = \"year\", value = \"gdp_population\", -country, -type)## # A tibble: 4,070 × 4\n##    country             type       year  gdp_population\n##    <fct>               <chr>      <chr>          <dbl>\n##  1 Albania             gdp        2000     3686649387 \n##  2 Albania             population 2000        3121965 \n##  3 Algeria             gdp        2000    54790058957 \n##  4 Algeria             population 2000       31183658 \n##  5 Angola              gdp        2000     9129180361 \n##  6 Angola              population 2000       15058638 \n##  7 Antigua and Barbuda gdp        2000      802526701.\n##  8 Antigua and Barbuda population 2000          77648 \n##  9 Argentina           gdp        2000   284203745280 \n## 10 Argentina           population 2000       37057453 \n## # ℹ 4,060 more rows\nmytib4 %>% pivot_longer(cols = starts_with(\"2\"),\n                      names_to = \"year\",\n                      values_to = \"gdp_population\")## # A tibble: 4,070 × 4\n##    country type  year  gdp_population\n##    <fct>   <chr> <chr>          <dbl>\n##  1 Albania gdp   2000      3686649387\n##  2 Albania gdp   2001      3944714844\n##  3 Albania gdp   2002      4059111575\n##  4 Albania gdp   2003      4290480934\n##  5 Albania gdp   2004      4543619309\n##  6 Albania gdp   2005      4793518372\n##  7 Albania gdp   2006      5033194290\n##  8 Albania gdp   2007      5330152753\n##  9 Albania gdp   2008      5740574515\n## 10 Albania gdp   2009      5930013474\n## # ℹ 4,060 more rows\nmytib5## # A tibble: 2 × 5\n##   Name  `2023 S1` `2023 S2` `2024 S1` `2024 S2`\n##   <chr>     <dbl>     <dbl>     <dbl>     <dbl>\n## 1 SV1         8.5       6.9       8.1       8.5\n## 2 SV2         8         8.2       8.8       8\nmytib5 %>% pivot_longer(cols = !Name, # Không tập hợp cột Names\n                      names_to = c(\"year\", \"semester\"),\n                      names_sep = \" \",\n                      values_to = \"GPA\")## # A tibble: 8 × 4\n##   Name  year  semester   GPA\n##   <chr> <chr> <chr>    <dbl>\n## 1 SV1   2023  S1         8.5\n## 2 SV1   2023  S2         6.9\n## 3 SV1   2024  S1         8.1\n## 4 SV1   2024  S2         8.5\n## 5 SV2   2023  S1         8  \n## 6 SV2   2023  S2         8.2\n## 7 SV2   2024  S1         8.8\n## 8 SV2   2024  S2         8\nmytib6## # A tibble: 4 × 5\n##   Lop   Ten_LopTruong Ten_BiThu Diem_LopTruong Diem_BiThu\n##   <chr> <chr>         <chr>              <int>      <int>\n## 1 Act61 LT1           BT1                    6          7\n## 2 Act62 LT2           BT2                    7          8\n## 3 Act63 LT3           BT3                    8          9\n## 4 Act64 LT4           BT4                    9         10\nmytib6%>%pivot_longer(cols = !Lop,\n                      names_to = c(\".value\", \"Chuc_vu\"),\n                      names_sep = \"_\")## # A tibble: 8 × 4\n##   Lop   Chuc_vu   Ten    Diem\n##   <chr> <chr>     <chr> <int>\n## 1 Act61 LopTruong LT1       6\n## 2 Act61 BiThu     BT1       7\n## 3 Act62 LopTruong LT2       7\n## 4 Act62 BiThu     BT2       8\n## 5 Act63 LopTruong LT3       8\n## 6 Act63 BiThu     BT3       9\n## 7 Act64 LopTruong LT4       9\n## 8 Act64 BiThu     BT4      10"},{"path":"biến-đổi-và-sắp-xếp-dữ-liệu.html","id":"mở-rộng-dữ-liệu-với-speard","chapter":"Chương 8 Biến đổi và sắp xếp dữ liệu","heading":"8.2.2 Mở rộng dữ liệu với speard()","text":"Ngược lại với gather(), chúng ta sử dụng speard() để mở rộng dữ liệu, nghĩa là biến đổi tên biến thành tên các cột. Dữ liệu cần được mở rộng khi giá trị trong các cột dữ liệu không được đồng nhất, giống như thông tin về tổng thu nhập quốc dân và dân số của các quốc gia trên thế giới trong dữ liệu được lưu trong mytib3Bạn đọc có thể thấy rằng trong cột gdp_population vừa có cả thông tin về tổng thu nhập quốc dân (biến gdp) và thông tin về dân số (biến population) của quốc gia đó. Đồng thời cột type cho biết giá trị đó là gdp hay population. Dữ liệu như trên có thể được sắp xếp lại bằng cách sử dụng hàm speard() như sau:Có thể thấy rằng dữ liệu mới đã được sắp xếp nhất quán và đã có thể được sử dụng trong phân tích và xây dựng mô hình. Trong hàm spread() ở trên,Tham số key cho biết cột nào trong dữ liệu ban đầu là cột chứa giá trị là tên các biến mới. cột type của dữ liệu mytib3 chỉ có chứa hai giá trị là gdp và population nên trong dữ liệu mới có hai cột mới được hình thành tương ứng với hai giá trị trong cột type.Tham số value được sử dụng để cho biết giá trị của biến nào trong dữ liệu ban đầu sẽ được lấy vào các cột mới được hình thành. Trong câu lệnh ở trên, cột value của mytib3 chứa giá trị tương ứng với tổng thu nhập quốc dân và dân số của mỗi nước được sử dụng để gán cho tham số value trong hàm spread().Trong các phiên bản mới của thư viện tidyr, hàm pivot_wider() được sử dụng để thay thế cho hàm spread() có nhiều ưu điểm hơn. cách sử dụng pivot_wider() tương tự như pivot_longer() nên chúng tôi không đi vào thảo luận chi tiết về hàm số này. Bạn đọc tự tham khảo cách sử dụng hàm pivot_wider() và thực hành trong phần bài tập của chương.","code":"\nmytib3## # A tibble: 4,070 × 4\n##    country              year type               value\n##    <fct>               <int> <chr>              <dbl>\n##  1 Albania              2000 gdp          3686649387 \n##  2 Albania              2000 population      3121965 \n##  3 Algeria              2000 gdp         54790058957 \n##  4 Algeria              2000 population     31183658 \n##  5 Angola               2000 gdp          9129180361 \n##  6 Angola               2000 population     15058638 \n##  7 Antigua and Barbuda  2000 gdp           802526701.\n##  8 Antigua and Barbuda  2000 population        77648 \n##  9 Argentina            2000 gdp        284203745280 \n## 10 Argentina            2000 population     37057453 \n## # ℹ 4,060 more rows\nmytib3%>%spread(key = type, value = value)## # A tibble: 2,035 × 4\n##    country  year        gdp population\n##    <fct>   <int>      <dbl>      <dbl>\n##  1 Albania  2000 3686649387    3121965\n##  2 Albania  2001 3944714844    3124093\n##  3 Albania  2002 4059111575    3123112\n##  4 Albania  2003 4290480934    3117045\n##  5 Albania  2004 4543619309    3103758\n##  6 Albania  2005 4793518372    3082172\n##  7 Albania  2006 5033194290    3050741\n##  8 Albania  2007 5330152753    3010849\n##  9 Albania  2008 5740574515    2968026\n## 10 Albania  2009 5930013474    2929886\n## # ℹ 2,025 more rows"},{"path":"biến-đổi-và-sắp-xếp-dữ-liệu.html","id":"phân-tách-và-kết-hợp-các-cột","chapter":"Chương 8 Biến đổi và sắp xếp dữ liệu","heading":"8.2.3 Phân tách và kết hợp các cột","text":"Đối với dữ liệu như mytib2 với biến gdp_per_capita trình bày dưới dạng chuỗi ký tự, với số tổng thu nhập quốc dân và dân số được lưu trong cùng một cột và cách nhau bởi dấu /,Việc biến đổi dữ liệu về nhất quán như mytib1 có thể được thực hiện một cách đơn giản bằng hàm seperate()Bạn đọc có thể thấy rằng dữ liệu mới đã nhất quán với hai cột mới được tạo thành là cột gdp là thu nhập quốc nội của các quốc gia và cột population là dân số của quốc gia đó.","code":"\nmytib2## # A tibble: 2,035 × 3\n##    country              year gdp_per_capita       \n##    <fct>               <int> <chr>                \n##  1 Albania              2000 3686649387/3121965   \n##  2 Algeria              2000 54790058957/31183658 \n##  3 Angola               2000 9129180361/15058638  \n##  4 Antigua and Barbuda  2000 802526700.6/77648    \n##  5 Argentina            2000 284203745280/37057453\n##  6 Armenia              2000 1911563665/3076098   \n##  7 Aruba                2000 1858659293/90858     \n##  8 Australia            2000 416887521196/19107251\n##  9 Austria              2000 192070749954/8050884 \n## 10 Azerbaijan           2000 5272617196/8117742   \n## # ℹ 2,025 more rows\nmytib2 %>% \n  \n  # Phân tách cột gdp_per_capita thành hai cột gdp và population\n  separate(gdp_per_capita, c(\"gdp\",\"population\"), sep = \"/\") %>%\n  \n  # Đổi giá trị cột gdp và population thành kiểu số\n  mutate(gdp = as.numeric(gdp), population = as.numeric(population))## # A tibble: 2,035 × 4\n##    country              year           gdp population\n##    <fct>               <int>         <dbl>      <dbl>\n##  1 Albania              2000   3686649387     3121965\n##  2 Algeria              2000  54790058957    31183658\n##  3 Angola               2000   9129180361    15058638\n##  4 Antigua and Barbuda  2000    802526701.      77648\n##  5 Argentina            2000 284203745280    37057453\n##  6 Armenia              2000   1911563665     3076098\n##  7 Aruba                2000   1858659293       90858\n##  8 Australia            2000 416887521196    19107251\n##  9 Austria              2000 192070749954     8050884\n## 10 Azerbaijan           2000   5272617196     8117742\n## # ℹ 2,025 more rows"},{"path":"trực-quan-hóa-dữ-liệu.html","id":"trực-quan-hóa-dữ-liệu","chapter":"Chương 9 Trực quan hóa dữ liệu","heading":"Chương 9 Trực quan hóa dữ liệu","text":"","code":""},{"path":"trực-quan-hóa-dữ-liệu.html","id":"giới-thiệu-về-trực-quan-hóa-dữ-liệu","chapter":"Chương 9 Trực quan hóa dữ liệu","heading":"9.1 Giới thiệu về trực quan hóa dữ liệu","text":"Trực quan hóa dữ liệu là nghệ thuật mô tả dữ liệu thông qua việc sử dụng đồ họa và hình ảnh như các biểu đồ, sơ đồ, hình vẽ, bao gồm cả hình ảnh động hoặc hình ảnh tương tác. Đây là phương pháp truyền đạt thông tin một cách trực quan và dễ hiểu từ người quản lý dữ liệu đến người tiếp nhận. Trực quan hóa giúp mô tả một cách hiệu quả các mối quan hệ dữ liệu phức tạp, các thông tin chuyên sâu, và các vấn đề bất thường ẩn chứa trong dữ liệu.Tại sao lại cần trực quan hóa dữ liệu? Thứ nhất, não bộ của con người phản ứng tốt hơn với hình ảnh, màu sắc, kích thước, hay khoảng cách với các ký hiệu, chuỗi ký tự, hay các con số. Thứ hai, dữ liệu ngày càng trở nên lớn hơn và phức tạp hơn. Trực quan hóa là phương pháp hiệu quả nhất để tìm ra các giá trị ẩn chứa bên trong dữ liệu. Đây chính là nguyên nhân khiến kỹ năng trực quan hóa dữ liệu được đánh giá là kỹ năng quan trọng nhất đối với những người phân tích dữ liệu.Có nhiều công cụ để trực quan hóa dữ liệu một cách chuyên nghiệp. Tiêu biểu phải kể đến hai công cụ quen thuộc là Power BI và Tableau. Đây là hai công cụ thân thiện với người dùng, cho phép người dùng tạo bảng điều khiển và báo cáo tương tác một cách nhanh chóng và dễ dàng. Cả hai đều có giao diện kiểu kéo và thả con trỏ giúp dễ dàng tạo hình ảnh trực quan mà không cần bất kỳ kỹ năng lập trình nào.Khác với Power BI hay Tableau, R sử dụng thư viện ggplot2 để trực quan hóa dữ liệu. Sẽ là không dễ dàng cho người mới bắt đầu vẽ được đồ thị bằng các câu lệnh của ggplot2. Điểm mạnh của thư viện ggplot2 với các công cụ như Power BI hay Tableau là cho phép người dùng tạo các hình ảnh có khả năng tùy biến cao. ggplot2 là lựa chọn phù hợp dành cho các nhà phân tích dữ liệu, những người cảm thấy hứng thú với việc viết các câu lệnh để tạo ra các hình ảnh trực quan phức tạp, và quan trọng nhất là đúng theo ý muốn của mình. Với một chút kinh nghiệm về Power BI và Tableau, cùng với nhiều hơn một chút kinh nghiệm về ggplot2, chúng tôi cho rằng bạn đọc nên làm quen với cả hai cách trực quan hóa dữ liệu. Khi bạn phải tạo các báo cáo trực quan trong một thời gian ngắn, Power BI hay Tableau sẽ là lựa chọn tối ưu. Khi bạn muốn vẽ những hình ảnh phức tạp, có tính cá nhân cao, và bạn có thời gian để làm việc đó, hãy sử dụng R và thư viện ggplot2.","code":""},{"path":"trực-quan-hóa-dữ-liệu.html","id":"tổng-quan-về-thư-viện-ggplot2","chapter":"Chương 9 Trực quan hóa dữ liệu","heading":"9.2 Tổng quan về thư viện ggplot2","text":"Như chúng tôi đã giới thiệu, ggplot2 là một thư viện để trực quan hóa dữ liệu trong R. Ngoài ggplot2, bạn đọc cũng có thể sử dụng các đồ thị cơ bản của R hoặc các thư viện khác nhưlattice để vẽ đồ thị. Tuy nhiên, không giống như hầu hết các công cụ khác, ggplot2 trực quan hóa dữ liệu dựa trên Ngữ pháp của đồ thị (Wilkinson 2005). Hai chữ gg trong ggplot2 có nghĩa là “Grammar Graphics” - Ngữ pháp của đồ thị. Ngữ pháp này cho phép bạn đọc vẽ đồ thị bằng cách kết hợp các cấu phần độc lập lại với nhau. Đây chính là điểm mạnh của ggplot2. Thay vì bị giới hạn ở các bộ đồ thị đã được xác định trước, bạn đọc có thể tạo đồ thị mới phù hợp với mục tiêu của mình.Ý tưởng phải học ngữ pháp để vẽ đồ thị có thể làm bạn đọc cảm thấy nản chí, nhưng sự thật là ngữ pháp của ggplot2 thực sự dễ học. Chỉ có một số nguyên tắc cốt lõi đơn giản và rất ít trường hợp đặc biệt. Khi đã thông thạo Ngữ pháp của đồ thị, ngoài việc tạo ra những đồ thị quen thuộc, bạn đọc còn có thể tạo ra những đồ thị mới hơn, đẹp hơn và mang tính cá nhân riêng. Bạn đọc có thể gặp khó khăn một chút ban đầu nhưng chúng tôi tin rằng khi đã quen với thư viện ggplot2, sẽ rất ít bạn đọc muốn quay lại với các công cụ trực quan hóa dữ liệu khác.Chúng ta hãy thử xem một ví dụ để hình dung về cách thư viện ggplot2 trực quan hóa dữ liệu. Chúng ta sẽ bắt đầu với một dữ liệu có tên là murders trong thư viện dslabs. Đây là dữ liệu FBI cung cấp về số vụ sát nhân bằng súng tại các bang của Mỹ vào năm 2010. Giả sử bạn muốn du lịch đến Mỹ nhưng lo ngại về việc cho phép sử dụng súng ở quốc gia này và muốn biết ở những bang nào có tỷ lệ số vụ sát nhân bằng súng cao. Chúng ta sẽ bắt đầu bằng việc tìm hiểu thông tin sơ bộ về dữ liệu này. Nếu bạn đã quen với đối tượng kiểu data.frame trong R, bạn cũng sẽ quen với các câu lệnh giúp tìm hiểu dữ liệu như head(), str(), view():Dù dữ liệu chỉ có hơn 51 dòng và 5 cột nhưng thật khó để có thể có được cái nhìn tổng thể về dữ liệu nếu chỉ nhìn vào các bảng, các con số, các véc-tơ kiểu chuỗi ký tự như trên. Thay vì trình bày dữ liệu dưới dạng con số hay ký tự, bạn đọc có thể mô tả dữ liệu murders dưới dạng một đồ thị phân tán như Hình 9.1, thì hiệu quả sẽ cao hơn rất nhiều.\nHình 9.1: Số vụ sát nhân bằng súng tại 51 bang của Mỹ vào năm 2010. Có mối liên hệ cùng chiều giữa dân số của các bang và số vụ sát nhân bằng súng tại mỗi bang.\nChúng tôi đã sử dụng một vài kỹ thuật biến đổi dữ liệu kết hợp với kỹ thuật trực quan hóa để vẽ đồ thị ở trên:Biến tổng số vụ sát nhân (total) và biến dân số của mỗi bang (population) đều có đuôi dài sang phía bên phải, nghĩa là có nhiều điểm tập trung ở khu vực trung tâm, và một số ít điểm tập trung ở phía đuôi bên phải. Nếu sử dụng chính xác giá trị của các biến này trên đồ thị, các điểm của đồ thị phân tán sẽ phân bố không đồng đều. Để hiển thị các điểm một cách rõ ràng hơn, giá trị hiển thị trên đồ thị đã được điều chính lại theo hàm log() cơ số 10. Điều này giải thích tại sao bạn đọc thấy rằng trên trục y khoảng cách từ 10 (vụ sát nhân) đến 100 (vụ sát nhân) sẽ bằng khoảng cách từ 100 (vụ sát nhân) đến 1000 (vụ sát nhân), hay trên trục x khoảng cách từ 1 (triệu người) đến 3 (triệu người) tương đương với khoảng cách từ 3 (triệu người) đến 10 (triệu người)Chúng tôi thêm vào một đường thẳng tuyến tính đi qua trung tâm các điểm để mô tả mối quan hệ chung giữa hai biến total và population. Đường thẳng có độ dốc dương cho thấy mối quan hệ cùng chiều giữa biến. Điều này có thể giải thích là ở các bang có dân số càng đông thì tổng số vụ sát nhân bằng súng càng cao.Để bạn đọc dễ dàng phân biệt một bang thuộc vào vùng (region) nào, chúng tôi sử dụng các màu sắc khác nhau hiển thị cho một vùng.Mỗi bang được ghi chú bằng tên viết tắt của bang đó.Dựa trên đồ thị phân tán ở trên, bạn đọc có thể đưa ra được các nhận xét như sauBang có dân số càng cao thì số vụ sát nhân bằng súng càng nhiều.Hầu hết các bang nằm phía trên đường trung bình là các bang ở miền Nam.Các vùng còn lại không có sự phân biệt rõ ràng về tỷ lệ số vụ sát nhân bằng súng.Bang District Columbia là bang nằm cao hơn hẳn với đường trung bình, và cũng là bang có tỷ lệ số vụ sát nhân bằng súng cao nhất.Bang California có tổng số vụ sát nhân bằng súng lớn nhất, nhưng tỷ lệ số vụ sát nhân bằng súng trên đầu người chỉ bằng mức trung bình chung.Rõ ràng là không dễ dàng để đưa ra được các nhận xét như trên nếu chỉ dựa trên quan sát con số và dữ liệu. Thay vào đó chúng ta có thể đưa ra nhiều phân tích có ý nghĩa về dữ liệu khi sử dụng đồ thị như Hình 9.1.Wilkinson (2005) giới thiệu khái niệm Ngữ pháp đồ thị để mô tả các thành phần cơ bản làm nền tảng cho tất cả các đồ thị và cách thức các thành phần đó tương tác với nhau khi mô tả một dữ liệu. Ngữ pháp đồ thị là mô tả chính xác nhất cho câu hỏi đồ thị trực quan hóa dữ liệu là gì? Thư viện ggplot2 được Wickham giới thiệu vào năm 2009 và được xây dựng dựa trên Ngữ pháp đồ thị mà Wilkinson đã đề cập.Ngữ pháp đồ thị, theo Wickham (2009), là các quy tắc cho tương ứng các biến của dữ liệu đến các thuộc tính thẩm mỹ, được gọi là các aesthetic attributions, của các đối tượng hình ảnh, được gọi là các geometries, xuất hiện trên đồ thị. Đồ thị được trực quan bằng thư viện ggplot2 cũng có thể bao gồm hình ảnh mô tả các mô hình thống kê của dữ liệu và được mô tả trên một hệ tọa độ cụ thể. Ngoài ra, khi dữ liệu muốn hiển thị quá phức tạp, bạn đọc cũng có thể chia dữ liệu thành các tập hợp con dựa trên các biến rời rạc và mô tả dữ liệu thông qua một nhóm các đồ thị con dựa trên kỹ thuật facetting. Sự kết hợp của các thành phần độc lập kể trên tạo nên một đồ thị trực quan mô tả dữ liệu.Bạn đọc không cần phải lo lắng nếu khái niệm Ngữ pháp đồ thị chúng tôi vừa giải thích ở trên không có ý nghĩa ngay lập tức. Trong phần sau của cuốn sách, chúng tôi sẽ nói về ngữ pháp đồ thị một cách chi tiết hơn. Bạn sẽ có nhiều cơ hội hơn để tìm hiểu về Ngữ pháp và các sử dụng ngữ pháp để kết hợp các cấu phần độc lập của một đồ thị hoạt động cùng nhau. Trong phần giới thiệu này, chúng tôi muốn bạn đọc hãy ghi nhớ bảy thành phần độc lập tạo nên một đồ thị cơ bản trong thư viện ggplot2:Dữ liệu (Data) là dữ liệu hay tập hợp các dữ liệu mà bạn đọc muốn trực quan hóa. Thông thường thì chỉ có một dữ liệu chính mà bạn đọc muốn minh họa cho người tiếp nhận dữ liệu, trong khi các dữ liệu khác được sử dụng với mục đích để mô tả và minh họa cho dữ liệu chính. Các dữ liệu được sử dụng để minh họa dữ liệu chính thường được gọi là các meta data. Một ví dụ điển hình của dữ liệu minh họa là dữ liệu kiểu bản đồ. Chẳng hạn như khi bạn đọc muốn mô tả về dữ liệu murders ở trên, bạn đọc có thể sử dụng dữ liệu về bản đồ nước Mỹ để có mô tả tốt hơn. Chúng ta sẽ thảo luận về dữ liệu kiểu bản đồ trong phần sau của chương.Dữ liệu (Data) là dữ liệu hay tập hợp các dữ liệu mà bạn đọc muốn trực quan hóa. Thông thường thì chỉ có một dữ liệu chính mà bạn đọc muốn minh họa cho người tiếp nhận dữ liệu, trong khi các dữ liệu khác được sử dụng với mục đích để mô tả và minh họa cho dữ liệu chính. Các dữ liệu được sử dụng để minh họa dữ liệu chính thường được gọi là các meta data. Một ví dụ điển hình của dữ liệu minh họa là dữ liệu kiểu bản đồ. Chẳng hạn như khi bạn đọc muốn mô tả về dữ liệu murders ở trên, bạn đọc có thể sử dụng dữ liệu về bản đồ nước Mỹ để có mô tả tốt hơn. Chúng ta sẽ thảo luận về dữ liệu kiểu bản đồ trong phần sau của chương.Hình dạng đồ họa, được gọi là các geometries hay viết tắt là các geoms, là những hình dạng đồ họa mà chúng ta muốn nhìn thấy trên đồ thị. Các hình dạng này có thể là các điểm, các thanh, các đường.Hình dạng đồ họa, được gọi là các geometries hay viết tắt là các geoms, là những hình dạng đồ họa mà chúng ta muốn nhìn thấy trên đồ thị. Các hình dạng này có thể là các điểm, các thanh, các đường.Các ánh xạ thẩm mỹ, được gọi là các aesthetic mapping, là các quy tắc cho tương ứng từ các biến (hay các cột của dữ liệu) đến các thuộc tính thẩm mỹ của các hình dạng đồ họa. Các thuộc tính thẩm mỹ có thể là hình dạng, màu sắc, độ đậm nhạt.Các ánh xạ thẩm mỹ, được gọi là các aesthetic mapping, là các quy tắc cho tương ứng từ các biến (hay các cột của dữ liệu) đến các thuộc tính thẩm mỹ của các hình dạng đồ họa. Các thuộc tính thẩm mỹ có thể là hình dạng, màu sắc, độ đậm nhạt.Các mô hình hay biến đổi thống kê, được gọi là các statistics hay viết tắt là stats là các quy tắc tóm tắt dữ liệu để làm nổi bật các xu thế và hiển thị các giá trị ẩn trong dữ liệu. Các mô hình xây dựng trên dữ liệu được mô tả dưới dạng một hình dạng đồ họa nhằm tăng tính dễ hiểu cho đồ thị và giúp cho người tiếp nhận nhanh chóng tiếp nhận thông tin. Ví dụ như trong đồ thị phân tán mô tả dữ liệu murders, chúng tôi đã sử dụng một mô hình tuyến tính nhằm mô tả mối quan hệ giữa biến total và biến population với mục đích cho thấy mối liên hệ cùng chiều giữa hai biến này, đồng thời cho bạn đọc biết được các bang nào có tỷ lệ số vụ sát nhân bằng súng thấp hơn trung bình và các bang nào có tỷ lệ số vụ sát nhân bằng súng cao hơn với mức trung bình chung.Các mô hình hay biến đổi thống kê, được gọi là các statistics hay viết tắt là stats là các quy tắc tóm tắt dữ liệu để làm nổi bật các xu thế và hiển thị các giá trị ẩn trong dữ liệu. Các mô hình xây dựng trên dữ liệu được mô tả dưới dạng một hình dạng đồ họa nhằm tăng tính dễ hiểu cho đồ thị và giúp cho người tiếp nhận nhanh chóng tiếp nhận thông tin. Ví dụ như trong đồ thị phân tán mô tả dữ liệu murders, chúng tôi đã sử dụng một mô hình tuyến tính nhằm mô tả mối quan hệ giữa biến total và biến population với mục đích cho thấy mối liên hệ cùng chiều giữa hai biến này, đồng thời cho bạn đọc biết được các bang nào có tỷ lệ số vụ sát nhân bằng súng thấp hơn trung bình và các bang nào có tỷ lệ số vụ sát nhân bằng súng cao hơn với mức trung bình chung.Hệ tọa độ, hay Cordinate mô tả cách dữ liệu được trực quan hóa trên mặt phẳng của đồ họa. Đa số các trường hợp chúng ta sẽ sử dụng hệ tọa độ Descartes, nhưng cũng có một số hệ tọa độ khác có thể sử dụng bao gồm tọa độ cực và bản đồ.Hệ tọa độ, hay Cordinate mô tả cách dữ liệu được trực quan hóa trên mặt phẳng của đồ họa. Đa số các trường hợp chúng ta sẽ sử dụng hệ tọa độ Descartes, nhưng cũng có một số hệ tọa độ khác có thể sử dụng bao gồm tọa độ cực và bản đồ.Một thành phần mô tả cách dữ liệu được hiển thị là chia nhỏ dữ liệu để mô tả bằng một nhóm các đồ thị thay vì một đồ thị duy nhất được gọi là facetting. Thành phần này thường được sử dụng để mô tả khi dữ liệu có kích thước lớn và hoặc chúng ta muốn sánh trực quan dữ liệu ở các nhóm khác nhau.Một thành phần mô tả cách dữ liệu được hiển thị là chia nhỏ dữ liệu để mô tả bằng một nhóm các đồ thị thay vì một đồ thị duy nhất được gọi là facetting. Thành phần này thường được sử dụng để mô tả khi dữ liệu có kích thước lớn và hoặc chúng ta muốn sánh trực quan dữ liệu ở các nhóm khác nhau.Thành phần cuối cùng của đồ thị là ngữ cảnh của đồ thị, còn được gọi là các themes. Theme quy định khung hoặc nền mà đồ thị được hiển thị, chẳng hạn như kích thước phông chữ hoặc màu nền. Mặc dù các giá trị mặc định trong thư viện ggplot2 đã được lựa chọn hợp lý nhưng bạn đọc cũng có thể cần tham khảo các tài liệu tham khảo khác để tạo ra một ngữ cảnh phù hợp hơn cho đồ thị của mình.Thành phần cuối cùng của đồ thị là ngữ cảnh của đồ thị, còn được gọi là các themes. Theme quy định khung hoặc nền mà đồ thị được hiển thị, chẳng hạn như kích thước phông chữ hoặc màu nền. Mặc dù các giá trị mặc định trong thư viện ggplot2 đã được lựa chọn hợp lý nhưng bạn đọc cũng có thể cần tham khảo các tài liệu tham khảo khác để tạo ra một ngữ cảnh phù hợp hơn cho đồ thị của mình.Mỗi khi vẽ một đồ thị sử dụng ggplot2, bạn đọc cần tự định nghĩa ít nhất ba thành phần: 1. Dữ liệu; 2. Các hình dạng đồ họa; và 3. Các ánh xạ thẩm mỹ. Các thành phần 5. Hệ tọa độ; và 7. Ngữ cảnh; sẽ được tự động gán cho các giá trị mặc định nếu bạn đọc không quy định trong câu lệnh. Và các thành phần 4. Mô hình; và 6. Facetting; chỉ xuất hiện khi bạn đọc gọi lên.Trước khi đi vào nội dung chi tiết về cách tạo nên một đồ thị, bạn đọc cũng cần biết được các hạn chế khi trực quan hóa dữ liệu bằng ggplot2 như sauggplot2 là một thư viện của R nên bạn đọc cần có kỹ năng viết câu lệnh R tương đối thành thạo.Thư viện ggplot2 không gợi ý bạn đọc nên sử dụng đồ thị nào khi gặp một dữ liệu cụ thể. Điều đó cũng có nghĩa là bạn đọc cần có một chút kinh nghiệm về trực quan hóa dữ liệu trước khi sử dụng thư viện này.Thư viện ggplot2 không được phát triển để vẽ các đồ thị động hay đồ thị tương tác mà chỉ tập trung vào vẽ các đồ thị tĩnh. Muốn vẽ các đồ thị tương tác hay đồ thị động, bạn đọc phải sử dụng các thư viện đi kèm như gganimate hay ggplotly.Để kết thúc phần giới thiệu, chúng tôi sẽ sử dụng thư viện ggplot2 kết hợp với thư viện vẽ hình ảnh động gganimate, để kể một câu chuyện về sự phát triển về sự tiến bộ y tế của các quốc gia trên thế giới trong khoảng thời gian từ năm 1960 đến năm 2010, thông qua hai khía cạnh là tuổi thọ trung bình của các quốc gia và tỷ lệ tử vong trung bình tính trên mỗi 1000 trẻ sơ sinh. Dữ liệu chính được sử dụng là dữ liệu gapminder trong thư viện dslabs mà bạn đọc đã làm quen trong phần phân tích dữ liệu. Sự thay đổi của tuối thọ trung bình (life_expectancy) và tỷ lệ trẻ sơ sinh tử vong (infant_mortality) của các quốc gia trên thế giới được mô tả lại một cách sinh động qua Hình 9.2\nHình 9.2: Sự thay đổi trong tỷ lệ tử vong trẻ sơ sinh, tính trên 1000 trẻ, và tuổi thọ trung bình của cá quốc gia trên thế giới từ năm 1960 đến năm 2010\nDựa trên đồ thị ở trên, bạn đọc có thể suy nghĩ về các câu hỏi dưới đây:Xu hướng di chuyển chung của các điểm dữ liệu là như thế nào ? Điều này cho biết tỷ lệ tử vong của trẻ sơ sinh và tuổi thọ bình quân của các quốc gia trên thế giới từ năm 1960 đến năm 2010 thay đổi như thế nào ? Điều này cho thấy việc phát triển về y tế nhìn chung là như thế nào ?Xu hướng di chuyển chung của các điểm dữ liệu là như thế nào ? Điều này cho biết tỷ lệ tử vong của trẻ sơ sinh và tuổi thọ bình quân của các quốc gia trên thế giới từ năm 1960 đến năm 2010 thay đổi như thế nào ? Điều này cho thấy việc phát triển về y tế nhìn chung là như thế nào ?Trong xu thế phát triển chung đó, sự khác biệt của các Châu lục là như thế nào ?Trong xu thế phát triển chung đó, sự khác biệt của các Châu lục là như thế nào ?Kích thước của các điểm trong đồ thị phân tán cho biết dân số quốc gia đó, bạn có nhận thấy sự thay đổi của kích thước của các điểm theo thời gian là như thế nào không ? Bạn có nhận ra các quốc gia đông dân như Trung Quốc, Ấn Độ, Mỹ, Nhật Bản từ đồ thị trên hay không ?Kích thước của các điểm trong đồ thị phân tán cho biết dân số quốc gia đó, bạn có nhận thấy sự thay đổi của kích thước của các điểm theo thời gian là như thế nào không ? Bạn có nhận ra các quốc gia đông dân như Trung Quốc, Ấn Độ, Mỹ, Nhật Bản từ đồ thị trên hay không ?Một vài quốc gia Châu Phi có quỹ đạo di chuyển bất thường khi điểm dữ liệu bị “rơi” xuống theo trục \\(\\overrightarrow{Oy}\\) trong một vài năm, theo bạn đó là nguyên nhân gì ?Một vài quốc gia Châu Phi có quỹ đạo di chuyển bất thường khi điểm dữ liệu bị “rơi” xuống theo trục \\(\\overrightarrow{Oy}\\) trong một vài năm, theo bạn đó là nguyên nhân gì ?","code":"\nlibrary(dslabs)\nhead(murders)##        state abb region population total\n## 1    Alabama  AL  South    4779736   135\n## 2     Alaska  AK   West     710231    19\n## 3    Arizona  AZ   West    6392017   232\n## 4   Arkansas  AR  South    2915918    93\n## 5 California  CA   West   37253956  1257\n## 6   Colorado  CO   West    5029196    65\n# str(murders)\n# View(murders)"},{"path":"trực-quan-hóa-dữ-liệu.html","id":"tạo-một-đồ-thị-cơ-bản","chapter":"Chương 9 Trực quan hóa dữ liệu","heading":"9.3 Tạo một đồ thị cơ bản","text":"Trước khi giới thiệu chi tiết về các thành phần độc lập của đồ thị trực quan và cách sử dụng ngữ pháp của đồ thị, chúng tôi cho rằng sẽ tốt hơn nếu bạn đọc bắt đầu vẽ các đồ thị đơn giản bằng cách sao chép và dán các câu lệnh vẽ đồ thị. Sau khi thực thi một vài lần, bạn đọc sẽ có cảm nhận được một phần cách mà một đồ thị của thư viện ggplot2 được xây dựng. Dữ liệu chúng tôi sử dụng để trực quan hóa trong suốt chương sách này là dữ liệu gapminder - dữ liệu mô tả về sức khỏe và thu nhập của tất cả các quốc gia trên thế giới bắt đầu từ năm 1960 đến năm 2016. Hình 9.3 mô tả tỷ lệ biến không dữ liệu không quan sát được của các biến qua các năm\nHình 9.3: Tỷ lệ giá trị không quan sát được của các biến trong dữ liệu gapminder theo năm bắt đầu từ 1960 đến 2016\nBạn đọc có thể thấy rằng dữ liệu gapminder có nhiều giá trị không quan sát được trong năm 2016. Hai cột có tỷ lệ không quan sát được qua các năm lớn là infant_mortality và gdp. Riêng biến gdp là gần như không quan sát được từ năm 2012 đến 2016. chỉ sử dụng dữ liệu với mục đích trực quan hóa nên chúng tôi sẽ xử lý việc dữ liệu không quan sát được này một cách đơn giản là xóa các quan sát của các năm 2012 đến 2016. Các giá trị không quan sát được từ năm 1960 đến 2011 sẽ được thay thế bằng cách nội suy tuyến tính theo chuỗi thời gian. Thư viện để nội suy tuyến tính các giá trị không quan sát được theo chuỗi thời gian là thư viện imputeTS.\nHình 9.4: Thu nhập bình quân đầu người và tuổi thọ trung bình của các quốc gia trên thế giới năm 2011\nKhi sử dụng hàm ggplot() ở trên, dữ liệu được đưa vào có tên là dat - dữ liệu được biến đổi từ dữ liệu gapminder bằng cách thêm vào cột thu nhâp bình quân đầu người (gdp_per_capita) và sau đó lọc theo năm 2011. Hình dạng đồ họa là các điểm trên trục tọa độ Descartes. Hình dạng đồ họa này được gọi bằng hàm geom_point(). Ánh xạ thẩm mỹ được khai báo thông qua hàm aes() nằm trong hàm ggplot(). Bên trong hàm aes(), chúng ta đã cho tương ứng (ánh xạ) biến life_expectancy với giá trị trên trục \\(\\overrightarrow{Ox}\\) và biến gdp_per_capita tương ứng (ánh xạ) với giá trị trên trục \\(\\overrightarrow{Oy}\\) của trục tọa độ Descartes.Mặc dù đồ thị trên Hình 9.4 còn đơn giản, nhưng chúng ta đã có thể nhận thấy được một số thông tin về tuổi thọ trung bình, thu nhập bình quân đầu người, và mối liên hệ giữa hai biến này:Có mối liên hệ đồng biến giữa tuổi thọ trung bình và thu nhập bình quân đầu người. Quốc gia nào có thu nhập bình quân đầu người cao thì tuổi thọ trung bình cũng sẽ cao. Điều này khá hợp lý bởi các quốc gia có thu nhập trung bình cao thường là các nước phát triển có hệ thống chăm sóc sức khỏe tốt, đó tuổi thọ trung bình cũng sẽ cao.Mối liên hệ đồng biến nhưng không tuyến tính, thu nhập bình quân đầu người tăng nhanh hơn rất nhiều ro với tuổi thọ trung bình.Có một vài điểm có khả năng là ngoại lai trong mối liên hệ đồng biến này. Nghĩa là có các quốc gia có mức thu nhập bình quân khá cao (từ 10 nghìn USD - 20 nghìn USD/1 người) nhưng lại có tuổi thọ trung bình không cao. Tuy nhiên chỉ với các thông tin như trên chúng ta không thể đưa ra giải thích cho các giá trị này.Hình dạng đồ họa là những hình dạng cụ thể mà bạn đọc nhìn thấy trên đồ thị, chẳng hạn như các điểm, các đường, thanh, hay các khối hình khác. Khi gọi các hình dạng đồ họa, thư viện ggplot2 luôn luôn sử dụng các hàm số bắt đầu bởi geom là viết tắt của từ geometries. Để làm quen với các khối hình khác trong ggplot2, bạn đọc có thể thử các câu lệnh để vẽ các khối hình khác như dưới đây:Còn nhiều hàm geom_() khác có thể được sử dụng để trực quan hóa dữ liệu. Bạn đọc có thể tham khảo danh sách các hàm geom_() thường sử dụng trong link dưới đây.https://www.maths.usyd.edu.au/u/UG/SM/STAT3022/r/current/Misc/data-visualization-2.1.pdfBạn đọc có thể thấy rằng trong danh sách các hàm geom_() có thể sử dụng bao gồm cả gợi ý cho người sử dụng nên dùng hàm geom_() nào trong từng trường hợp. Chẳng hạn như geom_point() được gợi ý sử dụng khi mô tả đông thời hai biến liên tục, hoặc geom_boxplot() được gợi ý khi mô tả đồng thời một biến liên tục và một biến rời rạc. Ngoài ra, bên cạnh gợi ý sử dụng, mỗi hàm geom_() sẽ có đi kèm với một danh sách các thuộc tính thẩm mỹ đi kèm. Ví dụ, khi sử dụng hàm geom_point() sẽ có các thuộc tính thẩm mỹ bao gồm x, y, alpha, color, fill, shape, size, và stroke. Bạn đọc cần tham khảo hướng dẫn sử dụng của hàm geom_point() (câu lệnh ? geom_point) để biết các thuộc tính thẩm mỹ này có ý nghĩa như thế nào. Trong các thuộc tính thẩm mỹ được sử dụng với geom_point(), các thuộc tính thẩm mỹ color, fill, shape, và size là các thuộc tính thẩm mỹ xuất hiện ở nhiều hàm geom_() khác. Đây là các thuộc tính thẩm mỹ thường xuyên được sử dụng để tăng khả năng mô tả dữ liệu của các đồ thị ggplot2.Chúng ta tiếp tục với ví dụ về mô tả trực quan mối liên hệ giữa thu nhập bình quân đầu người và tuổi thọ trung bình của các quốc gia trên thế giới vào năm 2011. Để đồ thị giải thích tốt hơn, chúng ta cần đưa thêm thông tin vào đồ thị trong Hình 9.4. Một phương pháp đơn giản để thêm biến khác vào một đồ thị là ánh xạ biến đó đến một trong các thuộc tính thẩm mỹ của đồ thị được vẽ bằng hàm geom_point(). Biến được thêm vào đồ thị trong Hình 9.5 là biến continent. Chúng ta sẽ ánh xạ biến này đến thuộc tính thẩm mỹ color như sau\nHình 9.5: Thu nhập bình quân đầu người và tuổi thọ trung bình của các quốc gia trên thế giới năm 2011. Biến continent được ánh xạ đến thuộc tính thẩm mỹ màu sắc của các điểm\nBằng cách thêm biến continent vào đồ thị và ánh xạ đến thuộc tính thẩm mỹ màu sắc, chúng ta đã có thể đưa ra thêm các phân tích về mối liên hệ giữa tuổi thọ trung bình và thu nhập bình quân đầu người của các quốc gia trên thế giới vào năm 2011:Có sự phân bố không đồng đều về thu nhập bình quân và tuổi thọ trung bình của các quốc gia trên thế giới theo châu lục. Đa số các quốc gia Châu Phi có thu nhập bình quân đầu người thấp và tuổi thọ trung bình thấp trong khi các quốc gia Châu Âu có thu nhập bình quân đầu người cao và tuổi thọ trung bình cao.Có sự phân hóa rõ ràng ở Châu Đại Dương và Châu Mỹ, một vài quốc gia nằm trong nhóm các nước có thu nhập cao, tuổi thọ trung bình cao trong khi đa số các quốc gia còn lại nằm trong nhóm thu nhập thấp và tuổi thọ trung bình thấp. Sự phân hóa ở các nước Châu Á không quá rõ ràng.Các nước có mối liên hệ giữa thu nhập bình quân và tuổi thọ trung bình ít giống như các nước khác là các quốc gia ở Châu Phi và Châu Mỹ.Có một số nguyên tắc chung, tuy không bắt buộc, nhưng khuyến khích khi sử dụng các thuộc tính thẩm mỹ như sauThuộc tính thẩm mỹ color thường được sử dụng với biến kiểu rời rạc.Thuộc tính thẩm mỹ size thường được sử dụng với biến liên tục.Thuộc tính thẩm mỹ shape chỉ có thể được sử dụng với biến rời rạc, R sẽ báo lỗi nếu bạn ánh xạ một biến liên tục vào thuộc tính thẩm mỹ này. Có tổng số 21 giá trị khác nhau cho thuộc tính thẩm mỹ shape và R sẽ đưa ra cảnh báo nếu bạn đọc ánh xạ một biến rời rạc có nhiều hơn 21 giá trị.Đồ thị trong hình 9.6 thêm biến population từ dữ liệu vào đồ thị bằng cách sử dụng thuộc tính thẩm mỹ size. Bạn đọc hãy luôn nhớ rằng khai báo ánh xạ thẩm mỹ từ một biến dữ liệu đến một thuộc tính thẩm mỹ luôn luôn phải thực hiện bên trong hàm aes(),\nHình 9.6: Thu nhập bình quân đầu người và tuổi thọ trung bình của các quốc gia trên thế giới năm 2011. Biến continent ánh xạ đến thuộc tính thẩm mỹ màu sắc của các điểm và biến population ánh xạ đến thuộc tính thẩm mỹ kích thước\nThuộc tính thẩm mỹ alpha sử dụng trong hàm geom_point() được nhận giá trị cố định là 0.5 và có thể hiểu là một phép thiết lập tham số cố định. Chúng ta sẽ phân biết về thiết lập tham số và xạ thẩm mỹ trong phần sau. Tham số alpha được sử dụng trong trường hợp dữ liệu có nhiều điểm bị trùng lên nhau; alpha nhận giá trị từ 0 đến 1 cho biết độ trong suốt của các điểm tăng dần. Khi đồ thị có quá nhiều điểm, để quan sát được dễ dàng hơn chúng ta có thể thiết lập cho tham số alpha nhận giá trị nhỏ hơn 1 để cho phép chúng ta quan sát được nhiều hơn các điểm trên đồ thị phân tán. Lưu ý rằng alpha cũng có thể được sử dụng trong hàm aes() như một thuộc tính thẩm mỹ. Chúng ta sẽ thảo luận chi tiết hơn về thuộc tính thẩm mỹ này trong phần sau của chương sách.Bạn đọc có thể thấy rằng khi thêm biến population bằng cách ánh xạ đến thuộc tính kích thước các điểm như Hình 9.6 đã giúp cho đồ thị có thêm thông tin:Chúng ta có thể nhận ra vị trí của các quốc gia đông dân tiêu biểu như Trung Quốc và Ấn Độ vào năm 2011, có thể nhận thấy hai quốc gia này vẫn nằm trong nhóm các nước có thu nhập bình quân đầu người thấp;Cũng có thể nhận ra Mỹ và Nhật Bản là các quốc gia nằm ở góc trên bên phải là các nước cũng có dân số tương đối lớn với các quốc gia khác. Tất nhiên, với thuộc tính thẩm mỹ màu sắc thì thuộc tính thẩm mỹ kích thước không hiệu quả bằng.Ngoài ra, bạn đọc cũng có thể nhận ra rằng khi cùng sử dụng nhiều thuộc tính thẩm mỹ trên một đồ thị, hiệu quả sẽ không được như mong muốn. Một phương pháp khác để tạo các đồ thị rõ ràng hơn, đặc biệt với các dữ liệu có nhiều quan sát, bạn đọc có thể chia nhỏ dữ liệu thành các nhóm và mô tả dữ liệu trong mỗi nhóm bằng một đồ thị khác nhau. Kỹ thuật này được gọi là facetting và được mô tả trong Hình 9.7\nHình 9.7: Chia dữ liệu thành năm nhóm tương ứng với năm lục địa và sử dụng năm đồ thị phân tán để mô tả phân bố của các điểm dữ liệu\nTrong các câu lệnh vẽ Hình 9.7, chúng tôi không sử dụng biến continent ánh xạ vào thuộc tính thẩm mỹ màu sắc, mà chia nhỏ dữ liệu ra thành 5 phần tương ứng với 5 giá trị trong biến này và mô tả mỗi thành phần của dữ liệu trong một đồ thị riêng biệt. Các đồ thị có miền giá trị trên các trục tọa độ x và y giống nhau để việc sánh trở nên dễ dàng.Có thể nhận thấy từ Hình 9.7 rằng sử dụng năm đồ thị phân tán có cùng khoảng giá trị của trục tọa độ x và y để mô tả mối quan hệ giữa thu nhập bình quân đầu người và tuổi thọ trung bình của các quốc gia thuộc năm châu lục là rõ ràng hơn rất nhiều với sử dụng một đồ thị duy nhất và phân biệt các lục địa khác nhau bằng màu sắc.Một thành phần tuy không bắt buộc nhưng bạn đọc có thể thêm vào đồ thị của ggplot2 để tăng tính thẩm mỹ và sự rành mạch là ngữ cảnh hay còn gọi là các themes. Có một số theme có sẵn khi chúng ta cài đặt thư viện và cũng có các ngữ cảnh nằm trong các thư viện cài đặt bổ sung như thư viện ggthemes. Chúng ta sẽ thảo luận chi tiết về cách tùy chỉnh ngữ cảnh và tự tạo ngữ cảnh cho đồ thị ở phần sau của chương. Để thay đổi ngữ cảnh mặc định của các đồ thị của ggplot2, chúng ta sử dụng các hàm theme_(). Ví dụ, trong Hình 9.8 chúng tôi thay đổi ngữ cảnh mặc định của Hình 9.7 thành ngữ cảnh khác bằng cách sử dụng hàm theme_minimal().\nHình 9.8: Thay đổi ngữ cảnh mặc định của ggplot sang ngữ cảnh khác giúp đồ thị rõ ràng hơn\nThành phần chưa được nhắc đến khi tạo đồ thị trực quan hóa dữ liệu là các statistics hay viết tắt là các stats. đây là thành phần phức tạp nhất và liên quan đến các kiến thức về xây dựng mô hình trên dữ liệu nên chúng tôi chưa đề cập đến trong phần này. Mục tiêu của chúng tôi trong phần giới thiệu là để bạn đọc làm quen với cách sử dụng các câu lệnh vẽ đồ thị trong ggplot2. Trong các phần tiếp theo, từng thành phần của đồ thị và các cấu phần thẩm mỹ quan trọng sẽ được thảo luận chi tiết cùng với ngữ pháp của đồ thị.","code":"\nlibrary(imputeTS) # Thư viện dùng để nội suy tuyến tính\nmydat<-gapminder%>%filter(year<=2011)\nlist_country<-unique(mydat$country)\nfor (ct in list_country){\n  ind<-(mydat$country == ct)\n  if (sum(!is.na(mydat$infant_mortality[ind]))>=2){\n    mydat$infant_mortality[ind]<-na.interpolation(mydat$infant_mortality[ind])\n  }\n  if (sum(!is.na(mydat$gdp[ind]))>=2){\n    mydat$gdp[ind] <- na.interpolation(mydat$gdp[ind])\n  }\n}\n# Biến đổi dữ liệu\ndat<-mydat%>%filter(year==2011)%>%\n  mutate(gdp_per_capita = gdp/population)\n\n# Trực quan hóa\nggplot(dat, aes(x = life_expectancy, y = gdp_per_capita)) +\n  geom_point()\n## geom_histogram() sử dụng các thanh\n## mô tả phân phối của một biến liên tục\nggplot(dat,aes(x = gdp_per_capita))+\n  geom_histogram()\n\n## geom_bar() sử dụng các thanh\n## mô tả phân phối của một biến rời rạc\nggplot(dat,aes(x = continent))+\n  geom_bar()\n\n## geom_boxplot() sử dụng các hình hộp\n## mô tả phân phối của biến liên tục\nggplot(dat,aes(x = continent, y = life_expectancy))+\n  geom_boxplot()\n\n## geom_line() sử dụng đường nối các điểm\n## mô tả các điểm theo thứ tự xuất hiện\ndat1<-filter(gapminder, year<=2011, country == \"United States\")%>%\n  select(year,gdp)\nggplot(dat1,aes(x = year, y = gdp))+\n  geom_line()\n# Vẽ đồ thị phân tán, ánh xạ biến continent đến màu sắc\nggplot(dat, aes(x=life_expectancy, y=gdp_per_capita,\n                color=continent))+\n  geom_point()\n# Vẽ đồ thị phân tán, ánh xạ biến continent đến màu sắc\n# ánh xạ population đến kích thước\nggplot(dat, aes(x = life_expectancy, y = gdp_per_capita,\n                color = continent, size = population)) +\n  geom_point(alpha = 0.5)\n# Dùng facet_wrap để chia dữ liệu ra thành các nhóm\nggplot(dat, aes(x = life_expectancy, y = gdp_per_capita,\n                size = population)) +\n  geom_point(alpha = 0.5)+\n  facet_wrap(~continent)\nggplot(dat, aes(x = life_expectancy, y = gdp_per_capita, size = population)) +\n  geom_point(shape = 21, alpha = 0.5, fill = \"#640514\")+\n  facet_wrap(~continent, ncol = 2)+\n  # thêm title\n  labs( title = \"Thu nhập bình quân và tuổi thọ trung bình\")+\n  xlab(\"Tuổi thọ trung bình (tuổi)\")+\n  ylab(\"Gdp bình quân đầu người (USD)\")+\n  theme_minimal()# thêm ngữ cảnh"},{"path":"trực-quan-hóa-dữ-liệu.html","id":"cấu-trúc-nhiều-lớp-và-ngữ-pháp-của-đồ-thị","chapter":"Chương 9 Trực quan hóa dữ liệu","heading":"9.4 Cấu trúc nhiều lớp và ngữ pháp của đồ thị","text":"Cấu trúc theo lớp (nhiều layers) của đồ thị ggplot2 giúp cho người phân tích trực quan dữ liệu xây dựng đồ thị của mình theo một đối tượng có cấu trúc. Đồ thị được tạo thành từ ggplot2 từ đơn giản đến phức tạp đều được tạo thành từ (ít nhất) một đến nhiều lớp. Mỗi lớp trong đồ thị có mục tiêu hiển thị khác nhau:Mục tiêu hiện thị đầu tiên và cũng là mục tiêu chính, đó là để hiển thị dữ liệu. Luôn luôn có một hoặc một vài lớp chính với mục tiêu mô tả dữ liệu thô, mô tả cấu trúc tổng thể, hoặc mô tả các giá trị ngoại lai của dữ liệu. Lớp này xuất hiện trên tất cả các đồ thị. Trong giai đoạn đầu của quá trình mô tả dữ liệu bằng trực quan hóa, lớp này thường xuất hiện duy nhất. Đơn giản như khi mô tả mỗi quan hệ giữa thu nhập bình quân đầu người và tuổi thọ trung bình của các quốc gia trên thế giới năm 2011, lớp được hiển thị bằng hàm geom_point() là lớp hiển thị dữ liệu chính.Các lớp có mục tiêu tóm tắt và mô tả ý nghĩa thống kê của dữ liệu. Bằng cách thêm vào đồ thị các mô hình hoặc bằng cách hiển thị các dự đoán dựa trên mô hình mà người phân tích dữ liệu và người tiếp nhận dữ liệu sẽ nhận biết được những giá trị bên trong dữ liệu và những chi tiết mà khi xây dựng mô hình có thể bỏ sót.Các lớp có mục tiêu thêm vào ngữ cảnh của dữ liệu. Các lớp này hiển thị bối cảnh nền, thêm vào các chú thích giúp mang lại ý nghĩa cho dữ liệu thô hoặc các giá trị tham chiếu nhằm hỗ trợ việc sánh hoặc đánh giá. Đây thường là lớp cuối cùng được thêm vào trong đồ thị.Lớp chính của đồ thị có thể bao gồm bảy thành phần độc lập giống như chúng ta đã giới thiệu ở phần đầu. Cấu trúc của các lớp còn lại của đồ thị ggplot2 có thể bao gồm các thành phần sau:Dữ liệu: nếu bạn không khai báo dữ liệu trong mỗi lớp, ggplot2 sẽ sử dụng dữ liệu ban đầu là dữ liệu mặc định.Dữ liệu: nếu bạn không khai báo dữ liệu trong mỗi lớp, ggplot2 sẽ sử dụng dữ liệu ban đầu là dữ liệu mặc định.Ánh xạ thẩm mỹ: được khai báo trong hàm aes() trong mỗi lớp, nếu không có khai báo riêng về ánh xạ thẩm mỹ, ggplot2 sẽ sử dụng ánh xạ thẩm mỹ được khai báo trong hàm ggplot().Ánh xạ thẩm mỹ: được khai báo trong hàm aes() trong mỗi lớp, nếu không có khai báo riêng về ánh xạ thẩm mỹ, ggplot2 sẽ sử dụng ánh xạ thẩm mỹ được khai báo trong hàm ggplot().Một, hoặc một vài hình dạng đồ họa được gọi bằng các hàm geom_().Một, hoặc một vài hình dạng đồ họa được gọi bằng các hàm geom_().Một biến đổi thống kê hoặc một tóm tắt của dữ liệu được gọi bằng hàm stat_().Một biến đổi thống kê hoặc một tóm tắt của dữ liệu được gọi bằng hàm stat_().Vị trí xuất hiện của lớp đó trong bố cục chung.Vị trí xuất hiện của lớp đó trong bố cục chung.Khi đồ thị chỉ có một lớp với mục tiêu hiển thị dữ liệu chính, bạn đọc không cần phải hiểu về ngữ pháp của đồ thị. Bạn đọc chỉ cần khai báo chính xác ánh xạ thẩm mỹ trong hàm aes() để có được kết quả mong muốn. Tuy nhiên khi xây dựng đồ thị phức tạp có nhiều lớp, bạn đọc cần phải nắm được ngữ pháp của đồ thị để kết hợp các lớp chính và các lớp phụ lại với nhau theo ý muốn của mình. Thảo luận chi tiết về cấu trúc nhiều lớp của đồ thị sẽ có trong phần tiếp theo.","code":""},{"path":"trực-quan-hóa-dữ-liệu.html","id":"ánh-xạ-thẩm-mỹ-trong-đồ-thị-có-nhiều-lớp","chapter":"Chương 9 Trực quan hóa dữ liệu","heading":"9.4.1 Ánh xạ thẩm mỹ trong đồ thị có nhiều lớp","text":"Để hiểu về cách các lớp tương tác với nhau trong một đồ thị, hãy quan sát Hình 9.9. Sau khi sử dụng một lớp chính là các điểm được gọi bằng hàm geom_point() để mô tả mối liên hệ giữa thu nhập bình quân đầu người và tuổi thọ trung bình của các quốc gia trên thế giới vào năm 2011, bạn muốn thêm vào một lớp phụ là một đường cong mô tả mối liên hệ giữa hai biến. Hàm số dùng để thêm vào một đường mô tả mối liên hệ là geom_smooth()\nHình 9.9: Đồ thị có hai lớp bao gồm một đồ thị phân tán và một đường mô tả mối liên hệ giữa các điểm. Hình bên trái: ánh xạ thẩm mỹ màu sắc được khai báo trong hàm ggplot(). Hình bên phải: ánh xạ thẩm mỹ màu sắc được khai báo trong hàm geom_point()\nBạn đọc có thể thấy sự khác nhau giữa hai đồ thị là như sau:Các đường mô tả mối liên hệ giữa tuổi thọ trung bình và thu nhập bình quân đầu người được xây dựng cho từng lục địa cho hình bên trái Hình 9.9Chỉ có duy nhất một đường được xây dựng cho tất cả các quốc gia trong hình bên phải của Hình 9.9.Sự khác biệt này là :Trong hình bên trái chúng ta khai báo ánh xạ từ biến continent đến thuộc tính thẩm mỹ color bên trong hàm aes() nằm trong hàm ggplot(),Trong khi đó, trong hình bên phải, chúng ta đã khai báo ánh xạ từ biến continent đến thuộc tính thẩm mỹ color bên trong hàm aes() nằm trong hàm geom_point()Như chúng tôi đã đề cập ở trên, hàm geom_point() là lớp chính mô tả dữ liệu thô, còn hàm geom_smooth() là lớp phụ được thêm vào nhằm tăng khả năng mô tả của dữ liệu. Các ánh xạ thẩm mỹ được khai báo trong hàm ggplot() có thể hiểu như khai báo các biến toàn cục trong một đồ thị, còn các ánh xạ thẩm mỹ được khai báo trong các hàm geom_() có thể hiểu như khai báo các biến cục bộ trong hàm số đó. Theo quy tắc chung, các biến cục bộ nếu không được khai báo trong các hàm geom_() sẽ được tìm trên môi trường toàn cục của hàm ggplot(). Trong trường hợp trong các hàm geom_() và ggplot() các thuộc tính thẩm mỹ đều không được khai báo giá trị thì các thuộc tính thẩm mỹ trong các lớp phụ sẽ nhận giá trị mặc định.Dựa trên nguyên tắc này, trong hình bên trái của Hình 9.9, các thuộc tính thẩm mỹ x, y, và color được khai báo trong hàm ggplot(); đồng thời trong các hàm geom_point() và geom_smooth() không khai báo các ánh xạ thẩm mỹ; đó cả hai hàm này đều hiểu các thuộc tính thẩm mỹ x, y, và color giống như khai báo trong hàm ggplot().Trong hình bên phải của Hình 9.9, hai thuộc tính thẩm mỹ x và y được khai báo trong hàm ggplot() trong khi thuộc tính thẩm mỹ color được khai báo bên trong hàm geom_point(). đó, hàm geom_smooth() chỉ hiểu hai thuộc tính thẩm mỹ x và y như được khai báo trong ggplot(). Thuộc tính thẩm mỹ color của hàm geom_smooth() trong hình bên phải của Hình 9.9 sẽ được gán giá trị mặc định.Cách ghi nhận các thuộc tính thẩm mỹ của hai lớp, được gọi bằng các hàm geom_point() và geom_smooth(), của các đồ thị trong Hình 9.9 được tổng kết lại như sau:Đường mô tả mối liên hệ giữa hai biến thu nhập bình quân và tuổi thọ trung bình được xây dựng bằng hàm geom_smooth() dựa trên phương pháp được gọi là hồi quy cục bộ, locally estimated scatterplot smoothing hay viết tắt là loess. Phương pháp này sẽ được chúng tôi sẽ thảo luận trong Chương Mô hình cộng tính tổng quát. Khi thuộc tính thẩm mỹ color được sử dụng và ánh xạ đến một biến rời rạc, hàm geom_smooth() sẽ chia dữ liệu thành các nhóm, mỗi nhóm tương ứng với một giá trị của biến rời rạc ánh xạ đến thuộc tính color, sau đó xây dựng mô hình hồi quy với thu nhập bình quân phụ thuộc vào tuổi thọ trung bình trong mỗi nhóm. Điều này lý giải tại sao trong hình bên trái có năm (5) mô hình được xây dựng tương ứng với năm Châu lục, trong khi trong hình bên phải chỉ có một mô hình duy nhất được xây dựng cho tất cả các quốc gia trên thế giới.Vậy khi nào nên khai báo ánh xạ thẩm mỹ trong hàm ggplot() và khi nào bạn nên khai báo ánh xạ thẩm mỹ bên trong hàm geom_() nói chung? Câu trả lời như sau: nếu đa số các lớp bạn đọc sử dụng có chung một dữ liệu và dùng chung ánh xạ thẩm mỹ, bạn nên khai báo ánh xạ thẩm mỹ bên trong hàm ggplot(). Còn trong trường hợp các lớp sử dụng dữ liệu khác nhau, hoặc có ánh xạ thẩm mỹ khác nhau, bạn hãy khai báo ánh xạ thẩm mỹ bên trong mỗi hàm geom_(). Trong trường hợp bạn dùng một hàm thuộc nhóm các hàm geom_() và không muốn sử dụng ánh xạ thẩm mỹ đã khai báo trong ggplot(), bạn có thể khai báo lại ánh xạ đó, hoặc khai báo thuộc tính thẩm mỹ đó bằng giá trị NULL.Một lưu ý quan trọng cần được thảo luận trong ngữ pháp của đồ thị đó là sự khác nhau giữa sử dụng ánh xạ thẩm mỹ và thiết lập tham số. Trước hết, bạn đọc hãy lưu ý các câu lệnh vẽ đồ thị trực quan và kết quả nhận được trong Hình 9.10\nHình 9.10: Sự khác nhau giữa ánh xạ thẩm mỹ và thiết lập tham số. Thuộc tính thẩm mỹ color trong geom_smooth() được sử dụng theo các cách khác nhau. Hình góc trên bên trái: trong hàm geom_smooth() sử dụng không sử dụng ánh xạ thẩm mỹ đến thuộc tính color. Hình phía trên bê phải, trong hàm geom_smooth() không sử dụng khai báo ánh xạ thẩm mỹ và color được thiết lập cho giá trị là ‘black’. Hình góc dưới bên trái: trong hàm geom_smooth sử dụng khai báo ánh xạ thẩm mỹ va color được ánh xạ đến giá trị ‘black’. Hình góc dưới bên phải: hàm geom_smooth() sử dụng ánh xạ thẩm mỹ và thuộc tính color ánh xạ đến một cột dữ liệu nhận toàn các giá trị là ‘black’\nCó hai cách để chúng ta tác động đến các thuộc tính thẩm mỹ của đồ thị, đó là dùng ánh xạ thẩm mỹ và thiết lập tham số. Sự khác nhau giữa hai cách này là việc bạn khai báo giá trị của thuộc tính thẩm mỹ bên trong hay bên ngoài hàm aes(). Hình 9.10 cho thấy rằng:Hình góc trên bên trái có đường hồi quy liên tục mô tả mối liên hệ giữa hai biến có màu mặc định. Nguyên nhân là khi gọi hàm geom_smooth() chúng ta đã cho thuộc tính thẩm mỹ color nhận giá trị mặc định. Bạn đọc có thể thấy rằng trong câu lệnh vẽ hình góc trên bên trái, thuộc tính color được gọi trong hàm aes() của geom_smooth() và được ánh xạ đến giá trị NULL.Hình góc trên bên phải đã thực hiện thiết lập cấu phần thẩm mỹ color thay vì gọi ánh xạ. Thuộc tính color được gọi bên trong hàm geom_smooth() nhưng không sử dụng aes(). Giá trị được thiết lập là ‘black’, đó đường hồi quy được tạo ra sẽ có màu đen đúng như yêu cầu từ thiết lập thuộc tính thẩm mỹ. Để thiết lập giá trị cho cấu phần thẩm mỹ, bạn đọc cần sử dụng giá trị tương ứng với cấu phần thẩm mỹ tương ứng đó và sử dụng ngoài hàm aes(). Giá trị tương ứng với thuộc tính color có thể là bất kỳ chuỗi ký tự mô tả màu sắc có ý nghĩa trong ngôn ngữ R. Điều gì xảy ra nếu trong cùng một lớp (trong một hàm geom_() bạn đọc vừa sử dụng có ánh xạ thẩm mỹ vừa thiết lập thuộc tính thẩm mỹ? Câu trả lời là ggplot2 sẽ ưu tiên giá trị nằm ngoài aes(), nghĩa là ưu tiên thiết lập tham số.Hình góc dưới bên trái phức tạp hơn các hình phía trên. Trước hết, thuộc tính color được ánh xạ từ biến color trong hàm ggplot(). Sau đó, thuộc tính thẩm mỹ màu sắc lại được được ánh xạ từ giá trị ‘black’ trong hàm aes() của geom_smooth().\nBạn đọc có thể thấy rằng đường hồi quy được tạo từ hàm geom_smooth() không có màu đen như hình ở phía trên bên phải. Nguyên nhân là khi khai báo thuộc tính thẩm mỹ trong hàm aes(), chúng ta đã ánh xạ thuộc tính color của hàm geom_smooth() đến một giá trị kiểu ký tự là ‘black’. ggplot2 sẽ hiểu ‘black’ là một biến kiểu ký tự được ánh xạ đến thuộc tính thẩm mỹ màu sắc.\nTrước đó, trong hàm ggplot() biến continent được ánh xạ đến thuộc tính thẩm mỹ color. Khi chúng ta tiếp tục ánh xạ một biến nhận giá trị ‘black’ tới thuộc tính color thì ggplot2 hiểu rằng có thêm một giá trị mới cho thuộc tính màu sắc là ‘black’ và thêm vào với các giá trị hiện có là tên của 5 châu lục. Điều này lý giải tại sao trong chú giải của hình có 6 loại màu sắc thay vì 5 loại màu sắc như các hình ở trên.\nBạn đọc có thể thấy rằng đường hồi quy được tạo từ hàm geom_smooth() không có màu đen như hình ở phía trên bên phải. Nguyên nhân là khi khai báo thuộc tính thẩm mỹ trong hàm aes(), chúng ta đã ánh xạ thuộc tính color của hàm geom_smooth() đến một giá trị kiểu ký tự là ‘black’. ggplot2 sẽ hiểu ‘black’ là một biến kiểu ký tự được ánh xạ đến thuộc tính thẩm mỹ màu sắc.Trước đó, trong hàm ggplot() biến continent được ánh xạ đến thuộc tính thẩm mỹ color. Khi chúng ta tiếp tục ánh xạ một biến nhận giá trị ‘black’ tới thuộc tính color thì ggplot2 hiểu rằng có thêm một giá trị mới cho thuộc tính màu sắc là ‘black’ và thêm vào với các giá trị hiện có là tên của 5 châu lục. Điều này lý giải tại sao trong chú giải của hình có 6 loại màu sắc thay vì 5 loại màu sắc như các hình ở trên.Hình góc dưới bên phải được vẽ trên dữ liệu sau khi được thêm vào cột mới có tên newcol có tất cả các giá trị đều là ‘black’. Đồ thị vẫn bao gồm hai lớp là geom_point() và geom_smooth(). Cả hai lớp đều sử dụng chung thuộc tính thẩm mỹ x và y. Thuộc tính thẩm mỹ color của geom_point() được ánh xạ đến biến continent trong khi thuộc tính thẩm mỹ color của geom_smooth() ánh xạ đến cột mới được tạo thành. Bạn đọc có thể thấy rằng đồ thị được tạo ra hoàn toàn giống như hình góc dưới bên phải khi chúng ta gán trực tiếp thuộc tính color của geom_smooth() với giá trị ‘black’.Câu hỏi đặt ra là khi nào sử dụng ánh xạ thẩm mỹ và khi nào sử dụng thiết lập giá trị cho các thuộc tính thẩm mỹ. Để trả lời, bạn đọc cần cân nhắc về việc có muốn tác động lên thuộc tính thẩm mỹ nữa hay không. Nếu bạn muốn cố định giá trị cho thuộc tính thẩm mỹ, hãy sử dụng thiết lập giá trị. Còn nếu bạn muốn tác động lên thuộc tính thẩm mỹ sau đó, hãy sử dụng ánh xạ thẩm mỹ. Chúng ta sẽ thảo luận thêm về vấn đề này khi nói về các hàm scale_().Bạn đọc hãy lưu ý về cách chú giải ghi nhận giá trị mới của một thuộc tính thẩm mỹ. Trong đồ thị góc dưới bên trái của Hình 9.10, khi chúng ta khai báo giá trị ‘black’ cho thuộc tính color, đồ thị ghi nhận thêm ‘black’ như một giá trị mới tương đương với tên các Châu lục đã sử dụng trong khai báo trước đó. Cách ghi nhận tên biến mới trong chú giải sẽ rất hữu ích khi chúng ta muốn tạo một đồ thị có nhiều lớp và đặt tên cho từng lớp trong phần chú giải của đồ thị. Ví dụ, khi chúng ta muốn sánh ba phương pháp xây dựng mô hình trong lớp geom_smooth() khi mô tả mối liên hệ giữa biến tuổi thọ trung bình và biến thu nhập bình quân đầu người bao gồm:Phương pháp hồi quy tuyến tính thông thường, sử dụng method = ‘lm’,Phương pháp hồi quy loess, sử dụng method = ‘loess’,Phương pháp hồi quy cộng tính tổng quát method = ‘gam’,chúng ta có thể sử dụng ánh xạ thẩm mỹ color như sau:\nHình 9.11: Sử dụng đồ thị trực quan để sánh ba phương pháp xây dựng mô hình mô tả mối liên hệ giữa tuổi thọ trung binh và thu nhập bình quân đầu người của các quốc gia trên thế giới năm 2011.\nĐồ thị trong Hình 9.11 có bốn lớp. Khi chúng ta khai báo thuộc tính thẩm mỹ x và y trong hàm ggplot(), cả bốn lớp đều sử dụng chung các thuộc tính thẩm mỹ này. Hàm geom_point() không sử dụng thêm ánh xạ thẩm mỹ nào. Mỗi hàm geom_smooth() thêm một đường hồi quy vào trong đồ thị, và thêm một giá trị vào thuộc tính thẩm mỹ color. Kết quả thu được là một đồ thị có ba màu sắc mô tả ba đường hồi quy tương ứng, với chú giải là tên của phương pháp xây dựng đường hổi quy.","code":"\n## Hình bên trái, khai báo color trong ggplot()\np1<-dat%>%ggplot(aes(x = life_expectancy, y = gdp_per_capita,\n                color = continent)) +\n  geom_point(alpha = 0.5)+\n  geom_smooth(se=FALSE)+\n  theme(legend.position = \"none\")+\n  theme_minimal()\n\n## Hình bên phải, khai báo color trong geom_point()\np2<-dat%>%ggplot(aes(x = life_expectancy, y = gdp_per_capita))+\n  geom_point(aes(color = continent), alpha = 0.5) +\n  geom_smooth(se=FALSE)+\n  theme(legend.position = \"none\")+\n  theme_minimal()\n\n## Vẽ p1 và p2 trên cùng một đồ thị\ngrid.arrange(p1,p2,nrow= 1 , ncol = 2)\n## Hình góc trên bên trái, geom_smooth có aes(color = NULL)\np1<-dat%>%ggplot(aes(x = life_expectancy, y = gdp_per_capita,\n                color = continent)) +\n  geom_point(alpha = 0.5)+\n  geom_smooth(aes(color=NULL), se = FALSE)+\n  theme(legend.position = \"right\")+\n  theme_minimal()\n\n## Hình góc trên bên phải, geom_smooth có thiết lập (color = back)\np2<-dat%>%ggplot(aes(x = life_expectancy, y = gdp_per_capita,\n                color = continent)) +\n  geom_point(alpha = 0.5)+\n  geom_smooth(color=\"black\", se = FALSE)+\n  theme(legend.position = \"right\")+\n  theme_minimal()\n\n## Hình góc dưới bên trái, geom_smooth có aes(color = \"black\")\np3<-dat%>%ggplot(aes(x = life_expectancy, y = gdp_per_capita,\n                color = continent)) +\n  geom_point(alpha = 0.5)+\n  geom_smooth(aes(color=\"black\") , se = FALSE)+\n  theme(legend.position = \"right\")+\n  theme_minimal()\n\n\n## Hình góc dưới bên phải, geom_smooth có aes(color = newcol)\np4<-dat%>%mutate(newcol = \"black\")%>%ggplot(aes(x = life_expectancy, y = gdp_per_capita)) +\n  geom_point(aes(color = continent))+\n  geom_smooth(aes(color = newcol) , se = FALSE)+\n  theme(legend.position = \"right\")+\n  theme_minimal()\n\n## Vẽ các đồ thị trên cùng một hình\ngrid.arrange(p1,p2,p3,p4, nrow= 2 , ncol = 2)\n# So sánh ba phương pháp xây dựng mô hình khác nhau của hàm geom_smooth\ndat%>%ggplot(aes(x = life_expectancy, y = gdp_per_capita)) +\n  #Layer 1: đồ thị rải điểm\n  geom_point(alpha = 0.4)+\n\n  # Layer 2: Đường hồi quy tuyến tính\n  geom_smooth(aes(color=\"Hồi quy tuyến tính\"), method = \"lm\" , se = FALSE)+\n\n  # Layer 3: Đường hồi quy loess\n  geom_smooth(aes(color=\"Hồi quy loess\"), method = \"loess\", span = 0.3 , se = FALSE)+\n\n  # Layer 4: Mô hình GAM (generalized additive model)\n  geom_smooth(aes(color=\"Mô hình cộng tính tổng quát\"), method = \"gam\" , se = FALSE)+\n  theme_minimal()"},{"path":"trực-quan-hóa-dữ-liệu.html","id":"các-hàm-geom_-cơ-bản","chapter":"Chương 9 Trực quan hóa dữ liệu","heading":"9.4.2 Các hàm geom_() cơ bản","text":"Các hình dạng đồ họa, gọi tắt là các geoms, là một cách phổ biến để hiển thị một lớp của một đồ thị trực quan. Ví dụ như sử dụng geom_point() sẽ tạo ra một đồ thị phân tán hay còn gọi là đồ thị rải điểm; khi sử dụng geom_line() sẽ tạo ra các đồ thị theo đường. Danh sách các geoms và các thuộc tính thẩm mỹ bạn đọc có thể tìm trong danh sách được liệt kê trong link ở phần @ref(sec:ggplot_intro). Trong phần này, chúng tôi phân loại và giải thích cách sử dụng các geoms chi tiết hơn.","code":""},{"path":"trực-quan-hóa-dữ-liệu.html","id":"mô-tả-một-biến","chapter":"Chương 9 Trực quan hóa dữ liệu","heading":"9.4.2.1 Mô tả một biến","text":"Khi sử dụng đồ thị trực quan bằng thư viện ggplot2 để mô tả một biến rời rạc, hàm thường được sử dụng là hàm geom_bar(). Tần suất xuất hiện của các giá trị trong các biến rời rạc được mô tả dưới dạng các thanh. Các thuộc tính thẩm mỹ quan trọng trong hàm geom_bar() bao gồm:Thuộc tính thẩm mỹ x được ánh xạ đến tên biến (rời rạc).Thuộc tính thẩm mỹ color là màu sắc của đường viền xung quanh các thanh.Thuộc tính thẩm mỹ fill là màu sắc của các thanh. fill ngoài ý nghĩa tăng tính thẩm mỹ cho đồ thị dạng thanh, còn có ý nghĩa khi mô tả mối liên hệ giữa biến rời rạc một biến rời rạc khác.Hình 9.12 sử dụng đồ thị dạng thanh mô tả phân phối xác suất của biến continent trong dữ liệu gapminder được lọc theo năm 2011. Khi sử dụng đồ thị dạng thanh, chúng ta thường sắp xếp sao cho các thanh có chiều cao tăng dần hoặc giảm dần.\nHình 9.12: Phân phối xác suất của biến continent trong dữ liệu gapminder trong năm 2011. Hình bên trái: phân phối xác suất của biến continent mô tả theo thứ tự của giá trị xuất hiện. Hình bên phải: phân phối xác suất của biến continent mô tả theo thứ tự của tần suất xuất hiện của các biến tăng dần.\nKhi mô tả biến rời rạc, geom_bar() luôn mặc định sắp xếp các thanh theo thứ tự các giá trị xuất hiện trong biến rời rạc tăng dần. Tuy nhiên, để đồ thị dạng thanh mô tả hiệu quả hơn, chúng ta thường để các thanh xuất hiện theo thứ tự có chiều cao tăng dần hoặc giảm dần giống như hình bên phải của Hình 9.12. Để thực hiện việc này, chúng ta cần thay đổi thứ tự xuất hiện của các giá trị trên trục \\(\\overrightarrow{Ox}\\) theo thứ tự tần suất tăng dần hay giảm dần bằng cách sử dụng hàm scale_x_discrete(). Chúng ta sẽ thảo luận về các hàm này trong phần sau của chương.Hàm geom_bar() cũng có thể được sử dụng để mô tả các giá trị liên tục được lưu trong một biến tương ứng với các giá trị rời rạc được lưu trong biến khác. Ví dụ, khi chúng ta muốn mô tả thu nhập bình quân của 10 nước có thu nhập bình quân đầu người lớn nhất thế giới năm 2011, chúng ta có thể sử dụng geom_bar() với đầy đủ hai thuộc tính thẩm mỹ x và y như sau:\nHình 9.13: 10 quốc gia có thu nhập bình quân đầu người cao nhất thế giới năm 2011. Hình bên trái: Thứ tự các nước không được sắp xếp theo thu nhập bình quân đầu người. Hình bên phải: các nước xuất hiện theo thứ tụ thu nhập bình quân đâu người giảm dần.\nKhi sử dụng geom_bar() để mô tả hai biến như Hình 9.13, chúng ta cần cho tham số stat nhận giá trị bằng identity để phân biệt với khi sử dụng geom_bar() khi mô tả một biến liên tục. Chúng tôi sẽ giải thích tham số stat trong phần lập trình ggplot2 ở phần cuối của chương. Để biểu diễn các cột theo thứ tự chiều cao tăng dần hay giảm dần, bạn đọc sử dụng hàm reorder(). Trong đồ thị bên phải của Hình 9.13, chúng tôi ánh xạ cấu phần thẩm mỹ y của geom_bar() đến biến country nhưng được sắp xếp theo thứ tự gdp_per_capita của giảm dần.Để mô tả phân phối xác suất của một biến liên tục, chúng ta sử dụng geom_histogram(). Hình dạng đồ họa của geom_histogram() giống với geom_bar() khi cùng sử dụng hình dạng kiểu các thanh/cột để mô tả phân phối xác suất của một biến. Nguyên tắc vẽ đồ thị của hàm geom_histogram() là chia miền giá trị được xác định từ giá trị nhỏ nhất đến giá trị lớn nhất của biến liên tục thành \\(k\\) khoảng bằng nhau, sau đó đếm trong mỗi khoảng có bao nhiêu giá trị của biến liên tục xuất hiện. Chiều cao của các cột là số lần xuất hiện của các giá của biến liên tục trong khoảng đó. Tham số bins trong hàm geom_histogram() được sử dụng để gán giá trị cho số khoảng giá trị. bins càng lớn thì số cột càng lớn và chiểu rộng của các cột càng nhỏ; trong khi bins càng nhỏ thì số cột càng nhỏ và chiều rộng của các cột càng lớn. Lựa chọn số bins phù hợp để mô tả chính xác phân phối xác suất của biến liên tục là rất quan trọng. Ngoài tham số bins, các cấu phần thẩm mỹ của geom_histogram() tương đối giống với geom_bar() nên chúng tôi sẽ không nhắc lại.Hình 9.14 mô tả biến thu nhập bình quân đầu người trong năm 2011 của tất cả các quốc gia trên thế giới.\nHình 9.14: Phân phối của biến thu nhập bình quân đầu người của các quốc gia trên thế giới năm 2011. Hình bên trái: Sử dụng 10 bins để mô tả. Hình bên phải: Sử dụng 30 bins để mô tả.\nNhư chúng tôi đã đề cập, lựa chọn số bins để hiển thị là rất quan trọng trong mô tả phân phối xác suất của một biết liên tục. Trong trường hợp biến có ít quan sát như Hình 9.13, lựa chọn giữa 10 bins hoặc 30 bins không dẫn đến sự khác biệt nhiều. Tuy nhiên, khi biến liên tục có nhiều quan sát, lựa chọn số bins quá ít sẽ làm cho chúng ta hiểu sai về phân phối của biến. Hãy quan sát ví dụ khi chúng ta mô tả biến price trong dữ liệu có tên là diamond. Đây là một dữ liệu nằm trong thư viện dslabs, với biến price là biến chứa giá của hơn 50 nghìn viên kim cương.\nHình 9.15: Phân phối của biến price trong dữ liệu diamonds với hơn 50 nghìn quan sát. Hình bên trái: Sử dụng 10 bins để mô tả. Hình bên phải: Sử dụng 60 bins để mô tả.\nCó thể nhận thấy rằng nếu chúng ta sử dụng quá ít bins để mô tả một số lượng quan sát lớn có thể dẫn đến kết luận sai về phân phối của biến liên tục. Hình bên trái của Hình 9.15 là phân phối của một biến ngẫu nhiên chỉ có một giá trị mode (một đỉnh) tại giá trị $2500 và sau đó có tần suất xuất hiện giảm dần. Hình bên phải của Hình 9.15 lại cho biết đây là một phân phối liên tục có hai đỉnh tại các giá trị 1000 và 4500. Phương pháp tốt nhất để lựa chọn số bins phù hợp đó là tăng dần tham số này cho đến khi phân phối xác suất của biến liên tục không còn bị thay đổi quá nhiều.Cũng để mô tả phân phối xác suất của biến liên tục, geom_density() có thể được sử dụng một cách độc lập hoặc bổ sung với geom_histogram() để mô tả phân phối của các biến liên tục một cách tốt hơn. Hình 9.16 mô tả cách sử dụng geom_density() cùng với geom_histogram() để mô tả phân phối của các biến liên tục.\nHình 9.16: Kết hợp geom_density() và geom_histogram() để mô tả phân phối xác suất của biến liên tục. Hình bên trái: phân phối của biến thu nhập bình quân đầu người của các quốc gia trên thế giới năm 2011. Hình bên phải: phân phối của giá của các viên kim cương trong dữ liệu diamonds\nĐể hiển thị đồng thời đồ thị vẽ bằng geom_histogram() và đồ thị vẽ bằng geom_density() trên cùng một đồ thị, chúng ta cần phải biến đổi đồ thị histogram từ mô tả số lần xuất hiện của biến liên tục thành tần suất xuất hiện. Bạn đọc thực hiện phép biến đổi này bằng cách thêm vào sau phần khai báo ánh xạ thẩm mỹ của geom_histogram() câu lệnh after_stat(density) để tổng diện tích của các hình được tạo bởi các thanh được quy đổi về 1 đơn vị.Phương pháp vẽ đồ thị của geom_density() cũng giống như hàm geom_density() có sẵn trong thư viện stats khi cho kết quả là một đường liên tục là ước lượng cho hàm mật độ xác suất của một biến ngẫu nhiên liên tục. Hàm mật độ này được ước lượng bằng phương pháp Kernel. Giá trị hàm mật độ tại một điểm x bất kỳ nằm trong miền giá trị của một biến liên tục được tính bằng trung bình giá trị hàm \\(K\\), được gọi là hàm Kernel, tính trên khoảng cách từ điểm x tới tất cả các quan sát. Ký hiệu \\(\\hat{f}(x)\\) là giá trị hàm mật độ tính tại x bằng phương pháp Kernel thì ta có\n\\[\\begin{align}\n\\hat{f}(x) = \\cfrac{1}{nh} \\times \\sum\\limits_{= 1}^{n} \\ K\\left( \\cfrac{x - x_i}{h} \\right)\n\\tag{9.1}\n\\end{align}\\]\ntrong đó \\(x_i\\) là giá trị quan sát thứ \\(\\) và h là được gọi là tham số làm mịn. h càng lớn thì hàm \\(\\hat{f}\\) sẽ càng mịn. Hàm \\(K\\) được sử dụng làm hàm Kernel mặc định cho geom_density() là hàm mật độ của biến ngẫu nhiên phân phối chuẩn.Một hàm số khác cũng có thể được sử dụng để mô tả phân phối của một biến liên tục là geom_boxplot() nhưng hàm số này có thể được sử dụng để mô tả mối liên hệ giữa biến rời rạc và biến liên tục nên chúng tôi sẽ thảo luận ở phần sau.","code":"\np1<-dat%>%ggplot() +\n  geom_bar(aes(x = continent), color = \"darkblue\", fill = \"#640514\", alpha = 0.5)+\n  theme_minimal()\np2<-dat%>%ggplot() +\n  geom_bar(aes(x = continent), color = \"darkblue\", fill = \"#640514\", alpha = 0.5)+\n  scale_x_discrete(limits = names(sort(table(dat$continent))))+\n  theme_minimal()\ngrid.arrange(p1,p2, nrow= 1 , ncol = 2)\np1<-dat%>%arrange(desc(gdp_per_capita))%>%head(10)%>%\n  ggplot() +\n  geom_bar(aes(x = gdp_per_capita, y = country),\n           stat = \"identity\",\n           color = \"darkblue\", fill = \"#640514\", alpha = 0.5)+\n  theme_minimal()\np2<-dat%>%arrange(desc(gdp_per_capita))%>%head(10)%>%\n  ggplot() +\n  geom_bar(aes(x = gdp_per_capita, y = reorder(country,gdp_per_capita)),\n           stat = \"identity\",\n           color = \"darkblue\", fill = \"#640514\", alpha = 0.5)+\n  ylab(\"country\")+\n  theme_minimal()\ngrid.arrange(p1,p2, nrow= 1 , ncol = 2)\np1<-dat%>%ggplot() +\n  geom_histogram(aes(x = gdp_per_capita), bins = 10,\n           color = \"darkblue\", fill = \"#640514\", alpha = 0.5)+\n  theme_minimal()\np2<-dat%>%ggplot() +\n  geom_histogram(aes(x = gdp_per_capita), bins = 40,\n           color = \"darkblue\", fill = \"#640514\", alpha = 0.5)+\n  theme_minimal()\ngrid.arrange(p1,p2, nrow= 1 , ncol = 2)\np1<-diamonds%>%ggplot() +\n  geom_histogram(aes(x = price), bins = 10,\n           color = \"darkblue\", fill = \"#640514\", alpha = 0.5)+\n  theme_minimal()\np2<-diamonds%>%ggplot() +\n  geom_histogram(aes(x = price), bins = 60,\n           color = \"darkblue\", fill = \"#640514\", alpha = 0.5)+\n  theme_minimal()\ngrid.arrange(p1,p2, nrow= 1 , ncol = 2)\np1<-dat%>%ggplot() +\n  geom_histogram(aes(x = gdp_per_capita, after_stat(density)), bins = 30,\n          fill = \"#640514\", alpha = 0.2)+\n  geom_density(aes(x = gdp_per_capita), color = \"darkblue\")+\n  theme_classic()\np2<-diamonds%>%ggplot() +\n  geom_histogram(aes(x = price, after_stat(density)), bins = 60,\n           fill = \"#640514\", alpha = 0.2)+\n  geom_density(aes(x = price),color = \"darkblue\")+\n  theme_classic()\ngrid.arrange(p1,p2, nrow= 1 , ncol = 2)"},{"path":"trực-quan-hóa-dữ-liệu.html","id":"mô-tả-hai-biến-liên-tục","chapter":"Chương 9 Trực quan hóa dữ liệu","heading":"9.4.2.2 Mô tả hai biến liên tục","text":"Đồ thị phân tán hay đồ thị rải điểm được trực quan bằng hàm geom_point() là cách hiệu quả nhất để mô tả trực quan hai biến liên tục. Bạn đọc đã làm quen với geom_point() cùng với các thuộc tính thẩm mỹ như x, y, color, shape, size, để mô tả mối liên hệ giữa các biến thu nhập bình quân đầu người và tuổi thọ bình quân của các quốc gia trên thế giới nên chúng tôi sẽ không nhắc lại cách sử dụng các cấu phần thẩm mỹ này khi sử dụng geom_point().Đồ thị phân tán thường được sử dụng cùng với geom_smooth() để mô tả mối liên hệ giữa hai biến. Phương pháp xây dựng mô hình trong hàm geom_smooth() bao gồm:Phương pháp loess là phương pháp mặc định để xây dựng đường hồi quy mô tả mối liên hệ giữa biến liên tục được ánh xạ tới y phụ thuộc vào biến liên tục được ánh xạ tới x. Phương pháp này được sử dụng khi số lượng dòng của dữ liệu nhỏ hơn hoặc bằng 1000. Nguyên nhân là thời gian xây dựng mô hình bằng phương pháp loess tăng lên tỷ lệ với bình phương của số lượng quan sát, đó sẽ mất nhiều thời gian để vẽ đường hồi quy mô tả mối liên hệ giữa hai biến khi dữ liệu lớn.Phương pháp loess là phương pháp mặc định để xây dựng đường hồi quy mô tả mối liên hệ giữa biến liên tục được ánh xạ tới y phụ thuộc vào biến liên tục được ánh xạ tới x. Phương pháp này được sử dụng khi số lượng dòng của dữ liệu nhỏ hơn hoặc bằng 1000. Nguyên nhân là thời gian xây dựng mô hình bằng phương pháp loess tăng lên tỷ lệ với bình phương của số lượng quan sát, đó sẽ mất nhiều thời gian để vẽ đường hồi quy mô tả mối liên hệ giữa hai biến khi dữ liệu lớn.Phương pháp lm sẽ ước lượng đường thẳng tuyến tính mô tả mối liên hệ giữa biến liên tục ánh xạ đến x và biến liên tục ánh xạ đến y.Phương pháp lm sẽ ước lượng đường thẳng tuyến tính mô tả mối liên hệ giữa biến liên tục ánh xạ đến x và biến liên tục ánh xạ đến y.Phương pháp gam sẽ ước lượng một hàm liên tục được biến đến với tên gọi là một smoothing spline mô tả mối liên hệ giữa hai biến liên tục. Đây là phương pháp mặc định để xây dựng mô hình khi dữ liệu có kích thước lớn hơn 1000. Chúng ta sẽ thảo luận về smoothing spline trong Chương mô hình cộng tính tổng quát.Phương pháp gam sẽ ước lượng một hàm liên tục được biến đến với tên gọi là một smoothing spline mô tả mối liên hệ giữa hai biến liên tục. Đây là phương pháp mặc định để xây dựng mô hình khi dữ liệu có kích thước lớn hơn 1000. Chúng ta sẽ thảo luận về smoothing spline trong Chương mô hình cộng tính tổng quát.Trong trường hợp mô tả trực quan hai biến liên tục nhưng có nhiều điểm bị trùng nhau lên nhau thì geom_point() có thể sẽ gây nhầm lẫn về mật độ xuất hiện của các điểm. Hãy quan sát ví dụ khi chúng tôi sử dụng geom_point() để mô tả trực quan hai biến city và hwy của dữ liệu mpg. Đây là dữ liệu nằm trong thư viện ggplot2 mô tả mức độ tiêu hao nhiên liệu của 234 loại xe ô tô khác nhau được sản xuất vào các năm 1998 và 2008 và hai biến city và hwy lần lượt là mức độ tiêu hao nhiên liệu khi ô tô di chuyển trong thành phố và ô tô di chuyển trên đường cao tốc. Đơn vị của hai biến này đều là miles per gallon, nghĩa là cho biết số dặm mà xe đi được trên mỗi gallon nhiên liệu.\nHình 9.17: Sử dụng đồ thị phân tán để mô tả trực quan hai biến liên tục là hwy và cty trong dữ liệu mpg. Hình bên trái: không sử dụng tham số alpha. Hình bên phải sử dụng tham số alpha bằng 0.2 để biết mật độ xuất hiện của các điểm\nBạn đọc có thể nhận thấy rằng số lượng điểm xuất hiện trên đồ thị bên trái của Hình 9.17 không tương ứng với số quan sát của dữ liệu. Nhận xét này được khẳng định thêm từ đồ thị bên phải của Hình 9.17. Khi chúng ta cho tham số alpha nhận giá trị 0.2, độ đậm nhạt của các điểm là rất khác nhau. Điều này cho thấy sự xuất hiện trùng lặp của của các điểm tại một số giá trị nhất định. Để có hiển thị tốt hơn với dữ liệu như vậy, bạn đọc có thể sử dụng geom_jitter() thay thế cho geom_point(). Hàm geom_jitter() cũng hiển thị các điểm trên hai trục tọa độ giống như geom_point(), tuy nhiên khác biệt của geom_jitter() đó là mỗi điểm sẽ được di chuyển các một cách ngẫu nhiên xung quanh điểm ban đầu để tránh việc hiển thị điểm bị trùng nhau.\nHình 9.18: Sử dụng đồ thị phân tán kết hợp với di chuyển ngẫu nhiên bằng geom_jitter() để mô tả trực quan hai biến liên tục là hwy và cty trong dữ liệu mpg. Hình bên trái: các điểm được di chuyển một cách ngẫu nhiên với trung bình bằng 1 đơn vị. Hình bên phải: các điểm được di chuyển một cách ngẫu nhiên với trung bình bằng 3 đơn vị.\nCó thể thấy rằng các điểm dữ liệu đã được hiển thị đầy đủ hơn trong Hình 9.18. Hai tham số quan trọng trong geom_jitter() là width và height cho biết các điểm được di chuyển theo chiều ngang và chiều dọc với giá trị trung bình là bao nhiêu. Nếu lựa chọn giá trị cho hai tham số này quá nhỏ, mục tiêu hiển thị đầy đủ các điểm sẽ không được đảm bảo, trong khi lựa chọn giá trị cho hai tham số này quá lớn sẽ làm cho dữ liệu bị thay đổi về bản chất.Một phương pháp khác được sử dụng để mô tả trực quan hai biến liên tục là geom_text() hoặc geom_label(). Thay vì hiển thị các điểm như geom_point(), geom_text() hiển thị một biến kiểu chuỗi ký tự thay vì hiển thị điểm. geom_text() thường được sử dụng kết hợp với geom_point() hoặc cũng có thể sử dụng độc lập trên những dữ liệu nhỏ. Khi dữ liệu có kích thước trung bình trở lên, geom_text() nên được sử dụng để nhấn mạnh hoặc chú thích cho một vài điểm quan trọng hơn là được sử dụng cho tất cả các điểm. Khi hiển thị biến kiểu ký tự và các điểm trên cùng một đồ thị, rất dễ dẫn đến hiện tượng ký tự và điểm bị trùng nhau hiển thị chồng lên nhau. Để điều chỉnh ký tự xuất hiện về các phía, bạn đọc cần phải sử dụng thêm các tham số như hjust, vjust.Theo kinh nghiệm của chúng tôi, để hiện thị biến kiểu ký tự tốt hơn, nên sử dụng các hàm geom_text_repel() và geom_label_repel() thay thế cho geom_text() và geom_label(). Để sử dụng hai hàm này cần cài đặt thêm thư viện bổ sung là thư viện \\(\\textbf{ggrepel}\\).\nHình 9.19: Tỷ lệ sinh trung bình mỗi phụ nữ và tỷ lệ tử vong trên 1000 trẻ sơ sinh cả các quốc gia Đông Nam Á năm 2011. Hình phía trên bên phải: Sử dụng geom_text. Hình phía trên bên trái: Sử dụng geom_label. Hình phía dưới bên trái: Sử dụng geom_text_rebel. Hình phía dưới bên phải: Sử dụng geom_label_rebel\nKhi một trong hai biến liên tục là biến dạng thời gian thì chúng ta thường sử dụng geom_line() để trực quan dữ liệu. geom_point() cũng có thể sử dụng cùng với geom_line() nếu số lượng điểm dữ liệu không quá lớn. Nguyên tắc vẽ hình của geom_line() là sử dụng các đường thẳng để nối các điểm xuất hiện trong dữ liệu theo thứ tự tăng dần của biến được ánh xạ tới cấu phần thẩm mỹ x. Khi vẽ đồ thị của một biến liên tục theo thời gian, biến thời gian luôn luôn được ánh xạ đến thuộc tính x.\nHình 9.20: Thu nhập bình quân đầu người thay đổi theo thời gian của bốn quốc gia là Pháp, Nhật Bản, Mỹ, và Vương quốc Anh từ năm 1960 đến năm 2016\nHình 9.20 mô tả sự thay đổi của thu nhập bình quân đầu người của các quốc gia phát triển trên thế giới từ năm 1960 đến 2016. Biến thu nhập bình quân đầu người được ánh xạ đến thuộc tính y trong khi biến thời gian (year) được ánh xạ đến thuộc tính thẩm mỹ x. Để mô tả mỗi quốc gia bằng một đường khác nhau, bạn đọc có các lựa chọn là ánh xạ biến country đến một trong các thuộc tính thẩm mỹ là: group, linetype, hoặc color.Khi trực quan hai biến liên tục với số lượng quan sát lớn thì việc hiển thị trực quan bằng các điểm trên trục tọa độ sẽ không hiệu quả. Thay vào đó, chúng ta có thể hiển thị tần xuất hay mật độ xuất hiện của các điểm để đồ thị được rõ ràng hơn. geom_bin2d() và geom_density2d() là các hàm để trực quan hóa phân phối của hai biến liên tục.geom_bin2d() chia miền giá trị của từng biến thành các khoảng bằng nhau sau đó đếm trên mặt phằng hai chiều trong mỗi một hình chữ nhật đơn vị có bao nhiêu điểm sau đó sử dụng màu sắc từ đậm tới nhạt để mô tả số lượng điểm trong từng hình chữ nhật từ lớn đến nhỏ.geom_bin2d() chia miền giá trị của từng biến thành các khoảng bằng nhau sau đó đếm trên mặt phằng hai chiều trong mỗi một hình chữ nhật đơn vị có bao nhiêu điểm sau đó sử dụng màu sắc từ đậm tới nhạt để mô tả số lượng điểm trong từng hình chữ nhật từ lớn đến nhỏ.geom_density2d() sử dụng phương pháp Kernel để tính giá trị hàm mật độ trong không gian hai chiều giống như cách tính toán của geom_density() trong công thức (9.1). Khoảng cách từ quan sát \\((x_{,1}, x_{,2})\\) đến điểm \\((x_1, x_2)\\) cần xác định mật độ được sử dụng là khoảng cách Euclide thay vì \\(|x_i-x|\\) trong không gian một chiều. Hàm Kernel được sử dụng là hàm mật độ của biến ngẫu nhiên phân phối chuẩn hai chiều. Hàm geom_density2d() vẽ các đường nối các điểm có giá trị hàm mật độ bằng nhau lại với nhau, đường này còn được gọi là các đường bàng quang (hay contour). Hình vẽ dưới đây mô tả phân phối đồng thời của hai biến price và carat trong dữ liệu diamond.geom_density2d() sử dụng phương pháp Kernel để tính giá trị hàm mật độ trong không gian hai chiều giống như cách tính toán của geom_density() trong công thức (9.1). Khoảng cách từ quan sát \\((x_{,1}, x_{,2})\\) đến điểm \\((x_1, x_2)\\) cần xác định mật độ được sử dụng là khoảng cách Euclide thay vì \\(|x_i-x|\\) trong không gian một chiều. Hàm Kernel được sử dụng là hàm mật độ của biến ngẫu nhiên phân phối chuẩn hai chiều. Hàm geom_density2d() vẽ các đường nối các điểm có giá trị hàm mật độ bằng nhau lại với nhau, đường này còn được gọi là các đường bàng quang (hay contour). Hình vẽ dưới đây mô tả phân phối đồng thời của hai biến price và carat trong dữ liệu diamond.\nHình 9.21: Biểu diễn phân phối của hai biến liên tục là price và carat trong dữ liệu diamonds. Hình bên trái: sử dụng geom_bin2d() để chia miền giá trị của các biến ra thành các ô vuông nhỏ và sử dụng màu sắc để mô tả mật độ xuất hiện. Hình bên phải: sử dụng geom_density2d() kết nối các điểm có cùng ước lượng của hàm mật độ.\nHình 9.21 cho thấy phân phối đồng thời của hai biến carat và price trong dữ liệu diamond là phân phối có nhiều mode. Các điểm có tập trung mật độ đặc biệt cao là các điểm có (price, carat) bằng (1000, 0.3), (2200,0.7), và (4700, 1.0).","code":"\np1<-mpg%>%ggplot(aes(x = cty,y = hwy)) +\n  geom_point(color = \"#640514\")+\n  geom_smooth(se = FALSE, linetype = 2, size = 0.7, color = \"darkblue\")+\n  theme_minimal()\np2<-mpg%>%ggplot(aes(x = cty,y = hwy)) +\n  geom_point(color = \"#640514\", alpha = 0.2)+\n  geom_smooth(se = FALSE, linetype = 2, size = 0.7, color = \"darkblue\")+\n  theme_minimal()\ngrid.arrange(p1,p2, nrow= 1 , ncol = 2)\np1<-mpg%>%ggplot(aes(x = cty,y = hwy)) +\n  geom_jitter(width = 1, height = 1, alpha = 0.5, color = \"#640514\")+\n  geom_smooth(se = FALSE, linetype = 2, size = 0.7, color = \"darkblue\")+\n  theme_minimal()\np2<-mpg%>%ggplot(aes(x = cty,y = hwy)) +\n  geom_jitter(width = 3, height = 3,alpha = 0.5, color = \"#640514\")+\n  geom_smooth(se = FALSE, linetype = 2, size = 0.7, color = \"darkblue\")+\n  theme_minimal()\ngrid.arrange(p1,p2, nrow= 1 , ncol = 2)\np1<-dat%>%filter(region == \"South-Eastern Asia\")%>%\n  ggplot(aes(fertility, infant_mortality))+\n  geom_point(color = \"#640514\")+\n  geom_text(aes(label = country), size = 2.5,vjust=  - 1.1, color = \"#640514\")+\n  ggtitle(\"Sử dụng geom_text()\")+\n  theme_minimal()\np2<-dat%>%filter(region == \"South-Eastern Asia\")%>%\n  ggplot(aes(fertility, infant_mortality))+\n  geom_point(color = \"#640514\")+\n  geom_label(aes(label = country), size = 2.5, vjust= - 1.1, color = \"#640514\")+\n  ggtitle(\"Sử dụng geom_label()\")+\n  theme_minimal()\n\np3<-dat%>%filter(region == \"South-Eastern Asia\")%>%\n  ggplot(aes(fertility, infant_mortality))+\n  geom_point(color = \"#640514\")+\n  geom_text_repel(aes(label = country), size = 2.5,color = \"#640514\")+\n  ggtitle(\"Sử dụng geom_text_repel()\")+\n  theme_minimal()\np4<-dat%>%filter(region == \"South-Eastern Asia\")%>%\n  ggplot(aes(fertility, infant_mortality), )+\n  geom_point(color = \"#640514\")+\n  geom_label_repel(aes(label = country),size = 2.5,color = \"#640514\")+\n  ggtitle(\"Sử dụng geom_label_repel()\")+\n  theme_minimal()\ngrid.arrange(p1,p2,p3,p4,nrow=2,ncol=2)\ngapminder%>%\n  filter(country %in% c(\"United States\",\"Japan\",\"France\",\"United Kingdom\"))%>%\n  ggplot(aes(x = year, y = gdp/population,color = country))+\n  geom_line()+\n  geom_point(size = 0.5, alpha = 0.5)+\n  theme_minimal()\np1<-diamonds%>%\n  ggplot(aes(price, carat))+geom_bin2d(bins = 40)+\n  scale_x_log10()+scale_y_log10()+\n  theme_minimal()+scale_fill_viridis_c()+\n  ggtitle(\"geom_bind2d\")\n\np2<-diamonds%>%\n  ggplot(aes(price, carat))+geom_density2d(color = \"#640514\",alpha = 0.5)+\n  scale_x_log10()+scale_y_log10()+\n  theme_minimal()+\n  ggtitle(\"geom_density2d\")\n\ngrid.arrange(p1,p2,nrow=1, ncol = 2)"},{"path":"trực-quan-hóa-dữ-liệu.html","id":"mô-tả-một-biến-liên-tục-và-một-biến-rời-rạc","chapter":"Chương 9 Trực quan hóa dữ liệu","heading":"9.4.2.3 Mô tả một biến liên tục và một biến rời rạc","text":"Đồ thị boxplot là phương pháp thông dụng nhất để mô tả mối liên hệ giữa một biến liên tục với một biến rời rạc. Đồ thị boxplot được vẽ bằng hàm geom_boxplot() với thuộc tính thẩm mỹ x được ánh xạ đến biến rời rạc và thuộc tính thẩm mỹ y được ánh xạ đên biến liên tục, hoặc ngược lại. Các thuộc tính thẩm mỹ như color và fill được sử dụng giống như geom_histogram(). Hình 9.22 mô tả mối liên hệ giữa biến thu nhập bình quân đầu người và biến Châu lục trong dữ liệu gapminder được lọc theo năm 2011.\nHình 9.22: Logarit của thu nhập bình quân đầu người của các quốc gia trên thế giới theo Châu lục năm 2011. Hình bên trái: Các châu lục được sắp xếp theo thứ tự tên Châu lục tăng dần. Hình bên phải: các châu lục được sắp xếp theo thứ tự giá trị trung bình của biến thu nhập bình quân đầu người tăng dần\nThứ tự của các đồ thị hình hộp sẽ xuất hiện trên trục \\(\\overrightarrow{Ox}\\) theo thứ tự của các biến rời rạc. Tuy nhiên, để hiệu quả trực quan được tốt hơn, các đồ thị hình hộp nên được sắp xếp theo thứ tự mà giá trị trung bình của biến liên tục tương ứng với mỗi giá trị rời rạc tăng dần giống như đồ thị bên phải của Hình 9.22. Chúng ta có thể thấy rằng thu nhập bình quân đầu người của Châu Mỹ thấp hơn Châu Á mặc dù các giá trị trung vị và tứ phân vị thứ nhất cao hơn. Nguyên nhân là phân phối của biến thu nhập bình quân đầu người của châu Á lệch phải mạnh hơn với phân phối của biến này tại các nước châu Mỹ.geom_boxplot() chỉ thể hiện các giá trị phân vị của phân phối xác suất, nên đôi khi sẽ không cung cấp đầy đủ thông tin về phân phối của biến liên tục. đó, người phân tích dữ liệu thường sử dụng geom_violin() kết hợp cùng với geom_boxplot() để cho mô tả tốt hơn về phân phối xác suất của biến liên tục trong từng nhóm. geom_violin() đơn giản là vẽ hàm mật độ của biến liên tục trong từng nhóm được định nghĩa bởi biến rời rạc.\nHình 9.23: Logarit thu nhập bình quân đầu người của các quốc gia trên thế giới theo Châu lục năm 2011 kết hợp geom_boxplot và geom_violin\nTham số draw_quantiles cho biết các giá ngưỡng phân vị mà chúng ta muốn vẽ cùng với các hàm mật độ. Trong Hình 9.23 sử dụng các ngưỡng phân vị 25%, 50% và 75% tương tự như geom_boxplot().","code":"\np1<-dat%>%ggplot(aes(x = continent, y = log(gdp_per_capita),\n                     fill=continent))+\n  geom_boxplot(color = \"#640514\")+\n  theme_minimal()+\n  theme(legend.position = \"none\")\np2<-dat%>%ggplot(aes(x = reorder(continent, gdp_per_capita),\n                     y = log(gdp_per_capita), fill=continent))+\n  geom_boxplot(color = \"#640514\")+\n  theme_minimal()+\n  theme(legend.position = \"none\")\ngrid.arrange(p1,p2,nrow=1, ncol = 2)\np1<-dat%>%ggplot(aes(x = reorder(continent, log(gdp_per_capita)),\n                     y = log(gdp_per_capita), fill=continent))+\n  geom_boxplot(color = \"#640514\")+\n  theme_minimal()+\n  theme(legend.position = \"none\")\np2<-dat%>%ggplot(aes(x = reorder(continent, log(gdp_per_capita)),\n                     y = log(gdp_per_capita)))+\n  geom_violin(draw_quantiles = c(0.25,0.5,0.75),\n              color = \"#640514\")+\n  theme_minimal()\ngrid.arrange(p1,p2,nrow=1, ncol = 2)"},{"path":"trực-quan-hóa-dữ-liệu.html","id":"mô-tả-hai-biến-rời-rạc","chapter":"Chương 9 Trực quan hóa dữ liệu","heading":"9.4.2.4 Mô tả hai biến rời rạc","text":"Đồ thị thường được sử dụng để mô tả trực quan phân phối của hai biến rời rạc là đồ thị kiểu bong bóng. Hàm số được sử dụng để trực quan đồ thị này là geom_count(). Tương ứng với mỗi cặp giá trị của hai biến rời rạc, hàm geom_count() tính toán số lượng điểm dữ liệu tương ứng với hai giá trị này và phản ánh số lượng điểm dữ liệu lên trên đồ thị thông qua kích thước của mỗi bong bóng. Để mô tả cách sử dụng của geom_count(), chúng ta mô tả phân phối của hai biến rời rạc là cut và color trong dữ liệu diamond như sau:\nHình 9.24: Đồ thị bong bóng mô tả phân phối của hai biến rời rạc là cut và color trong dữ liệu diamonds. Hình bên trái sử dụng hình dạng tròn (shape = 21) để mô tả số điểm dữ liệu. Hình bên phải sử dụng hình vuông (shape = 22) để dễ phân biệt kích thước hơn\nCác thuộc tính thẩm mỹ của geom_count() hoàn toàn giống với geom_point() nên chúng ta có thể lựa chọn các hình dạng cho phép phân biệt kích thước tốt hơn thay vì sử dụng hình tròn như mặc định. Đồ thị trực quan trong Hình 9.24 còn sử dụng hàm scale_size() để giúp cho hiển thị được tốt hơn. Chúng ta sẽ thảo luận về các hàm scale_() trong các phần tiếp theo. Từ Hình 9.24 để nhận ra tỷ lệ lớn các viên kim cương có biến cut nhận giá trị ‘ideal’ và tỷ lệ lớn các viên kim cương có màu sắc nhận giá trị ‘G’. Các viên kim cương có biến cut nhận giá trị \\('ideal'\\) và biến color nhận giá trị ‘G’ cũng xuất hiện nhiều nhất trong dữ liệu với số lượng hơn 4000 viên.Một phương pháp khác để trực quan hai biến rời rạc là trực quan một biến bằng geom_bar() sau đó ánh xạ thuộc tính fill đến biến rời rạc còn lại.","code":"\np1<-diamonds%>%ggplot(aes(cut,color))+\n  geom_count(fill=\"#640514\",alpha = 0.5,shape = 21)+\n  theme_minimal()+scale_size(range = c(1,12))+\n  theme(legend.position = \"top\")\n\np2<-diamonds%>%ggplot(aes(cut,color))+\n  geom_count(fill=\"#640514\",alpha = 0.5,shape = 22)+\n  theme_minimal()+scale_size(range = c(1,12))+\n  theme(legend.position = \"top\")\ngrid.arrange(p1,p2,nrow=1, ncol = 2)"},{"path":"trực-quan-hóa-dữ-liệu.html","id":"mô-tả-ba-biến","chapter":"Chương 9 Trực quan hóa dữ liệu","heading":"9.4.2.5 Mô tả ba biến","text":"Thư viện ggplot2 có hỗ trợ vẽ các hình ảnh trong không gian ba chiều, nhưng sử dụng hình ảnh kiểu 3 chiều không phải là một phương pháp tốt để hiển thị ba biến trên cùng một đồ thị. Khi mô tả mối liên hệ giữa ba biến, phương pháp đơn giản và hiệu quả nhất là sử dụng ba đồ thị, và mỗi đồ thị trực quan mô tả mối liên hệ giữa hai biến với nhau. Hướng tiếp cận khác để mô tả ba biến trên cùng một đồ thị là lựa chọn đồ thị trực quan cho hai biến trước sau đó ánh xạ biến còn lại vào một thuộc tính thẩm mỹ của đồ thị đó.Trong trường hợp hai trong ba biến là biến liên tục và biến còn lại là biến rời rạc, hãy sử dụng đồ thị phân tán để mô tả hai biến liên tục và sử dụng màu sắc để mô tả (ánh xạ) từ biến rời rạc còn lại.Trong trường hợp hai trong ba biến là biến liên tục và biến còn lại là biến rời rạc, hãy sử dụng đồ thị phân tán để mô tả hai biến liên tục và sử dụng màu sắc để mô tả (ánh xạ) từ biến rời rạc còn lại.Khi hai trong ba biến là các biến rời rạc trong khi biến còn lại là biến liên tục, chúng ta có thể sử dụng geom_tile(). Thật vậy, hàm geom_tile() là một phương pháp hiệu quả khi mô tả mối liên hệ giữa hai biến rời rạc với một biến liên tục bằng cách ánh xạ hai biến rời rạc vào các thuộc tính thẩm mỹ x và y, trong khi giá trị của biến liên tục được mô tả thông qua màu sắc. Nếu tương ứng với một cặp giá trị của biến rời rạc ánh xạ đến thuộc tính x và thuộc tính y có nhiều giá trị của biến liên tục thì màu sắc được hiển thị nên là một giá trị thống kê đại diện cho giá trị biến liên tục, giá trị trung bình là một ví dụ. Thật vậy, hãy lấy ví dụ khi chúng ta muốn mô tả ba biến region, year, và life_expectancy của dữ liệu gapminder trên cùng một đồ thị. Các biến region và year là các biến rời rạc, đó sẽ được ánh xạ đến các thuộc tính thẩm mỹ y và x của geom_tile(). Tương ứng với mỗi giá trị của region và year, chúng ta có một véc-tơ giá trị của biến liên tục life_expectancy. Để có thể trực quan hóa được bằng geom_tile(), chúng ta sẽ cần tính life_expectancy trung bình của mỗi vùng trước khi thực hiện trực quan hóa.Khi hai trong ba biến là các biến rời rạc trong khi biến còn lại là biến liên tục, chúng ta có thể sử dụng geom_tile(). Thật vậy, hàm geom_tile() là một phương pháp hiệu quả khi mô tả mối liên hệ giữa hai biến rời rạc với một biến liên tục bằng cách ánh xạ hai biến rời rạc vào các thuộc tính thẩm mỹ x và y, trong khi giá trị của biến liên tục được mô tả thông qua màu sắc. Nếu tương ứng với một cặp giá trị của biến rời rạc ánh xạ đến thuộc tính x và thuộc tính y có nhiều giá trị của biến liên tục thì màu sắc được hiển thị nên là một giá trị thống kê đại diện cho giá trị biến liên tục, giá trị trung bình là một ví dụ. Thật vậy, hãy lấy ví dụ khi chúng ta muốn mô tả ba biến region, year, và life_expectancy của dữ liệu gapminder trên cùng một đồ thị. Các biến region và year là các biến rời rạc, đó sẽ được ánh xạ đến các thuộc tính thẩm mỹ y và x của geom_tile(). Tương ứng với mỗi giá trị của region và year, chúng ta có một véc-tơ giá trị của biến liên tục life_expectancy. Để có thể trực quan hóa được bằng geom_tile(), chúng ta sẽ cần tính life_expectancy trung bình của mỗi vùng trước khi thực hiện trực quan hóa.\nHình 9.25: Tuổi thọ trung bình của các vùng trên thế giới thay đổi qua các năm từ năm 1960 đến năm 2015\nTrong các câu lệnh vẽ Hình 9.25 chúng tôi đã sử dụng thêm các hàm scale_() để kiểm soát các ánh xạ thẩm mỹ: giá trị của biến year xuất hiện trên trục \\(\\overrightarrow{Ox}\\) sẽ là cách đều 5 năm, các vùng trên trục \\(\\overrightarrow{Oy}\\) được sắp xếp theo thứ tự có tuổi thọ trung bình trên toàn bộ dữ liệu tăng dần. Dải màu sắc cũng được gán giá trị cho dải màu liên tục hai giá trị màu sắc khai báo trong các câu lệnh. Chúng ta sẽ thảo luận về scale_() trong phần sau của chương sách.","code":"\n# Tạo danh sách các vùng có tuổi thọ trung bình tăng dần\ndat1<-gapminder%>%group_by(region)%>%\n     summarise(life_expectancy = mean(life_expectancy,na.rm = TRUE))%>%\n    arrange(desc(life_expectancy))\nregion_list<-dat1$region\n\n# Tạo dải màu rời rạc từ cam tới xanh\n#\nmycol<-colorRampPalette(c(\"#EB492E\", \"grey95\"),space = \"Lab\")(5)\nmycol<-c(mycol,colorRampPalette(c(\"grey95\",\"#4C99EB\"),space = \"Lab\")(5) )\n\n#mycol<-hcl_palettes(palette = \"Red-Blue\", n = 5)\n\n# Trực quan hóa 3 biến region, year, và life_expectancy\ngapminder%>%group_by(year,region)%>%\n     summarise(life_expectancy = mean(life_expectancy,na.rm = TRUE))%>%\n     ggplot()+\n     geom_tile(aes(x = year, y = region , fill = life_expectancy),color = \"grey30\", size = 0.1)+\n     scale_fill_gradientn(colours = mycol,\n                          guide = guide_legend(title = \"Tuổi thọ trung bình\") )+\n\n     scale_x_continuous(breaks = seq(1960,2015,5),\n                        limits = c(1960,2015))+\n     scale_y_discrete(limits = region_list)+\n     theme_minimal()+\n    xlab(\"\")+ylab(\"\")+\n    theme(legend.position = \"top\")"},{"path":"trực-quan-hóa-dữ-liệu.html","id":"các-hàm-stat_","chapter":"Chương 9 Trực quan hóa dữ liệu","heading":"9.4.3 Các hàm stat_()","text":"Bạn đọc cũng có thể xây dựng các lớp cho đồ thị ggplot2 bằng cách sử dụng các hàm stat_(). Các hàm số này không hiển thị dữ liệu ở trạng thái ban đầu mà thường hiển thị dữ liệu dưới một phép biến đổi thống kê hoặc một phương pháp tóm tắt dữ liệu. Có sự tương đương giữa các hàm stat_() với các hàm geom_(), nghĩa là chúng ta có thể gọi hàm geom_() bằng một hàm stat_() và ngược lại. Ví dụ stat_bin() tương đương với geom_histogram() và geom_bar(); stat_smooth() tương đương với geom_smooth(). Về bản chất, các geom và các stat đều có nguồn gốc từ chung một hàm tạo một lớp mới cho đồ thị là hàm layer_(). Chúng ta sẽ thảo luận về các hàm này trong phần Kiến thức nâng cao về ggplot2.Ví dụ, thay vì sử dụng geom_(), chúng ta có thể sử dụng stat_() để mô tả phân phối của các biến liên tục:\nHình 9.26: Phân phối của biến tỷ lệ sinh trung bình một phụ nữ (fertility) năm 2011. Hình bên trái: histogram và mật độ của biến fertility. Hình bên phải: phân phối của biến fertility theo các lục địa\nBạn đọc có thể thấy rằng các đồ thị trong Hình 9.26 được tạo bằng các hàm stat_() và cho kết quả hoàn toàn giống với các hàm geom_() tương ứng.","code":"\np1<-dat%>%ggplot(aes(fertility, after_stat(density)))+\n  stat_bin(fill = \"#640514\",alpha = 0.5)+\n  stat_density(color = \"darkblue\",alpha = 0.1)+\n  theme_minimal()\np2<-dat%>%ggplot(aes(x = reorder(continent,fertility), y = fertility,fill = continent))+\n  stat_boxplot(alpha = 0.7, color = \"#640514\")+\n  theme_minimal()+\n  theme(legend.position = \"none\")\ngrid.arrange(p1,p2,nrow=1,ncol=2)"},{"path":"trực-quan-hóa-dữ-liệu.html","id":"nhóm-hàm-scale_","chapter":"Chương 9 Trực quan hóa dữ liệu","heading":"9.5 Nhóm hàm scale_()","text":"Các hàm scale_() trong thư viện ggplot2 được sử dụng để kiểm soát ánh xạ thẩm mỹ từ các biến trong dữ liệu đến thuộc tính thẩm mỹ của đồ thị. Các hàm này sử dụng dữ liệu thô ban đầu để biến đổi thành các đối tượng trực quan mà chúng ta có thể nhìn thấy như kích thước, màu sắc, vị trí, hoặc hình dạng. Bạn đọc có thể tạo đồ thị bằng thư viện ggplot2 mà không cần biết chính xác các ánh xạ thẩm mỹ hoạt động như thế nào vì các cài mặc định của thư viện ggplot2 đã được lựa chọn kỹ càng. Tuy nhiên, hiểu về nguyên tắc biến đổi từ giá trị thành đối tượng trực quan của ggplot2 và hiểu cách hoạt động của các hàm scale_() sẽ giúp bạn kiểm soát tốt những đối tượng trực quan trên đồ thị và tạo được đồ thị trực quan theo ý của mình.","code":""},{"path":"trực-quan-hóa-dữ-liệu.html","id":"vị-trí-xuất-hiện-trên-trục-tọa-độ","chapter":"Chương 9 Trực quan hóa dữ liệu","heading":"9.5.1 Vị trí xuất hiện trên trục tọa độ","text":"Đa số các đồ thị trực quan được vẽ bằng thư viện ggplot2 hiển thị dữ liệu trên trục tọa độ Descartes nên chúng tôi sẽ tập trung vào cách dữ liệu được mô tả khi ánh xạ đến trục tọa độ \\(\\overrightarrow{Ox}\\) và \\(\\overrightarrow{Oy}\\). Khi ánh xạ các biến của dữ liệu tới các trục tọa, nếu chúng ta không sử dụng các hàm scale_(), dữ liệu sẽ được hiển thị đúng như giá trị ban đầu trên các trục tọa độ. Trong nhiều trường hợp, hiển thị tại vị trí đúng như dữ liệu ban đầu sẽ không mang lại hiệu quả. Chúng ta quay trở lại ví dụ khi mô tả hai biến total và population của dữ liệu murders trong thư viện dslabs. Bạn đọc có thể sánh cách hiển thị giữa việc không kiểm soát và có kiểm soát ánh xạ thẩm mỹ như Hình 9.27\nHình 9.27: Hiển thị dân số và số vụ sát nhân bằng súng của các bang trong dữ liệu murders. Hình bên trái: vị trí các điểm trên trục tọa độ là giá trị dữ liệu thô. Hình bên phải: vị trí trên các trục tọa độ đã được biến đổi bằng cách lấy logarit cơ số 10\nCó thể thấy rằng đồ thị bên phải của Hình 9.27 hiển thị rõ ràng hơn đồ thị bên trái. Các điểm dữ liệu hiển thị rõ ràng hơn nhờ vào chúng ta đã gọi các hàm bằng các hàm scale_x_continuous() và scale_y_continuous() để tác động đến ánh xạ từ biến population đến thuộc tính x và ánh xạ từ biến total đến thuộc tính y. Đây là hai hàm số được dùng để kiểm soát vị trí xuất hiện của các điểm trên trục tọa độ khi các biến trong ánh xạ là các biến kiểu số liên tục. Các tham số có thể được sử dụng trong các hàm này bao gồm có:Tham số trans, là viết tắt của transformation, nhận giá trị mặc định là ‘identity’, nghĩa là lấy chính xác giá trị của dữ liệu khi ánh xạ đến các thuộc tính x hoặc y. Để biết các giá trị khác mà tham số này có thể nhận được, bạn đọc có thể tham khảo trong tài liệu đi kèm với các hàm scale_x_continuous() và scale_y_continuous(). Khi sử dụng hàm các hàm này với tham số trans, với các biến \\(X_1\\) và \\(X_2\\) là các biến của dữ liệu được ánh xạ tới các thuộc tính thẩm mỹ x và y, và một hàm \\(f\\) được gán giá trị cho tham số trans, giá trị xuất hiện trên trục tọa độ \\(\\overrightarrow{Ox}\\) và \\(\\overrightarrow{Oy}\\) sẽ tương ứng là \\(f(X_1)\\) và \\(f(X_2)\\). Chẳng hạn như trong đồ thị bên phải của Hình 9.27, khi chúng ta sử dụng hàm scale_x_continuous() và scale_y_continuous(), với tham số trans được gán bằng ‘log10’, tọa độ (giá trị xuất hiện) của mỗi quốc gia trên các trục \\(\\overrightarrow{Ox}\\) và \\(\\overrightarrow{Oy}\\) sẽ là log10(population) và log10(total). Việc chuyển đổi này sẽ hữu ích bởi rất đa số các bang có dân số nhỏ, trong khi có một vài bang có dân số rất lớn. Thực hiện chuyển đổi dữ liệu bằng hàm ‘log10’ sẽ giúp cho khoảng cách của các điểm cách đều nhau hơn và dễ dàng phân biệt hơn với người quan sát đồ thị.Tham số limits giới hạn giá trị trên các thuộc tính thẩm mỹ x và y của đồ thị. Mỗi khi chúng ta vẽ đồ thị và sử dụng thư viện ggplot2, tham số limits mặc định sẽ đảm bảo việc hiển thị được đầy đủ nhất. Trong một vài trường hợp, khi chúng ta cần phải thay đổi các giá trị giới hạn của các trục tọa độ, việc thay đổi miền giá trị là cần thiết để truyền tải ý nghĩa của dữ liệu. Ví dụ như khi muốn sánh hai dữ liệu trên cùng một miền giá trị các biến được ánh xạ tới các trục tọa độ \\(\\overrightarrow{Ox}\\) và \\(\\overrightarrow{Oy}\\): các đồ thị trong Hình 9.28 mô tả hai biến fertility và life_expectancy trong các năm 1960 và 2011 và không sử dụng các hàm scale_()\nHình 9.28: Mối liên hệ giữa hai biến tỷ lệ sinh trung bình của mỗi phụ nữ và tuổi thọ trung bình của tất cả các quốc gia trên thế giới trong năm 1960 và năm 2010. Miền giá trị trên các trục tọa độ là khác nhau. Hình bên trái: dữ liệu năm 1960. Hình bên phải: dữ liệu năm 2011\nKhông thể dễ dàng nhận biết được sự khác biệt giữa hai đồ thị mô tả mối liên hệ giữa hai biến tỷ lệ sinh trung bình và tuổi thọ trung bình của các quốc gia trên thế giới sau 50 năm, từ năm 1960 đến năm 2010, chúng ta không biểu diễn các biến trên cùng một miền giá trị của các trục tọa độ. Để khắc phục vấn đề này, Hình 9.29 sử dụng tham số limits trong các hàm scale_x_continuous() và scale_y_continuous(). Để khai báo tham số cho tham số này, chúng ta sử dụng một véc-tơ hai chiều chứa giá trị nhỏ nhất và giá trị lớn nhất trên trục tọa độ mà bạn muốn hiển thị.\nHình 9.29: Mối liên hệ giữa tỷ lệ sinh trung bình của mỗi phụ nữ và tuổi thọ trung bình của các quốc gia trên thế giới trong năm 1960 và 2010 sử dụng cùng một miền giá trị trên mỗi trục tọa độ. Hình bên trái: dữ liệu năm 1960. Hình bên phải: dữ liệu năm 2010\nTham số breaks kiểm soát vị trí các điểm được đánh dấu xuất hiện trên các trục tọa độ. Chúng tôi thường kết hợp breaks với tham số labels để kiểm soát đồng thời vị trí và cách hiển thị trên các trục số. Ví dụ như trong đồ thị mô tả hai biến fertility và life_expectancy của các năm 1960 và năm 2010, chúng ta muốn giá trị xuất hiện trên trục \\(\\overrightarrow{Ox}\\) là các số 2, 4, 6, 8 và các số trên trục \\(\\overrightarrow{Oy}\\) xuất hiện tại các vị trí 10, 30, 50, 70, và 90, chúng ta chỉ cần khai báo giá trị cho tham số breaks bằng một véc-tơ chứa các giá trị mà chúng ta muốn hiển thị. Lưu ý rằng breaks có chữ s ở cuối để phân biệt với từ khóa break.\nHình 9.30: Mối liên hệ giữa hai biến tỷ lệ sinh trung bình và tuổi thọ trung bình của các quốc gia trên thế giới biểu diễn trên cùng một miền giá trị của mỗi trục tọa độ. Giá trị trên trục tọa độ được định nghĩa lại bằng các tham số breaks và labels. Hình bên trái: dữ liệu năm 1960. Hình bên phải: dữ liệu năm 2011\nKhi một trong hai biến liên tục là biến kiểu thời gian thì hàm số sử dụng để kiểm soát giá trị hiển thị trên trục tọa độ là scale_x_date(), cùng với hai tham số thường được sử dụng là date_break và date_labels. Cách sử dụng của các tham số này tương tựng như các tham số breaks và labels. Bạn đọc có thể tham khảo cách sử dụng của hàm scale_x_date() thông qua ví dụ sau: chúng ta mô tả biến số lượng hành khách trung bình từng tháng được lưu trong dữ liệu AirPassengers theo một biến thời gian bắt đầu từ tháng 01 năm 1949:\nHình 9.31: Số lượng hành khách trung bình theo tháng trong dữ liệu AirPassenger. Hình bên trái: không sử dụng scale_x_date. Hình bên phải: sử dụng scale_x_date để hiển thị tốt hơn giá trị ngày tháng trên trục tọa độ\nKhi giá trị trên trục \\(\\overrightarrow{Ox}\\) hoặc trục \\(\\overrightarrow{Oy}\\) là các giá trị rời rạc, các hàm số sử dụng để kiểm soát ánh xạ thẩm mỹ từ các biến đến các trục tọa độ là các hàm scale_x_discrete() và scale_y_discrete() và các tham số thường sử dụng đi kèm với các hàm này là limits và labels. Tham số limits được sử dụng để cho biết các giá trị nào của biến rời rạc xuất hiện trên đồ thị, trong khi tham số labels cho biết từng giá trị của biến rời rạc xuất hiện như thế nào\nHình 9.32: Phân phối của biến tỷ lệ số vụ xả súng trên một triệu người dân theo vùng vào năm 2010 tại Mỹ. Hình bên trái: không sử dụng scale. Hình bên phải: sử dụng limits trên trục y và labels trên trục x cho hiển thị tốt hơn\n","code":"\np1<-murders%>%ggplot(aes(x = population,y = total))+\n  geom_point(size = 2, shape = 21, color = \"#640514\")+\n  theme_minimal()+ggtitle(\"Không sử dụng scale\")\np2<-murders%>%ggplot(aes(x = population,y = total))+\n  geom_point(size = 2, shape = 21, color = \"#640514\")+\n  theme_minimal()+\n  scale_x_continuous(trans = \"log10\")+\n  scale_y_continuous(trans = \"log10\")+\n  ggtitle(\"Có sử dụng scale (log10)\")\ngrid.arrange(p1,p2,nrow=1,ncol=2)\np1<-gapminder%>%filter(year==1960)%>%\n  ggplot(aes(fertility,life_expectancy))+\n  geom_point(size = 2, shape = 21 ,color = \"#640514\")+\n  ggtitle(\"Năm 1960\")+\n  theme_minimal()\np2<-gapminder%>%filter(year==2011)%>%\n  ggplot(aes(fertility,life_expectancy))+\n  geom_point(size = 2, shape = 21,color = \"#640514\")+\n  ggtitle(\"Năm 2011\")+\n  theme_minimal()\ngrid.arrange(p1,p2,nrow=1,ncol=2)\np1<-gapminder%>%filter(year==1960)%>%\n  ggplot(aes(fertility,life_expectancy))+\n  geom_point(size = 2, shape = 21 ,color = \"#640514\")+\n  ggtitle(\"Năm 1960\")+\n  scale_x_continuous(limits = c(1,9))+\n  scale_y_continuous(limits = c(25,85))+\n  theme_minimal()\np2<-gapminder%>%filter(year==2010)%>%\n  ggplot(aes(fertility,life_expectancy))+\n  geom_point(size = 2, shape = 21,color = \"#640514\")+\n  ggtitle(\"Năm 2010\")+\n  scale_x_continuous(limits = c(1,9))+\n  scale_y_continuous(limits = c(25,85))+\n  theme_minimal()\ngrid.arrange(p1,p2,nrow=1,ncol=2)\np1<-gapminder%>%filter(year==1960)%>%\n  ggplot(aes(fertility,life_expectancy))+\n  geom_point(size = 2, shape = 21 ,color = \"#640514\")+\n  ggtitle(\"Năm 1960\")+\n  scale_x_continuous(limits = c(1,9),\n                     breaks = c(2,4,6,8),\n                     labels = paste(c(2,4,6,8),\"trẻ em\"))+\n  scale_y_continuous(limits = c(25,85),\n                     breaks = c(10,30,50,70,90),\n                     labels = paste(c(10,30,50,70,90),\"tuổi\"))+\n  theme_minimal()\np2<-gapminder%>%filter(year==2010)%>%\n  ggplot(aes(fertility,life_expectancy))+\n  geom_point(size = 2, shape = 21 ,color = \"#640514\")+\n  ggtitle(\"Năm 2010\")+\n  scale_x_continuous(limits = c(1,9),\n                     breaks = c(2,4,6,8),\n                     labels = paste(c(2,4,6,8),\"trẻ em\"))+\n  scale_y_continuous(limits = c(25,85),\n                     breaks = c(10,30,50,70,90),\n                     labels = paste(c(10,30,50,70,90),\"tuổi\"))+\n  theme_minimal()\ngrid.arrange(p1,p2,nrow=1,ncol=2)\ndat0<-data.frame(Number_Passengers = AirPassengers,\n                Month = seq(as.Date(\"1949-01-01\"), by = \"month\", length.out = 144))\np1<-dat0%>%ggplot(aes(x = Month, y = Number_Passengers))+\n  geom_line(color = \"#640514\") + ggtitle(\"Không sử dụng scale\")+\n  theme_minimal()\np2<-dat0%>%ggplot(aes(x = Month, y = Number_Passengers))+\n  geom_line(color = \"#640514\")+ ggtitle(\"Sử dụng scale_x_date()\")+\n  scale_x_date(date_break = \"2 years\", date_labels = \"%b\\n%Y\" )+\n  scale_y_continuous(breaks = seq(100,600,length=6))+\n  theme_minimal()\n\ngrid.arrange(p1,p2,nrow=1,ncol=2)\n# Không sử dụng scale\np1<-murders%>%mutate(rate = total/population*10^6)%>%\n  ggplot(aes(reorder(region,rate),rate, fill = region))+\n  geom_boxplot(color = \"#640514\")+\n  ggtitle(\"Không sử dụng scale\")+\n  theme_minimal()+xlab(\"\")+\n  theme(legend.position = \"none\")\n# Sử dụng tham số labels cho trục x\n# và sử dụng limits cho trục y\np2<-murders%>%mutate(rate = total/population*10^6)%>%\n  ggplot(aes(reorder(region,rate),rate,fill = region))+\n  geom_boxplot(color = \"#640514\")+\n  scale_y_continuous(limits = c(0,50))+\n  # Thay thế giá trị hiển thị trên trục số bằng labels\n  scale_x_discrete(labels = c(\"Northeast\" = \"Đông Bắc\",\n                              \"West\" = \"Miền Tây\",\n                              \"South\" = \"Miền Nam\",\n                              \"North Central\" = \"Miền Bắc\"))+\n  ggtitle(\"Sử dụng limits và labels\")+\n  theme_minimal()+xlab(\"\")+\n  theme(legend.position = \"none\")\ngrid.arrange(p1,p2,nrow=1,ncol=2)"},{"path":"trực-quan-hóa-dữ-liệu.html","id":"màu-sắc-hiển-thị","chapter":"Chương 9 Trực quan hóa dữ liệu","heading":"9.5.2 Màu sắc hiển thị","text":"Thuộc tính thẩm mỹ được sử dụng phổ biến nhất là màu sắc. Có nhiều cách để ánh xạ giá trị của biến tới màu sắc khi trực quan hóa dữ liệu bằng thư viện ggplot2. Vì màu sắc là một chủ đề phức tạp, chúng tôi sẽ bắt đầu bằng thảo luận sơ lược về lý thuyết màu sắc. Sau đó, chúng tôi sẽ giới thiệu đến bạn đọc về thang màu liên tục, thang màu rời rạc và thang màu tổng hợp được sử dụng để ánh xạ các biến rời rạc và biến liên tục trong trực quan hóa dữ liệu. Chúng tôi cũng sẽ đề cập đến các thang màu dành cho biến kiểu thời gian, kiểu ngày tháng, độ trong của các màu sắc hiển thị, và nguyên tắc chú giải cho màu sắc được thiết lập trong các đồ thị của ggplot2.","code":""},{"path":"trực-quan-hóa-dữ-liệu.html","id":"cảm-nhận-của-con-người-về-màu-sắc","chapter":"Chương 9 Trực quan hóa dữ liệu","heading":"9.5.2.1 Cảm nhận của con người về màu sắc","text":"Trong vật lý, màu sắc được tạo ra bởi hỗn hợp các bước sóng ánh sáng. Để mô tả đầy đủ về một màu sắc, chúng ta cần biết sự kết hợp chính xác của các bước sóng. Thực tế, mắt con người chỉ có ba cơ quan cảm nhận màu sắc khác nhau, vì vậy chúng ta có thể tóm tắt khả năng cảm nhận bất kỳ màu nào chỉ bằng ba con số. Một không gian màu quen thuộc với bạn đọc là không gian màu RGB, không gian mà mọi màu sắc được xác định theo cường độ ánh sáng đỏ, xanh da trời và xanh lá cây để tạo ra màu đó. Ưu điểm của không gian màu này là sự đơn giản mỗi màu sắc đều được mô tả bằng ba con số từ 0 đến 255 hoàn toàn độc lập với nhau. Một vấn đề với không gian này là các dải màu liên tục nhận được bằng cách tăng giảm các cường độ màu đỏ, xanh lá cây, và xanh dương lại không giống như cách nhận thức về màu sắc của con người. Khi nhìn vào một màu cụ thể, chúng ta không thể ước tính được cường độ mỗi màu là bao nhiêu, điều này có thể gây khó khăn cho việc tạo ánh xạ từ một biến liên tục sang một dải màu.Mỗi khi hiển thị một giá trị màu sắc trong không gian RGB,ngôn ngữ R cũng như đa số các ngôn ngữ khác thường sử dụng kiểu chuỗi ký tự có 6 chữ số viết theo hệ 16, bắt đầu từ 0 và kết thúc ở F, và bắt đầu bằng một dấu ‘#’. Hai chữ số đầu đại diện cho sắc đỏ, 2 chữ số tiếp theo đại diện cho màu xanh lá cây và 2 chữ số cuối đại diện cho màu xanh da trời. Cường độ ánh sáng của mỗi màu sẽ bắt đầu từ ‘00’ cho đến ‘FF’, nghĩa là có 256 mức độ cho mỗi màu. Bạn đọc có thể dễ dàng suy diễn ra mã của các màu sắc quen thuộc:Màu đen: ‘#000000’Màu trắng: ‘#FFFFFF’Màu đỏ: ‘#FF0000’Màu xanh lá cây: ‘#00FF00’Màu xanh da trời: ‘#0000FF’Dễ dàng suy diễn và nhận biết chính là điểm mạnh của không gian màu RGB. Tuy nhiên, như chúng tôi đã thảo luận, sự liên tục của màu sắc trong không gian này lại không liên tục giống như cách cảm nhận màu sắc của con người. Chính vì thế những nhà nghiên cứu về màu sắc luôn cố gắng xây dựng các không gian màu sắc giống với cảm nhận về màu sắc của con người hơn không gian RGB.Một không gian màu có thể được sử dụng thay thế cho không gian RGB là không gian Lab mà trong đó:L đại diện cho độ tương phản sáng-tối của màu sắc;trục tọa độ cho biết các vị trí của màu trên trục từ xanh lá cây đến đỏ;trục tọa độ b cho biết các vị trí của màu trên trục từ xanh da trời đến màu vàng.Cải tiến từ không gian RGB sang không gian Lab giúp cho các dải màu sắc tương ứng hơn với khả năng nhận biết màu sắc của con người, tuy nhiên vẫn còn khoảng cách giữa không gian Lab với nhận thức màu sắc. Nhìn chung, không gian Lab có các ưu điểm vượt trội hơn không gian RGB đó thư viện ggplot2 mặc định sử dụng không gian Lab khi nội suy tuyến tính các màu sắc nằm giữa hai màu bất kỳ khi chúng ta ánh xạ một biến liên tục lên thuộc tính thẩm mỹ màu sắc.Một không gian màu khác có thể hạn chế vấn đề của không gian RGB là không gian màu HCL với ba thành phần màu: sắc độ (Hue), độ bão hòa (Chroma) và độ sáng (Luminance):Sắc độ (Hue) nằm trong khoảng từ 0 đến 360 (một góc) và cho biết màu sắc chính muốn hiển thị.Độ bão hòa (Chroma) là “độ tinh khiết” hay đậm của một màu, nằm trong khoảng từ 0 (xám) đến mức tối đa thay đổi theo mức độ đậm.Độ sáng là độ sáng của màu, dao động từ 0 (đen) đến 1 (trắng).Ba chiều có những đặc tính khác nhau. Tương tự như không gian màu Lab, màu sắc trong HCL được sắp xếp xung quanh một hình tròn và không được coi là có trật tự; ví dụ: màu xanh lá cây không lớn hơn hay nhỏ hơn màu đỏ và màu xanh da trời không lớn hơn hay nhỏ hơn màu vàng. Ngược lại, độ bão hòa (đậm nhạt) và độ sáng đều được coi là có trật tự: màu hồng được coi là nằm giữa màu đỏ và trắng, và màu xám được coi là nằm giữa màu đen và trắng. Tạo các thang màu sắc từ không gian HCL thường được dựa trên nguyên tắc cố định 2 tham số và thay đổi tham số còn lại. không gian màu HCL gần với nhận thức màu sắc của con người hơn nên các dải màu được tạo ra sẽ “cách đều” nhau hơn theo cách mà chúng ta nhận thức.Xin được nhắc lại với bạn đọc rằng màu sắc là một chủ đề phức tạp mà phạm vi của nó vượt rất xa những gì mà chúng tôi đề cập ở trên. Bạn đọc nên tham khảo thêm các tài liệu chuyên ngành khoa học máy tính để có thể sử dụng màu sắc một cách hiệu quả nhất.","code":""},{"path":"trực-quan-hóa-dữ-liệu.html","id":"dải-màu-liên-tục","chapter":"Chương 9 Trực quan hóa dữ liệu","heading":"9.5.2.2 Dải màu liên tục","text":"Dải màu liên tục được sử dụng để hiển thị giá trị của một biến liên tục trên bề mặt phẳng. Để kiểm soát màu sắc trong thư viện ggplot2, chúng ta sử dụng các hàm scale_color_(). Lưu ý rằng các thuộc tính thẩm mỹ color và fill có thể được sử dụng song song với đa số các hình dạng đồ họa, đó bất kỳ hàm scale_color_() cũng có hàm scale_fill_() tương ứng.Dải màu liên tục thường được sử dụng cùng với các hàm geom_() có hình dạng đồ họa cần màu sắc để phân biệt trên trên mặt phẳng như geom_polygon(), geom_tile(), geom_raster()), và geom_bin2d(). Mỗi khi chúng ta cho một biến liên tục ánh xạ đến thuộc tính thẩm mỹ màu sắc, thư viện ggplot2 sẽ tự động hiểu rằng chúng ta sử dụng dải màu liên tục để mô tả biến đó.Chúng ta sẽ làm quen với các dải màu liên tục thông qua trực quan hóa hàm mật độ của biến ngẫu nhiên phân phối chuẩn hai chiều với trung bình 0, phương sai 1 và hệ số tương quan \\(\\rho = 0.8\\). Lưu ý rằng hàm mật độ của hai biến ngẫu nhiên phân phối chuẩn \\(\\mathcal{N}(0,1)\\) với hệ số tương quan \\(\\rho = 0.8\\) được tính như sau\n\\[\\begin{align}\nf(x,y) = \\cfrac{1}{2 \\pi \\sqrt{1-\\rho^2}} \\ \\exp \\left(- \\cfrac{x^2 + y^2 - 2\\rho x y}{1-\\rho^2}  \\right)\n\\end{align}\\]Đồ thị hàm mật độ của véc-tơ ngẫu nhiên có phân phối chuẩn hai chiều được lưu trong đối tượng có tên p và được tạo thành từ các đoạn câu lệnh như dưới như dưới đâyPhương pháp đơn giản nhất để kiểm soát ánh xạ thẩm mỹ từ một biến liên tục đến thuộc tính màu sắc là lựa chọn các dải màu liên tục có sẵn trong thư viện ggplot2, hoặc trong các thư viện được cài đặt bổ sung. Các dải màu có sẵn này đều đã được xây dựng để ngay cả những người gặp khó khăn trong phân biệt màu sắc cũng có thể cảm nhận được. Trong Hình 9.32, chúng tôi lựa chọn các dải màu như sau:Dải màu liên tục mặc định của thư viện ggplot2;Dải màu liên tục viridis, được gọi bằng hàm scale_fill_viridis_c();Dải màu liên tục distiller, được gọi bằng hàm scale_fill_distiller();Dải màu liên tục fermenter, được gọi bằng hàm scale_fill_fermenter.Trong mỗi hàm scale_fill_() chúng ta đều sử dụng tham số palette để lựa chọn dải các dải màu sắc có sẵn.\nHình 9.33: Hàm mật độ của véc-tơ ngẫu nhiên phân phối chuẩn hai chiều với hệ số tương quan bằng 0.8. Hình góc trên bên trái: dải màu liên tục mặc định của ggplot2. Hình góc trên bên phải: dải màu viridis với option = ‘’. Hình góc dưới bên trái: dải màu liên tục distiller số 3. Hình góc dưới bên phải: dải màu liên tục fermenter số 2\nĐể dải màu sắc liên tục có tính cá nhân hóa cao hơn, bạn đọc có thể chỉ định thang màu sắc thay vì sử dụng các thang màu có sẵn. Nhóm các hàm scale_*_gradient() là các công cụ mạnh mẽ giúp bạn đọc thực hiện việc này. Bạn đọc cần cung cấp các giá trị màu sắc tương ứng với giá trị bắt đầu của dải màu, giá trị kết thúc của dải màu, và có thể thêm một vài giá trị trung gian, thư viện ggplot2 sẽ nội suy tuyến tính ra các màu sắc thành một dải màu tương ứng với các giá trị mà bạn khai báo. Các hàm số có thể được sử dụng để tạo dải màu liên tục bao gồm có:Hàm scale_fill_gradient() tạo một thang màu liên tục giữa hai màu sắc mà bạn khai báo. Hai tham số được sử dụng để khai báo là giá trị bắt đầu và giá trị kết thúc của dải màu là tham số low và tham số high. Đây cũng chính là cách tạo dải màu liên tục mặc định của thư viện ggplot2; mỗi khi chúng ta sử dụng ánh xạ thẩm mỹ từ một biến liên tục đến thuộc tính màu sắc, thư viện ggplot2 sử dụng dải màu liên tục theo hàm số scale_fill_gradient() với giá trị tham số low là #132B43 và giá trị tham số high là #56B1F7. Không gian nội suy tuyến tính thang màu là luôn luôn là không gian màu Lab.Một hàm số khác cũng được dùng để tạo một dải màu liên tục là scale_fill_gradient2(). Ngoài hai tham số low và high tương ứng với là điểm bắt đầu và điểm kết thúc của thang màu, chúng ta cần khai báo thêm một màu ở giữa bằng tham số mid. Ngoài ra, chúng ta cần khai báo tham số midpoint, là giá trị của biến liên tục tương ứng với màu được khai báo với tham số mid! Nếu không khai báo, tham số midpoint sẽ nhận giá trị mặc định là 0.Hàm scale_fill_gradientn() tạo một thang màu liên tục từ một véc-tơ chứa các màu sắc mà bạn đọc khai báo. Dải màu này bắt đầu từ màu sắc có vị trí đầu tiên trong véc-tơ, đi qua lần lượt các màu sắc được khai báo, và kết thúc ở màu sắc tương ứng với giá trị cuối cùng của véc-tơ.Cách sử dụng các hàm scale_*_gradient() được thể hiện thông qua ví dụ trong Hình 9.34\nHình 9.34: Hàm mật độ của biến ngẫu nhiên phân phối chuẩn hai chiều với hệ số tương quan bằng 0.8. Hình góc trên bên trái: dải màu liên tục mặc định của ggplot2. Hình góc trên bên phải: dải màu liên tục bắt đầu từ xanh da trời (low) và kết thúc tại màu đỏ (high). Hình góc dưới bên trái: dải màu liên tục bắt đầu từ xanh da trời (low) đi qua màu trắng (mid) và kết thúc tại màu đỏ (high). Hình góc dưới bên phải: dải màu liên tục bắt đầu từ xanh lá cây đi qua, màu trắng, màu xanh da trời và kết thúc tại màu vàng.\nCả ba hàm số ở trên đều nội suy tuyến tính trong không gian màu Lab để tạo ra các giải màu liên tục. Khi nói đến nội suy tuyến tính giữa hai màu sắc, sẽ dễ hiểu nếu bạn đọc sử dụng không gian RGB mà tất cả các màu đều nằm trong một hình lập phương với điểm (0,0,0) là màu đen, (1,1,1) là màu trắng. Có thể hiểu nội suy màu sắc một cách đơn giản như sau: mỗi màu sắc hiển thị có ba thành phần là cường độ màu đỏ (r), cường độ màu xanh lá (g) cường độ màu xanh lam (b) … Một dải màu bao gồm \\(n\\) màu, bắt đầu từ màu \\(m_1\\) bao gồm các thành phần \\((r_1, g_1, b_1)\\), đến màu \\(m_n\\) với thành phần \\((r_n, g_n, b_n)\\) sẽ là các màu \\(m_i\\) có các thành phần tương ứng\n\\[\\begin{align}\nr_i = \\left[r_1 + (-1) * \\cfrac{r_n - r_1}{(n-1)} \\right] \\\\\ng_i = \\left[g_1 + (-1) * \\cfrac{g_n - g_1}{(n-1)} \\right] \\\\\nb_i = \\left[b_1 + (-1) * \\cfrac{b_n - b_1}{(n-1)} \\right]\n\\end{align}\\]Nói một cách đơn giản, trong không gian RGB dải màu liên tục sẽ là tất cả các điểm nằm trên đường thẳng nối điểm bắt đầu (low) và điểm kết thúc (high). Đáng tiếc là trong không gian Lab việc nội suy màu sắc không đơn giản như vậy. Việc nội suy dựa trên các tính toán phức tạp và kết quả cuối cùng là các công thức gần đúng. Ưu điểm của nội suy màu sắc trong không gian Lab với không gian RGB sự chuyển đổi màu sắc giữa các điểm mượt mà hơn rất nhiều trong cách nhận biết màu sắc của con người.Hàm số để nội suy một véc-tơ màu rời rạc từ hai màu sắc bất kỳ trên không gian RGB hoặc không gian Lab là hàm colorRampPalette() của thư viện grDevices. Ví dụ, chúng ta có thể sử dụng hàm colorRampPalette() để nội suy các véc-tơ màu bắt đầu từ màu xanh da trời và kết thúc ở màu cam như sau:Để sánh hiệu quả khi nội suy dải màu trên không gian RGB và không gian Lab, chúng ta sẽ sử dụng hai dải màu kể trên mô tả hàm mật độ xác suất của biến phân phối chuẩn hai chiều. các hàm scale_fill_gradient() luôn nội suy trên không gian Lab nên để hiển thị dải màu RGB, chúng ta cần rời rạc hóa giá trị mật độ hàm mật độ trước khi ánh xạ đến dải màu rời rạc được tạo ra từ hàm colorRampPalette()\nHình 9.35: Hàm mật độ của véc-tơ ngẫu nhiên phân phối chuẩn hai chiều với hệ số tương quan bằng 0.8. Hình bên trái: dải màu nội suy trên không gian RGB. Hình bên phải: dải màu nội suy trên không gian Lab\nCả hai đồ thị đều sử dụng dải màu liên tục từ màu xanh da trời đến màu cam để mô tả mật độ của phân phối chuẩn hai chiều có hệ số tương quan \\(\\rho=0.8\\). Bạn đọc có thể thấy rằng việc chuyển hóa màu sắc trên không gian màu Lab ít làm thay đổi độ sáng tối của màu sắc và tự nhiên với mắt quan sát hơn với không gian RGB. Đây là lý tại sao các dải màu liên tục của thư viện ggplot2 mặc định sử dụng không gian Lab để nội suy màu sắc.Các tham số limits, breaks, và label cũng có thể được sử dụng trong các hàm scale_fill_() và scale_color_() để kiểm soát các thang màu liên tục.Tham số limits khi sử dụng cần được gán giá trị là một véc-tơ hai phần tử, phần tử thứ nhất cho biết màu sắc bắt đầu trong thang màu tương ứng với giá trị nào trong biến liên tục và phần tử thứ hai cho biết màu sắc kết thúc của thang màu ứng với giá trị nào của biến liên tục.Các tham số breaks và labels được sử dụng để thay đổi giá trị trên thang màu của chú giải.\nHình 9.36: Hàm mật độ của véc-tơ ngẫu nhiên phân phối chuẩn hai chiều với hệ số tương quan bằng 0.8. Hình bên trái: sử dụng tham số limits trong scale_fill_gradient(). Hình bên phải: sử dụng limits, breaks và labels trong scale_fill_gradient()\nĐồ thị bên trái của Hình 9.36 sử dụng giá trị của tham số limits là từ 0 đến 0.8 trong khi giá trị lớn nhất của hàm mật độ tại tâm ellipse chỉ khoảng 0.3. Màu bắt đầu của dải màu là màu xanh da trời tương ứng với giá trị thứ nhất của tham số limits là 0 và màu kết thúc của dải màu là màu cam tương ứng với giá trị thứ hai của tham số limits là 0.8. Điều này giải thích tại sao cả các giá trị nằm trong tâm của hình ellipse chưa chuyển thành màu cam.Đồ thị bên phải của Hình 9.36 sử dụng tham số limits từ 0 đến 0.3 nên các giá trị càng nằm gần tâm đường ellipse càng chuyển sang màu cam. Tham số breaks thay đổi các vị trí giải thích thang màu trên chú giải của thuộc tính màu sắc, trong khi tham số labels kiểm soát cách hiển thị tại các vị trí trên thang màu.","code":"\n# tạo lưới điểm trên hình vuông [-2,2] * [2-,2]\nn<-100\nx<-rep(1:n,n)/n*4-2\ny<-sort(x, decreasing = FALSE)\nrho<-0.8\ndat0<-data.frame(X=x,Y=y,\n                dens = 1/(2*pi*sqrt(1-rho^2)) *\n                  exp(-(x^2+y^2-2*rho*x*y)/(1-rho^2)))\np<-dat0%>%ggplot(aes(x,y,fill=dens))+geom_raster()+\n  theme_minimal()\np1<-p + scale_fill_continuous()+\n  ggtitle(\"Màu mặc định\") # sử dụng dải màu mặc định\np2<-p + scale_fill_viridis_c(option = \"A\")+ # Dải màu viridis liên tục\n  ggtitle(\"Dải màu viridis\")\np3<-p + scale_fill_distiller(palette = 3)+ # Dải màu distiller\n  ggtitle(\"Dải màu distiller\")\np4<-p + scale_fill_fermenter(palette = 2)+ # Dải màu fermenter\n  ggtitle(\"Dải màu fermenter\")\ngrid.arrange(p1,p2,p3,p4,nrow=2,ncol=2)\np1<-p + ggtitle(\"Màu mặc định\") # sử dụng dải màu mặc định\np2<-p + scale_fill_gradient(low = \"blue\", high = \"red\")+\n  ggtitle(\"Dải màu từ xanh lam đến đỏ\") # sử dụng dải màu từ xanh lam đến đỏ\np3<-p + scale_fill_gradient2(low = \"blue\", high = \"red\", mid = \"white\", midpoint = 0.12)+\n  ggtitle(\"Dải màu từ xanh lam đến đỏ điểm giữa là trắng\")\np4<-p +  scale_fill_gradientn(colours = c(\"#00FF00\",\"#FFFFFF\",\"#0000FF\", \"#FFFF00\"))+\n  ggtitle(\"Dải màu đi qua nhiều điểm màu\")\ngrid.arrange(p1,p2,p3,p4,nrow=2,ncol=2)\nn<-200\nmy_rgb_palette<-colorRampPalette(c(\"blue\", \"orange\"),space = \"rgb\")(n+1)\nmy_lab_palette<-colorRampPalette(c(\"blue\", \"orange\"),space = \"Lab\")(n+1)\nx<-rep(1:n,n)/n*4-2\ny<-sort(x, decreasing = FALSE)\nrho<-0.8\ndat0<-data.frame(x=x,y=y,dens = 1/(2*pi*sqrt(1-rho^2)) * exp(-(x^2+y^2-2*rho*x*y)/(1-rho^2)))\nh<-(max(dat0$dens)-min(dat0$dens))/n\ndat0<-mutate(dat0,dens.d = round((dens - min(dat0$dens))/h))\ndat0$dens.d<-as.factor(dat0$dens.d)\n\nmycol<-colorRampPalette(c(\"blue\", \"orange\"),space = \"rgb\")(n+1)\n\np1<-dat0%>%ggplot(aes(x,y,fill=dens.d))+geom_raster()+theme_minimal()+\n  scale_fill_manual(values= mycol)+\n  theme(legend.position = \"none\")+\n  ggtitle(\"Nội suy trên RGB\")\n\nmycol<-colorRampPalette(c(\"blue\", \"orange\"),space = \"Lab\")(n+1)\np2<-dat0%>%ggplot(aes(x,y,fill=dens.d))+geom_raster()+theme_minimal()+\n  scale_fill_manual(values= mycol)+\n  theme(legend.position = \"none\")+\n  ggtitle(\"Nội suy trên Lab\")\ngrid.arrange(p1,p2,ncol=2,nrow=1)\n# limits cho biết hai giá trị tương ứng với điểm đầu và cuối của dải màu\np1<-p + scale_fill_gradient(low = \"blue\", high = \"orange\",\n                            limits = c(0,0.8))+\n  ggtitle(\"Tham số limits\")\n# breaks cho biết các giá trị nào xuất hiện trên chú giải\n# labels cho biết giá trị hiển thị trong chú giải\np2<-p + scale_fill_gradient(low = \"blue\", high = \"orange\",\n                            limits = c(0,0.3),\n                            breaks = c(0.1,0.15,0.25),\n                            labels = paste(\"Density at\", c(0.1,0.15,0.25)))+\n  ggtitle(\"Tham số breaks và labels\")\ngrid.arrange(p1,p2,nrow=1,ncol=2)"},{"path":"trực-quan-hóa-dữ-liệu.html","id":"dải-màu-rời-rạc","chapter":"Chương 9 Trực quan hóa dữ liệu","heading":"9.5.2.3 Dải màu rời rạc","text":"Dải màu rời rạc dùng để mô tả thuộc tính thẩm mỹ màu sắc khi ánh xạ đến thuộc tính thẩm mỹ này là các biến rời rạc. Hàm số dùng để kiểm soát màu sắc rời rạc khi trực quan hóa dữ liệu bằng thư viện ggplot2 là các hàm scale_fill_discrete() và scale_color_discrete(). Mỗi khi sử dụng các hàm số kể trên, thư viện ggplot2 sẽ mặc định sử dụng dải màu rời rạc cách đều nhau trong không gian màu HCL. Dải màu rời rạc mặc định có cùng độ bão hòa (Chroma), được ký hiệu bằng tham số c, cùng độ sáng (Luminance), được ký bằng tham số l, và khác nhau về sắc độ (Hue), được ký hiệu bằng tham số h. Sắc độ là tham số chính trong ba cấu phần và nhận giá trị từ 0 đến 360 (độ). Sắc độ h của một dải màu rời rạc mặc định luôn cách đều nhau với giá trị ban đầu là 15 (độ).Bạn đọc muốn sử dụng các dải màu rời rạc trong không gian HCL để ánh xạ tới biến rời rạc thì có thể sử dụng các hàm scale_fill_hue() và scale_color_hue() thay thế cho scale_fill_discrete() và scale_color_discrete(). Cách sử dụng các hàm số này được thể hiện qua ví dụ trong Hình 9.37\nHình 9.37: Mô tả phân phối của biến continent trong dữ liệu gapminder lọc theo năm 2011. Hình bên trái: Sử dụng dải màu rời rạc mặc định. Hình ở giữa: sử dụng dải màu rời rạc trong không gian HCL với sắc độ (tham số h) thay đổi. Hình bên phải: sử dụng dải màu rời rạc trong không gian HCL với độ bão hòa (tham số c) thay đổi\nDải màu mặc định đối với biến rời rạc sử dụng tham số c bằng 100 và tham số l bằng 65 trong khi tham số h nhận các giá trị cách đều nhau, bắt đầu từ h = 15 (độ). Lưu ý rằng:h nhận giá trị từ 0 độ đến 360 độ nên trong trong đồ thị ở trên, khi biến continent có năm giá trị, các màu sắc sẽ lần lượt nhận các giá trị với tham số h là \\(15\\), \\(15 + 360/5\\), \\(15 + 2 \\times 360/5\\), \\(15 + 3 \\times 360/5\\) và \\(15 + 4 \\times 360/5\\). Đó là màu sắc của các thanh trong đồ thị bên trái của Hình 9.37 theo thứ tự từ trái qua phải.Trong đồ thị ở giữa, khi chúng ta tăng giá trị ban đầu của h thêm 360/5 (độ), chúng ta có thể thấy các màu sắc bắt đầu từ \\(h = 15 + 360/5\\) và kết thúc ở \\(h = 15\\). Nghĩa là màu sắc trong thanh thứ nhất của đồ thị bên trái đã trở thành màu sắc của thanh thứ năm trong đồ thị ở giữa.Trong đồ thị bên phải, chúng tôi giảm độ chói (tham số c) xuống còn 40. Chúng ta có thể thấy dải màu vẫn tương tự như hai đồ thị còn lại nhưng có sự khác biệt về độ chói.Bạn đọc cũng có thể sử dụng các dải màu rời rạc được thiết kế sẵn trong thư viện ggplot2. Dải màu rời rạc mà chúng tôi thường sử dụng là dải màu brewer. Những dải màu này được thiết kế để hoạt động tốt trong nhiều tình huống khác nhau kể cả đối với những người khó khăn khi nhận biết màu sắc hay khi sử dụng để hiển thị trên những bề mặt lớn. Hàm số để kiểm soát ánh xạ thẩm mỹ màu sắc sử dụng dải màu brewer là scale_color_brewer() và scale_fill_brewer(). Bạn đọc cần cài đặt thư viện RColorBrewer để sử dụng được các hàm này. Để xem các dải màu có sẵn trong thư viện này, bạn đọc sử dụng câu lệnh sauTham số palette trong hàm scale_color_brewer() được sử dụng để lựa chọn dải màu:\nHình 9.38: Mô tả phân phối xác suất của biến continent trong dữ liệu gapminder lọc theo năm 2011 và màu sắc sử dụng là dải màu brewer. Hình bên trái: sử dụng dải màu Dark2. Hình ở giữa: sử dụng dải màu Set1. Hình bên phải: sử dụng dải màu Spectral\nBạn đọc có thể tự tạo ra dải màu rời rạc cho các giá trị của thuộc tính thẩm mỹ màu sắc bằng cách sử dụng các hàm scale_fill_manual() và scale_color_manual(). Tham số values được sử dụng để nhận giá trị là véc-tơ chứa màu sắc mà bạn đọc tự tạo. Số lượng phần tử trong véc-tơ phải tương ứng với số lượng phần tử trong biến rời rạc.Như chúng tôi đã giới thiệu trong phần dải màu sắc liên tục, hàm số colorRampPalette() của thư viện grDevices có thể được sử dụng để nội suy ra một véc-tơ màu rời rạc giữa hai giá trị màu cho trước. Ví dụ, để tạo ra một véc-tơ có độ dài 5, mỗi giá trị là một màu sắc được nội suy tuyến tính từ màu xanh da trời đến màu cam chúng ta viết câu lệnh như sau:Các đồ thị trong Hình 9.39 sử dụng các véc-tơ màu sắc rời rạc tự định nghĩa bằng cách liệt kê các màu sắc trong scale_fill_manual() và bằng nội suy tuyến tính trong không gian RGB và không gian Lab.\nHình 9.39: Mô tả phân phối của biến continent trong dữ liệu gapminder được lọc theo năm 2011 và sử dụng dải màu tự định nghĩa. Hình bên trái: dải màu được tự định nghĩa bằng cách liệt kê tên các màu sắc. Hình ở giữa: nội suy trong không gian RGB giữa xanh da trời và màu cam. Hình bên phải: nội suy trong không gian Lab giữa màu xanh da trời và màu cam\nCách sử dụng tham số limits, breaks, và label trong các hàm scale_fill_manual() và scale_color_manual() cũng tương tự như khi sử dụng đối với dải màu liên tục:Tham số limits cho biết các giá trị nào trong biến rời rạc được ánh xạ tới dải màu sắc.Tham số breaks cho biết các giá trị nào không được sử dụng trong ánh xạ thẩm mỹ.Tham số label cho biết cách các màu sắc hiển thị trong phần chú giải.Sự thật thì tham số breaks không có nhiều ý nghĩa khi sử dụng đối với dải màu sắc rời rạc, trong khi tham số limits có ý nghĩa quan trọng khi bạn đọc cần cố định ánh xạ màu sắc lên biến rời rạc khi vẽ nhiều biểu đồ khác nhau và để kiểm soát thứ tự xuất hiện của biến liên tục trên đồ thị. Hãy quan sát ví dụ sau để thấy sự quan trọng của tham số limits\nHình 9.40: Dân số của ba nước Philippines, Việt Nam, và Indonesia trong top 10 nước đông dân nhất châu Á. Hình bên trái: Dữ liệu năm 1960. Hình bên phải: Dữ liệu năm 2011\nChúng tôi đã sử dụng tham số limits để cố định màu sắc tương ứng với các giá trị của biến rời rạc như trong Hình 9.40. Giá trị biến rời rạc là Philippines được cố định với màu sắc là xanh da trời, Vietnam được cố định với màu đỏ, trong khi Indonesia được cố định với màu vàng. Bạn đọc có thể dễ dàng nhận ra sự thay đổi về thứ hạng về quy mô dân số của ba 3 quốc gia Philippines, Việt Nam, và Indonesia trong nhóm 10 nước có dân số lớn nhất châu Á trong các năm 1960 và 2010.","code":"\np1<-gapminder%>%filter(year==2011)%>%\n  ggplot(aes(continent,fill=continent))+\n  geom_bar()+ggtitle(\"Màu rời rạc mặc định\")+\n  theme_minimal()+\n  theme(legend.position = \"none\")\np2<-gapminder%>%filter(year==2011)%>%\n  ggplot(aes(continent,fill=continent))+geom_bar()+\n  scale_fill_hue(h=c(0,360)+15+360/5)+\n  ggtitle(\"Thay đổi tham số h\")+\n  theme_minimal()+\n  theme(legend.position = \"none\")\np3<-gapminder%>%filter(year==2011)%>%\n  ggplot(aes(continent,fill=continent))+geom_bar()+\n  scale_fill_hue(c=30)+ggtitle(\"Thay đổi tham số c\")+\n  theme_minimal()+\n  theme(legend.position = \"none\")\ngrid.arrange(p1,p2,p3,nrow=1,ncol=3)\ndisplay.brewer.all()\np1<-gapminder%>%filter(year==2011)%>%\n  ggplot(aes(continent,fill=continent))+geom_bar()+\n  scale_fill_brewer(palette = \"Dark2\")+\n  ggtitle(\"Sử dụng dải màu Dark2\")+\n  theme_minimal()+\n  theme(legend.position = \"none\")\np2<-gapminder%>%filter(year==2011)%>%\n  ggplot(aes(continent,fill=continent))+geom_bar()+\n  scale_fill_brewer(palette = \"Set1\")+\n  ggtitle(\"Sử dụng dải màu Set1\")+\n  theme_minimal()+\n  theme(legend.position = \"none\")\np3<-gapminder%>%filter(year==2011)%>%\n  ggplot(aes(continent,fill=continent))+geom_bar()+\n  scale_fill_brewer(palette = \"Spectral\")+\n  ggtitle(\"Sử dụng dải màu Spectral\")+\n  theme_minimal()+\n  theme(legend.position = \"none\")\ngrid.arrange(p1,p2,p3,nrow=1,ncol=3)\n# nội suy trong RGB\nmypalette1<-colorRampPalette(c(\"blue\",\"orange\"), space = \"rgb\")(5)\n# nội suy trong Lab\nmypalette2<-colorRampPalette(c(\"blue\",\"orange\"), space = \"Lab\")(5)\np1<-gapminder%>%filter(year==2011)%>%\n  ggplot(aes(continent,fill=continent))+geom_bar()+\n  scale_fill_manual(values = c(\"blue\",\"green\",\"grey\",\"yellow\",\"orange\"))+\n  ggtitle(\"Màu tự định nghĩa\")+\n  theme_minimal()+\n  theme(legend.position = \"none\")\np2<-gapminder%>%filter(year==2011)%>%\n  ggplot(aes(continent,fill=continent))+geom_bar()+\n  scale_fill_manual(values = mypalette1)+\n  ggtitle(\"Màu nội suy trong RGB\")+\n  theme_minimal()+\n  theme(legend.position = \"none\")\np3<-gapminder%>%filter(year==2011)%>%\n  ggplot(aes(continent,fill=continent))+geom_bar()+\n  scale_fill_manual(values = mypalette2)+\n  ggtitle(\"Màu nội suy trong Lab\")+\n  theme_minimal()+\n  theme(legend.position = \"none\")\ngrid.arrange(p1,p2,p3,nrow=1,ncol=3)\np1<-gapminder%>%filter(year==1960, continent == \"Asia\")%>%\n  arrange(-population)%>%head(10)%>%\n  ggplot(aes(fill=country))+\n  geom_bar(aes(x = population, y = reorder(country,population)),\n           stat=\"identity\",col=\"darkblue\", alpha = 0.7)+\n  ylab(\"\")+ggtitle(\"Năm 1960\")+theme_minimal()+\n  theme(legend.position = \"top\")+\n  scale_x_continuous(labels = scales::comma)+\n  scale_fill_manual(values = c(\"blue\",\"red\",\"yellow\"), limits = c(\"Philippines\",\"Vietnam\", \"Indonesia\"))\np2<-gapminder%>%filter(year==2011, continent == \"Asia\")%>%\n  arrange(-population)%>%head(10)%>%\n  ggplot(aes(fill=country))+\n  geom_bar(aes(x = population, y = reorder(country,population)),\n           stat=\"identity\",col=\"darkblue\", alpha = 0.7)+\n  ylab(\"\")+ggtitle(\"Năm 2010\")+theme_minimal()+theme(legend.position = \"top\")+\n  scale_x_continuous(labels = scales::comma)+\n  scale_fill_manual(values = c(\"blue\",\"red\",\"yellow\"), limits = c(\"Philippines\",\"Vietnam\", \"Indonesia\"))\n\ngrid.arrange(p1,p2,nrow=1,ncol=2)"},{"path":"trực-quan-hóa-dữ-liệu.html","id":"các-thuộc-tính-thẩm-mỹ-khác","chapter":"Chương 9 Trực quan hóa dữ liệu","heading":"9.5.3 Các thuộc tính thẩm mỹ khác","text":"Ngoài vị trí trên các trục tọa độ và màu sắc, còn có một số thuộc tính thẩm mỹ khác mà thư viện ggplot2 có thể sử dụng để mô tả trực quan dữ liệu. Trong phần này, chúng ta sẽ xem xét thuộc tính kích thước (size), hình dạng (shape), chiều rộng của line (linewidth) và kiểu line (linetype) khi sử dụng cùng với các thuộc tính vị trí trên trục tọa độ và màu sắc để trực quan một cách hiệu quả nhất các biến trong dữ liệu. Ngoài đề cập đến các giá trị mặc định, chúng tôi cũng sẽ thảo luận về các hàm số thuộc nhóm scale_() tương ứng với các thuộc tính thẩm mỹ để bạn đọc có thể sử dụng để kiểm soát tốt các thuộc tính này.","code":""},{"path":"trực-quan-hóa-dữ-liệu.html","id":"thuộc-tính-thẩm-mỹ-kích-thước","chapter":"Chương 9 Trực quan hóa dữ liệu","heading":"9.5.3.1 Thuộc tính thẩm mỹ kích thước","text":"Thuộc tính thẩm mỹ kích thước (size) thường được sử dụng để mô tả hình dạng đồ họa kiểu điểm hoặc ký tự. Như chúng tôi đã đề cập trong phần giới thiệu, thuộc tính kích thước cho hiệu quả tốt nhất khi được ánh xạ từ các biến liên tục. Nếu không có hàm kiểm soát ánh xạ thẩm mỹ, bán kính của điểm tương ứng với giá trị nhỏ nhất luôn là 1 và bán kính của điểm có giá trị lớn nhất luôn là 6, nghĩa là có bán kính gấp 6 lần bán kính của điểm nhỏ nhất. Khi nội suy ra kích thước của các điểm khác, thư viện ggplot2 mặc định cho kích thước của điểm là diện tích của hình tròn mô tả điểm đó chứ không phải bán kính của hình tròn. Hơn thế nữa, kích thước của điểm sẽ phụ thuộc vào thứ hạng (rank) của giá trị đó trong biến liên tục chứ không được tính bằng giá trị thực của điểm đó. Thực vậy, nếu \\(area_m\\) là diện tích của hình tròn tương ứng với giá trị nhỏ nhất và \\(area_M\\) tương ứng với diện tích của hình tròn tương ứng với giá trị lớn nhất thì diện tích của hình tròn tương ứng với giá trị có thứ hạng \\(k\\) trong tổng số \\(n\\) giá trị của biến liên tục là\n\\[\\begin{align}\narea = area_m + (k-1) \\times \\cfrac{area_M - area_m}{n - 1}\n\\end{align}\\]Để hiểu về cách thư viện ggplot2 ánh xạ kích thước đến giá trị các biến, bạn đọc có thể quan sát kích thước của các hình tròn trong Hình 9.41\nHình 9.41: Ánh xạ véc-tơ số tự nhiên đến thuộc tính thẩm mỹ kích thước. Hình bên trái: ánh xạ các số 1, 2, 3 đến kích thước ba hình tròn. Hình ở giữa: ánh xạ bình phương của 1, 2, 3 đến kích thước các hình tròn. Hình bên phải: ánh xạ hai số 2 và 3 đến kích thước của hai hình tròn.\nTừ Hình 9.41 chúng ta có thể thấy rằng:Trong đồ thị bên trái: kích thước của các hình tròn ở các tọa độ (1,1), (2,2), và (3,3) được ánh xạ đến các giá trị số lần lượt là 1, 2, và 3. Tham số mặc định của các hàm scale_size_() là range = c(1,6) nên hình tròn tại vị trí tọa độ (1,1) có bán kính là 1 trong khi hình tròn ở vị trí tọa độ (3,3) có bán kính là 6. Diện tích của hình tròn nằm ở tọa độ (2,2) được nội suy theo diện tích của hai hình tròn tại tọa độ (1,1) và (3,3) bằng trung bình cộng diện tích của hình tròn nằm ở vị trí (1,1) và (3,3). diện tích của hình tròn nằm ở vị trí (3,3) bằng \\(6^2 = 36\\) lần diện tích của hình tròn tại vị trí \\((1,1)\\) nên diện tích của hình tròn tại (2,2) bằng \\(\\cfrac{36+1}{2} = 18,5 \\textit{(lần)}\\) diện tích hình tròn tại tọa độ (1,1), hay nói cách khác bán kính của hình tròn tại vị trí (2,2) bằng \\(\\sqrt{18,5} \\sim 4,3 \\text{ (lần)}\\) bán kính của hình tròn tại vị trí (1,1).Đồ thị ở giữa: chúng ta ánh xạ thuộc tính thẩm mỹ kích thước với \\(z^2\\), nghĩa là các giá trị thực của biến được ánh xạ đến thuộc tính kích thước là \\(1^2\\), \\(2^2\\), và \\(3^2\\). Tuy nhiên, kích thước các hình tròn xuất hiện vẫn không hề thay đổi với hình bên trái. Giống như chúng ta đã thảo luận, thư viện ggplot2 sử dụng thứ hạng của các giá trị trong véc-tơ số chứ không sử dụng giá trị thực. Thứ hạng của \\(1^2\\), \\(2^2\\), và \\(3^2\\) vẫn là 1, 2, và 3, đồng thời hình tròn nhỏ nhất vẫn có bán kính bằng 1 và đường tròn lớn nhất vẫn có bán kính bằng 6. Kết quả là kích thước của các hình xuất hiện vẫn không thay đổi.Đồ thị bên phải: khi chúng ta chỉ trực quan hai điểm tại vị trí (2,2) và (3,3) và thuộc tính thẩm mỹ kích thước được ánh xạ vào hai giá trị là 2 và 3; có thể thấy rằng diện tích hình tròn nhỏ nhất và diện tích hình tròn lớn nhất vẫn không thay đổi.Hàm số dùng để kiểm soát giá trị của ánh xạ thẩm mỹ kích thước là hàm scale_size(). Để thay đổi miền giá trị của thuộc tính kích thước, chúng ta sử dụng tham số range.\nHình 9.42: Ánh xạ các số tự nhiên đến thuộc tính thẩm mỹ kích thước. Hình bên trái: ánh xạ các số 1, 2, 3 đến kích thước ba hình tròn. Hình bên phải: sử dụng scale_size để hình nhỏ nhất có bán kính bằng 6 và hình lớn nhất có bán kính bằng 12.\nHình 9.42 mô tả cách sử dụng tham số range trong hàm scale_size().Đồ thị bên trái: bán kính của hình tròn nhỏ nhất là 1, bán kính của hình tròn lớn nhất là 6.Đồ thị bên trái: bán kính của hình tròn nhỏ nhất là 1, bán kính của hình tròn lớn nhất là 6.Đồ thị bên phải: bán kính của hình nhỏ nhất là 6 bằng với kích thước của hình lớn nhất của đồ thị bên trái. Trong khi đó, bán kính của hình tròn lớn nhất là 12. Mặc dù khi khai báo tham số range được hiểu là bán kính của các hình, nhưng khi nội suy kích thước, hình tròn ở tọa độ (2,2) lại được nội suy theo diện tích, chứ không phải theo bán kính.Đồ thị bên phải: bán kính của hình nhỏ nhất là 6 bằng với kích thước của hình lớn nhất của đồ thị bên trái. Trong khi đó, bán kính của hình tròn lớn nhất là 12. Mặc dù khi khai báo tham số range được hiểu là bán kính của các hình, nhưng khi nội suy kích thước, hình tròn ở tọa độ (2,2) lại được nội suy theo diện tích, chứ không phải theo bán kính.Trong trường hợp bạn đọc muốn sử dụng nội suy kích thước theo bán kính thay vì nội suy theo diện tích, hàm scale_radius() thay thế cho scale_size(). Hình 9.43 mô tả sự khác nhau khi sử dụng scale_size() và scale_radius():\nHình 9.43: Sự khác nhau giữa scale_size() và scale_radius(). Hình bên trái: sử dụng scale_size() với range = c(1,7). Hình ở giữa: sử dụng scale_radius() với range = c(1,7). Hình bên phải: sử dụng scale_radius() với range = c(4,10)\nĐồ thị bên trái sử dụng scale theo diện tích và bán kính hình tròn lớn nhất bằng 7 lần đường tròn nhỏ; hình tròn ở giữa có bán kính bằng \\(\\sqrt{\\cfrac{7^2+1^2}{2}} = 5 \\text{ (lần)}\\) bán kính hình tròn nhỏ nhất.Đồ thị ở giữa, scale theo bán kính hình tròn nên hình ở giữa có bán kính bằng \\(\\cfrac{7+1}{2} = 4 \\textit{ (lần)}\\) bán kính hình tròn nhỏ. Bạn đọc có thể thấy rằng kích thước của hình tròn ở vị trí tọa độ (2,2) trong đồ thị ở giữa nhỏ hơn hình tròn ở vị trí tọa độ (2,2) trong hình bên trái.Trong đồ thị bên phải, bán kính của hình nhỏ nhất là 4, của hình tròn lớn nhất là 10, nên bán kính hình ở giữa là \\(\\cfrac{4+10}{2} = 7\\) nội suy bằng hàm scale_radius(). Bạn đọc có thể thấy rằng kích thước của hình ở vị trí (2,2) của đồ thị này bằng với kích thước của hình tròn ở vị trí (3,3) của hình ở giữa.Các tham số limits, breaks, và label được sử dụng tương tự như khi sử dụng với thuộc tính thẩm mỹ màu sắc:Tham số limits cho biết miền giá trị nào của biến được ánh xạ đến thuộc tính thẩm mỹ size.Tham số breaks cho biết các giá trị nào của kích thước nào xuất hiện trên chú giải về cấu phần thẩm mỹ kích thước.Tham số labels mô tả thuộc tính thẩm mỹ kích thước trên chú giải của đồ thị.Bạn đọc tham khảo cách sử dụng các tham số này trong ví dụ dưới đây khi mô tả hai biến liên tục là tỷ lệ trẻ sơ sinh tử vong và tuổi thọ trung bình bằng đồ thị phân tán và ánh xạ biến dân số vào kích thước của các điểm\nHình 9.44: Tỷ lệ trẻ sơ sinh tử vong và tuổi thọ trung bình của các quốc gia trên thế giới năm 2011\nÁnh xạ thẩm mỹ từ biến dân số đến thuộc tính thẩm mỹ kích thước của các điểm được điều chỉnh bằng hàm scale_size() như sau:Tham số range = c(1,12) cho biết bán kính của hình tròn tương ứng với nước có dân số nhỏ nhất bằng 1 và bán kính của hình tròn tương ứng với nước có dân số lớn nhất là 12.Tham số limits cho biết chỉ các nước có dân số 10 triệu trở lên được đưa vào trong đồ thị.Tham số breaks cho biết các giá trị xuất hiện trên chú giải là các giá trị 100 triệu, 200 triệu, 500 triệu và 1 tỷ.Tham số labels cho biết các số viết trên chú giải sử dụng cách viết giá trị lên chú giải là theo đơn vị triệu.","code":"\ndat0<-data.frame(x=1:3,y=1:3,z=1:3)\n# Hình bên trái\np1<-dat0%>%ggplot(aes(x,y,size=z))+\n  geom_point(shape=21,fill= \"#640514\", alpha = 0.7, color = \"darkblue\")+\n  theme_minimal()+\n  theme(legend.position = \"none\")+\n  scale_x_continuous(limits = c(0.9,3.1))+\n  scale_y_continuous(limits = c(0.9,3.1))\n# Hình ở giữa\np2<-dat0%>%ggplot(aes(x,y,size=z^2))+\n  geom_point(shape=21,fill= \"#640514\", alpha = 0.7, color = \"darkblue\")+\n  theme_minimal()+\n  theme(legend.position = \"none\")+\n  scale_x_continuous(limits = c(0.9,3.1))+\n  scale_y_continuous(limits = c(0.9,3.1))\n# Hình bên phải\np3<-dat0%>%filter(z>1)%>%ggplot(aes(x,y,size=z))+\n  geom_point(shape=21,fill= \"#640514\", alpha = 0.7, color = \"darkblue\")+\n  theme_minimal()+\n  theme(legend.position = \"none\")+\n  scale_x_continuous(limits = c(0.9,3.1))+\n  scale_y_continuous(limits = c(0.9,3.1))\ngrid.arrange(p1,p2,p3,ncol=3,nrow=1)\ndat0<-data.frame(x=1:3,y=1:3,z=1:3)\n# Hình bên trái\np1<-dat0%>%ggplot(aes(x,y,size=z))+\n  geom_point(shape=21,fill= \"#640514\", alpha = 0.7, color = \"darkblue\")+\n  theme(legend.position = \"none\")+\n  scale_x_continuous(limits = c(0.9,3.25))+\n  scale_y_continuous(limits = c(0.9,3.25))+\n  theme_minimal()+ggtitle(\"Không sử dụng scale_size\")\n# Hình bên phải\np2<-dat0%>%ggplot(aes(x,y,size=z))+\n  geom_point(shape=21,fill= \"#640514\", alpha = 0.7, color = \"darkblue\")+\n  scale_size(range=c(6,12))+\n  theme(legend.position = \"none\")+\n  scale_x_continuous(limits = c(0.9,3.25))+\n  scale_y_continuous(limits = c(0.9,3.25))+\n  theme_minimal()+ggtitle(\"Sử dụng scale_size với range = (6,12)\")\ngrid.arrange(p1,p2,ncol=2,nrow=1)\ndat0<-data.frame(x=1:3,y=1:3,z=1:3)\n# Hình bên trái\np1<-dat0%>%ggplot(aes(x,y,size=z))+\n  geom_point(shape=21,fill= \"#640514\", alpha = 0.7, color = \"darkblue\")+\n  theme_minimal()+\n  theme(legend.position = \"none\")+\n  scale_x_continuous(limits = c(0.9,3.1))+\n  scale_y_continuous(limits = c(0.9,3.1))+\n  scale_size(range = c(1,7))\n\np2<-dat0%>%ggplot(aes(x,y,size=z))+\n  geom_point(shape=21,fill= \"#640514\", alpha = 0.7, color = \"darkblue\")+\n  theme_minimal()+\n  theme(legend.position = \"none\")+\n  scale_x_continuous(limits = c(0.9,3.1))+\n  scale_y_continuous(limits = c(0.9,3.1))+\n  scale_radius(range=c(1,7))\n# Hình bên phải\np3<-dat0%>%ggplot(aes(x,y,size=z))+\n  geom_point(shape=21,fill= \"#640514\", alpha = 0.7, color = \"darkblue\")+\n  theme_minimal()+\n  theme(legend.position = \"none\")+\n  scale_x_continuous(limits = c(0.9,3.1))+\n  scale_y_continuous(limits = c(0.9,3.1))+\n  theme(legend.position = \"none\")+\n  scale_radius(range=c(4,10))\ngrid.arrange(p1,p2,p3,ncol=3,nrow=1)\ndat%>%\n  ggplot(aes(infant_mortality,life_expectancy, size = population))+\n  geom_point(shape=21,fill= \"#640514\", alpha = 0.5, color = \"darkblue\")+\n  theme_minimal()+\n  scale_size(range = c(1,12),\n             limits = c(10^7,max(gapminder$population)),\n             breaks = c(10^8,2*10^8,5*10^8,10^9),\n             labels = c(paste(c(10^8,2*10^8,5*10^8)/10^6,\"triệu\"), \"Một tỷ\"))"},{"path":"trực-quan-hóa-dữ-liệu.html","id":"thuộc-tính-thẩm-mỹ-hình-dạng","chapter":"Chương 9 Trực quan hóa dữ liệu","heading":"9.5.3.2 Thuộc tính thẩm mỹ hình dạng","text":"Thuộc tính thẩm mỹ hình dạng được sử dụng trong các đồ thị trực quan để mô tả một hoặc một vài biến rời rạc với điều kiện biến này không có quá nhiều giá trị riêng biệt. Theo kinh nghiệm của chúng tôi thì hình dạng chỉ nên sử dụng với các biến có nhỏ hơn năm giá trị riêng biệt. Mặc dù thư viện ggplot2 cho phép sử dụng lên đến 25 hình dạng khác nhau, nhưng sử dụng nhiều hơn hoặc bằng năm hình dạng trong một đồ thị sẽ làm cho đồ thị trở nên rắc rối và khó khăn khi nhận diện. Tại phiên bản ggplot2 mà chúng tôi đang sử dụng, có 25 hình dạng khác nhau có thể dùng để mô tả biến rời rạc. Các hình dạng này được sử dụng bằng cách ánh xạ thẩm mỹ một biến rời rạc đến thuộc tính shape hoặc thiết lập tham số cho shape tương ứng với 25 số tự nhiên từ 1 đến 25 được mô tả trong Hình 9.45\nHình 9.45: Các hình dạng có thể được sử dụng trong trực quan hóa dữ liệu của thư viện ggplot2\nBạn đọc cần lưu ý rằng có một số hình dạng trông giống nhau nhưng lại có thuộc tính thẩm mỹ khác nhau. Chẳng hạn như hình dạng tương ứng với số 1 là một điểm hình tròn với thuộc tính thẩm mỹ color là màu sắc của toàn bộ hình tròn đó, trong khi hình dạng tương ứng với số 21 có thuộc tính thẩm mỹ color là màu viền bên ngoài của hình tròn và thuộc tính thẩm mỹ fill mới là màu sắc bên trong hình tròn.Để kiểm soát ánh xạ thẩm mỹ đến thuộc tính hình dạng, bạn đọc sử dụng hàm scale_shape_manual(). Bạn đọc có thể tham khảo cách sử dụng hàm này thông qua ví dụ dưới đây:\nHình 9.46: Thu nhập bình quân đầu người và tuổi thọ trung bình của các quốc gia châu Á vào năm 2011\nTham số được sử dụng để gán giá trị cho biến rời rạc đến hình dạng cụ thể là tham số values. Nhìn chung, ánh xạ biến rời rạc đến thuộc tính thẩm mỹ hình dạng chỉ cho hiệu quả tốt khi dữ liệu không có quá nhiều quan sát và số lượng giá trị riêng biệt của biến rời rạc là nhỏ. Trong trường hợp dữ liệu có nhiều quan sát và biến rời rạc nhận nhiều hơn năm giá trị khác nhau, bạn đọc nên thận trọng khi sử dụng thuộc tính thẩm mỹ này!","code":"\ngapminder%>%filter(year==2011, continent == \"Asia\")%>%\n  ggplot(aes(gdp/population,life_expectancy, shape = region))+\n  geom_point(color = \"#640514\")+\n  scale_x_continuous(trans=\"log10\")+\n  scale_shape_manual(values=c(21:24,8) )+\n  theme_minimal()"},{"path":"trực-quan-hóa-dữ-liệu.html","id":"kích-thước-và-hình-dạng-của-các-đường","chapter":"Chương 9 Trực quan hóa dữ liệu","heading":"9.5.3.3 Kích thước và hình dạng của các đường","text":"Đối với hình dạng đồ họa là các đường được vẽ bằng các hàm như geom_line(), geom_path() hay geom_segment(), chúng ta có thể ánh xạ các biến rời rạc vào độ rộng hoặc hình dạng của đường. Hình dạng và kích thước của các đường được sử dụng tương đương như hình dạng và kích thước của các điểm nên không có nhiều kiến thức mới cần thảo luận trong phần này.Hình 9.47 mô tả sự thay đổi của biến tổng thu nhập quốc dân (gdp) của ba quốc gia bao gồm Mỹ, Trung Quốc và Nhật Bản theo thời gian từ năm 1960 đến năm 2011 sử dụng dữ liệu gapminder. Các đường mô tả được phân biệt bằng cách sử dụng ba kiểu đường khác nhau:\nHình 9.47: Tổng thu nhập quốc dân của Mỹ, Trung Quốc, và Nhật Bản từ năm 1960 đến 2011\nHàm số dùng để kiểm soát ánh xạ thẩm mỹ vào hình dạng của các đường là scale_linetype_manual(). Thư viện ggplot2 có 13 hình dạng cho các đường được đánh số từ 1 đến 13 như Hình 9.48 dưới đây\nHình 9.48: Hình dạng của các đường có thể sử dụng để trực quan hóa dữ liệu trong thư viện ggplot2\nĐể các đường có hình dạng như mong muốn, chúng ta gán giá trị tham số values trong hàm scale_linetype_manual() cho một véc-tơ chứa các số nhận giá trị từ 1 đến 13 tương ứng với hình dạng mà bạn lựa chọn như sau\nHình 9.49: Tổng thu nhập quốc dân của Mỹ, Trung Quốc, và Nhật Bản từ năm 1960 đến 2011\n","code":"\ngapminder%>%filter(country %in% c(\"United States\", \"China\", \"Japan\"), year <= 2011)%>%\n  ggplot(aes(x = year,y = gdp/10^9))+\n  geom_line(aes(linetype = country), color = \"#640514\")+\n  theme_minimal()+\n  ylab(\"GDP in Billion USD\")+\n  scale_x_continuous(breaks = seq(1960,2010,10))+\n  scale_y_continuous(labels = scales::label_comma())\ngapminder%>%filter(country %in%\n                     c(\"United States\", \"China\", \"Japan\"), year <= 2011)%>%\n  ggplot(aes(x = year,y = gdp/10^9))+\n  geom_line(aes(linetype = country),\n            color = \"#640514\")+\n  theme_minimal()+\n  ylab(\"GDP in $B\")+\n  scale_x_continuous(breaks = seq(1960,2010,10))+\n  scale_y_continuous(labels = scales::label_comma())+\n  scale_linetype_manual(values = c(4,7,12))"},{"path":"trực-quan-hóa-dữ-liệu.html","id":"tùy-chỉnh-chú-giải-của-ánh-xạ-thẩm-mỹ","chapter":"Chương 9 Trực quan hóa dữ liệu","heading":"9.6 Tùy chỉnh chú giải của ánh xạ thẩm mỹ","text":"Về mặt hình thức, nếu coi các hàm scale_() như các ánh xạ từ tập hợp các giá trị của biến đến tập hợp các giá trị của thuộc tính thẩm mỹ thì chú giải là ánh xạ ngược từ thuộc tính thẩm mỹ đến miền giá trị của biến. Chú giải cho phép bạn chuyển đổi các thuộc tính trực quan trở lại giá trị của dữ liệu. Giá trị xuất hiện trên các trục tọa độ và các chú giải có cách hiển thị khác nhau nhưng về bản chất lại có cùng một mục đích là cho phép người tiếp nhận quan sát các hình ảnh đồ họa trực quan và ánh xạ chúng trở lại giá trị của dữ liệu. Các cấu phần khác nhau của chú giải được mô tả trong Hình 9.50\nHình 9.50: Các thành phần và tên gọi của chú giải\nChú giải có khả năng giải thích tốt hơn giá trị xuất hiện trên các trục tọa độ bởi các nguyên nhân sauChú giải có thể giải thích nhiều biến cùng lúc trong khi giá trị trên trục tọa độ chỉ cho phép một biến.Chú giải có thể giải thích nhiều biến cùng lúc trong khi giá trị trên trục tọa độ chỉ cho phép một biến.Chú giải có thể tùy biến dễ hơn: có thể xuất hiện ở các vị trí theo ý muốn của người xây dựng đồ thị, có thể xuất hiện theo bất kỳ hướng nào.Chú giải có thể tùy biến dễ hơn: có thể xuất hiện ở các vị trí theo ý muốn của người xây dựng đồ thị, có thể xuất hiện theo bất kỳ hướng nào.Bạn đọc hãy lưu ý rằng dù chúng ta không gọi bất kỳ hàm scale_() nào trong các câu lệnh thì mỗi khi vẽ đồ thị, thư viện ggplot2 vẫn luôn luôn sử dụng một nhóm hàm scale_() mặc định để ánh xạ từ các biến đến các thuộc tính thẩm mỹ của đồ thị trực quan. Mỗi khi bạn đọc gọi hàm scale_() để kiểm soát ánh xạ thẩm mỹ, các giá trị mà bạn khai báo sẽ thay thế cho các giá trị mặc định. Trong trường hợp bạn gọi nhiều hàm scale_() cùng tác động đến một thuộc tính thẩm mỹ thì chỉ có hàm scale_() được gọi ra sau cùng được sử dụng. Hãy quan sát ví dụ dưới đây:\nHình 9.51: Thu nhập bình quân đầu người của các quốc gia Đông Nam Á năm 2011\nĐồ thị trực quan trong Hình 9.51 được vẽ bằng các câu lệnh gọi hàm scale_x_continuous() lặp lại hai (2) lần và hàm scale_y_continuous() lặp lại ba (3) lần. Quan sát kết quả, bạn đọc có thể thấy rằng chỉ có câu lệnh sau cùng được chấp nhận. Ngoài ra, khi thực thi đoạn lệnh ở trên, thư viện ggplot2 cũng sẽ đưa ra các cảnh báo về các hàm scale_() đã xuất hiện và sẽ bị thay thế bằng các hàm cùng tên.Để kiểm soát chú giải của các ánh xạ thẩm mỹ, bạn đọc sử dụng tham số guide trong các hàm scale_() tương ứng. Giá trị gán cho tham số guide là một trong các hàm số sau đây:Hàm guide_axis() là hàm số dùng để gán cho tham số guide khi chúng ta sử dụng các hàm scale_() nhằm kiểm soát ánh xạ thẩm mỹ đến các trục tọa độ.\nHình 9.52: Thu nhập bình quân đầu người của các quốc gia Tây Âu năm 2011. Hàm guide_axis() được sử dụng để kiểm soát chú giải cho các trục tọa độ x và y\nBạn đọc có thể thấy rằng tham số title trong hàm guide_axis() đã thay thế cho tham số name trong các hàm scale_x_discrete() và scale_y_discrete(). Tham số angle cho biết hướng các giá trị xuất hiện trên trục tọa độ. Trong Hình 9.52, tên các quốc gia trên trục tọa độ x đã được xoay một góc 90 độ. Bạn đọc tham khảo hướng dẫn sử dụng hàm guide_axis() để hiểu về các tham số khác như n.dodge, order, hay position.Hàm guide_legend() là hàm số dùng để gán cho tham số guide khi gọi các hàm scale_() kiểm soát ánh xạ từ các biến rời rạc đến màu sắc. Có rất nhiều tham số có thể sử dụng trong hàm số này. Bạn đọc tham khảo hướng dẫn sử dụng hàm để biết đầy đủ các tham số.\nHình 9.53: Thu nhập bình quân đầu người của các quốc gia Nam Mỹ năm 2011. Hàm guide_legend được sử dụng để kiểm soát chú giải cho ánh xạ thẩm mỹ màu sắc\nTương tự như hàm guide_legend(), hàm guide_colorbar() được sử dụng khi chú giải cho các ánh xạ từ biến liên tục đến dải màu liên tụcHàm guide_bin() được dùng khi chú giải cho các ánh xạ từ biến liên tục đến thuộc tính thẩm mỹ kích thước (size).","code":"\np<-gapminder%>%filter(year==2011,region==\"South-Eastern Asia\")%>%\n  mutate(gdp_per_capita = gdp/population)%>%\n  ggplot(aes(reorder(country,gdp_per_capita),\n             gdp_per_capita,fill = country))+\n  geom_bar(stat=\"identity\")+\n  theme_minimal()\np+theme(legend.position = \"none\")+\n  scale_y_continuous(name = \"Thu nhập bình quân đầu người\",\n                     labels = scales::label_comma())+\n  scale_x_discrete(name = \"Country\")+\n  scale_y_continuous(trans = \"sqrt\")+\n  scale_x_discrete(name = \"Quốc gia\", labels = c(\n    \"Vietnam\" = \"VN\",\n    \"Thailand\" = \"TL\",\n    \"Timor-Leste\" = \"Đông Timor\"))+\n  scale_y_continuous(name = \"Thu nhập bình quân đầu người\",\n                     labels = scales::label_dollar())\ngapminder%>%filter(year==2011,region==\"Western Europe\")%>%\n  mutate(gdp_per_capita = gdp/population)%>%\n  filter(!is.na(gdp_per_capita))%>%\n  ggplot(aes(reorder(country,gdp_per_capita),gdp_per_capita,fill = country))+\n  geom_bar(stat=\"identity\")+\n  theme_minimal()+\n  theme(legend.position = \"none\")+\n  scale_x_discrete(name = \"Country\",\n                   guide = guide_axis(title = \"Quốc gia\",\n                                      angle = 90))+\n  scale_y_continuous(name = \"GDP per capita\",\n                     labels = scales::label_dollar(),\n                     guide = guide_axis(title = \"Thu nhập bình quân đầu người\"))\ngapminder%>%filter(year==2011,region==\"South America\")%>%\n  mutate(gdp_per_capita = gdp/population)%>%\n  filter(!is.na(gdp_per_capita))%>%\n  ggplot(aes(reorder(country,gdp_per_capita),gdp_per_capita,fill = country))+\n  geom_bar(stat=\"identity\")+\n  theme_minimal()+\n  theme(legend.position = \"none\")+\n  scale_x_discrete(guide = guide_axis(title = \"Quốc gia\",angle = 90))+\n  scale_y_continuous(labels = scales::label_dollar(),\n                     guide = guide_axis(title = \"Thu nhập bình quân đầu người\"))+\n  scale_fill_brewer(palette = \"Paired\",\n                    guide = guide_legend(title = \"Quốc gia\",\n                                         title.position = \"top\",ncol = 2))"},{"path":"trực-quan-hóa-dữ-liệu.html","id":"chủ-đề-và-ngữ-cảnh-của-đồ-thị","chapter":"Chương 9 Trực quan hóa dữ liệu","heading":"9.7 Chủ đề và ngữ cảnh của đồ thị","text":"Ngữ cảnh cho phép bạn đọc kiểm soát tốt các cấu phần không ánh xạ đến dữ liệu trong đồ thị như phông chữ, hình nền, vị trí chú giải, … Sự phân tách giữa các thành các phần có ánh xạ đến dữ liệu và thành phần không ánh xạ đến dữ liệu trong các đồ thị trực quan của thư viện ggplot2 là điểm khác biệt với đồ họa cơ sở. Trong đồ họa cơ sở hầu hết các hàm đều có một số lượng lớn các tham số số chỉ định đồng thời cho dữ liệu và cho cả phần không liên quan đến dữ liệu, điều này làm cho các hàm trong đồ thị cơ sở của R trở nên phức tạp và khó kiểm soát. Thư viện ggplot2 tiếp cận theo cách khác: khi tạo đồ thị, bạn xác định cách hiển thị dữ liệu trước, sau đó bạn có thể chỉnh sửa mọi chi tiết không liên quan đến dữ liệu bằng các hàm kiểm soát chủ đề và ngữ cảnh. Để kiểm soát chủ đề và ngữ cảnh của đồ thị, bạn đọc cần nắm vững các nội dung sau:Các chủ đề và ngữ cảnh đã được hoàn chỉnh và sẵn có trong thư viện ggplot2 và trong các thư viện cài đặt bổ sung, chẳng hạn như ggthemes.Cách kiểm soát các thành phần của chủ đề và ngữ cảnh như: tiêu đề của đồ thị (kiểu chữ, kích thước, vị trí), cách hiển thị các số trên các trục, cách hiển thị các hình dạng đồ họa trên chú giải, kiểu chữ, kích thước hay vị trí của chú giải…Kiểm soát các tùy biến của các hàm dùng để gán giá trị cho các thành phần của chủ đề. Ví dụ như hàm element_text() có thể dùng để chỉnh kích thước phông chữ, màu sắc và giao diện của các thành phần văn bản.Cách sử dụng hàm theme() với một danh sách dài các tùy biến cho phép bạn sao chép lên các thành phần của chủ đề và ngữ cảnh mặc định.","code":""},{"path":"trực-quan-hóa-dữ-liệu.html","id":"tạo-đồ-thị-tương-tác-và-đồ-thị-động-với-r","chapter":"Chương 9 Trực quan hóa dữ liệu","heading":"9.8 Tạo đồ thị tương tác và đồ thị động với R","text":"Các đồ thị trực quan được vẽ bằng các câu lệnh của thư viện ggplot2 đều là các đồ thị tĩnh. Như chúng tôi đã đề cập trong phần giới thiệu, thư viện ggplot2 không có tính năng vẽ các đồ thị dạng động và các đồ thị tương tác. Tuy nhiên, trong yêu cầu của trực quan hóa dữ liệu nói chung, các đồ thị động và đồ thị tương tác luôn có vị trí nhất định. Các lợi thế của đồ thị động và đồ thị tương tác với đồ thị tĩnh có thể liệt kê ra như sau:Các đồ thị dạng động đặc biệt hiệu quả trong việc mô tả sự thay đổi dữ liệu theo thời gian. Một đồ thị tĩnh khi mô tả biến theo thời gian thường chỉ có thể mô tả yếu tố thời gian lên một trục tọa độ và rất khó để có thể mô tả sự thay đổi đồng thời của một nhóm các biến theo thời gian trên một đồ thị tĩnh.Cùng với đồ thị động, các đồ thị tương tác có lợi thế ở việc thu hút thị giác của người tiếp nhận và có khả năng mô tả dữ liệu một cách đầy đủ thông tin hơn. Các đồ thị tương tác cho phép hiển thị thông tin bằng con trỏ, hoặc phóng , thu nhỏ từng phần của đồ thị. Người trực quan hóa dữ liệu không cần phải hiển thị quá nhiều thông tin lên đồ thị cùng lúc, đặc biệt là với các dữ liệu có nhiều biến.Ngoài các ưu điểm kể trên, khuyết điểm lớn nhất của các đồ thị tương tác và các đồ thị động đó là không thể biểu diễn trên các bản cứng.Trong phần này của chương sách, chúng tôi sẽ thảo luận về hai thư viện dùng để tạo đồ thị tương tác và đồ thị dạng động là ggiraph và plotly. Nếu như ggiraph là thư viện bổ sung cho ggplot2 và được xây dựng dựa trên cấu trúc ngữ pháp đồ thị thì plotly là một thư viện độc lập với ggplot2 và chuyên được sử dụng để tạo đồ thị dạng động và tương tác. Ưu nhược điểm và cách vẽ đồ thị của các thư viện này sẽ được thảo luận trong phần tiếp theo của chương.","code":""},{"path":"trực-quan-hóa-dữ-liệu.html","id":"tạo-đồ-thị-tương-tác-với-ggiraph","chapter":"Chương 9 Trực quan hóa dữ liệu","heading":"9.8.1 Tạo đồ thị tương tác với ggiraph","text":"Ưu điểm lớn nhất của thư viện ggiraph đó là các câu lệnh tạo đồ thị cũng được dựa trên ngữ pháp của đồ thị, nghĩa là hoàn toàn tương đồng với các câu lệnh trong thư viện ggplot2. Để tạo một đồ thị trực quan tương tác hoặc đồ thị động với ggiraph, bạn đọc chỉ cần thêm các thuộc tính thẩm mỹ của đồ thị tương tác và đồ thị động cùng với các thuộc tính của đồ thị tĩnh mà chúng ta đã làm quen khi vẽ đồ thị với thư viện ggplot2. Tại thời điểm chúng tôi viết chương sách này, thư viện ggiraph đang ở phiên bản 0.8.7 và hướng dẫn sử dụng ở trong đường dẫn như sau:https://cloud.r-project.org/web/packages/ggiraph/ggiraph.pdfTrong danh sách các hàm số trong thư viện ggiraph, bạn đọc có thể thấy rằng đa số các hàm geom_() của thư viện ggplot2 đều có một hàm tương ứng để tạo đồ thị tương tác tương ứng là geom_*_interactive(). Chẳng hạn như hàm geom_point() của thư viện ggplot2 sẽ có hàm tương ứng trong thư viện ggiraph là geom_point_interactive().Hai cấu phần thẩm mỹ thường được sử dụng để tạo đồ thị tương tác là tooltip và data_id. Bạn đọc cần lưu ý là hàm geom_point_interactive() không trực tiếp tạo ra đồ thị tương tác, mà bạn đọc cần lưu đối tượng được tạo bằng hàm số này, sau đó thực hiện câu lệnh tạo đồ thị tương tác bằng hàm girafe() của thư viện ggiraphHình 9.54 là đồ thị trực quan tương tác được vẽ bằng thư viện ggiraph mô tả số vụ sát nhân bằng súng tại Mỹ năm 2010 từ dữ liệu murders.\nHình 9.54: Đồ thị tương tác mô tả số vụ sát nhân bằng súng tại 51 bang của Mỹ vào năm 2010\nThuộc tính thẩm mỹ tooltip được ánh xạ đến các biến chứa thông tin mà bạn đọc muốn hiển thị của các điểm trên đồ thị khi sử dụng con trỏ. Trong Hình 9.54 chúng tôi ánh xạ thuộc tính này đến các biến state, region và population. Các biến này chỉ hiển thị khi bạn đọc sử dụng con trỏ di chuyển đến một điểm trên đồ thị.Thuộc tính thẩm mỹ data_id khi được ánh xạ đến một biến sẽ cho biết (làm nổi bật) các điểm dữ liệu có cùng giá trị trên biến đó. Trong đồ thị trên, biến region ánh xạ đến thuộc tính thẩm mỹ data_id, đó mỗi khi di chuyển con trỏ đến một bang, tất cả các bang có cùng giá trị của biến region sẽ được làm nổi bật. Bạn đọc có thể sử dụng con trỏ di chuyển đến từng các điểm trên đồ thị trong Hình 9.54 để xem kết quả của ánh xạ đến thuộc tính tooltip và data_id như miêu tả.Không chỉ trong đồ thị phân tán, các loại đồ thị cơ bản các cũng có cách sử dụng các thuộc tính thẩm mỹ tooltip và data_id hoàn toàn tương tự. Dưới đây là một vài ví dụ:Hình 9.55 mô tả đồ thị bong bóng dạng tương tác. mắt quan sát không dễ dàng đánh giá được kích thước của các hình tròn, kể cả khi chúng ta sử dụng chú giải cho kích thước, đó việc sử dụng đồ thị tương tác để hiển thị số lượng điểm tại mỗi hình tròn giúp cho dữ liệu càng trở nên sinh động và trực quan hơn\nHình 9.55: Đồ thị bong bóng tương tác mô tả số lượng viên kim cương theo màu sắc (color) và giác cắt (cut)\nHình 9.56 sử dụng đồ thị dạng đường và đồ thị dạng hình hộp chữ nhật có tương tác\nHình 9.56: Đồ thị tương tác mô tả tỷ lệ thất nghiệp của nước Mỹ qua các thời kỳ Tổng thống và các Đảng cầm quyền\nĐỒ thị trong Hình 9.56 được trực quan từ hai dữ liệu. Dữ liệu chính là dữ liệu economics với biến unemploy cho biết số lượng người thất nghiệp tại Mỹ được quan sát theo tháng từ năm 1967 đến năm 2015. Dữ liệu thứ hai là dữ liệu về các nhiệm kỳ của các tổng thống Mỹ trong các khoảng thời gian tương ứng. Bạn đọc có thể nhận thấy sự khác biệt về sự biến động của tỷ lệ thất nghiệp qua các thời kỳ cầm quyền của các đảng cầm quyền: tỷ lệ thất nghiệp luôn có xu thế giảm trong giai đoạn đảng Dân chủ nắm quyền, trong khi lại có xu thế tăng trong giai đoạn đảng Cộng hòa nắm chính quyền.Hình 9.57 sử dụng các đồ thị tương tác dạng thanh để mô tả thu nhập bình quân đầu người của các quốc gia vùng Đông Á trong năm 2011\nHình 9.57: Thu nhập bình quân đầu người của các quốc gia vùng Đông Á năm 2011\nHình 9.58 sử dụng bản đồ tương tác để mô tả biến tỷ lệ trẻ sơ sinh tử vong trong dữ liệu gapminder lọc theo năm 2011.\nHình 9.58: Bản đồ mô tả tỷ lệ trẻ sơ sinh tử vong các quốc gia trên thế giới năm 2011\nBạn đọc có thể thấy rằng bản đồ tương tác đặc biệt hiệu quả trong hiển thị thông tin thay thế cho chú giải. Màu sắc từ xanh da trời đến màu cam cho biết vùng quốc gia - vùng lãnh thổ nào có tỷ lệ trẻ sơ sinh tử vong thấp và quốc gia - vùng lãnh thổ nào có tỷ lệ trẻ sơ sinh tử vong cao. Bạn đọc muốn biết thông tin chi tiết về quốc gia đó có thể sử dụng con trỏ để hiển thị thông tin, bao gồm có thông tin về tên nước, dân số, và tỷ lệ trẻ sơ sinh tử vong.","code":"\np<-murders %>% ggplot(aes(y = total, x = population)) +\n  geom_point_interactive(aes(fill=region,\n                             tooltip = paste0(\"Bang: \", state, \n                                              \"\\n Vùng: \", region, \n                                              \"\\n Dân số: \", round(population/1000,0)*1000),\n                             data_id = region),\n                         size = 4, shape=21, alpha = 0.8, color = \"black\") +\n  geom_smooth(method = \"lm\", se = FALSE, linetype = 2, color=\"#640514\")+\n  scale_x_continuous(trans = \"log10\", labels = scales::label_comma()) +\n  scale_y_log10() +\n  scale_fill_brewer(palette = \"Dark2\",\n                    guide = guide_legend(title = \"Vùng\"))+\n  theme_minimal()+\n  ggtitle(\"Số vụ sát nhân bằng súng tại các bang năm 2010\")+\n  xlab(\"Dân số\")+ylab(\"Số vụ sát nhân bằng súng\")\n\ngirafe(ggobj = p, width_svg = 6.5, height_svg = 3.5,\n       options = list(\n                opts_sizing(width = .7),\n                opts_tooltip(css= \"font-family: Source Code Pro; \n                              color: white; \n                               background-color: #640514\")\n                ))\np<-diamonds%>%group_by(cut,color)%>%mutate(ave_price = mean(price))%>%ungroup()%>%\n  as.data.frame()%>%\n  ggplot(aes(cut,color))+\n  geom_count_interactive(aes(tooltip = paste0(\"Số lượng: \", after_stat(n))),\n                         alpha = 0.7, color = \"darkblue\",\n                         shape = 21, fill = \"#640514\")+\n  scale_size(range=c(1,12))+\n  theme_minimal()+\n  theme(legend.position = \"none\")+xlab(\"Giác cắt\")+ylab(\"Màu sắc\")\ngirafe(ggobj = p, width_svg = 5, height_svg = 4,\n        options = list(opts_sizing(width = .55),\n        opts_tooltip(css = \"font-family: Source Code Pro; \n                               padding:3pt; color: white; \n                               background-color: #640514\")\n                ))\ndat1<-presidential[3:11,]\np<-economics%>%mutate(unemploy_rate = unemploy/pop)%>%\n  ggplot()+\n  geom_rect_interactive(data=dat1,\n            aes(xmin = start, xmax = end,\n                ymin = 0.005, ymax = 0.06,\n                tooltip = paste(\"Tổng thống: \", name),\n                data_id = name,\n                fill = party),color = \"white\",size=0.08,alpha = 0.7)+\n  theme_minimal()+\n  scale_fill_manual(values=c(\"blue\",\"red\"),\n                    labels = c(\"Democratic\" = \"Dân chủ\",\"Republican\" = \"Cộng hòa\"),\n                    guide = guide_legend(title = \"Đảng cầm quyền:\"))+\n  geom_line(aes(x = date, y = unemploy_rate),size = 0.5, col= \"#A1FDFD\")+\n  geom_point_interactive(aes(x = date, y = unemploy_rate,\n                             tooltip = paste(round(unemploy_rate*100,2),\"%\")), \n                         col = \"#A1FDFD\",size = 0.3, alpha = 0.5)+\n  scale_y_continuous(limits = c(0.005,0.06),labels = scales::label_percent())+\n  xlab(\"Năm\") + ylab(\"Tỷ lệ thất nghiệp\")+\n  theme(legend.position = \"top\")\n\ngirafe(ggobj = p, width_svg = 6, height_svg = 3.5,\n        options = list(opts_sizing(width = .65),\n        opts_tooltip(css = \"font-family: Source Code Pro; \n                               padding:3pt; color: white; \n                               background-color: #640514 ; \n                               border-radius:5px\")\n                ))\np<-gapminder%>%filter(year==2011,region==\"Eastern Asia\")%>%\n  mutate(gdp_per_capita = gdp/population)%>%\n  filter(!is.na(gdp_per_capita))%>%\n  ggplot(aes(reorder(country,gdp_per_capita),gdp_per_capita,fill = country))+\n  geom_bar_interactive(aes(\n    tooltip = paste0(\"GDP: \", round(gdp/10^9,2), \n                     \" tỷ USD \\n Dân số: \", round(population/10^6,2),\n                     \" triệu \\n Tuổi thọ bình quân: \", life_expectancy)),\n                       stat=\"identity\")+\n  theme_minimal()+\n  scale_x_discrete(guide = guide_axis(title = \"\", angle = 90))+\n  scale_y_continuous(labels = scales::label_dollar(),\n                     guide = guide_axis(title = \"Thu nhập bình quân đầu người\"))+\n  scale_fill_brewer(palette = \"Paired\",\n                    guide = guide_legend(title = \"Quốc gia\",\n                                         title.position = \"top\"))+\n  ggtitle(\"Các nước Đông Á năm 2011\")+\n  theme(legend.position = \"none\")\ngirafe(ggobj = p, width_svg = 6, height_svg = 4,\n        options = list(opts_sizing(width = .6),\n        opts_tooltip(css = \"font-family: Source Code Pro; \n                               padding:3pt; color: white; \n                               background-color: #640514 ; \n                               border-radius:5px\")\n                ))\ndat_map<-map_data(\"world\")\nmycol<-colorRampPalette(c(\"#5AA0EB\",\"grey95\"), space = \"Lab\")(5)\nmycol<-c(mycol,colorRampPalette(c(\"grey95\",\"#EB5541\"), space = \"Lab\")(5))\n\ndat<-filter(gapminder,year == 2011)\ndat$country<-as.character(dat$country)\ndat$country[dat$country == \"Congo, Dem. Rep.\"]<-\"Democratic Republic of the Congo\"\ndat$country[dat$country == \"Congo, Rep.\"]<-\"Republic of Congo\"\ndat$country[dat$country == \"Dominican Republic\"]<-\"Dominica\"\ndat$country[dat$country == \"Kyrgyz Republic\"]<-\"Kyrgyzstan\"\ndat$country[dat$country == \"Lao\"]<-\"Laos\"\ndat$country[dat$country == \"St. Lucia\"]<-\"Saint Lucia\"\ndat$country[dat$country == \"United States\"]<-\"USA\"\ndat$country[dat$country == \"United Kingdom\"]<-\"UK\"\ndat$country[dat$country == \"Trinidad and Tobago\"]<-\"Trinidad\"\ndat$country<-as.factor(dat$country)\n\nind<-match(dat_map$region,dat$country)\ndat_map<-dat_map%>%mutate(gdp = dat$gdp[ind],\n                 population = dat$population[ind],\n                 infant_mortality = dat$infant_mortality[ind])\nind<-is.na(dat_map$infant_mortality)\ndat_map$infant_mortality[ind]<-round(mean(dat_map$infant_mortality,na.rm=TRUE),2)\n\np<-dat_map%>%\n  ggplot(aes(x=long,y=lat,group=group,label = region, fill = infant_mortality))+\n  geom_polygon_interactive(aes(tooltip = paste0(region, \"\\n Dân số: \", round(population/10^6,2),\n                                                \" triệu \\n Tỷ lệ tử vong trẻ sơ sinh: \", infant_mortality,\"/1000\")),\n                           color=\"black\",size = 0.1)+\n  scale_x_continuous(expand=c(0,0))+\n  scale_fill_gradientn(colors = mycol,\n                       guide = guide_legend(title = \"Tỷ lệ (phần nghìn)\"))+\n  theme_minimal()+xlab(\"\")+ylab(\"\")+ggtitle(\"Tỷ lệ trẻ sơ sinh tử vong năm 2011\")+\n  theme(legend.position = \"top\")\ngirafe(ggobj = p, \n        options = list(opts_sizing(width = .9),\n        opts_tooltip(css = \"font-family: Source Code Pro; \n                               padding:3pt; color: white; \n                               background-color: #640514\")\n                ))"},{"path":"trực-quan-hóa-dữ-liệu.html","id":"tạo-đồ-thị-tương-tác-bằng-thư-viện-plotly","chapter":"Chương 9 Trực quan hóa dữ liệu","heading":"9.8.2 Tạo đồ thị tương tác bằng thư viện plotly","text":"Để tạo một đồ thị tương tác bằng thư viện plotly dễ dàng hơn với sử dụng thư viện ggiraph vì chúng ta không cần viết các câu lệnh có ngữ pháp. Việc duy nhất cần làm là sử dụng hàm ggplotly() trên một đối tượng được tạo bằng hàm ggplot(). Đoạn câu lệnh dưới đây mô tả dữ liệu murders dưới dạng đồ thị tương tác.\nHình 9.59: Đồ thị tương tác mô tả số vụ sát nhân bằng súng tại 51 bang của Mỹ vào năm 2010\nBạn đọc có thể tương tác với đồ thị tạo bằng ggplotly() bằng các thao tác như sau:Sử dụng con trỏ chỉ vào các điểm để xem thông tin chính xác về dân số, số vụ sát nhân, tên bang, và tên vùng của mỗi điểm.Sử dụng con trỏ chỉ vào các điểm để xem thông tin chính xác về dân số, số vụ sát nhân, tên bang, và tên vùng của mỗi điểm.Sử dụng con trỏ, hoặc các nút phóng , thu nhỏ để xem từng phần của đồ thị.Sử dụng con trỏ, hoặc các nút phóng , thu nhỏ để xem từng phần của đồ thị.Sử dụng con trỏ trên chú giải để lựa chọn các vùng nào hiển thị, hoặc không hiển thị trên đồ thị.Sử dụng con trỏ trên chú giải để lựa chọn các vùng nào hiển thị, hoặc không hiển thị trên đồ thị.Sử dụng con trỏ trượt theo đường thẳng tạo bởi geom_smooth() để biết giá trị trên trục total và population của mỗi điểm trên đường thẳng. Lưu ý rằng giá trị xuất hiện là giá trị sau khi đã chuyển đổi bằng hàm log10().Sử dụng con trỏ trượt theo đường thẳng tạo bởi geom_smooth() để biết giá trị trên trục total và population của mỗi điểm trên đường thẳng. Lưu ý rằng giá trị xuất hiện là giá trị sau khi đã chuyển đổi bằng hàm log10().Các thông tin bạn đọc muốn hiển thị bằng con trỏ là tất cả các biến dữ liệu đã được ánh xạ vào trong các thuộc tính thẩm mỹ của đồ thị. Đồ thị trong Hình 9.59 hiển thị thông tin trên các điểm bao gồm giá trị các biến population, total, state, và region là tất cả các biến được sử dụng trong ánh xạ thẩm mỹ của hàm geom_point(). Dọc theo đường hồi quy tuyến tính, chúng ta sẽ có thông tin về giá trị của các biến population và total là các biến được sử dụng trong ánh xạ thẩm mỹ của hàm geom_smooth().Tham số tooltip trong hàm ggplotly() được sử dụng để kiểm soát các thuộc tính thẩm mỹ xuất hiện trên đồ thị tương tác. Ví dụ như trong đồ thị phân tán trong Hình 9.59, nếu chúng ta chỉ muốn hiển thị thông tin về tên của bang và thông tin về vùng. tên của bang được ánh xạ tới thuộc tính thẩm mỹ group và vùng được ánh xạ tới thuộc tính thẩm mỹ fill, nên chúng ta có thể sử dụng tham số tooltip như sauHàm số ggplotly() có thể được sử dụng để tạo đồ thị tương tác với đa số các đồ thị được tạo bởi \\(ggplot2\\), dưới đây là một số ví dụHình 9.60 vẽ đồ thị kiểu bong bóng tương tác mô tả số lượng và giá trung bình của kim cương khi phân loại theo màu sắc và giác cắt.\nHình 9.60: Số lượng và giá trung bình của kim cương phân loại theo màu sắc và giác cắt\nHình 9.61 sử dụng đồ thị tương tác dạng đường để mô tả tổng thu nhập quốc dân của năm quốc gia phát triển trên thế giới là Mỹ, Đức, Pháp, Nhật và Trung Quốc từ năm 1970 đến năm 2011.\nHình 9.61: Tổng thu nhập quốc dân của các quốc gia Mỹ, Đức, Pháp, Nhật và Trung Quốc từ năm 1970 đến năm 2011\nHình 9.62 sử dụng đồ thị tương tác dạng thanh để trực quan hóa hai biến rời rạc là thu nhập bình quân đầu người và châu lục vào năm 2011. Thu nhập bình quân đầu người được phân loại theo ba mức độ: thấp tương ứng với thu nhập bình quân dưới 3000 USD, trung bình tương ứng với thu nhập bình quân từ 3000 USD đến 8000 USD, và cao tương ứng với thu nhập bình quân trên 8000 USD.\nHình 9.62: Tỷ lệ các nước thu nhập thấp, trung bình, và cao tại các châu lục năm 2011\n","code":"\np<-murders %>%\n  ggplot(aes(x = population/10^6, y = total)) +\n  geom_point(aes(group = state, fill = region ), size = 3, shape=21, alpha = 0.8) +\n  geom_smooth(method = \"lm\", se = FALSE, linetype = 2, color=\"#640514\", alpha = 0.5)+\n  scale_x_log10() +\n  scale_y_log10() +\n  scale_fill_brewer(palette = \"Dark2\",\n                    guide = guide_legend(title = \"Vùng\"))+\n  xlab(\"Dân số của bang (triệu dân)\") +\n  ylab(\"Tổng số vụ sát nhân bằng súng\") +\n  ggtitle(\"Số vụ sát nhân bằng súng trong năm 2010 tại Mỹ\")+\n  theme_minimal()\nggplotly(p)\n# Thông tin chỉ bao gồm tên bang và vùng\nggplotly(p, tooltip = c(\"group\",\"fill\"))\nmycol = colorRampPalette(c(\"#5AA0EB\",\"grey90\"), space = \"Lab\")(5)\nmycol = c(mycol,colorRampPalette(c(\"grey90\",\"orange\"), space = \"Lab\")(5))\n\np<-diamonds%>%group_by(cut,color)%>%mutate(ave_price = round(mean(price)))%>%ungroup()%>%\n  as.data.frame()%>%\n  ggplot(aes(cut,color,fill = ave_price))+\n  geom_count(shape = 21, alpha = 0.9, color = \"#640514\")+\n  scale_fill_gradientn(colors = mycol,\n                       guide = guide_legend(title = \"Giá trung bình\"))+\n  scale_size(range=c(1,12))+\n  theme_minimal()\nggplotly(p, tooltip = c(\"n\", \"color\"))\np<-gapminder%>%filter(country %in% c(\"United States\",\"Japan\",\"Germany\",\"France\", \"China\"),\n                   year <= 2011, year >= 1970)%>%mutate(gdp_bil_usd = gdp/10^9)%>%\n  ggplot(aes(x = year, y = gdp_bil_usd, color = country))+\n  geom_line()+\n  scale_y_continuous(labels = scales::label_comma())+\n  theme_minimal()\nggplotly(p, tooltip = c(\"x\",\"y\", \"color\"))\np<-gapminder%>%filter(year == 2011)%>%drop_na()%>%\n  mutate(gdp_per_capita = gdp/population,\n         gdp_levels = ifelse(gdp_per_capita<3000,\"Thấp\",\n                            ifelse(gdp_per_capita<8000,\"Trung bình\",\"Cao\")),\n         gdp_range = factor(gdp_levels, levels = c(\"Cao\",\"Trung bình\",\"Thấp\")))%>%\n  ggplot(aes(x = continent,fill = gdp_range))+\n  geom_bar(color=\"#640514\",alpha=0.7)+\n  scale_fill_manual(values = c(\"orange\",\"grey90\",\"#5AA0EB\"))+\n  theme_minimal()\nggplotly(p, tooltip = \"count\")"},{"path":"trực-quan-hóa-dữ-liệu.html","id":"đồ-thị-động","chapter":"Chương 9 Trực quan hóa dữ liệu","heading":"9.8.3 Đồ thị động","text":"Đồ thị động là một phương pháp thường được sử dụng để mô tả dữ liệu biến đổi theo thời gian. Đồ thị dạng động ngoài yếu tố thẩm mỹ còn giúp cho người tiếp nhận dữ liệu nhanh chóng tiếp nhận được sự thay đổi của các biến liên tục và rời rạc theo thời gian một cách trực quan nhất. Chúng tôi sẽ giới thiệu đến bạn đọc cách tạo các đồ thị động với thư viện plotly và thư viện gganimate.Để hiểu cách tạo đồ thị động, hãy bắt đầu với một dữ liệu đơn giản bao gồm hai biến liên tục là x, y và một biến thời gian:Để trực quan hóa ba biến đều là kiểu số, bao gồm x, y và time, phương pháp thường được sử dụng là trực quan hóa hai biến x và y bằng một đồ thị phân tán, sau đó ánh xạ biến time vào một thuộc tính thẩm mỹ phù hợp, chẳng hạn như kích thước của các điểm. Đồ thị như vậy được mô tả trong Hình 9.63\nHình 9.63: Mô tả ba biến liên tục sử dụng thuộc tính thẩm mỹ kích thước\nMột phương pháp khác để mô tả ba biến trong dữ liệu kể trên là sử dụng đồ thị động, một tập hợp của nhiều đồ thị tĩnh xuất hiện liên tục mà mỗi đồ thị tương ứng với một giá trị của biến time. Bạn đọc có thể thực hiện thao tác này bằng thư viện plotly. Thuộc tính thẩm mỹ để tạo đồ thị dạng động là frame. Chúng ta khai báo thêm ánh xạ thẩm mỹ từ tham số frame đến biến time để tạo một đồ thị động trực quan như Hình 9.64\nHình 9.64: Đồ thị động mô tả sự chuyển động của một điểm theo biến time của dữ liệu\nĐồ thị dạng động sẽ được kích hoạt mỗi khi chúng ta bấm nút play. Bạn đọc có thể thấy rằng cách mô tả sự thay đổi của điểm theo thời gian của đồ thị động trong Hình 9.64 trực quan và hiệu quả hơn với Hình 9.63.Vẽ đồ thị dạng động bằng ggplotly() có ưu điểm là đơn giản và hiệu quả đặc biệt là khi mô tả các biến thay đổi theo thời gian. Bạn đọc có thể sử dụng tham số frame với biến year trong dữ liệu gapminder để mô tả sự thay đổi của các biến khác trong dữ liệu này theo thời gian. Ví dụ, chúng ta muốn mô tả hai biến tuổi thọ trung bình và tỷ lệ sinh trung bình của một phụ nữ qua các năm, chúng ta có thể ánh xạ hai biến lên hai trục tọa độ, sau đó sử dụng màu sắc để mô tả biến châu lục, sử dụng kích thước để mô tả biến dân số, và sau cùng sẽ gán tham số frame với biến year:\nHình 9.65: Đồ thị động mô tả sự thay đổi của tỷ lệ sinh trung bình và tuổi thọ trung bình của tất cả các quốc gia trên thế giới từ năm 1960 đến 2011\n","code":"\ndat<-data.frame(x=1:30,y=(1:30)^2,time=1:30)\ndat%>%ggplot(aes(x,y,size = time))+\n  geom_point(alpha=0.3,shape = 21, color = \"darkblue\", fill = \"#640514\")+\n  scale_size(range=c(1,15))+\n  theme_minimal()\np<-dat%>%ggplot(aes(x=x,y=y,size=time,frame = time))+\n  geom_point(alpha=0.3,shape = 21, color = \"darkblue\", fill = \"#640514\")+\n  scale_size(range=c(1,15))+\n  theme_minimal()\nggplotly(p, tooltip = \"size\")\np<-gapminder%>%filter(year %in% 1960:2011)%>%\n  ggplot(aes(x = fertility, y = life_expectancy, size = population,\n             fill = continent, frame = year))+\n  geom_point(alpha = 0.5,shape=21)+\n  scale_fill_brewer(palette = \"Set1\")+\n  scale_size(range=c(1,15))+\n  theme_minimal()+\n  ggtitle(\"Tuổi thọ và tỷ lệ sinh trung bình 1960 đến 2011\")\nggplotly(p, tooltip = c())"},{"path":"trực-quan-hóa-dữ-liệu.html","id":"bài-tập-1","chapter":"Chương 9 Trực quan hóa dữ liệu","heading":"9.9 Bài tập","text":"","code":""},{"path":"trực-quan-hóa-dữ-liệu.html","id":"phụ-lục-3","chapter":"Chương 9 Trực quan hóa dữ liệu","heading":"9.10 Phụ lục","text":"","code":""},{"path":"trực-quan-hóa-dữ-liệu.html","id":"lập-trình-trong-ggplot2","chapter":"Chương 9 Trực quan hóa dữ liệu","heading":"9.10.1 Lập trình trong ggplot2","text":"","code":""},{"path":"trực-quan-hóa-dữ-liệu.html","id":"tạo-dashboard-với-thư-viện-shiny","chapter":"Chương 9 Trực quan hóa dữ liệu","heading":"9.10.2 Tạo dashboard với thư viện shiny","text":"","code":""},{"path":"mô-hình-hồi-quy-tuyến-tính.html","id":"mô-hình-hồi-quy-tuyến-tính","chapter":"Chương 10 Mô hình hồi quy tuyến tính","heading":"Chương 10 Mô hình hồi quy tuyến tính","text":"","code":""},{"path":"mô-hình-hồi-quy-tuyến-tính.html","id":"giới-thiệu-chung-1","chapter":"Chương 10 Mô hình hồi quy tuyến tính","heading":"10.1 Giới thiệu chung","text":"Chương sách này thảo luận về mô hình hồi quy tuyến tính - phương pháp cơ bản nhất trong các phương pháp xây dựng mô hình học máy có giám sát. Mặc dù đơn giản nhưng mô hình hồi quy tuyến tính lại là một công cụ hữu ích để đưa ra các dự đoán hoặc mô tả sự tác động của một biến giải thích lên các biến khác. Hồi quy tuyến tính là một chủ đề đã được nghiên cứu từ rất lâu, từ trước khi có máy tính điện tử, đồng thời cũng là chủ đề của vô số sách tham khảo. Trong thời đại ngày nay, mặc dù mô hình này có vẻ hơi nhàm chán với một số phương pháp học thống kê/học máy hiện đại, hồi quy tuyến tính vẫn là một phương pháp học thống kê hữu ích và được sử dụng rộng rãi. Hơn nữa, đây còn là điểm khởi đầu tốt cho các phương pháp tiếp cận mới hơn như chúng ta sẽ thấy trong các chương sau. Nhiều phương pháp học máy tiên tiến nhất hiện nay có thể được coi là sự khái quát hóa hoặc mở rộng của hồi quy tuyến tính. đó, tầm quan trọng của việc hiểu rõ về hồi quy tuyến tính trước khi nghiên cứu các phương pháp phức tạp hơn là không thể phủ nhận.Trong phần đầu của chương này, chúng ta xem xét một số ý tưởng chính làm cơ sở cho mô hình hồi quy tuyến tính, cũng như phương pháp bình phương nhỏ nhất được sử dụng phổ biến nhất để ước lượng tham số cho mô hình này. Trong phần sau của chương, chúng ta sẽ thảo luận về các phương pháp lựa chọn mô hình và các phương pháp rút gọn tham số (shrinkage methods).","code":""},{"path":"mô-hình-hồi-quy-tuyến-tính.html","id":"những-nội-dung-cơ-bản-của-mô-hình-quy-tuyến-tính","chapter":"Chương 10 Mô hình hồi quy tuyến tính","heading":"10.2 Những nội dung cơ bản của mô hình quy tuyến tính","text":"Trước khi đi vào các nội dung cơ bản, hãy lấy một ví dụ đơn giản về một yêu cầu phân tích dữ liệu mà có thể tìm được lời giải được bằng mô hình hồi quy tuyến tính. Một công ty thực hiện một chiến dịch quảng cáo sản phẩm cho 55 cửa hàng trên phạm vi toàn quốc trong một năm thông qua ba phương thức quảng cáo làQuảng cáo qua truyền hình: biến TV;Quảng cáo qua truyền hình: biến TV;Quảng cáo qua các nền tảng mạng xã hội: biến Social_Media; và,Quảng cáo qua các nền tảng mạng xã hội: biến Social_Media; và,Quảng cáo qua phát tờ rơi: biến Flyer.Quảng cáo qua phát tờ rơi: biến Flyer.Hình 10.1 mô tả mối liên hệ giữa doanh thu bán hàng, biến Sales, với chi phí thực hiện các phương thức quảng cáo ở 55 cửa hàng. Lưu ý rằng doanh thu bán hàng được tính bằng đơn vị tỷ đồng trong khi các chi phí quảng cáo được tính bằng đơn vị triệu đồng.\nHình 10.1: Doanh thu bán hàng (tỷ đồng) và mối liên hệ với chi phí quảng cáo. Hình bên trái: Chi phí quảng cáo trên Tivi. Hình ở giữa: chi phí quảng cáo qua các nền tảng mạng xã hội. Hình bên phải: quảng cáo theo hình thức phát tờ rơi\nGiả sử với vai trò là một chuyên gia tư vấn, chúng ta được yêu cầu đưa ra đề xuất trên cơ sở dữ liệu quan sát được, một kế hoạch quảng cáo cho năm tới nhằm mang lại doanh thu bán sản phẩm cao. Thông tin nào từ dữ liệu sẽ hữu ích để đưa ra khuyến nghị cho chiến dịch quảng cáo? Dưới đây là một số câu hỏi quan trọng mà chúng ta cần tìm cách giải quyết nhằm đưa ra khuyến nghị:Có mối quan hệ giữa ngân sách chi cho từng hình thức quảng cáo và doanh thu bán hàng không? Để trả lời câu hỏi này chúng ta cần xác định xem dữ liệu có cung cấp minh chứng về mối liên hệ tuyến tính giữa chi phí cho từng hình thức quảng cáo với doanh thu bán hàng. Nếu mối liên hệ là yếu, hoặc thậm chí là mối liên hệ âm thì chúng ta có thể lập luận rằng không nên chi tiền cho quảng cáo.Có mối quan hệ giữa ngân sách chi cho từng hình thức quảng cáo và doanh thu bán hàng không? Để trả lời câu hỏi này chúng ta cần xác định xem dữ liệu có cung cấp minh chứng về mối liên hệ tuyến tính giữa chi phí cho từng hình thức quảng cáo với doanh thu bán hàng. Nếu mối liên hệ là yếu, hoặc thậm chí là mối liên hệ âm thì chúng ta có thể lập luận rằng không nên chi tiền cho quảng cáo.Mối liên hệ giữa ngân sách chi cho quảng cáo và doanh thu nếu tồn tại thì mạnh đến mức nào? Nếu mối liên hệ là mạnh, thì với một ngân sách quảng cáo nhất định, liệu chúng ta có thể dự đoán doanh số bán hàng với độ chính xác cao không, hay dự đoán về doanh số bán hàng dựa trên chi tiêu quảng cáo chỉ tốt hơn một chút với dự đoán một cách ngẫu nhiên?Mối liên hệ giữa ngân sách chi cho quảng cáo và doanh thu nếu tồn tại thì mạnh đến mức nào? Nếu mối liên hệ là mạnh, thì với một ngân sách quảng cáo nhất định, liệu chúng ta có thể dự đoán doanh số bán hàng với độ chính xác cao không, hay dự đoán về doanh số bán hàng dựa trên chi tiêu quảng cáo chỉ tốt hơn một chút với dự đoán một cách ngẫu nhiên?Phương tiện truyền thông nào góp phần tăng doanh số bán hàng? Cả ba phương tiện truyền thông qua Tivi, mạng xã hội và phát tờ rơi có đóng góp vào doanh số bán hàng hay chỉ một hoặc hai phương tiện quảng cáo có đóng góp? Để trả lời câu hỏi này, chúng ta phải tìm cách tách biệt những tác động riêng lẻ của từng phương tiện quảng cáo đến doanh thu khi chúng ta đã chi tiền cho cả ba phương tiện.Phương tiện truyền thông nào góp phần tăng doanh số bán hàng? Cả ba phương tiện truyền thông qua Tivi, mạng xã hội và phát tờ rơi có đóng góp vào doanh số bán hàng hay chỉ một hoặc hai phương tiện quảng cáo có đóng góp? Để trả lời câu hỏi này, chúng ta phải tìm cách tách biệt những tác động riêng lẻ của từng phương tiện quảng cáo đến doanh thu khi chúng ta đã chi tiền cho cả ba phương tiện.Chúng ta có thể ước tính chính xác tác động của chi phí từng phương tiện quảng cáo đến doanh thu bán hàng như thế nào? Với cùng một mức chi cho quảng cáo trên một phương tiện cụ thể, doanh thu bán hàng sẽ tăng bao nhiêu? Chúng ta có thể dự đoán mức tăng này chính xác đến mức nào?Chúng ta có thể ước tính chính xác tác động của chi phí từng phương tiện quảng cáo đến doanh thu bán hàng như thế nào? Với cùng một mức chi cho quảng cáo trên một phương tiện cụ thể, doanh thu bán hàng sẽ tăng bao nhiêu? Chúng ta có thể dự đoán mức tăng này chính xác đến mức nào?Mối liên hệ hay sự tác động của chi phí cho từng hình thức quảng cáo đến doanh số bán hàng có tuyến tính không? Nếu không, liệu có phương pháp biến đổi như thế nào để mối liên hệ vẫn là tuyến tính?Mối liên hệ hay sự tác động của chi phí cho từng hình thức quảng cáo đến doanh số bán hàng có tuyến tính không? Nếu không, liệu có phương pháp biến đổi như thế nào để mối liên hệ vẫn là tuyến tính?Có sự tác động qua lại giữa các chi phí cho các phương tiện quảng cáo không? Chẳng hạn như nên chi đồng thời 10 triệu đồng cho quảng cáo trên mạng xã hội và 10 triệu đồng cho quảng cáo tờ rơi liệu có mang lại doanh thu cao hơn việc phân bổ 20 triệu cho riêng từng hình thức? Trong tiếp thị quảng cáo, đây được gọi là hiệu ứng tổng hợp.Có sự tác động qua lại giữa các chi phí cho các phương tiện quảng cáo không? Chẳng hạn như nên chi đồng thời 10 triệu đồng cho quảng cáo trên mạng xã hội và 10 triệu đồng cho quảng cáo tờ rơi liệu có mang lại doanh thu cao hơn việc phân bổ 20 triệu cho riêng từng hình thức? Trong tiếp thị quảng cáo, đây được gọi là hiệu ứng tổng hợp.Những cơ sở của mô hình hồi quy tuyến tính được thảo luận trong chương này sẽ giúp bạn đọc lần lượt trả lời các câu hỏi đặt ra ở trên.","code":""},{"path":"mô-hình-hồi-quy-tuyến-tính.html","id":"mô-hình-hồi-quy-tuyến-tính-đơn-biến","chapter":"Chương 10 Mô hình hồi quy tuyến tính","heading":"10.2.1 Mô hình hồi quy tuyến tính đơn biến","text":"Hồi quy tuyến tính đơn biến, đúng như tên gọi, đó là một cách tiếp cận tuyến tính đơn giản để dự đoán phản ứng định lượng của một biến mục tiêu \\(Y\\) trên cơ sở biến độc lập \\(X\\). Mô hình giả định rằng có mối quan hệ tuyến tính giữa \\(Y\\) và \\(X\\). Về mặt toán học, chúng ta có thể viết mối quan hệ tuyến tính này như sau\n\\[\\begin{align}\nY \\sim \\beta_0 + \\beta_1 \\cdot X\n\\tag{10.1}\n\\end{align}\\]\nBạn có thể hiểu ‘\\(\\sim\\)’ theo nghĩa xấp xỉ hoặc gần đúng. Đôi khi chúng ta sẽ mô tả phương trình (10.1) bằng cách nói rằng chúng ta đang hồi quy \\(Y\\) theo \\(X\\). Trong ví dụ trình bày ở trên, \\(X\\) có thể đại diện cho chi phí quảng cáo trên truyền hình (TV) và \\(Y\\) có thể đại diện cho doanh số bán hàng (Sales) tại các cửa hàng. Sau đó, chúng ta có thể hồi quy doanh số bán hàng theo chi phí quảng cáo trên truyền hình theo một mô hình hồi quy tuyến tính đơn biến như sau\n\\[\\begin{align}\n\\text{Sales} \\sim \\beta_0 + \\beta_1 \\cdot \\text{TV}\n\\tag{10.2}\n\\end{align}\\]Trong phương trình (10.2), \\(\\beta_0\\) và \\(\\beta_1\\) là hai hằng số chưa biết biểu thị hệ số chặn và hệ số góc của đường thẳng trong mô hình tuyến tính. Cùng với nhau, \\((\\beta_0, \\beta_1)\\) được gọi là các hệ số tuyến tính hoặc tham số của mô hình. Các hệ số này sẽ được ước lượng dựa trên dữ liệu thu thập được dựa trên phương pháp người xây dựng mô hình lựa chọn. Các ước lượng cho hệ số tuyến tính thường được thêm dấu mũ ở trên để phân biệt với tham số của mô hình tuyến tính. Nói cách khác chúng ta có \\(\\hat{\\beta}_0\\) và \\(\\hat{\\beta}_1\\) là các ước lượng của \\(\\beta_0\\) và \\(\\beta_1\\). Với \\(Y\\) là biến doanh thu bán hàng và \\(X\\) là biến chi phí quảng cáo trên truyền hình chúng ta sẽ có một dự đoán cho doanh thu bán hàng \\(\\hat{y}\\) dựa trên một quan sát của chi phí quảng cáo \\(X = x\\)\n\\[\\begin{align}\n\\hat{y} = \\hat{\\beta}_0 + \\hat{\\beta}_1 \\cdot x\n\\tag{10.3}\n\\end{align}\\]Lưu ý rằng chúng tôi luôn sử dụng dấu mũ để mô tả một ước lượng cho một tham số, hoặc là giá trị dự đoán cho một giá trị không biết.","code":""},{"path":"mô-hình-hồi-quy-tuyến-tính.html","id":"ước-lượng-hệ-số-trong-mô-hình-đơn-biến","chapter":"Chương 10 Mô hình hồi quy tuyến tính","heading":"10.2.1.1 Ước lượng hệ số trong mô hình đơn biến","text":"Trong mô hình hồi quy tuyến tính đơn biến, \\(\\beta_0\\) và \\(\\beta_1\\) là các tham số không biết và cần được ước lượng từ dữ liệu. Giả sử dữ liệu chúng ta quan sát được bao gồm \\(n\\) cặp \\((x_i, y_i)\\) như sau\n\\[\\begin{align}\n(x_1, y_1), (x_2, y_2), \\cdots, (x_n, y_n)\n\\end{align}\\]\ntrong đó \\(x_i\\) là các giá trị quan sát được của biến \\(X\\) và \\(y_i\\) là các giá trị quan sát được tương ứng của biến ngẫu nhiên \\(Y\\), với \\(\\) = \\(1\\), \\(2\\), …, \\(n\\). Trong ví dụ về chi phí cho quảng cáo, tập dữ liệu bao gồm ngân sách quảng cáo qua truyền hình và doanh số bán sản phẩm với \\(n = 55\\) cửa hàng khác nhau. Mục tiêu của chúng ta là thu được các ước lượng cho hệ số \\(\\beta_0\\) và \\(\\beta_1\\) sao cho mô hình tuyến tính (10.2) phù hợp tốt với dữ liệu có sẵn. Nói cách khác, chúng ta muốn tìm hệ số chặn \\(\\beta_0\\) và hệ số góc \\(\\beta_1\\) sao cho đường thẳng ước lượng được càng gần \\(n = 55\\) điểm dữ liệu càng tốt.Có nhiều phương pháp khác nhau để định nghĩa thế nào là một đường thẳng gần nhất với một tập hợp điểm. Tuy nhiên, cho đến nay cách tiếp cận phổ biến nhất là tối thiểu hóa tổng bình phương các khoảng cách từ các điểm đến đường thẳng đó. Phương pháp này được gọi là phương pháp bình phương nhỏ nhất và chúng ta áp dụng cách tiếp cận này để ước lượng các tham số. Chi tiết về phương pháp bình phương nhỏ nhất sẽ được trình bày trong các phần sau của chương.\nHình 10.2: Phương pháp bình phương nhỏ nhất được sử dụng để ước lượng hệ số cho mô hình hồi quy đơn biến doanh thu bán hàng theo chi phí quảng cáo trên truyền hình. Đường hồi quy tuyến tính được ước lượng bằng phương pháp bình phương nhỏ nhất là đường thẳng có tổng bình phương khoảng cách các điểm dữ liệu đến đường thẳng này là nhỏ nhất\nHình 10.2 mô tả đường hồi quy tuyến tính đơn được xây dựng trên dữ liệu về quảng cáo với biến phụ thuộc là doanh số bán hàng và biến độc lập là chi phí quảng cáo trên truyền hình, với \\(\\hat{\\beta}_0\\) = 5.103 và \\(\\hat{\\beta}_1\\) = 0.024.Điểm \\((x_i,y_i)\\) trên hình vẽ mô tả chi phí quảng cáo trên truyền hình, \\(x_i\\), của cửa hàng thứ \\(\\) và doanh thu bán hàng \\(y_i\\) tương ứng. Với \\(\\hat{y}_i = \\hat{\\beta}_0 + \\hat{\\beta}_1 \\cdot x_i\\) là dự đoán cho \\(Y\\) dựa trên giá trị thứ \\(\\) của \\(X\\). Khi đó ta ký hiệu \\(e_i = y_i − \\hat{y}_i\\) đại diện cho sai số, hay còn gọi là phần dư, của quan sát thứ \\(\\). Nói một cách khác phần dư là sự khác biệt giữa giá trị quan sát được của biến mục tiêu và giá trị ước lượng được cho biến mục tiêu được tính toán bởi mô hình. Chúng ta xác định tổng bình phương của phần dư, được gọi là Residual Sum Squares hay RSS, bằng công thức như sau\n\\[\\begin{align}\nRSS & = e_1^2 + e_2^2 + \\cdots + e_n^2 \\\\\n& = (y_1 - \\hat{\\beta}_0 - \\hat{\\beta}_1 \\cdot x_1)^2 + (y_2 - \\hat{\\beta}_0 - \\hat{\\beta}_1 \\cdot x_2)^2 + \\cdots + (y_n - \\hat{\\beta}_0 - \\hat{\\beta}_1 \\cdot x_n)^2\n\\end{align}\\]Phương pháp bình phương nhỏ nhất lựa chọn \\(\\hat{\\beta}_0\\) và \\(\\hat{\\beta}_1\\) sao cho giá trị của RSS là nhỏ nhất trong công thức phía trên là nhỏ nhất. Bạn đọc có thể giải bài toán tối ưu bằng cách cho đạo hàm của RSS theo \\(\\hat{\\beta}_0\\) và \\(\\hat{\\beta}_1\\) bằng 0 và cho kết quả như sau\n\\[\\begin{align}\n\\hat{\\beta}_1 & = \\cfrac{\\sum\\limits_{=1}^n (x_i - \\bar{x}) (y_i - \\bar{y})}{\\sum\\limits_{=1}^n (x_i - \\bar{x})^2} \\\\\n\\hat{\\beta}_0 & = \\bar{y} - \\hat{\\beta}_1 \\bar{x}\n\\tag{10.4}\n\\end{align}\\]\ntrong đó \\(\\bar{y} = \\sum\\limits_{=1}^n y_i\\) và \\(\\bar{x} = \\sum\\limits_{=1}^n x_i\\) là các giá trị trung bình của các quan sát. Chúng ta sẽ quay trở lại với phương pháp ước lượng bình phương nhỏ nhất trong phần sau của chương.Để thực hiện ước lượng mô hình tuyến tính trong R, chúng ta sử dụng hàm lm() của thư viện stat. Câu lệnh thực hiện ước lượng mô hình tuyến tính như sauTừ kết quả của mô hình, chúng ta có một ước lượng cho mô hình tuyến tính đơn giản mà biến Sales phụ thuộc vào biến TV như sau:\n\\[\\begin{align}\n\\text{Sales} = 5.10303  + 0.02409  \\cdot \\text{TV}\n\\tag{10.5}\n\\end{align}\\]Theo như mô hình ước lượng được, khi thêm 1 triệu đồng chi cho quảng cáo trên truyền hình dẫn đến việc doanh thu bán hàng trung bình tăng thêm khoảng 24.1 triệu đồng. Doanh thu bán hàng trung bình tăng thêm 24.1 triệu có thể hiểu là có cửa hàng có doanh thu tăng nhiều hơn, có cửa hàng có doanh thu tăng ít hơn, nhưng khi lấy giá trị trung bình phần doanh thu tăng thêm của các cửa hàng sẽ có kết quả là khoảng 24.1 triệu đồng.Bạn đọc có thể thực hiện tương tự các bước như trên để đưa ra đánh giá về mối liên hệ giữa chi phí quảng cáo trên mạng xã hội, hoặc chi phí quảng cáo bằng hình thức phát tờ rơi, đến doanh thu bán sản phẩm trung bình của các cửa hàng.","code":"\n# Lấy dữ liệu advertising vào R\nAdvertising<-read.csv(\".../advertise.csv\")\n# Thực hiện ước lượng mô hình\n# Doanh thu bán hàng (Sales) hồi quy theo TV\nlm(Sales~TV, data = Advertising)"},{"path":"mô-hình-hồi-quy-tuyến-tính.html","id":"đánh-giá-sự-chính-xác-của-các-hệ-số-ước-lượng","chapter":"Chương 10 Mô hình hồi quy tuyến tính","heading":"10.2.1.2 Đánh giá sự chính xác của các hệ số ước lượng","text":"Trong mô hình tuyến tính đơn biến mà Sales phụ thuộc vào biến TV chúng ta ước lượng được hệ số \\(\\beta_2\\) bằng 0.02409 và dẫn đến kết luận là khi chi phí quảng cáo trên truyền hình tăng thêm 1 triệu đồng sẽ dẫn đến doanh thu bán hàng trung bình tăng thêm khoảng 24.1 triệu đồng. Chúng tôi dùng từ “khoảng” vì không có gì đảm bảo là doanh thu bán hàng trung bình sẽ tăng thêm đúng 24.1 triệu đồng. Để đưa ra được mối liên hệ giữa doanh thu trung bình thực tế tăng thêm, một giá trị mà chúng ta không biết, với giá trị ước lượng được là 24.1 triệu đồng, chúng ta cần đánh giá sự chính xác khi ước lượng hệ số \\(\\beta_0\\) và \\(\\beta_1\\).Nhắc lại rằng trong các mô hình học máy có giám sát, mối liên hệ thực tế giữa biến mục tiêu \\(Y\\) và biến phụ thuộc \\(X\\) được mô tả thông qua một hàm \\(f\\),\n\\[\\begin{align}\nY = f(X) + \\epsilon\n\\end{align}\\]\ntrong đó \\(\\epsilon\\) là thành phần hoàn toàn độc lập với biến \\(X\\), nghĩa là không thể đưa ra được thêm bất kỳ thông tin nào về \\(\\epsilon\\) dựa trên dữ liệu \\(X\\). Trong trường hợp hàm \\(f\\) thực sự là một hàm tuyến tính, mối liên hệ giữa \\(X\\) và \\(Y\\) sẽ được mô tả thông qua hệ số chặn \\(\\beta_0\\) và hệ số góc \\(\\beta_1\\)\n\\[\\begin{align}\nY = \\beta_0 + \\beta_1 X + \\epsilon\n\\tag{10.6}\n\\end{align}\\]Mô hình tuyến tính trong phương trình (10.6) được gọi là đường hồi quy tuyến tính thực. Trong thực tế, không thể biết được đường hồi quy tuyến tính thực, mà chúng ta chỉ có thể dựa trên các giá trị quan sát được của biến mục tiêu và biến phụ thuộc để ước lượng ra các tham số \\(\\hat{\\beta}_0\\) và \\(\\hat{\\beta}_1\\). Đường thẳng có hệ số chặn \\(\\hat{\\beta}_0\\) và hệ số góc \\(\\hat{\\beta}_1\\) được gọi là đường hồi quy tuyến tính ước lượng được.\nHình 10.3: Hình bên trái: Đường hồi quy tuyến tính thật màu đen và đường hồi quy tuyến tính ước lượng màu đỏ. Hình bên phải: Đường hồi quy tuyến tính thật màu đen và các đường hồi quy tuyến tính ước lượng màu xanh\nĐường màu đen ở phần bên trái của Hình 10.3 hiển thị đường hồi quy tuyến tính thực, \\(f(x) = 1 + 2 \\cdot x\\), trong khi đường màu đỏ là ước lượng bình phương nhỏ nhất dựa trên dữ liệu quan sát được. Mối quan hệ thực sự thường không thể biết được, nhưng đường bình phương nhỏ nhất luôn có thể được ước lượng bằng cách sử dụng phương trình (10.4). Nói cách khác, trong các ứng dụng thực tế, mỗi khi chúng ta có một tập hợp các quan sát là từ đó chúng ta có thể tính toán đường bình phương nhỏ nhất; đồng thời, đường hồi quy tuyến tính thực là không thể tính toán được. Trong phần bên phải của Hình 10.3, chúng tôi đã tạo ra mười bộ dữ liệu khác nhau từ mô hình hồi quy thực và vẽ mười đường bình phương nhỏ nhất tương ứng. Lưu ý rằng các tập dữ liệu khác nhau được tạo ra từ cùng một mô hình thực sẽ dẫn đến các đường bình phương nhỏ nhất hơi khác nhau một chút, nhưng đường hồi quy tổng thể không quan sát được không thay đổi.Quan sát trên hình vẽ, sự khác biệt giữa đường hồi quy tổng thể và đường bình phương nhỏ nhất có vẻ khó nhận thấy và khó hiểu. Về cơ bản, khái niệm hai đường này là sự mở rộng của phương pháp thống kê tiêu chuẩn về việc sử dụng thông tin từ một mẫu để ước tính các đặc điểm của một tổng thể. Ví dụ, giả sử chúng ta muốn biết trung bình tổng thể \\(\\mu\\) của một biến ngẫu nhiên \\(Y\\) nào đó. \\(\\mu\\) là một giá trị không biết, nhưng chúng ta có \\(n\\) quan sát từ của \\(Y\\), mà chúng ta có thể viết là \\(y_1, y_2, \\cdots , y_n\\) và chúng ta có thể sử dụng để ước lượng \\(\\mu\\). Một ước lượng hợp lý cho \\(\\mu\\) là \\(\\hat{\\mu} = \\bar{y}\\), trong đó\n\\[\\begin{align}\n\\bar{y} = \\cfrac{\\sum\\limits_{=1}^n y_i}{n}\n\\end{align}\\]\nlà giá trị trung bình mẫu. Trung bình mẫu và trung bình tổng thể là khác nhau, nhưng nói chung trung bình mẫu sẽ cung cấp ước tính tốt về trung bình tổng thể. Theo cách tương tự, các hệ số chưa biết \\(\\beta_0\\) và \\(\\beta_1\\) trong hồi quy tuyến tính là hệ số đường hồi quy tổng thể hay đường hồi quy thực. Chúng ta ước lượng các hệ số chưa biết này bằng cách sử dụng \\(\\hat{\\beta_0}\\) và \\(\\hat{\\beta_1}\\).Sự tương tự giữa ước lượng các hệ số của hồi quy tuyến tính và ước lượng giá trị trung bình của một biến ngẫu nhiên còn được thể hiện qua các tính chất của các ước lượng. Chẳng hạn như chúng ta luôn mong muốn các ước lượng là các ước lượng không chệch. Đối với trung bình tổng thể, chúng ta luôn tìm các ước lượng \\(\\hat{\\mu}\\) sao cho \\(\\mathbb{E}(\\hat{\\mu}) = \\mu\\). Tính chất không chệch của ước lượng đảm bảo rằng nếu chúng ta có thể có một số lượng đủ lớn các quan sát thì giá trị trung bình mẫu sẽ xấp xỉ giá trị trung bình tổng thể. Các hệ số của mô hình hồi quy tuyến tính bằng phương pháp bình phương nhỏ nhất cũng là các ước lượng không chệch, nghĩa là nếu chúng ta ước lượng \\(\\beta_0\\) và \\(\\beta_1\\) trên cơ sở một tập dữ liệu cụ thể và nhỏ thì các ước lượng sẽ không chính xác bằng \\(\\beta_0\\) và \\(\\beta_1\\). Nhưng nếu chúng ta có thể lặp lại việc ước lượng nhiều lần và tính trung bình các ước lượng thu được từ một số lượng lớn tập dữ liệu cùng lấy ra từ tổng thể thì giá trị trung bình của các ước lượng này sẽ xấp xỉ \\(\\beta_0\\) và \\(\\beta_1\\). Trên thực tế, chúng ta có thể thấy từ hình bên phải của Hình 10.3 rằng giá trị trung bình của nhiều đường bình phương tối thiểu, mỗi đường được ước tính từ một tập dữ liệu riêng biệt, khá gần với đường hồi quy tổng thể thực.Một câu hỏi khác cần được đặt ra với ước lượng trung bình tổng thể \\(\\mu\\) của biến ngẫu nhiên \\(Y\\) là: giá trị trung bình mẫu \\(\\hat{\\mu}\\) ước tính của \\(\\mu\\) chính xác như thế nào? Chúng ta đã biết rằng giá trị trung bình của các \\(\\hat{\\mu}\\) trên nhiều tập dữ liệu sẽ rất gần với \\(\\mu\\), nhưng một ước tính duy nhất của \\(\\hat{\\mu}\\) trên một dự liệu cụ thể sẽ chênh lệch với \\(\\mu\\) như thế nào? Nhìn chung, để trả lời câu hỏi này chúng ta cần tính độ lệch chuẩn của \\(\\hat{\\mu}\\), được ký hiệu là \\(SE(\\hat{\\mu})\\). Chúng ta đã biết rằng\n\\[\\begin{align}\nSE(\\hat{\\mu}) = \\cfrac{\\sigma}{\\sqrt{n}}\n\\end{align}\\]\nvới \\(\\sigma\\) là độ lệch chuẩn của biến \\(Y\\).\\(SE(\\hat{\\mu})\\) cho chúng ta biết một ước lượng cụ thể \\(\\hat{\\mu}\\) sẽ chênh lệch với \\(\\mu\\) như thế nào. Có thể dễ dàng thấy rằng khi số lượng quan sát \\(n\\) đủ lớn, \\(SE(\\hat{\\mu})\\) sẽ càng gần đến 0 và chênh lệch giữa \\(\\hat{\\mu}\\) với \\(\\mu\\) sẽ càng nhỏ. Lập luận hoàn toàn tương tự, để biết các ước lượng cho các hệ số chặn và hệ số góc trong mô hình tuyến tính chênh lệch với các giá trị thật \\(\\beta_0\\) và \\(\\beta_1\\) như thế nào, chúng ta cần tính toán độ lệch chuẩn của các ước lượng đó. Tính toán độ lệch chuẩn của các ước lượng cho hệ số sẽ được trình bày chi tiết trong các phần sau. Bạn đọc cần biết là độ lệch chuẩn của \\(\\hat{\\beta_0}\\) và \\(\\hat{\\beta_1}\\) có thể tính toán được như sau\n\\[\\begin{align}\nSE(\\hat{\\beta}_0) = \\sigma \\cdot \\sqrt{\\cfrac{1}{n} + \\cfrac{\\bar{x}^2}{\\sum\\limits_{=1}^n (x_i - \\bar{x})^2} } ; SE(\\hat{\\beta}_1) = \\cfrac{\\sigma}{\\sqrt{\\sum\\limits_{=1}^n (x_i - \\bar{x})^2} }\n\\tag{10.8}\n\\end{align}\\]\ntrong đó \\(\\sigma = \\sqrt{Var(\\epsilon)}\\).Các công thức cho độ lệch tiêu chuẩn của \\(\\hat{\\beta_0}\\) và \\(\\hat{\\beta_1}\\) ở trên đi kèm với giả định là các sai số \\(\\epsilon_i\\) là độc lập với nhau và có cùng phương sai là \\(\\sigma^2\\). Giả thiết này thường không đạt được trong thực tế tuy nhiên công thức (10.8) vẫn là một xấp xỉ tốt cho phương sai của các ước lượng. Lưu ý trong công thức ở trên rằng \\(SE(\\hat{\\beta}_1)\\) nhỏ hơn khi \\(x_i\\) trải rộng hơn quanh giá trị trung bình của nó. Chúng ta cũng thấy rằng \\(SE(\\hat{\\beta}_0)\\) sẽ giống như độ lệch chuẩn của trung bình mẫu nếu \\(\\bar{x}\\) bằng 0. Điểm đáng lưu ý là \\(\\sigma^2\\) cũng là một đại lượng chưa biết chưa nhưng có thể ước lượng được từ dữ liệu. Ước lượng cho \\(\\sigma\\) được gọi là độ lệch chuẩn của phần dư, ký hiệu RSE (hay Residual Standard Error), và được tính theo công thức\n\\[\\begin{align}\nRSE = \\sqrt{RSS/(n - 2)}\n\\tag{10.9}\n\\end{align}\\]RSE có thể được sử dụng để tính toán các khoảng tin cậy cho các hệ số ước lượng. Khoảng tin cậy ở một mức xác suất, chẳng hạn như mức \\(\\alpha\\), được định nghĩa là một khoảng giá trị sao cho với xác suất \\(\\alpha\\), khoảng giá trị đó sẽ chứa giá trị thực chưa biết của tham số. Với giả thiết phần dư \\(\\epsilon\\) có phân phối chuẩn, có thể chứng minh được rằng \\(\\hat{\\beta_0}\\) và \\(\\hat{\\beta_1}\\) cũng có phân phối chuẩn. Khoảng tin cậy ở mức xác suất \\(\\alpha\\) được sử dụng là làm các khoảng tin cậy cho tham số \\(\\beta_i\\); với = 1, 2; có dạng\n\\[\\begin{align}\n\\left[\\hat{\\beta_i} - z_{1 + \\alpha/2} \\cdot SE(\\hat{\\beta_i}); \\hat{\\beta_i} + z_{1 + \\alpha/2} \\cdot SE(\\hat{\\beta_i}) \\right]\n\\end{align}\\]\ntrong đó \\(z_{1+\\alpha/2}\\) là giá trị tại mức xác suất \\((1 + \\alpha/2)\\) của biến ngẫu nhiên phân phối chuẩn \\(\\mathcal{N}(0,1)\\).Quay trở lại với ví dụ về doanh thu bán hàng phụ thuộc vào chi phí quảng cáo trên truyền hình. Với mức độ tin cậy \\(\\alpha = 95\\%\\), giá trị tại mức xác suất \\((1 + \\alpha/2)\\) của phân phối chuẩn \\(\\mathcal{N}(0,1)\\) xấp xỉ bằng 1.96. Ta có các khoảng tin cậy tại mức xác suất 95% cho hệ số chặn là\n\\[\\begin{align}\n(3.372; 6.834)\n\\end{align}\\]\nvà khoảng tin cậy cho hệ số góc là\n\\[\\begin{align}\n(0.012; 0.036)\n\\end{align}\\]Điều này có nghĩa là, không tính đến quảng cáo trên truyền hình, doanh thu trung bình của các cửa hàng rơi vào khoảng 3.372 tỷ đồng đến 6.834 tỷ đồng. Đồng thời, mỗi triệu đồng tăng thêm cho quảng cáo trên truyền hình, sẽ làm cho doanh thu trung bình tăng thêm khoảng 12.4 triệu đồng đến 35.7 triệu đồng.Độ lệch tiêu chuẩn còn được sử dụng để trả lời câu hỏi là liệu mối liên hệ giữa biến \\(X\\) và \\(Y\\) có thực sự có ý nghĩa. Theo thống kê toán, chúng ta cần phải kiểm đinh cặp giả thuyết:\n\\[\\begin{align}\nH_0: \\beta_1 = 0 \\\\\nH_1: \\beta_1 \\neq 0\n\\end{align}\\]\nvì nếu \\(\\beta_1 = 0\\) thì mô hình hồi quy tuyến tính đơn trở thành \\(Y = \\beta_0 + \\epsilon\\) và \\(X\\) không có liên hệ với \\(Y\\) . Để kiểm định giả thuyết \\(H_0\\), chúng ta cần xác định xem liệu ước lượng của \\(\\beta_1\\), là \\(\\hat{\\beta}_1\\), có đủ xa giá trị 0 để chúng ta có thể tin tưởng rằng \\(\\beta_1\\) khác 0 hay không. Nhưng như thế nào là đủ xa thì lại phụ thuộc vào độ chính xác của \\(\\hat{\\beta}_1\\), nghĩa là cũng phụ thuộc vào \\(SE(\\hat{\\beta}_1)\\). Nếu \\(\\hat{\\beta}_1\\) tương đối nhỏ, nhưng \\(SE(\\hat{\\beta}_1)\\) lại rất nhỏ, thì chúng ta vẫn có thể khá chắc chắn rằng \\(\\beta \\neq 0\\), và đó có mối liên hệ giữa \\(X\\) và \\(Y\\). Ngược lại, nếu \\(\\hat{\\beta}_1\\) tương đối xa giá trị 0, nhưng \\(SE(\\hat{\\beta}_1)\\) lại rất lớn, thì cũng rất khó để khẳng định rằng \\(\\beta \\neq 0\\). Trong thực tế, chúng ta tính toán một thống kê t\n\\[\\begin{align}\nt = \\cfrac{\\hat{\\beta}_1 - 0}{SE(\\hat{\\beta}_1)}\n\\tag{10.10}\n\\end{align}\\]\ndùng để đo độ lệch tương đối giữa \\(\\hat{\\beta}\\) với giá trị 0. Với giả thiết \\(\\epsilon\\) có phân phối chuẩn, và dưới giả thuyết \\(H_0\\), có thể chứng minh được rằng thống kê \\(t\\) sẽ có phân phối student với bậc tự là \\(n-2\\). Phân phối student cũng có hình dạng quả chuông giống như phân phối chuẩn và sẽ tiệm cận phân phối chuẩn nếu tham số bậc tự đủ lớn. đó, chúng ta có thể tính toán được xác suất mà một biến ngẫu nhiên phân phối student bất kỳ có giá trị tuyệt đối lớn hơn hoặc bằng giá trị thống kê \\(t\\) tính toán được trong phương trình (10.10). Xác suất này còn thường được gọi là p-value.Nhìn chung, chúng ta có thể suy diễn p-value như sau: nếu p-value nhận giá trị nhỏ thì rất khó có thể có được giá trị thống kê \\(t\\) có giá trị tuyệt đối lớn như vậy dưới giả thuyết \\(H_0\\), nghĩa là có cơ sở để bác bỏ giả thuyết \\(H_0\\). Hay nói một cách khác, có mối liên hệ giữa biến độc lập và biến mục tiêu. Giá trị p-value thường được coi là nhỏ nếu nằm dưới các ngưỡng như 5% hoặc thậm chí 1%.Bảng ?? cung cấp thông tin chi tiết về tham số ước lượng được trong mô hình hồi quy tuyến tính đơn bằng phương pháp bình phương nhỏ nhất để hồi quy doanh thu bán hàng đơn theo ngân sách quảng cáo trên truyền hình trong dữ liệu về quảng cáo. Lưu ý rằng các hệ số \\(\\hat{\\beta_0}\\) và \\(\\hat{\\beta_1}\\) rất lớn với độ lệch chuẩn của các ước lượng này, đó giá trị thống kê t cũng lớn. Xác suất để một biến ngẫu nhiên phân phối Student có tham số bậc tự bằng 55 - 2 = 53 có giá trị lớn hơn các giá trị tuyệt đối của thống kê \\(t\\) dưới giả thuyết \\(H_0\\) đúng là gần như bằng 0. đó chúng ta có thể kết luận rằng \\(\\beta_0 \\neq 0\\) và \\(\\beta_1 \\neq 0\\).","code":""},{"path":"mô-hình-hồi-quy-tuyến-tính.html","id":"đánh-giá-mô-hình-hồi-quy-tuyến-tính-đơn","chapter":"Chương 10 Mô hình hồi quy tuyến tính","heading":"10.2.1.3 Đánh giá mô hình hồi quy tuyến tính đơn","text":"Sau khi chúng ta đã bác bỏ giả thuyết về các hệ số bằng 0, công việc tiếp theo sẽ là đưa ra các đánh giá định lượng về mức độ phù hợp của mô hình với dữ liệu. Chất lượng của mô hình hồi quy tuyến tính thường được đánh giá bằng cách sử dụng hai đại lượng: thứ nhất: độ lệch chuẩn của phần dư và hệ số \\(R^2\\).Độ lệch chuẩn của phần dư được gọi là Residual Standard Error hay viết tắt là RSE. Phần dư trong mô hình tuyến tính là phần độc lập với biến giải thích \\(X\\), nghĩa là trong các mô hình có sự hiện diện của phần dư, ngay cả khi chúng ta biết được đường hồi quy thực cũng không thể đưa ra dự đoán chính xác được về \\(Y\\) từ \\(X\\). Phần dư luôn có giá trị trung bình bằng 0 vì nếu không phần giá trị trung bình sẽ được giải thích bằng hệ số chặn. đó, người xây dựng mô hình thường quan tâm đến độ lệch chuẩn của phần dư, hay còn gọi là RSE\n\\[\\begin{align}\nRSE = \\sqrt{\\cfrac{RSS}{n-2}} = \\sqrt{\\cfrac{\\sum\\limits_{=1}^n (y_i - \\hat{y}_i)^2 }{n-2}}\n\\tag{10.11}\n\\end{align}\\]Trong mô hình hồi quy tuyến tính trên dữ liệu Quảng cáo, RSE bằng 0.235. Điều này có nghĩa là sai số giữa doanh thu bán hàng thực tế và doanh thu bán hàng được ước lượng từ mô hình hồi quy tuyến tính sẽ có độ lệch chuẩn khoảng 0.235 tỷ đồng. Tuy nhiên, độ lệch chuẩn là 0.235 có phải là con số có thể chấp nhận được hay không còn tùy thuộc vào bối cảnh. Trong dữ liệu quảng cáo, giá trị trung bình của doanh số bán hàng trên tất cả các cửa hàng là khoảng 8.7 tỷ đồng và đó sai số phần trăm là khoảng 0.235/8.7 \\(\\approx\\) 2.7 %. RSE có thể coi là thước đo mức độ phù hợp của các mô hình tuyến tính trên dữ liệu. Nếu các dự đoán thu được bằng cách sử dụng mô hình rất gần với giá trị kết quả thực, tức là, nếu các \\(\\hat{y}_i\\) rất gần với các \\(y_i\\), với \\(= 1, 2, \\cdots, n\\), khi đó RSE sẽ nhỏ và chúng ta có thể kết luận rằng mô hình rất phù hợp với dữ liệu. Mặt khác, \\(\\hat{y}_i\\) rất xa các \\(y_i\\) đối với một hoặc nhiều quan sát thì RSE có thể khá lớn, cho thấy mô hình không phù hợp với dữ liệu.RSE là một thước đo tuyệt đối về sự phù hợp của mô hình hồi quy tuyến tính trên dữ liệu. Nhưng vì RSE được đo bằng độ lệch chuẩn nên khi tính toán RSE tương đối trên giá trị trung bình của \\(Y\\) sẽ không cho chúng ta một cái nhìn chính xác thế nào là một RSE tốt. Thay vào đó, hệ số \\(R^2\\) cung cấp một thước đo tương đối về mức độ phù hợp của mô hình. \\(R^2\\) có dạng tỷ lệ giữa phương sai được giải thích trên tổng phương sai nên hệ số này luôn nhận giá trị từ 0 đến 1 và không phụ thuộc vào đơn vị của biến \\(Y\\). Hệ số \\(R^2\\), hay còn được gọi là R-squared, được tính bằng công thức sau\n\\[\\begin{align}\nR^2 = \\cfrac{TSS - RSS}{TSS} = 1 - \\cfrac{RSS}{TSS}\n\\tag{10.11}\n\\end{align}\\]\nvới \\(TSS = \\sum (y_i - \\bar{y})^2\\).TSS là viết tắt của Total Sum Squares, là tổng phương sai của biến mục tiêu \\(Y\\) và có thể được coi là mức độ biến thiên của biến mục tiêu xung quanh giá trị trung bình của nó. Giá trị này không phụ thuộc vào mô hình hồi quy tuyến tính. RSS đo lường mức độ biến thiên mà không giải thích được bởi mô hình hồi quy tuyến tính. đó, TSS − RSS đo lường mức độ biến thiên được giải thích bằng cách thực hiện hồi quy và hệ số \\(R^2\\) đo lường tỷ lệ biến thiên của biến mục tiêu \\(Y\\) có thể được giải thích bằng biến giải thích \\(X\\) trong mô hình tuyến tính. Hệ số \\(R^2\\) gần bằng 1 cho biết rằng phần lớn sự biến thiên trong biến mục tiêu đã được giải thích bằng mô hình hồi quy. Giá trị \\(R^2\\) càng gần 0 cho thấy mô hình hồi quy không giải thích được nhiều về sự biến thiên của biến mục tiêu. Mô hình có \\(R^2\\) nhỏ thông thường là dạng của mô hình tuyến tính là sai, mô hình tuyến tính bị thiếu biến giải thích, hoặc phần dư \\(\\epsilon\\) có phương sai lớn.Các đại lượng RSS, RSE, TSS, và R-squared trong mô hình tuyến tính đơn mà biến mục tiêu doanh thu được hồi quy theo chi phí quảng cáo trên truyền hình được cho trong bảng ??Trong Bảng ??, R-squared bằng 0.237 có nghĩa là chỉ 23.7% sự biến thiên trong doanh số được giải thích bằng hồi quy tuyến tính theo chi phí quảng cáo trên truyền hình. Rõ ràng, hệ số R-squared dễ dàng diễn giải hơn với RSE, vì đại lượng này luôn nằm trong khoảng từ 0 đến 1. Tuy nhiên, vẫn có thể gặp khó khăn khi xác định thế nào là giá trị R-squared tốt. Điều này lại tùy thuộc vào từng ngữ cảnh thực tế. Ví dụ, trong một số dữ liệu thu thập được từ vật lý hay khoa học máy tính, chúng ta có thể đã biết rằng dữ liệu thực sự đến từ một mô hình tuyến tính có phần dư nhỏ. Hệ số R-squared thu được sẽ xấp xỉ bằng 1 và giá trị R-squared nhỏ hơn 0.9 có thể cho thấy có vấn đề với thử nghiệm mà dữ liệu được tạo ra. Mặt khác, trong các ứng dụng điển hình về kinh tế hay xã hội học, các mô hình tuyến tính thường có phần dư có phương sai rất lớn có rất nhiều các yếu tố khác không được đo lường được từ dữ liệu. Trong trường hợp này, chúng ta chỉ cần một tỷ lệ phương sai được giải thích rất nhỏ. Hệ số R-squared bằng 0.05 hoặc 0.1 trong các dữ liệu như vậy lại có thể là bằng chứng cho một mô hình tốt!Trong mô hình hồi quy tuyến tính đơn biến, giá trị R-squared chính là bình phương của hệ số tương quan giữa biến mục tiêu \\(Y\\) và biến độc lập \\(X\\). Hệ số tương quan giữa \\(Y\\) và \\(X\\), ký hiệu \\(\\rho(X,Y)\\), và được ước lượng bằng công thức như sau\n\\[\\begin{align}\n\\hat{\\rho}(X,Y) = \\cfrac{ \\sum\\limits_{=1}^n (x_i - \\bar{x})(y_i - \\bar{y}) }{ \\sqrt{\\sum\\limits_{=1}^n (x_i - \\bar{x})^2}\\sqrt{\\sum\\limits_{=1}^n (y_i - \\bar{y})^2} }\n\\end{align}\\]Hệ số tương quan \\(\\rho(X,Y)\\) đo lường mối liên hệ tuyến tính giữa hai biến \\(X\\) và \\(Y\\). Hệ số \\(\\rho(X,Y)\\) nằm trong khoảng \\([-1,1]\\), vàKhi \\(\\rho(X,Y) = 0\\), chúng ta nói rằng giữa \\(X\\) và \\(Y\\) không có mối liên hệ tuyến tính.Khi \\(\\rho(X,Y) > 0\\), chúng ta nói rằng giữa \\(X\\) và \\(Y\\) có mối liên hệ tuyến tính cùng chiều, nghĩa là khi \\(X\\) tăng thì nhiều khả năng \\(Y\\) cũng sẽ tăng theo một tỷ lệ cố định.Khi \\(\\rho(X,Y) < 0\\), chúng ta nói rằng giữa \\(X\\) và \\(Y\\) có mối liên hệ tuyến tính ngược chiều, nghĩa là khi \\(X\\) tăng thì nhiều khả năng \\(Y\\) sẽ giảm theo một tỷ lệ cố định.Bạn đọc hãy lưu ý rằng đẳng thức \\(R^2 = \\left(\\hat{\\rho}(X,Y)\\right)^2\\) chỉ đúng trong mô hình hồi quy đơn biến. Trong phần tiếp theo, chúng ta sẽ thảo luận về mô hình hồi quy tuyến tính đa biến, trong đó chúng ta sẽ sử dụng đồng thời nhiều biến độc lập để giải thích một biến mục tiêu. Chúng ta sẽ thấy nhiều hơn ý nghĩa của hệ số R-squared trong các mô hình như vậy.","code":""},{"path":"mô-hình-hồi-quy-tuyến-tính.html","id":"mô-hình-hồi-quy-tuyến-tính-đa-biến","chapter":"Chương 10 Mô hình hồi quy tuyến tính","heading":"10.2.2 Mô hình hồi quy tuyến tính đa biến","text":"Hồi quy tuyến tính đơn là một phương pháp hữu ích để dự đoán biến mục tiêu dựa trên một biến giải thích duy nhất. Tuy nhiên, trong thực tế chúng ta thường có nhiều hơn một yếu tố để giải thích biến mục tiêu. Ví dụ: trong dữ liệu Quảng cáo, chúng ta đã kiểm tra mối quan hệ giữa doanh thu bán hàng và ngân sách quảng cáo trên truyền hình. Chúng ta cũng có dữ liệu về số tiền chi cho quảng cáo trên các mạng xã hội và quảng cáo thông qua tờ rơi, đó chúng ta sẽ muốn biết rằng liệu các phương thức quảng cáo này có liên quan đến doanh thu bán hàng hay không; nghĩa là làm cách nào chúng ta có thể mở rộng phân tích dữ liệu Quảng cáo để phù hợp khi bổ sung thêm hai biến giải thích.Bạn đọc có thể sử dụng ba mô hình hồi quy tuyến tính đơn riêng biệt, mỗi mô hình sử dụng một biến giải thích tương ứng với một phương thức quảng cáo khác nhau làm biến giải thích. Kết quả ước lượng ba mô hình tuyến tính đơn được cho trong các Bảng ??, ??, và ??Bạn đọc có thể nhận thấy rằng nếu sử dụng ba mô hình hồi quy đơn, các biến giải thích đều có tác động lên biến mục tiêu một cách có ý nghĩa các giá trị p-value đều rất nhỏ. Chúng ta sẽ thảo luận chi tiết về các hệ số tuyến tính trong các Bảng ??, ??, và ?? trong phần sau của cuốn sách. Tuy nhiên cách tiếp cận như trên sẽ gặp phải hai vấn đề. Thứ nhất: chúng ta sẽ không biết làm thế nào để đưa ra một dự đoán duy nhất về doanh thu bán hàng tương ứng với một phân bổ ngân sách quảng cáo cho ba hình thức quảng cáo, vì khi phân bổ ngân sách đến từng phương tiện quảng cáo sẽ có ba giá trị dự đoán riêng biệt. Thứ hai, mỗi phương trình hồi quy đơn đều bỏ qua hai phương tiện còn lại trong việc hình thành ước tính cho các hệ số hồi quy. Chúng ta sẽ sớm thấy rằng nếu ngân sách truyền thông có tương quan với nhau tại các cửa hàng, điều mà rất có thể xảy ra, thì điều này có thể dẫn đến những ước lượng có sai lệch rất lớn về tác động của từng phương tiện quảng cáo lên doanh thu bán hàng.Thay vì điều chỉnh một mô hình hồi quy tuyến tính đơn giản riêng biệt cho từng yếu tố dự đoán, cách tiếp cận tốt hơn là mở rộng mô hình hồi quy tuyến tính đơn bằng cách cho tương ứng với mỗi biến giải thích một hệ số góc riêng. Nói chung, giả sử rằng chúng ta có \\(p\\) biến giải thích riêng biệt. Khi đó mô hình hồi quy tuyến tính đa biến có dạng\n\\[\\begin{align}\nY = \\beta_0 + \\beta_1 \\cdot X_1 + \\beta_2 \\cdot X_2 + \\cdots + \\beta_p \\cdot X_p + \\epsilon\n\\tag{10.12}\n\\end{align}\\]\ntrong đó \\(X_j\\) đại diện cho biến giải thích thứ \\(j\\) và hệ số \\(\\beta_j\\) định lượng mối liên hệ tuyến tính giữa biến giải thích đó và biến mục tiêu. Có thể coi \\(\\beta_j\\) là đại lượng phản ánh sự thay đổi của biến mục tiêu \\(Y\\) khi biến giải thích \\(X_j\\) tăng thêm một đơn vị trong khi tất cả các biến giải thích khác không thay đổi. Trong ví dụ về dữ liệu quảng cáo, ta có mô hình hồi quy tuyến tính như sau\n\\[\\begin{align}\n\\text{Sales} = \\beta_0 + \\beta_1 \\times \\text{TV} + \\beta_2 \\times \\text{Social_Media} + \\beta_3 \\times \\text{Flyer} + \\epsilon\n\\tag{10.13}\n\\end{align}\\]\ntrong đó Sales là doanh thu từ bán hàng của 55 cửa hàng, TV là chi phí quảng cáo trên truyền hình, Social_Media là chi phí quảng cáo qua mạng xã hội, và Flyer là chi phí quảng cáo qua tờ rơi.","code":""},{"path":"mô-hình-hồi-quy-tuyến-tính.html","id":"ước-lượng-tham-số-cho-mô-hình-đa-biến","chapter":"Chương 10 Mô hình hồi quy tuyến tính","heading":"10.2.2.1 Ước lượng tham số cho mô hình đa biến","text":"Tương tự như trong mô hình hồi quy tuyến tính đơn, các hệ số hồi quy \\(\\beta_0\\), \\(\\beta_1\\), \\(\\cdots\\), \\(\\beta_p\\) trong phương trình (10.12) là chưa biết và cần được ước lượng. Các tham số này cũng được ước lượng bằng cách sử dụng phương pháp bình phương nhỏ nhất. Tuy nhiên, không giống như các ước lượng hồi quy tuyến tính đơn, các ước lượng hệ số hồi quy đa biến khá phức tạp cần được biểu diễn dưới dạng véc-tơ và ma trận. Chính vì lý này, chúng tôi không đi sâu vào vấn đề này ở đây. Chi tiết của phương pháp bình phương nhỏ nhất trong hồi quy đa biến bạn đọc có thể tham khảo trong phần 10.3. Với các ước lượng \\(\\hat{\\beta}_0\\), \\(\\hat{\\beta}_1\\), \\(\\cdots\\), \\(\\hat{\\beta}_p\\) chúng ta có thể đưa ra dự đoán cho biến mục tiêu \\(y\\) như sau\n\\[\\begin{align}\n\\hat{y} = \\hat{\\beta}_0 + \\hat{\\beta}_1 \\cdot x_1 + \\hat{\\beta}_2 \\cdot x_2 + \\cdots + \\hat{\\beta}_p \\cdot x_p\n\\tag{10.14}\n\\end{align}\\]Bạn đọc có thể sử dụng hàm lm() để thực hiện ước lượng các hệ số tuyến tính. Chúng ta thực hiện ước lượng mô hình đa biến cho dữ liệu Quảng cáo như sauBảng ?? hiển thị ước tính hệ số hồi quy đa biến khi ngân sách quảng cáo trên truyền hình, mạng xã hội, và tờ rơi được sử dụng để dự đoán doanh thu bán sản phẩm trên dữ liệu Quảng cáo. Chúng ta có thể giải thích những kết quả này như sau:Đối với một ngân sách cố định cho quảng cáo trên truyền hình và một ngân sách cố định cho quảng cáo qua mạng xã hội, việc chi thêm 1 triệu đồng cho quảng cáo bằng hình thức tờ rơi sẽ dẫn đến doanh thu bán hàng trung bình tăng khoảng 11.7 triệu đồng. Con số tương tự với quảng cáo trên truyền hình và qua mạng xã hội lần lượt là 63 triệu đồng và 16.3 triệu đồng. sánh các con số này với các Bảng ??, ??, và ??, chúng ta có thể nhận thấy rằng các hệ số ước lượng đã thay đổi đáng kể với việc sử dụng ba mô hình hồi quy đơn biến.Trong mô hình hồi quy đơn, hệ số tuyến tính của biến chi phí quảng cáo qua tờ rơi (Flyer) là có ý nghĩa, trong khi trong mô hình hồi quy đa biến, hệ số của biến này lại không khác 0 một cách có ý nghĩa. Điều này thể hiện qua giá trị của thống kê \\(t\\) khá nhỏ và p-value khá lớn (khoảng 0.343)Sự khác biệt này xuất phát từ thực tế là trong trường hợp hồi quy đơn, hệ số góc thể hiện tác động trung bình của việc tăng 1 triệu đồng trong quảng cáo qua tờ rơi và bỏ qua các yếu tố dự đoán khác là quảng cáo qua truyền hình và qua mạng xã hội. Ngược lại, trong mô hình hồi quy đa biến, hệ số tuyến tính của biến Flyer thể hiện tác động trung bình của việc tăng chi phí quảng cáo qua tờ rơi thêm 1 triệu đồng trong khi giữ nguyên chi phí quảng cáo trên truyền hình và qua mạng xã hội. Vậy liệu có hợp lý không khi mô hình hồi quy đa biến cho thấy không có mối quan hệ giữa doanh thu bán hàng và chi phí quảng cáo qua tờ rơi trong khi hồi quy tuyến tính đơn lại hàm ý ngược lại? Câu trả lời là có! Hãy quan sát ma trận hệ số tương quan của ba biến giải thích và biến mục tiêu trong Bảng ??.\nHình 10.4: Ma trận hệ số tương quan của các biến trong dữ liệu Quảng cáo\nBạn đọc có thể thấy rằng hệ số tương quan giữa chi phí quảng cáo qua tờ rơi,biến Flyer, với hai biến giải thích còn lại bao gồm TV và Social_Media là khá cao, lần lượt là 0.54 và 0.51. Điều này cho thấy xu hướng chi tiêu nhiều hơn cho quảng cáo qua hình thức tờ rơi ở các cửa hàng nơi chi tiêu nhiều hơn cho quảng cáo trên qua truyền hình hoặc qua mạng xã hội. Các mô hình hồi quy đơn và mô hình hồi quy đa biến đều cho kết luận là tăng chi tiêu quảng cáo qua truyền hình và quảng cáo qua mạng xã hội thực sự có ý nghĩa làm tăng doanh thu. Giả sử rằng kết luận này là đúng, khi đó việc sử dụng mô hình hồi quy tuyến tính đơn để kiểm tra mối liên hệ giữa doanh thu bán hàng theo quảng cáo trên tờ rơi cho hệ số ước lượng có ý nghĩa là cả hai biến này đều có tương quan cao với chi phí quảng cáo qua truyền hình và mạng xã hội, chứ thực sự thì chi tiêu cho quảng cáo qua tờ rơi không có tác động đến doanh thu bán hàng.Đây là kết quả rất thường gặp khi xây dựng mô hình trên dữ liệu thực tế. Một ví dụ thường được nhắc đến để mô tả tình huống này trong nhiều sách tham khảo là khi hồi quy số các cuộc tấn công của cá mập theo doanh số bán kem trên các bãi biển trong một khoảng thời gian nhất định. Đây là hai biến về bản chất không có mối liên hệ nhưng sẽ cho hệ số góc là một số dương có ý nghĩa thống kê. Cũng giống như chi phí quảng cáo qua hình thức tờ rơi và doanh thu bán hàng, việc không tính đến các biến giải thích có tác động thực sự lên biến mục tiêu sẽ khiến cho chúng ta lầm tưởng rằng doanh số bán kem có tác động đến số cuộc tấn công của cá mập! Trên thực tế, nhiệt độ cao hơn khiến nhiều người đến bãi biển hơn, từ đó dẫn đến doanh số bán kem nhiều hơn và nhiều vụ cá mập tấn công hơn. Nếu chúng ta xây dựng mô hình hồi quy bội mà số các cuộc tấn công của cá mập phụ thuộc vào doanh số bán kem và nhiệt độ của vùng đó sẽ cho kết quả là doanh số bán kem không còn có ý nghĩa giải thích số các cuộc tấn công!","code":"\n# Doanh thu bán hàng (Sales) hồi quy theo 3 biến\nlm(Sales ~ TV + Social_Media + Flyer, data = Advertising)"},{"path":"mô-hình-hồi-quy-tuyến-tính.html","id":"kiểm-định-mô-hình-tuyến-tính-đa-biến","chapter":"Chương 10 Mô hình hồi quy tuyến tính","heading":"10.2.2.2 Kiểm định mô hình tuyến tính đa biến","text":"Mục đích của kiểm định mô hình đa biến là để trả lời hai câu hỏiThứ nhất: mô hình hồi quy có ý nghĩa giải thích biến mục tiêu \\(Y\\) hay không? Hay nói một cách khác là có ít nhất một biến trong số các biến giải thích \\(X_1\\), \\(X_2\\), \\(\\cdots\\), \\(X_p\\) có tác động lên biến mục tiêu?Thứ hai: nếu mô hình hồi quy đa biến có ý nghĩa, thì tất cả các biến đều có ý nghĩa tác động lên biến mục tiêu, hay chỉ một tập hợp con các biến có tác động?Để trả lời cho câu hỏi thứ nhất, nhắc lại với bạn đọc rằng trong kiểm định mô hình hồi quy tuyến tính đơn, để xác định liệu có mối quan hệ giữa biến mục tiêu và biến giải thích hay không, chúng ta chỉ cần kiểm định giả thuyết \\(H_0: \\beta_1 = 0\\). Trong mô hình hồi quy đa biến với \\(p\\) biến giải thích dự đoán, chúng ta cần kiểm định giả thuyết liệu hệ số hồi quy của tất cả các biến giải thích đều bằng 0, tức là liệu có xảy ra trường hợp \\(\\beta_1 = \\beta_2 = \\cdots = \\beta_p = 0\\). Cặp giả thuyết \\(H_0\\) - \\(H_1\\) trong mô hình hồi quy đa biến được viết như sau\n\\[\\begin{align}\n& H_0: \\beta_1 = \\beta_2 = \\cdots = \\beta_p = 0 \\\\\n& H_1: \\text{ít nhất có một } \\beta_j \\text{ khác 0}\n\\tag{10.15}\n\\end{align}\\]Để kiểm định cặp giả thuyết trong (10.15), chúng ta sử dụng thống kê \\(F\\)\n\\[\\begin{align}\nF = \\cfrac{(TSS - RSS)/p}{RSS/(n-p-1)}\n\\tag{10.16}\n\\end{align}\\]Nếu giả thuyết \\(H_0\\) là đúng thì thống kê \\(F\\) sẽ có phân phối \\(\\mathcal{F}(p, n - p - 1)\\). giá trị trung bình của biến ngẫu nhiên phân phối \\(\\mathcal{F}(p, n - p - 1)\\) là 1 nên khi giá trị thống kê \\(F\\) lớn thì khả năng bác bỏ giả thuyết \\(H_0\\) là lớn. Để hiểu được tại sao lại sử dụng phân phối \\(\\mathcal{F}\\) để kiểm định giả thuyết, bạn đọc tham khảo phần 10.3. Chúng tôi không giải thích chi tiết vấn đề này tại đây để tránh sự phức tạp không cần thiết.Thống kê \\(F\\) cho mô hình hồi quy tuyến tính đa biến thu được bằng cách hồi quy doanh thu bán hàng theo chi phí quảng cáo qua truyền hình, mạng xã hội, và tờ rơi được trình bày là 18.75. Vì giá trị này lớn hơn 1 rất nhiều nên đây có cơ sở để bác bỏ giả thuyết \\(H_0\\). Nói cách khác, giá trị thống kê \\(F\\) lớn cho thấy rằng ít nhất một trong các phương tiện quảng cáo phải liên quan đến doanh thu bán hàng. Tuy nhiên, thống kê \\(F\\) cần phải lớn đến mức nào để chúng ta có thể bác bỏ \\(H_0\\) và kết luận rằng có mối quan hệ và điều gì sẽ xảy ra nếu thống kê \\(F\\) gần với 1 hơn? Để trả lời câu hỏi này còn phụ thuộc vào giá trị của \\(n\\) và \\(p\\). Khi \\(n\\) lớn, ngay cả khi thống kê \\(F\\) chỉ lớn hơn 1 một chút chúng ta vẫn có thể có cơ sở để bác bỏ \\(H_0\\). Ngược lại, thống kê \\(F\\) cần lớn hơn để có cơ sở bác bỏ \\(H_0\\) nếu \\(n\\) nhỏ.Bạn đọc có thể quan sát hàm mật độ của biến ngẫu nhiên phân phối \\(\\mathcal{F}\\) với các tham số \\((p = 3,n - p - 1 = 51)\\) trong Hình 10.5. Khả năng một biến ngẫu nhiên có phân phối \\(\\mathcal{F}\\) lớn hơn giá trị thống kê \\(F\\) tính toán từ dữ liệu là khoảng \\(2.5 \\times 10^{-8}\\). Nói cách khác, với p-value rất nhỏ, chúng ta có cơ sở để bác bỏ giả thuyết \\(H_0\\). Điều này đồng nghĩa với việc có ít nhất một ngân sách chi cho quảng cáo có tác động đến doanh thu bán hàng.\nHình 10.5: Hàm mật độ của phân phối F(3,51) cho dữ liệu quảng cáo. Giá trị thống kê F (F - value) đủ lớn để bác bỏ giả thuyết \\(H_0\\)\nĐể trả lời cho câu hỏi thứ hai, chúng ta cần thực hiện các kiểm định liệu một nhóm biến giải thích có tác động lên biến mục tiêu hay không. Giả sử các biến giải thích có số thứ tự lần lượt là \\(1 \\leq i_1 < i_2 < \\cdots < i_h \\leq p\\). Khi đó, giả thuyết \\(H_0\\) - \\(H_1\\) để thực hiện kiểm định giả thuyết được viết như sau\n\\[\\begin{align}\n& H_0: \\beta_{i_j} = 0 \\ \\ \\forall j = 1,2, \\cdots, h \\\\\n& H_1: \\text{Tồn tại ít nhất $j$ sao cho} \\ \\beta_{i_j} > 0\n\\tag{10.17}\n\\end{align}\\]Tương tự như trong trường hợp kiểm định giả thuyết trong phương trình (10.15), chúng ta có thể sử dụng phân phối \\(\\mathcal{F}\\) để thực hiện kiểm định giả thuyết. Thống kê \\(F\\) được tính toán như sau\n\\[\\begin{align}\nF = \\cfrac{(RSS_1 - RSS)/h}{RSS/(n-p-1)}\n\\tag{10.18}\n\\end{align}\\]\ntrong đó \\(RSS_1\\) là tổng bình phương sai số của mô hình hồi quy tuyến tính không bao gồm các biến độc lập có hệ số tuyến tính được liệt kê trong phương trình (10.17).Nếu giả thuyết \\(H_0\\) trong (10.17) là đúng, thì có thể chứng minh được rằng (tham khảo phần 10.3) thống kê \\(F\\) sẽ có phân phối \\(\\mathcal{F}(h,n-p-1)\\). Giá trị của thống kê \\(F\\) lớn cho thấy rằng ít nhất một trong các biến độc lập có hệ số tuyến tính được liệt kê trong phương trình (10.17) có tác động tuyến tính lên biến phụ thuộc. Trong trường hợp đặc biệt khi \\(h=1\\), nghĩa là khi chúng ta cần kiểm định từng biến có ý nghĩa ở trong mô hình hồi quy tuyến tính, giá trị thống kê \\(F\\) trong phương trình (10.18) chính là bình phương của thống kê \\(t\\) khi kiểm định từng biến độc lập riêng lẻ. Giá trị của thống kê \\(t\\) và giá trị p-value tương ứng khi kiểm định từng chi phí quảng cáo có tác động đến doanh thu bán hàng hay không được cho trong Bảng ??. Các giá trị p-value này chỉ ra rằng truyền hình và mạng xã hội có liên quan đến doanh thu bán hàng, nhưng không có bằng chứng nào cho thấy quảng cáo qua hình thức tờ rơi có liên quan đến doanh thu bán hàng khi tính đến cả quảng cáo trên truyền hình và quảng cáo qua mạng xã hội.Khi đã có các p-value riêng lẻ cho từng biến, tại sao chúng ta cần xem xét thống kê \\(F\\) trong kiểm định đồng thời? Liệu có phải rằng p-value của một biến riêng lẻ là nhỏ thì ít nhất một trong các yếu tố dự đoán có liên quan đến phản hồi? Điều này không phải lúc nào cũng đúng, đặc biệt khi số lượng biến giải thích \\(p\\) khá lớn. Chẳng hạn như khi số lượng biến giải thích \\(p = 20\\) và chúng ta kiểm định giả thuyết \\(H_0: \\beta_1 = \\beta_2 = \\cdots = \\beta_{20} = 0\\). Ngay cả khi \\(H_0\\) là thực sự đúng, thì với mức ý nghĩa 5%, vẫn sẽ có trung bình \\(5\\% \\times 20 = 1\\) (biến) không có p-value lớn hơn 0.05! Điều này cũng giống như khi chúng ta tạo ra 20 biến ngẫu nhiên phân phối Student thì sẽ có trung bình một biến rơi vào miền giá trị có xác suất 0.05. đó, nếu chúng ta sử dụng thống kê \\(t\\) riêng lẻ và các p-value liên quan để quyết định xem có bất kỳ mối liên hệ nào giữa các biến giải thích và biến mục tiêu hay không, thì có khả năng cao là chúng ta sẽ kết luận sai rằng có một mối quan hệ. Sử dụng thống kê \\(F\\) không gặp phải vấn đề này vì thống kê \\(F\\) có tính toán đến số lượng biến giải thích đưa vào trong kiểm định. Nếu giả thuyết \\(H_0\\) thực sự đúng thì chỉ có 5% khả năng thống kê \\(F\\) có p-value nhỏ hơn 0.05, bất kể số lượng biến giải thích là bao nhiêu.Bước đầu tiên trong xây dựng một mô hình tuyến tính thường là ước lượng mô hình và tính toán giá trị thống kê \\(F\\). Nếu chúng ta kết luận dựa trên p-value của thống kê \\(F\\) rằng ít nhất một trong các biến giải thích có liên quan đến biến mục tiêu, thì câu hỏi tiếp theo cần trả lời sẽ là các biến nào sẽ thực sự có ý nghĩa trong mô hình. Chúng ta có thể xem xét các p-value riêng lẻ cho từng biến như trong bảng ??, nhưng như đã thảo luận, nếu \\(p\\) khá lớn thì chúng ta có thể thực hiện một số nhận định sai. Quá trình xác định những biến giải thích có liên quan đến biến mục tiêu để tìm ra một mô hình duy nhất chỉ bao gồm các biến có liên quan được gọi là quá trình lựa chọn biến. Vấn đề lựa chọn biến được thảo luận kỹ hơn trong phần 10.4.","code":""},{"path":"mô-hình-hồi-quy-tuyến-tính.html","id":"kiểm-tra-sự-phù-hợp-của-mô-hình","chapter":"Chương 10 Mô hình hồi quy tuyến tính","heading":"10.2.2.3 Kiểm tra sự phù hợp của mô hình","text":"Như đã đề cập trong mô hình tuyến tính đơn, hai thước đo định lượng phổ biến nhất về mức độ phù hợp của mô hình hồi quy tuyến tính là sai số của phần dư RSE và hệ số R-squared. Hãy nhớ lại rằng trong hồi quy đơn, R-squared là bình phương hệ số tương quan giữa biến mục tiêu và biến giải thích. Trong hồi quy tuyến tính đa biến, có thể chứng minh được rằng hệ số R-squared chính là bình phương hệ số tương quan giữa \\(Y\\) và \\(\\hat{Y}\\). Giá trị R-squared gần bằng 1 cho thấy mô hình giải thích được phần lớn phương sai của biến mục tiêu. Hệ số R-squared trong hồi quy đa biến được tính toán dựa trên phương trình @ref\\tag{10.11} tương tự như hồi quy đơn. Hệ số R-squared được tính toán bằng hàm lm() như sauĐối với Dữ liệu quảng cáo, mô hình sử dụng cả ba phương tiện quảng cáo để dự đoán doanh thu bán hàng có hệ số R-squared là 0.524. Mặt khác, mô hình chỉ sử dụng TV và Social_Media để dự đoán doanh thu bán hàng có giá trị R-squared là 0.516. Nói cách khác, có một sự gia tăng nhỏ trong R-squared nếu chúng ta đưa quảng cáo bằng tờ rơi vào mô hình đã có sẵn quảng cáo trên truyền hình và mạng xã hội, mặc dù trước đó chúng ta đã thấy rằng giá trị p-value cho quảng cáo trên tờ rơi trong Bảng ?? là không đáng kể.Thực ra thì hệ số R-squared sẽ luôn tăng khi có nhiều biến hơn được thêm vào mô hình, ngay cả khi những biến đó không có liên quan hoặc liên quan yếu đến biến mục tiêu. Bạn đọc cần lưu ý vấn đề này khi lựa chọn mô hình. Hệ số R-squared mà chúng ta thảo luận ở đây chỉ là hệ số R-squared tính trên dữ liệu huấn luyện mô hình chứ không phải là trên dữ liệu kiểm tra mô hình. Theo kinh nghiệm thực tế thì khi thêm các biến như quảng cáo tờ rơi vào mô hình chỉ làm cho R-squared tăng thêm một chút là bằng chứng cho thấy biến Flyer nên bị loại khỏi mô hình. Ngược lại, khi chúng ta sử dụng mô hình chỉ chứa biến TV là biến giải thích có hệ số R-squared là 0.237. Việc thêm biến Social_Media vào mô hình sẽ dẫn đến sự cải thiện đáng kể về R-squared. Điều này ngụ ý rằng mô hình sử dụng hai biến chi phí quảng cáo qua truyền hình và qua mạng xã hội để dự đoán doanh thu bán hàng sẽ tốt hơn đáng kể với mô hình chỉ sử dụng quảng cáo trên truyền hình.RSE cũng có thể là một thước đo định lượng để đánh giá sự phù hợp của mô hình. Bạn đọc có thể quan sát RSE của các mô hình với tổ hợp các biến giải thích khác nhau trong bảng ??Trong các mô hình đơn biến, có thể thấy rằng mô hình sử dụng biến Flyer làm biến giải thích có RSE thậm chí còn nhỏ hơn với mô hình chỉ sử dụng biến TV. Khi sử dụng hai biến để giải thích biến doanh thu, bạn đọc có thể nhận thấy rằng mô hình sử dụng TV và Social_Media có RSE nhỏ hơn hẳn với các mô hình còn lại. Khi thêm biến Flyer vào mô hình đã bao gồm TV và Social_Media, RSE gần như không thay đổi. Không giống như hệ số R-squared luôn tăng khi thêm biến vào mô hình, các mô hình có nhiều biến hơn có thể có RSE cao hơn nếu mức giảm RSS nhỏ hơn với sự gia tăng số lượng biến.Sau khi chúng ta đã tìm ra mô hình phù hợp, có thể sử dụng các hệ số tuyến tính để đưa ra dự đoán cho biến mục tiêu \\(Y\\) trên giá trị các biến giải thích \\(X_1, X_2, \\cdots , X_p\\). Tuy nhiên, trước khi đưa ra dự đoán cho biến mục tiêu dựa trên mô hình tuyến tính, có những vấn đề mà bạn đọc cần lưu ý:Thứ nhất: kể cả khi mối quan hệ giữa biến mục tiêu với các biến giải thích là mối quan hệ tuyến tính, thì chúng ta cũng không biết được giá trị thực của các hệ số tuyến tính. Các hệ số \\(\\hat{\\beta}_0, \\hat{\\beta}_1, \\cdots, \\hat{\\beta}_p\\) chỉ là các ước lượng cho các hệ số tuyến tính thực \\(\\beta_0, \\beta_1, \\cdots, \\beta_p\\) dựa trên dữ liệu quan sát được. Sai số giữa ước lượng và các giá trị thực có thể giảm bớt được dựa trên độ lớn của dữ liệu và kỹ năng của người xây dựng mô hình.Thứ hai: đó là sai số về mặt mô hình, nghĩa là mối quan hệ giữa các biến giải thích và các biến mục tiêu không phải là mối liên hệ tuyến tính nhưng chúng ta sử dụng mô hình tuyến tính để đưa ra dự đoán. Sai số này có thể được giảm bớt tùy theo kỹ năng của người xây dựng mô hình, chẳng hạn như sử dụng các phép biến đổi dữ liệu, hoặc thay đổi kiểu mô hình. Các mô hình phi tuyến sẽ được trình bày trong các Chương tiếp theo của cuốn sách.Thứ ba: đó là ngay cả khi chúng ta biết được mối quan hệ thực giữa biến mục tiêu và các biến giải thích, vẫn có những sai số mà hoàn toàn không thể được giải thích dựa trên dữ liệu. Các sai số này là không thể giảm bớt được.Với các ước lượng cho hệ số tuyến tính và sai số của phần dư, chúng ta có thể xây dựng được khoảng tin cậy với mức xác suất \\(\\alpha\\) cho giá trị trung bình của biến mục tiêu\n\\[\\begin{align}\n\\left(\\hat{\\beta}_0 + \\hat{\\beta}_1 \\cdot x_1 + \\cdots + \\hat{\\beta}_p \\cdot x_p - z_{1 - \\alpha/2} \\cdot \\hat{\\sigma} ; \\hat{\\beta}_0 + \\hat{\\beta}_1 \\cdot x_1 + \\cdots + \\hat{\\beta}_p \\cdot x_p + z_{1 - \\alpha/2} \\cdot \\hat{\\sigma}\\right)\n\\tag{10.19}\n\\end{align}\\]Đối với dữ liệu về chi phí quảng cáo và doanh thu bán sản phẩm, giả sử mô hình được lựa chọn là mô hình với hai biến giải thích là TV và Social_Media. Với ngân sách cho quảng cáo trên trền hình là 150 triệu đồng và ngân sách cho quảng cáo trên mạng xã hội 30 triệu, chúng ta có khoảng tin cậy 95% cho doanh thu bán sản phẩm là \\(\\left(8.64 , 9.38 \\right)\\) tỷ đồng. Khoảng tin cậy này được tính bởi các tham số được ước lượng từ dữ liệu quan sát như sau:\n\\[\\begin{align}\n& 8.64 = 4.06 + 0.019 \\times 150 + 0.07 \\times 30 - 1.96 \\times 0.189 \\\\\n& 9.38 = 4.06 + 0.019 \\times 150 + 0.07 \\times 30 + 1.96 \\times 0.189\n\\end{align}\\]","code":"\n# Hệ số R-squared trong mô hình hồi quy 3 biến\nsummary(lm(Sales ~ TV + Social_Media + Flyer,data = Advertising))$r.squared\n\n# Hệ số R-squared trong mô hình hồi quy 2 biến: TV, Social_Media\nsummary(lm(Sales ~ TV + Social_Media,data = Advertising))$r.squared"},{"path":"mô-hình-hồi-quy-tuyến-tính.html","id":"những-cân-nhắc-khi-xây-dựng-mô-hình-hồi-quy-tuyến-tính","chapter":"Chương 10 Mô hình hồi quy tuyến tính","heading":"10.2.3 Những cân nhắc khi xây dựng mô hình hồi quy tuyến tính","text":"Trong phần này chúng ta sẽ thảo luận thêm về các vấn đề thường gặp phải khi xây dựng mô hình tuyến tính trên dữ liệu thực tế bao gồm có vấn đề biến giải thích có kiểu định tính và vấn đề về tồn tại mối liên hệ phi tuyến tính giữa biến mục tiêu và biến giải thích","code":""},{"path":"mô-hình-hồi-quy-tuyến-tính.html","id":"biến-giải-thích-là-biến-định-tính","chapter":"Chương 10 Mô hình hồi quy tuyến tính","heading":"10.2.3.1 Biến giải thích là biến định tính","text":"Khi ước lượng hệ số tuyến tính bằng phương pháp bình phương nhỏ nhất, chúng ta luôn giả định rằng tất cả các biến trong mô hình đều là biến định lượng. Trong thực tế chúng ta lại rất hay thường gặp các biến giải thích là kiểu biến định tính. Ví dụ: dữ liệu có tên exposure.csv là dữ liệu về yêu cầu bồi thường viện phí và tổng số tiền bồi thường của khác hàng tham gia bảo hiểm sức khỏe tự nguyện tại một công ty bảo hiểm. Chúng tôi để mô hình ở dạng đơn giản nhất khi chỉ có hai biến phụ thuộc là giới tính, biến Gender, và độ tuổi, biến Age, của người được thanh toán bảo hiểm sức khỏe. Biến mục tiêu trong mô hình sẽ là số tiền bồi thường trung bình của những khách hàng, biến Ave_Claim, được tính bằng tổng số tiền của tất cả các lần bồi thường chia cho tổng số lần khách hàng gửi yêu cầu.Giả sử chúng ta muốn xây dựng mô hình tuyến tính mà biến mục tiêu là số tiền bồi thường trung bình của khách hàng và các biến giải thích là độ tuổi và giới tính của khách hàng. Biến Age có thể coi là biến định lượng và có thể nhận giá trị là mọi số tự nhiên từ 18 tuổi đến 65 tuổi. Biến Gender không thể được sử dụng như một biến định lượng bởi vì chúng ta không thể đưa ra các sánh định lượng giữa hai giá trị mà biến giới tính nhận.\nHình 10.6: Hình bên trái: Mối liên hệ giữa số tiền bồi thường trung bình với độ tuổi của người được bảo hiểm; Hình bên phải: Mối liên hệ giữa số tiền bồi thường trung bình với giới tính của người được bảo hiểm\nHình 10.6 mô tả mối liên hệ giữa số tiền yêu cầu bồi thường trung bình với độ tuổi và giới tính của người được bảo hiểm. Bạn đọc có thể thấy rằng có mối liên hệ giữa các biến giải thích đến các biến mục tiêu, số tiền yêu cầu bồi thường trung bình có xu hướng tăng khi tuổi của người được bảo hiểm tăng, và số tiền yêu cầu bồi thường trung bình của nam giới cao hơn với nữ giới. Như vậy tuổi và giới tính có nhiều khả năng là các biến có liên hệ đến biến mục tiêu. Ước lượng hệ số tuyến tính của biến Age có thể được thực hiện giống như các biến định lượng thông thường. Để sử dụng biến giới tính như một biến giải thích, chúng ta tạo một biến mới có dạng như sau\n\\[\\begin{align}\n\\text{Gender}_i = \\begin{cases}\n1 \\text{ nếu giới tính là nữ} \\\\\n0 \\text{ nếu giới tính là nam}\n\\end{cases}\n\\tag{10.20}\n\\end{align}\\]Mô hình tuyến tính với biến mục tiêu là số tiền yêu cầu bồi thường trung bình, biến \\(Y_i\\), và hai biến giải thích là độ tuổi, biến Age\\(_i\\), và giới tính, biến Gender\\(_i\\), được viết như sau\n\\[\\begin{align}\nY_i = & \\beta_0 + \\beta_1 \\cdot Gender_i + \\beta_2 \\cdot Age_i + \\epsilon_i \\\\\n= &\n\\begin{cases}\n(\\beta_0 + \\beta_1) + \\beta_2 \\cdot Age_i + \\epsilon_i  \\ \\ \\text{ nếu giới tính là nữ} \\\\\n\\beta_0 + \\beta_2 \\cdot Age_i + \\epsilon_i \\ \\ \\text{ nếu giới tính là nam}\n\\end{cases}\n\\tag{10.21}\n\\end{align}\\]Hệ số \\(\\beta_0\\) trong phương trình (10.19) là hệ số chặn trong mô hình hồi quy tuyến tính đơn mà số tiền yêu cầu bồi thường trung bình phụ thuộc vào độ tuổi nếu người được bảo hiểm là nam giới, trong khi \\((\\beta_0 + \\beta_1)\\) là hệ số chặn trong mô hình hồi quy tuyến tính đơn mà số tiền yêu cầu bồi thường trung bình phụ thuộc vào độ tuổi nếu người được bảo hiểm là nữ giới. Để ước lượng mô hình hồi quy tuyến tính với biến định tính Gender bằng hàm lm(), bạn đọc hãy đảm bảo biến định tính có kiểu factor trước khi đưa vào trong hàm ước lượng.Hệ số ước lượng của mô hình tuyến tính với biến phụ thuộc định tính được trình bày trong bảng ??. Bạn đọc thấy rằng các hệ số ước lượng được đều có ý nghĩa thống kê vì giá trị p-value đều rất nhỏ. Hệ số tuyến tính của biến độ tuổi bằng 1.95 cho thấy rằng nếu tuổi của người được bảo hiểm tăng thêm 1 tuổi, thì số tiền bồi thường trung bình sẽ tăng khoảng 1.95 triệu đồng. Hệ số tuyến tính của biến Gender-Nữ là số âm cho biết cùng một độ tuổi, trung bình mỗi lần yêu cầu bồi thường nữ giới sẽ có số tiền yêu cầu ít hơn nữ giới khoảng 14.32 triệu đồng. Việc lựa chọn mã hóa giới tính trong phương trình (10.20) là hoàn toàn tự và không ảnh hưởng đến kết quả của mô hình hồi quy. Nếu bạn đọc sử dụng cách mã hóa nữ giới là 0 và nam giới là 1, kết quả thu được sẽ có hệ số của biến Gender-Nam là số dương, có giá trị bằng với giá trị tuyệt đối của hệ số của biến Gender-Nữ trong bảng ??.Bạn đọc có thể đặt ra câu hỏi về việc biến định tính nhận nhiều hơn hai giá trị. Cách ước lượng của mô hình tuyến tính là hoàn toàn tương tự như trường hợp hai biến. Giả sử mô hình hồi quy tuyến tính có biến \\(Y\\) là biến mục tiêu và hai biến giải thích: \\(X_1\\) là biến định lượng và \\(X_2\\) là biến định tính. \\(X_2\\) có thể nhận \\(J\\) giá trị khác nhau lần lượt là \\(1, 2, \\cdots, J\\). Khi đó, ước lượng mô hình tuyến tính có \\(J + 1\\) hệ số tuyến tính cần được ước lượng như sau\n\\[\\begin{align}\nY = \\begin{cases}\n\\beta_0 + \\beta_1 \\cdot X_1 + \\epsilon \\ \\ \\text{ nếu } X_2 = 1 \\\\\n(\\beta_0 + \\beta_2) + \\beta_1 \\cdot X_1 + \\epsilon \\ \\ \\text{ nếu } X_2 = 2 \\\\\n(\\beta_0 + \\beta_3) + \\beta_1 \\cdot X_1 + \\epsilon \\ \\ \\text{ nếu } X_2 = 3 \\\\\n\\cdots \\\\\n(\\beta_0 + \\beta_J) + \\beta_1 \\cdot X_1 + \\epsilon \\ \\ \\text{ nếu } X_2 = J\n\\end{cases}\n\\tag{10.22}\n\\end{align}\\]\nCó thể thấy rằng, nếu biến định tính nhận quá nhiều giá trị, số lượng tham số của mô hình tuyến tính tăng lên tương ứng. Khi mô hình sử dụng quá nhiều hệ số sẽ dễ dẫn đến hiện tượng overfitting. Giải pháp khi gặp biến định tính nhận nhiều giá trị là nhóm các giá trị có hệ số tuyến tính \\(\\beta\\) không khác nhau vào cùng một nhóm để giảm số lượng biến. Chúng ta sẽ thảo luận kỹ hơn về giải pháp này trong phần thực hành trên mô hình tuyến tính.","code":"\n# Load dữ liệu exposure\ndat<-read.csv(\"../KHDL_KTKD Final/Dataset/exposure.csv\")\n\n# Chỉ giữ lại những khác hàng có yêu cầu bồi thường\ndat<-filter(dat,Claim_Count>0)\ndat$Gender <- ifelse(dat$Gender == 0, \"Nam\", \"Nữ\")\n# Đổi dữ liệu cột Gender thành factor\ndat$Gender<-as.factor(dat$Gender)"},{"path":"mô-hình-hồi-quy-tuyến-tính.html","id":"basiclmconsideration2","chapter":"Chương 10 Mô hình hồi quy tuyến tính","heading":"10.2.3.2 Mối quan hệ phi tuyến giữa biến mục tiêu và biến giải thích","text":"Mối liên hệ giữa số tiền yêu cầu bồi thường trung bình và biến độ tuổi trong Hình 10.6 không phải là một mối liên hệ tuyến tính. Bạn đọc có thể thấy rằng khi độ tuổi tăng thì số tiền yêu cầu bồi thường tăng lên nhanh hơn, điều này giải thích tại sao đường thẳng mô tả mối liên hệ giữa hai biến có độ dốc tăng dần khi độ tuổi tăng. Điều này gợi ý cho người xây dựng mô hình rằng mối liên hệ giữa số tiền bồi thường trung bình và độ tuổi là mối liên hệ phi tuyến hơn là tuyến tính. Đa số các phương pháp xây dựng mô hình hiện đại đều được xây dựng để mô tả mối quan hệ phi tuyến giữa biến giải thích và biến mục tiêu. Trong khuôn khổ mô hình hồi quy tuyến tính, chúng tôi giới thiệu một phương pháp tiếp cận đơn giản nhất, đó là hồi quy theo đa thức. Mối liên hệ giữa biến mục tiêu và biến giải thích trong Hình 10.6 có dạng parabol, đó chúng ta có thể thêm vào mô hình biến giải thích là bình phương của độ tuổi với hi vọng là sẽ có một mô hình giải thích tốt hơn biến mục tiêu. Mô hình có dạng như sau\n\\[\\begin{align}\nY = \\begin{cases}\n(\\beta_0 + \\beta_1) + \\beta_2 \\cdot \\text{Age} + \\beta_3 \\cdot \\text{Age}^2 + \\epsilon  \\ \\ \\text{ nếu giới tính là nam} \\\\\n\\beta_0 + \\beta_2 \\cdot \\text{Age}_i + \\beta_3 \\cdot \\text{Age}^2 + \\epsilon \\ \\ \\text{ nếu giới tính là nữ}\n\\end{cases}\n\\tag{10.23}\n\\end{align}\\]Kết quả ước lượng trong Bảng ?? cho thấy tất cả các hệ số tuyến tính đều có ý nghĩa, điều này cho thấy mối liên hệ giữa số tiền bồi thường trung bình và độ tuổi là mối liên hệ phi tuyến hơn là tuyến tính. Mô hình có biến giải thích độ tuổi bình phương có hệ số R-squared lớn hơn nhiều với mô hình không có biến độ tuổi bình phương, điều này cho thấy mô hình có biến độ tuổi bình phương phù hợp hơn để giải thích biến mục tiêu. Tuy nhiên, bạn đọc cũng có thể nhận thấy rằng, mô hình đã trở nên khó giải thích hơn một chút. Chúng ta không thể đưa ra đánh giá ngay lập tức cho biến mục tiêu khi tuổi của người yêu cầu bồi thường tăng 1 hay giảm 1 tuổi. Đó là sự đánh đổi giữa khả năng giải thích và khả năng dự đoán mà bạn đọc sẽ thường xuyên gặp phải khi xây dựng mô hình trên dữ liệu. Chúng ta sẽ thảo luận về các kỹ thuật mô tả mối liên hệ phi tuyến giữa biến mục tiêu và biến giải thích trong Chương mô hình cộng tính tổng quát.","code":""},{"path":"mô-hình-hồi-quy-tuyến-tính.html","id":"sự-kết-hợp-giữa-các-biến-giải-thích-không-chỉ-là-cộng-tính","chapter":"Chương 10 Mô hình hồi quy tuyến tính","heading":"10.2.3.3 Sự kết hợp giữa các biến giải thích không chỉ là cộng tính","text":"Trong nhiều trường hợp, biến mục tiêu không chỉ phụ thuộc vào từng biến giải thích một cách riêng lẻ, mà còn phụ thuộc vào sự tương tác giữa các biến giải thích. Một ví dụ điển hình cho trường hợp này là khi sử dụng mô hình tuyến tính để mô tả mối liên hệ giữa số lượng thành phẩm của một nhà máy sản xuất, biến products, với số lượng công nhân, biến workers, và số lượng máy chế tạo, biến machines. Nhìn chung khi tăng số lượng công nhân hoặc tăng số lượng máy thì số lượng thành phẩm sẽ tăng lên. Tuy nhiên mô hình hồi quy tuyến tính chỉ bao gồm hai biến giải thích workers và machines sẽ không mô tả được thực tế là khi tăng số lượng công nhân lên quá nhiều sẽ dẫn đến việc công nhân không có máy để sản xuất nên số lượng thành phẩm cũng sẽ không tăng theo tương ứng. Chính vì thế, để mô tả được thực tế đó, mô hình tuyến tính cần có biến giải thích mô tả sự tương tác giữa workers và machines:\n\\[\\begin{align}\n\\text{products} = \\beta_0 + \\beta_1 \\cdot \\text{workers}  + \\beta_2 \\cdot \\text{machines} + \\beta_3 \\cdot \\text{workers} \\times \\text{machines} + \\epsilon\n\\tag{10.24}\n\\end{align}\\]\nhoặc chúng ta cũng có thể viết mô hình (10.24) dưới dạng mô hình tuyến tính mà hệ số tuyến tính của biến machines phụ thuộc vào biến workers\n\\[\\begin{align}\n\\text{products} = & \\beta_0 + \\beta_1 \\cdot \\text{workers}  + \\left(\\beta_2 + \\beta_3 \\cdot \\text{workers}\\right) \\cdot \\text{machines} + \\epsilon \\\\\n= & \\beta_0 + \\left(\\beta_1 + \\beta_3 \\cdot \\text{machines} \\right)  \\cdot \\text{workers}  + \\beta_2 \\cdot \\text{machines} + \\epsilon\n\\tag{10.25}\n\\end{align}\\]Có thể giải thích mô hình (10.25) rằng mỗi khi tăng thêm 1 máy sản xuất, số lượng thành phẩm sẽ tăng lên tương ứng là bằng \\((\\beta_2 + \\beta_3 \\cdot \\text{workers})\\), hoặc tăng thêm 1 công nhân, số lượng thành phẩm sẽ tăng lên là \\((\\beta_1 + \\beta_3 \\cdot \\text{machines})\\). Hay nói một cách khác, số lượng thành phẩm tăng khi số lượng máy móc tăng nhưng tốc độ tăng còn phụ thuộc vào số lượng công nhân hiện tại; hoặc số lượng thành phẩm tăng khi tăng số lượng công nhân nhưng tốc độ tăng còn phụ thuôc vào số máy móc hiện có. Mô hình (10.25) sẽ phù hợp hơn mô hình tuyến tính chỉ bao gồm hai biến giải thích workers và machines khi giải thích biến mục tiêu products.","code":""},{"path":"mô-hình-hồi-quy-tuyến-tính.html","id":"những-khiếm-khuyết-của-mô-hình-hồi-quy-tuyến-tính","chapter":"Chương 10 Mô hình hồi quy tuyến tính","heading":"10.2.4 Những khiếm khuyết của mô hình hồi quy tuyến tính","text":"Khi chúng ta ước lượng mô hình hồi quy tuyến tính cho dữ liệu cụ thể những vấn đề dưới đây có thể xảy ra làm cho kết quả ước lượng của mô hình trở nên kém hiệu quả:Tồn tại mối liên hệ phi tuyến giữa biến mục tiêu và biến giải thích;Tồn tại mối liên hệ phi tuyến giữa biến mục tiêu và biến giải thích;Các sai số \\(\\epsilon_i\\) có tương quan với nhau.Các sai số \\(\\epsilon_i\\) có tương quan với nhau.Phương sai của các \\(\\epsilon_i\\) không phải là hằng số.Phương sai của các \\(\\epsilon_i\\) không phải là hằng số.Trong dữ liệu có điểm ngoại lai.Trong dữ liệu có điểm ngoại lai.Các biến giải thích có tương quan cao với nhau, hay còn gọi là đa cộng tuyến.Các biến giải thích có tương quan cao với nhau, hay còn gọi là đa cộng tuyến.Trong thực tế, việc xác định và khắc phục những vấn đề này là những chủ đề khoa học được nghiên cứu xuyên suốt cho đến thời điểm hiện tại. Có nhiều cuốn sách có chủ đề tập trung vào mô hình hồi quy tuyến tính có thể giải quyết một hoặc một vài vấn đề được nêu ở trên. Vì mô hình hồi quy tuyến tính không phải là trọng tâm của cuốn sách này nên chúng tôi sẽ chỉ tóm tắt ngắn gọn về các vấn đề và một số hướng giải quyết ngắn gọn.","code":""},{"path":"mô-hình-hồi-quy-tuyến-tính.html","id":"dữ-liệu-quan-sát-được-có-dạng-phi-tuyến-tính","chapter":"Chương 10 Mô hình hồi quy tuyến tính","heading":"10.2.4.1 Dữ liệu quan sát được có dạng phi tuyến tính","text":"Như chúng tôi đã trình bày ở phần ??, khi tồn tại mối liên hệ phi tuyến giữa biến mục tiêu và biến giải thích, sử dụng mô hình tuyến tính thông thường sẽ không phù hợp và làm cho kết quả dự đoán không được chính xác. Mối liên hệ phi tuyến có thể được phát hiện khi vẽ đồ thị biến mục tiêu theo biến giải thích giống như Hình 10.6 hoặc chúng ta vẽ đồ thị phần dư của mô hình theo biến mục tiêu\nHình 10.7: Đồ thị mô tả phần dư của mô hình tuyến tính theo biến mục tiêu trên dữ liệu về số tiền yêu cầu bồi thường bảo hiểm y tế. Hình bên trái: Phần dư của mô hình tuyến tính thông thường theo biến mục tiêu. Hình bên phải: Phần dư của mô hình hồi quy đa thức bậc hai theo biến mục tiêu\nHình 10.7 mô tả mối liên hệ giữa phần dư của mô hình tuyến tính thông thường và mô hình hồi quy đa thức với biến mục tiêu là số tiền yêu cầu bồi thường trung bình. Đường mô tả mối liên hệ trong mô hình hồi quy đa thức gần với đường trung bình của phần dư hơn cho thấy mối liên hệ phi tuyến giữa biến mục tiêu và phần dư tuy đã giảm bớt với hồi quy tuyến tính thông thường nhưng vẫn còn tồn tại trong mô hình hồi quy đa thức. Như vậy, có thể thấy rằng thêm các biến giải thích là các hàm phi tuyến của các biến giải thích ban đầu vào trong mô hình hồi quy tuyến tính là một phương pháp để mô tả mối quan hệ phi tuyến trong dữ liệu.Các biến đổi phi tuyến thường được dùng có dạng hàm mũ, hàm \\(\\log\\) của biến giải thích. Nghĩa là từ biến giải thích \\(X\\) ban đầu, nếu đồ thị mô tả mối liên hệ giữa \\(X\\) và \\(Y\\) cho thấy có mối liên hệ phi tuyến, tùy theo hình dạng của đồ thị mà chúng ta có thể thêm vào mô hình các biến giải thích như \\(\\sqrt{X}\\), \\(X^2\\), \\(X^3\\), \\(\\log(X)\\), \\(\\cdots\\), để có được mô hình phù hợp hơn. Trong phần sau của cuốn sách chúng ta sẽ thảo luận về các kỹ thuật hiện đại hơn để mô tả tốt hơn mối liên hệ phi tuyến như vậy.","code":""},{"path":"mô-hình-hồi-quy-tuyến-tính.html","id":"tồn-tại-tương-quan-giữa-các-phần-dư","chapter":"Chương 10 Mô hình hồi quy tuyến tính","heading":"10.2.4.2 Tồn tại tương quan giữa các phần dư","text":"Một giả thiết quan trọng của mô hình hồi quy tuyến tính là các sai số \\(\\epsilon_1\\), \\(\\epsilon_2\\), \\(\\cdots\\), \\(\\epsilon_n\\) không tương quan với nhau. Điều này có nghĩa là kể cả khi đã biết \\(\\epsilon_i\\), chúng ta cũng không có thông tin gì về các \\(\\epsilon_j\\) khi \\(j \\neq \\). Giả thiết các sai số có phân phối chuẩn và không tương quan có ý nghĩa quan trọng trong xây dựng các khoảng tin cậy cho hệ số tuyến tính. Trên thực tế, nếu có mối tương quan giữa các sai số thì phương sai của các hệ số tuyến tính ước lượng bằng phương pháp bình phương nhỏ nhất sẽ nhỏ hơn nhiều cho với phương sai thực. Kết quả là khoảng tin cậy ước lượng được sẽ nhỏ hơn các khoảng tin cậy thực sự.Bạn đọc có thể hình dung việc phần dư có tương quan với nhau cũng giống như việc chúng ta trong quá trình thu thập dữ liệu có sai sót dẫn đến dữ liệu bị trùng lặp. Chẳng hạn như mỗi dòng dữ liệu bị lặp lại một lần, nghĩa là dữ liệu đúng để ước lượng mô hình chỉ có \\(n\\) dòng nhưng chúng ta đã nhân đôi dữ liệu lên trước khi thực hiện ước lượng. Khi tính toán độ lệch chuẩn của sai số, chúng ta sử dụng \\(2 n\\) quan sát để tính toán thay vì \\(n\\) làm cho độ lệch chuẩn bị giảm xuống một tỷ lệ là \\(\\sqrt{2}\\). Các khoảng tin cậy khi tính toán với dữ liệu bị trùng lặp sẽ bị thu hẹp lại với khoảng tin cậy được tính toán với dữ liệu chính xác.Khi nào thì chúng ta sẽ gặp phải hiện tượng phần dư có tương quan với nhau? Ngoài việc sai sót trong quá trình thu thập dữ liệu làm cho dữ liệu vị trùng lặp, chúng ta cũng thường gặp phải hiện tượng phần dư có tương quan khi sử dụng mô hình hồi quy tuyến tính trong dữ liệu dạng chuỗi thời gian. Trong trường hợp mà mỗi dòng dữ liệu là một quan sát thu được tại các thời điểm liền kề nhau thì rất có nhiều khả năng các biến mục tiêu sẽ có tương quan với nhau, dẫn đến tương quan giữa các phần dư trong mô hình tuyến tính.Để xác định xem phần dư từ một mô hình hồi quy tuyến tính có tương quan hay không, chúng ta có thể quan sát đồ thị phần dư. Nếu phần dư không có tương quan thì sẽ không có mối liên hệ rõ ràng nào. Mặt khác, nếu có tồn tại tương quan dương thì chúng ta có thể thấy có sự liên kết giữa các giá trị phần dư.\nHình 10.8: Đồ thị mô tả phần dư có tương quan và không có tương quan. Hình trên: các phần dư có tương quan bằng 0. Hình ở giữa: hai giá trị phần dư cạnh nhau có tương quan 0.5. Hình dưới: hai giá trị phần dư cạnh nhau có tương quan 0.9\nHình ?? minh họa đồ thị phần dư của ba mô hình khác nhau. Trong hình trên cùng, chúng ta thấy phần dư từ mô hình hồi quy tuyến tính tương ứng với dữ liệu mà biến mục tiêu không có tương quan với nhau. Bạn đọc có thể thấy rằng không có mối liên hệ nào rõ ràng về xu hướng của các giá trị phần dư. Ngược lại, phần dư ở hình dưới cùng là từ tập dữ liệu trong đó các sai số liền kề có hệ số tương quan là 0.9. Bạn đọc có thể nhận thấy có một sự liên kết rõ ràng trong phần dư mà trong đó các giá trị liền kề nhau có xu hướng nhận các giá trị tương tự hay cùng dấu. Cuối cùng, hình ở giữa minh họa một trường hợp ít rõ ràng hơn mà trong đó phần dư có hệ số tương quan là 0,5. Vẫn có bằng chứng về sự liên hệ nhưng không rõ ràng như trường hợp có hệ số tương quan 0.9.Nhìn chung giả định về phần dư không tương quan là vô cùng quan trọng đối với mô hình hồi quy tuyến tính nói riêng cũng như đối với các mô hình học máy hiện đại. Ngoài nguyên nhân từ xây dựng mô hình hay cách lựa chọn biến, sự tương quan giữa các phần dư cũng có thể tồn tại ngay trong chính cách dữ liệu được thu thập, đặc biệt là những dữ liệu mà biến mục tiêu và biến giải thích cùng chịu sự tác động từ các yếu tố bên ngoài. Có nhiều phương pháp đã được phát triển để xác định các mối tương quan của phần dư trong mô hình hồi quy tuyến tính, đặc biệt là đối với mô hình tuyến tính có các biến mục tiêu và biến giải thích có dạng dữ liệu chuỗi thời gian. Nội dung của các phương pháp này bạn đọc có thể tham khảo trong các sách tham khảo dành riêng cho mô hình hồi quy tuyến tính.","code":""},{"path":"mô-hình-hồi-quy-tuyến-tính.html","id":"phương-sai-của-phần-dư-thay-đổi","chapter":"Chương 10 Mô hình hồi quy tuyến tính","heading":"10.2.4.3 Phương sai của phần dư thay đổi","text":"Một giả thiết quan trọng khác của mô hình hồi quy tuyến tính là các phần dư có phương sai không thay đổi: \\(\\mathbb{V}(\\epsilon_i) = \\sigma^2\\) \\(\\forall = 1, 2, \\cdots, n\\). Ước lượng các hệ số tuyến tính, xây dựng các khoảng tin cậy cho hệ số, hay kiểm định các giả thuyết của mô hình tuyến tính đều dựa trên giả định này. Thực tế là chúng ta rất hay gặp phải trường hợp phương sai của phần dư là không cố định. Một nguyên nhân hiện tượng phần dư có phương sai thay đổi đến từ việc mối liên hệ giữa biến mục tiêu và biến giải thích là phi tuyến. Nếu xuất phát từ nguyên nhân này, chúng ta có thể biến đổi biến mục tiêu trước khi thực hiện ước lượng.Một ví dụ điển hình thường gặp phải là khi phương sai của phần dư tăng theo giá trị của biến mục tiêu. Chúng ta có thể xác định được hiện tượng phương sai của phần dư thay đổi bằng cách sử dụng đồ thị của phần dư theo giá trị ước lượng được của biến mục tiêu.\nHình 10.9: Đồ thị mô tả phần dư có phương sai thay đổi. Hình ở trên: cho thấy phần dư có phương sai thay đổi. Hình ở dưới: Phần dư có phương sai ổn định\nMột ví dụ cho phần dư có phương sai thay đổi được thể hiện trong hình 10.9. Khi chúng ta sử dụng biến mục tiêu \\(Y\\), độ lớn của phần dư có xu hướng tăng theo các giá trị của biến mục tiêu. Khi gặp vấn đề này, một giải pháp đơn giản là biến đổi biến mục tiêu \\(Y\\) bằng cách sử dụng \\(\\log(Y)\\) hoặc \\(\\sqrt{Y}\\) làm biến mục tiêu. Các phép biến đổi này có thể làm giảm hiện tượng phương sai thay đổi. Hình phía dưới của Hình 10.9 mô tả phần dư theo giá trị của biến mục tiêu sau khi sử dụng phép biến đổi \\(\\log\\). Phần dư đã trở nên ổn định hơn mặc dù có một số dấu hiệu về mối quan hệ phi tuyến trong giữa biến mục tiêu và biến giải thích.Hiện tượng phương sai của sai số thay đổi có thể là kết quả của quá trình dữ liệu được thu thập, khi mà biến mục tiêu thứ \\(\\) là giá trị trung bình của \\(n_i\\) quan sát độc lập. Ví dụ, dữ liệu về yêu cầu bồi thường của các khách hàng của một công ty bảo hiểm, mỗi khách hàng có thể yêu cầu bồi thường nhiều lần trong khoảng thời gian một năm nhưng dữ liệu chỉ được lưu trữ dưới dạng tổng số tiền khách hàng yêu cầu bồi thường và tổng số lần khách hàng yêu cầu bồi thường. Khi xây dựng mô hình với biến mục tiêu là số tiền yêu cầu bồi thường trung bình thì độ lệch chuẩn của biến mục tiêu sẽ tỷ lệ nghịch với số lần khách hàng yêu cầu bồi thường. Trong trường hợp như vậy, một phương pháp khắc phục đơn giản là ước lượng mô hình sử dụng phương pháp bình phương nhỏ nhất có trọng số. Trọng số được sử dụng tỷ lệ nghịch với phương sai của biến mục tiêu. Chẳng hạn như trong ví dụ về yêu cầu bồi thường, trọng số được sử dụng đối với quan sát thứ \\(\\) chính là số lần khách hàng yêu cầu bồi thường.","code":""},{"path":"mô-hình-hồi-quy-tuyến-tính.html","id":"dữ-liệu-có-giá-trị-ngoại-lai","chapter":"Chương 10 Mô hình hồi quy tuyến tính","heading":"10.2.4.4 Dữ liệu có giá trị ngoại lai","text":"Điểm ngoại lai là điểm dữ liệu mà giá trị biến mục tiêu \\(y_i\\) khác xa giá trị được dự đoán bởi mô hình \\(\\hat{y}_i\\). Các ngoại lai có thể phát sinh vì nhiều lý , chẳng hạn như sự không chính xác trong quá trình thu thập dữ liệu. Bạn đọc có thể tham khảo thêm về giá trị ngoại lai trong phần @ref(#ourlier)\nHình 10.10: Dữ liệu chứa giá trị ngoại lai là điểm màu đỏ. Hình bên trái: đồ thị mô tả biến mục tiêu theo biến giải thích và các đường hồi quy tuyến tính được xây dụng cho hai trường hợp là có chứa điểm ngoại lai (nét liền màu xanh) và không chứa điểm ngoại lai (nét đứt màu đen). Hình bên phải: Đồ thị phần dư được điều chỉnh theo biến giải thích, điểm ngoại lai có phần dư có giá trị tuyệt đối lớn hơn hẳn các phần dư khác\nĐiểm màu đỏ ở hình bên trái của Hình 10.10 minh họa một ngoại giá trị ngoại lai điển hình. Đường liền màu xanh dương là đường hồi quy tuyến tính sử dụng đầy đủ dữ liệu, trong khi đường nét đứt màu đen là đường hồi quy tuyến tính sau khi loại bỏ đi điểm ngoại lai. Trong trường hợp này, việc loại bỏ giá trị ngoại lai ít ảnh hưởng đến đường hồi quy tuyến tính vì bạn đọc có thể thấy hai đường hồi quy khá gần nhau. Thông thường, một giá trị ngoại lai duy nhất sẽ ít ảnh hưởng đến sự hình dạng của đường hồi quy tuyến tính, tuy nhiên, điểm ngoại lai này lại có thể gây ra các vấn đề khác. Trong ví dụ ở trên, RSE là 1.8 khi giá trị ngoại lai được đưa vào hồi quy, và RSE chỉ bằng 0.9 khi giá trị ngoại lai bị loại bỏ. Vì chúng ta sẽ sử dụng RSE để tính toán các khoảng tin cậy và các p-value, nên sự thay đổi đáng kể của RSE như vậy sẽ có tác động đến việc giải thích sự phù hợp của mô hình. Tương tự, việc đưa giá trị ngoại lai vào làm cho R-squared giảm từ 0.973 xuống 0.89.Chúng ta có thể xác định một quan sát là ngoại lai hay không bằng cách vẽ đồ thị phần dư. Trong ví dụ ở trên, giá trị ngoại lai có thể được xác định rõ ràng trong Hình 10.10. Nhưng trong thực tế, có thể khó đưa ra được quyết định là phần dư cần phải lớn đến mức nào để chúng ta coi điểm đó là điểm bất thường. Để giải quyết vấn đề này, thay vì vẽ đồ thị phần dư, chúng ta có thể vẽ đồ thị phần dư sau khi chia phần dư cho RSE. Nếu các giả thiết của mô hình hồi quy tuyến tính là đúng, phần dư được điều chỉnh (sau khi chia cho RSE) sẽ có phân phối Student. Nếu một giá trị quan sát của phần dư vượt quá các ngưỡng xác suất của phân phối Student, nhiều khả năng đó là giá trị ngoại lai. Trong Hình 10.10, tất cả các quan sát có phần dư nằm trong khoảng -2 đến 2 trong khi giá trị ngoại lai có giá trị là gần 6. Nói một cách khác khả năng điểm có phần dư được điểu chỉnh gần bằng 6 có khả năng rất cao là giá trị ngoại lai.","code":""},{"path":"mô-hình-hồi-quy-tuyến-tính.html","id":"đa-cộng-tuyến","chapter":"Chương 10 Mô hình hồi quy tuyến tính","heading":"10.2.4.5 Đa cộng tuyến","text":"Đa cộng tuyến là sự kiện mà trong đó hai hoặc nhiều biến giải thích có tương quan cao với nhau với nhau. Để giải thích rõ ràng khái niệm này, chúng ta hãy lấy một ví dụ khi xây dựng mô hình hồi quy tuyến tính trên dữ liệu có tên là Credit_Card. Đây là dữ liệu về thông tin thẻ tín dụng của các khách hàng tại một ngân hàng với biến mục tiêu là số dư tài khoản, có tên là Balance, và 10 biến giải thích. Để giải thích về đa cộng tuyến, chúng tôi chỉ sử dụng ba biến giải thích là 1. Độ tuổi của chủ thẻ tín dụng, ký hiệu Age, 2. Hạn mức thẻ tín dụng, ký hiệu Limit, và 3. Điểm tín dụng của khách hàng, ký hiệu Rating. Hiện tượng đa cộng tuyến là hiện tượng mà mô hình có các biến giải thích có mối liên hệ rất chặt chẽ với nhau như được minh họa trên Hình 10.11\nHình 10.11: Mối liên hệ giữa các biến giải thích trong dữ liệu Credit. Hình bên trái: Không cho thấy có mối tương quan giữa hạn mức tín dụng với tuổi của khách hàng. Hình ở giữa: Tương quan giữa hạn mức tín dụng và điểm tính dụng là rất cao. Hình bên phải: Không có tương quan giữa tuổi của khách hàng với xếp hạng tín dụng\nTrong hình bên trái của Hình 10.11, hai biến giải thích là hạn mức tín dụng và độ tuổi không có mối tương quan rõ ràng. Tương tự, trong hình bên phải, cũng không có mối tương quan rõ ràng giữa độ tuổi với xếp hạng tín dụng. Ngược lại, trong hình ở giữa của Hình 10.11, hạn mức tín dụng và điểm tín dụng có mối tương quan rất cao với nhau bởi các điểm gần như nằm trên một đường thẳng.Hiện tượng đa cộng tuyến gây ra các vấn đề khi ước lượng và giải thích mô hình hồi quy bởi khó có thể tách biệt các tác động riêng lẻ của các biến có tương quan cao lên biến mục tiêu. Trong ví dụ ở trên. hạn mức tín dụng và điểm tín dụng có xu hướng tăng hoặc giảm cùng nhau nên khó có thể xác định xem từng biến riêng biệt có liên quan như thế nào đến biến mục tiêu là số dư tài khoản. Một vấn đề đáng kể khác khi gặp hiện tượng đa cộng tuyến đó là phương sai của các hệ số ước lượng sẽ rất lớn dẫn đến các ước lượng trở nên ít tin cậy hơn và chúng ta sẽ rất khó bác bỏ được giả thuyết hệ số tuyến tính bằng 0.Bảng ?? và ?? sánh các hệ số tuyến tính ước lượng được được từ hai mô hình hồi quy riêng biệt. Trước tiên là hồi quy số dư tài khoản thẻ tín dụng theo độ tuổi và hạn mức tín dụng, sau đó là hồi quy số dư tài khoản theo hạn mức tín dụng và điểm tín dụng. Trong mô hình hồi quy đầu tiên, độ tuổi và hạn mức tín dụng đều có ý nghĩa giá trị p-value rất nhỏ. Trong mô hình thứ hai, hiện tượng đa cộng tuyến giữa hạn mức tín dụng và điểm tín dụng đã khiến độ lệch chuẩn của hệ số ước lượng của biến hạn mức tín dụng tăng lên gấp 13 lần và p-value tăng lên thành 0.701. Nói cách khác, sự quan trọng của biến hạn mức tín dụng đã bị che khuất hiện tượng đa cộng tuyến. Để tránh rơi vào tình trạng như vậy, cần xác định và giải quyết vấn đề đa cộng tuyến trước khi ước lượng mô hình.Một cách đơn giản để phát hiện hiện tượng đa cộng tuyến là xem xét ma trận tương quan của các biến giải thích. Một phần tử của ma trận này có giá trị tuyệt đối lớn là dấu hiệu cho thấy một cặp biến có tương quan cao và đó có hiện tượng đa cộng tuyến trong dữ liệu. Tuy nhiên, bạn đọc cần lưu ý là vấn đề về đa cộng tuyến không phải lúc nào cũng có thể được phát hiện bằng cách kiểm tra ma trận tương quan bởi vì có thể tồn tại sự đa cộng tuyến giữa ba hoặc nhiều biến ngay cả khi không có cặp biến nào có tương quan cao. Thay vì kiểm tra ma trận tương quan, cách tốt hơn để đánh giá hiện tượng đa cộng tuyến là tính hệ số lạm phát phương sai, Variance Inflation Factor hay viết tắt là VIF. Hệ số này là tỷ lệ giữa phương sai của hệ số \\(\\hat{\\beta}_j\\) khi ước lượng mô hình đầy đủ biến và hệ số \\(\\hat{\\beta}_j\\) khi ước lượng mô hình với riêng biến đó. Một cách đơn giản hơn để tính VIF là sử dụng hệ số R-squared trong mô hình hồi quy tuyến tính biến \\(X_j\\) theo các biến giải thích còn lại:\n\\[\\begin{align}\nVIF_j = \\cfrac{1}{1 - R^2_{X_j|X_{-j}}}\n\\tag{10.24}\n\\end{align}\\]\ntrong đó \\(R^2_{X_j|X_{-j}}\\) là hệ số R-squared trong mô hình hồi quy biến \\(X_j\\) theo các biến giải thích còn lại. Nếu không tồn tại đa cộng tuyến, hệ số \\(R^2_{X_j|X_{-j}}\\) sẽ gần bằng 0 và \\(VIF_j\\) sẽ lớn hơn 1 một chút. Ngược lại, nếu biến \\(X_j\\) có thể được xấp xỉ bằng tổ hợp tuyến tính của các biến giải thích còn lại, hệ số \\(R^2_{X_j|X_{-j}}\\) sẽ xấp xỉ 1 và dẫn đến \\(VIF_j\\) có giá trị rất lớn.Từ Bảng ?? được tính toán từ dữ liệu về thẻ tín dụng, có thể thấy rằng các biến giải thích độ tuổi, hạn mức tín dụng, và điểm tín dụng có giá trị VIF lần lượt là \\(1.01\\), \\(160.67\\) và \\(160.59\\). Có thể kết luận là có hiện tượng đa cộng tuyến trong dữ liệu thẻ tín dụng! Khi gặp hiện tượng đa cộng tuyến như vậy, có hai giải pháp đơn giản thường được sử dụng:Giải pháp trước tiên là loại bỏ một trong các biến có hệ số VIF cao ra khỏi mô hình hồi quy. Giải pháp này thường được thực hiện mà không ảnh hưởng nhiều đến sự phù hợp của mô hình hồi quy. Trong ví dụ về dữ liệu thẻ tín dụng, chúng ta có thể hồi quy số dư tài khoản theo độ tuổi và hạn mức tín dụng và bỏ qua biến điểm tín dụng mà không làm cho hệ số R-squared giảm một cách đáng kể.Giải pháp thứ hai là kết hợp các biến có đa cộng tuyến lại với nhau thành một biến giải thích duy nhất. Chẳng hạn như chúng ta có thể lấy giá trị trung bình biến hạn mức tín dụng và biến điểm tín dụng để tạo ra một biến giải thích mới trong mô hình hồi quy tuyến tính.Trong phần tiếp theo, chúng tôi sẽ đi sâu vào giải thích phương pháp bình phương nhỏ nhất được sử dụng để ước lượng mô hình hồi quy tuyến tính và tính chất của các ước lượng. Mục tiêu là để bạn đọc hiểu rõ hơn những kết quả đã được sử dụng hoặc công nhận ở phần trên. Những bạn đọc cảm thấy không cần thiết có thể bỏ qua và chuyển sang các phần tiếp theo mà không gặp bất kỳ khó khăn nào khi sử dụng các kết quả của mô hình hồi quy tuyến tính.","code":""},{"path":"mô-hình-hồi-quy-tuyến-tính.html","id":"leastsquared","chapter":"Chương 10 Mô hình hồi quy tuyến tính","heading":"10.3 Mô hình hồi quy tuyến tính và phương pháp bình phương nhỏ nhất","text":"Mô hình hồi quy tuyến tính, đúng như tên gọi của nó, cho rằng hàm \\(f\\) được sử dụng để mô tả tác động của các biến giải thích \\(X_1, X_2, \\cdot ,X_p\\) lên biến mục tiêu \\(Y\\) là có dạng hàm tuyến tính\n\\[\\begin{align}\nf(\\textbf{X}) = \\beta_0 + \\beta_1 \\cdot X_1 + \\beta_2 \\cdot X_2 + \\cdots + \\beta_p \\cdot X_p\n\\tag{10.26}\n\\end{align}\\]\nCác hệ số \\(\\beta_i\\) trong phương trình (10.26) được gọi là các tham số của mô hình hồi quy tuyến tính hoặc còn được gọi là các hệ số hồi quy. Các biến \\(X_i\\) có thể được đưa vào mô hình từ những cách như sau:Biến \\(X_i\\) là một biến kiểu số.Biến \\(X_i\\) là một biến kiểu số.Biến \\(X_i\\) là một biến đổi của một biến kiểu số để mối liên hệ giữa \\(Y\\) và \\(X_i\\) trở nên tuyến tính. Các phép biến đổi thường gặp có thể là phép biến đổi \\(\\log\\), phép lấy căn, lấy bình phương, …Biến \\(X_i\\) là một biến đổi của một biến kiểu số để mối liên hệ giữa \\(Y\\) và \\(X_i\\) trở nên tuyến tính. Các phép biến đổi thường gặp có thể là phép biến đổi \\(\\log\\), phép lấy căn, lấy bình phương, …Biến \\(X_i\\) có thể là một đa thức của một biến trong dữ liệu ban đầu. Trong trường hợp này chúng ta thường gọi là mô hình hồi quy đa thức.Biến \\(X_i\\) có thể là một đa thức của một biến trong dữ liệu ban đầu. Trong trường hợp này chúng ta thường gọi là mô hình hồi quy đa thức.Trong trường hợp \\(X_i\\) là một biến kiểu factor và nhận \\(J\\) giá trị khác nhau. Giả sử \\(J\\) giá trị được mã hóa thành \\(1, 2, \\cdots, J\\) thì để mô tả tác động của biến \\(X_i\\) lên biến mục tiêu trong mô hình tuyến tính, chúng ta cần \\(J\\) giá trị hệ số tuyến tính: \\(\\beta_{,1}\\), \\(\\beta_{,2}\\), \\(\\cdots\\), \\(\\beta_{,J}\\); trong đó hệ số tuyến tính \\(\\beta_{,j}\\) mô tả tác động của giá trị \\(X_i = j\\) lên biến mục tiêu.Trong trường hợp \\(X_i\\) là một biến kiểu factor và nhận \\(J\\) giá trị khác nhau. Giả sử \\(J\\) giá trị được mã hóa thành \\(1, 2, \\cdots, J\\) thì để mô tả tác động của biến \\(X_i\\) lên biến mục tiêu trong mô hình tuyến tính, chúng ta cần \\(J\\) giá trị hệ số tuyến tính: \\(\\beta_{,1}\\), \\(\\beta_{,2}\\), \\(\\cdots\\), \\(\\beta_{,J}\\); trong đó hệ số tuyến tính \\(\\beta_{,j}\\) mô tả tác động của giá trị \\(X_i = j\\) lên biến mục tiêu.Biến giải thích được tính toán từ tác động qua lại giữa các biến giải thích khác, chẳng hạn như \\(X_i \\cdot X_j\\).Biến giải thích được tính toán từ tác động qua lại giữa các biến giải thích khác, chẳng hạn như \\(X_i \\cdot X_j\\).","code":""},{"path":"mô-hình-hồi-quy-tuyến-tính.html","id":"ước-lượng-các-hệ-số-tuyến-tính","chapter":"Chương 10 Mô hình hồi quy tuyến tính","heading":"10.3.1 Ước lượng các hệ số tuyến tính","text":"Dù biến giải thích được tính toán như thế nào, biến mục tiêu \\(Y\\) vẫn là một hàm số tuyến tính của các hệ số \\((\\beta_0, \\beta_1, \\beta_2,\\cdots, \\beta_p)\\). Để dễ dàng triển khai các công thức, chúng tôi sẽ sử dụng ký hiệu \\(\\boldsymbol{\\beta}\\) tương đương như véc-tơ hệ số \\((\\beta_0, \\beta_1, \\beta_2,\\cdots, \\beta_p)\\). Phương pháp thông dụng nhất để ước lượng các hệ số của mô hình hồi quy tuyến tính là phương pháp bình phương nhỏ nhất, nghĩa là tham số \\(\\boldsymbol{\\beta}\\) được tính toán từ bài toán tối ưu\n\\[\\begin{align}\n\\hat{\\boldsymbol{\\beta}} = \\underset{\\boldsymbol{\\beta}}{\\operatorname{argmin}} \\sum\\limits_{=1}^n \\left( y_i - \\beta_0 - \\sum\\limits_{j = 1}^p \\beta_j \\cdot x_{,j} \\right)^2\n\\tag{10.27}\n\\end{align}\\]Sai số giữa \\(y_i\\) và \\(\\beta_0 + \\sum\\limits_{j = 1}^p \\beta_j \\cdot x_{,j}\\) được gọi là phần dư, hay residuals, trong mô hình hồi quy tuyến tính. Vế bên phải của công thức (10.27) là tổng bình phương của các phần dư và được viết tắt là RSS. Nếu coi tổng bình phương sai số là hàm số của các hệ số hồi quy \\(\\boldsymbol{\\beta}\\) và viết công thức tổng bình phương sai số dưới dạng ma trận ta sẽ có\n\\[\\begin{align}\nRSS(\\boldsymbol{\\beta}) = (\\textbf{y} - \\textbf{x} \\boldsymbol{\\beta})^T \\ (\\textbf{y} - \\textbf{x} \\boldsymbol{\\beta})\n\\tag{10.28}\n\\end{align}\\]\nvới\\(\\textbf{x}\\) là dữ liệu huấn luyện mô hình có kích thước \\(n \\times (p+1)\\) với cột đầu tiên bao gồm toàn các giá trị 1;\\(\\textbf{y}\\) là véc-tơ biến mục tiêu có kích thước \\(n \\times 1\\); vàvéc-tơ tham số \\(\\boldsymbol{\\beta}\\) có kích thước \\((p+1) \\times 1\\).Véc-tơ gradient của \\(RSS(\\boldsymbol{\\beta})\\) là véc-tơ có độ dài \\((p+1)\\) mà phần tử thứ \\((j+1)\\) là giá trị đạo hàm của RSS theo \\(\\beta_j\\) với \\(j = 0, 1, \\cdots, p\\); và được xác định như sau\n\\[\\begin{align}\n\\cfrac{\\nabla RSS(\\boldsymbol{\\beta})}{\\nabla \\boldsymbol{\\beta}} &= \\cfrac{\\nabla (\\textbf{y} - \\textbf{x} \\boldsymbol{\\beta})^T (\\textbf{y} - \\textbf{x} \\boldsymbol{\\beta}) }  {\\nabla \\boldsymbol{\\beta}} \\\\\n& = - 2 \\textbf{x}^T \\  (\\textbf{y} - \\textbf{x} \\boldsymbol{\\beta})\n\\tag{10.29}\n\\end{align}\\]Ma trận Hessian là ma trận kích thước \\((p+1) \\times (p+1)\\) mà phần tử hàng \\(+1\\) cột \\(j+1\\) là đạo hàm cấp hai của RSS lần lượt theo \\(\\beta_i\\) rồi theo \\(\\beta_j\\)\n\\[\\begin{align}\n\\cfrac{\\nabla RSS(\\boldsymbol{\\beta})}{\\nabla \\boldsymbol{\\beta} \\ \\nabla \\boldsymbol{\\beta}^T} = 2 \\textbf{x}^T \\ \\textbf{x}\n\\tag{10.30}\n\\end{align}\\]Giả sử rằng ma trận biến giải thích không có cột nào là tổ hợp tuyến tính của các cột còn lại, hay nói cách khác, hạng của ma trận \\(\\textbf{x}\\) là \\((p+1)\\). Khi đó ta có \\((\\textbf{x}^T \\ \\textbf{x})\\) là ma trận xác định dương. Giá trị \\(\\hat{\\boldsymbol{\\beta}}\\) làm tối thiểu hóa \\(RSS(\\boldsymbol{\\beta})\\) là nghiệm của\n\\[\\begin{align}\n\\cfrac{\\nabla RSS(\\boldsymbol{\\beta})}{\\nabla \\boldsymbol{\\beta}} = \\textbf{0}\n\\tag{10.31}\n\\end{align}\\]\nnghĩa là \\(\\hat{\\beta}\\) được tính toán như sau\n\\[\\begin{align}\n\\textbf{x}^T \\  (\\textbf{y} - \\textbf{x} \\hat{\\boldsymbol{\\beta}}) = \\textbf{0} \\\\\n\\rightarrow \\textbf{x}^T \\ \\textbf{y} = \\textbf{x}^T \\ \\textbf{x} \\hat{\\boldsymbol{\\beta}} \\\\\n\\rightarrow \\hat{\\boldsymbol{\\beta}} = (\\textbf{x}^T \\ \\textbf{x} )^{-1} \\textbf{x}^T \\ \\textbf{y}\n\\tag{10.32}\n\\end{align}\\]Với ma trận dữ liệu \\(\\textbf{x}\\), giá trị dự báo \\(\\hat{\\textbf{y}}\\) được tính toán từ công thức dưới đây\n\\[\\begin{align}\n\\hat{\\textbf{y}} = \\textbf{x} \\hat{\\boldsymbol{\\beta}} =   \\textbf{x} \\ (\\textbf{x}^T \\ \\textbf{x} )^{-1} \\textbf{x}^T \\ \\textbf{y}\n\\tag{10.33}\n\\end{align}\\]Điều gì xảy ra nếu hạng của ma trận \\(\\textbf{x}\\) nhỏ hơn \\((p+1)\\), nghĩa là một (hoặc một vài cột dữ liệu) là tổ hợp tuyến tính của các cột dữ liệu khác, hoặc trong trường hợp ma trận \\(\\textbf{x}\\) có số hàng ít hơn số cột. Khi đó ma trận \\(\\textbf{x}^T \\ \\textbf{x}\\) sẽ không khả nghịch và phương trình tuyến tính (10.31) sẽ có vô số nghiệm \\(\\boldsymbol{\\beta}\\). Đa số các hàm có sẵn khi xây dựng và ước lượng mô hình tuyến tính đều tính toán đến vấn đề này, mỗi khi thêm một biến vào trong mô hình, luôn có bước kiểm tra nếu biến được thêm vào có phải là tổ hợp tuyển tính (hoặc xấp xỉ bằng tổ hợp tuyến tính) của các biến sẵn có để loại bỏ biến đó khỏi mô hình.Để có thể đưa ra các suy diễn về véc-tơ hệ số, chúng ta cần có giả thiết về phân phối của biến phụ thuộc \\(Y\\). Mô hình hồi quy tuyến tính có giả thiết quan trọng là biến mục tiêu \\(Y\\) có phân phối chuẩn. Nói một cách khác, biến ngẫu nhiên \\(Y|X = x_i\\), được viết tắt là \\(Y_i\\), là các biến ngẫu nhiên phân phối chuẩn độc lập có giá trị trung bình \\(\\textbf{x}_i^T \\boldsymbol{\\beta}\\) và phương sai cố định là \\(\\sigma^2\\) (không phụ thuộc vào \\(\\)). Mô hình hồi quy tuyến tính được viết như sau\n\\[\\begin{align}\n& Y_i = \\beta_0 + \\beta_1 \\cdot x_{,1} + \\beta_2 \\cdot x_{,2} + \\cdots + \\beta_p \\cdot x_{,p} + \\epsilon_i \\\\\n& \\epsilon_i \\sim \\mathcal{N}(0, \\sigma^2) \\\\\n& Cov(\\epsilon_i, \\epsilon_j) = 0 \\ \\ \\forall \\neq j\n\\tag{10.34}\n\\end{align}\\]Hoặc chúng ta có thể viết mô hình hồi quy tuyến tính dưới dạng ma trận\n\\[\\begin{align}\nY \\sim \\mathcal{N}(\\textbf{x} \\boldsymbol{\\beta}, \\sigma^2 \\ \\textbf{}_n)\n\\tag{10.35}\n\\end{align}\\]\ntrong đó \\(\\textbf{}_n\\) là ma trận đơn vị kích thước \\(n \\times n\\). Với giả thiết phân phối chuẩn của \\(Y\\) trong phương trình (10.35) và kết hợp với (10.32) chúng ta thấy rằng \\(\\hat{\\boldsymbol{\\beta}}\\) là một phép biến đổi tuyến tính của một véc-tơ phân phối chuẩn nên cũng là một véc-tơ phân phối chuẩn. Véc-tơ trung bình của \\(\\hat{\\boldsymbol{\\beta}}\\) được xác định như sau\n\\[\\begin{align}\n\\mathbb{E}(\\hat{\\boldsymbol{\\beta}}) &= \\mathbb{E}\\left((\\textbf{x}^T \\ \\textbf{x} )^{-1} \\textbf{x}^T \\ \\textbf{y}\\right) \\\\\n& = (\\textbf{x}^T \\ \\textbf{x} )^{-1} (\\textbf{x}^T \\textbf{x}) \\ \\boldsymbol{\\beta} \\\\\n& = \\boldsymbol{\\beta}\n\\tag{10.36}\n\\end{align}\\]\nMa trận hiệp phương sai của véc-tơ \\(\\hat{\\boldsymbol{\\beta}}\\)\n\\[\\begin{align}\n\\mathbb{V}ar(\\hat{\\boldsymbol{\\beta}}) &= \\mathbb{V}ar\\left((\\textbf{x}^T \\ \\textbf{x} )^{-1} \\textbf{x}^T \\ \\textbf{y}\\right) \\\\\n&= \\sigma^2 \\ (\\textbf{x}^T \\ \\textbf{x} )^{-1}\n\\tag{10.37}\n\\end{align}\\]Có thể thấy rằng véc-tơ trung bình của \\(\\hat{\\boldsymbol{\\beta}}\\) hoàn toàn phụ thuộc vào dữ liệu trong khi ma trận hiệu phương sai lại phụ thuộc vào một tham số không biết là \\(\\sigma\\). Để xây dựng được các khoảng tin cậy hoặc kiểm định được các hệ số có khác không hay không, chúng ta cần ước lượng tham số \\(\\sigma^2\\).","code":""},{"path":"mô-hình-hồi-quy-tuyến-tính.html","id":"ước-lượng-phương-sai-của-biến-phụ-thuộc.","chapter":"Chương 10 Mô hình hồi quy tuyến tính","heading":"10.3.2 Ước lượng phương sai của biến phụ thuộc.","text":"Tham số thứ hai của mô hình hồi quy tuyến tính là phương sai của biến phụ thuộc, ký hiệu \\(\\sigma^2\\), được ước lượng như sau\n\\[\\begin{align}\n\\hat{\\sigma}^2 = RSS(\\hat{\\boldsymbol{\\beta}}) = \\cfrac{\\sum \\hat{\\epsilon}_i^2}{n - (p+1)} = \\cfrac{\\hat{\\boldsymbol{\\epsilon}}^{T} \\ \\hat{\\boldsymbol{\\epsilon}}}{n - (p+1)}\n\\tag{10.37}\n\\end{align}\\]\nvới \\(\\hat{\\epsilon}_i = y_i - \\hat{y}_i\\).Với giả thiết phân phối chuẩn của các \\(\\epsilon_i\\) như (10.34), \\(\\hat{\\boldsymbol{\\beta}}\\) được ước lượng từ phương trình (10.32), và \\(\\hat{y}\\) được tính toán từ (10.33), ta có thể chứng minh được rằng \\(\\cfrac{\\sum \\hat{\\epsilon}_i^2}{\\sigma^2}\\) là một biến ngẫu nhiên phân phối \\(\\chi^2\\) với bậc tự là \\(n-(p+1)\\). Thật vậy, ta có\n\\[\\begin{align}\n\\hat{\\boldsymbol{\\epsilon}} &= \\textbf{y} - \\hat{\\textbf{y}} = y - \\textbf{x} \\hat{\\beta} \\\\\n& = \\textbf{y} - \\textbf{x} (\\textbf{x}^T \\ \\textbf{x} )^{-1} \\textbf{x}^T \\ \\textbf{y} \\\\\n& = Q \\textbf{y}\n\\end{align}\\]\nvới ma trận \\(Q = I_n - \\textbf{x} (\\textbf{x}^T \\ \\textbf{x} )^{-1} \\textbf{x}^T\\). Ma trận \\(Q\\) có các tính chất sau:Thứ nhất là Q có vết (\\(Trace\\)) bằng \\(n - (p+1)\\), thật vậy\n\\[\\begin{align}\nTrace(Q) = Trace(\\textbf{}_n) - Trace(\\textbf{x} (\\textbf{x}^T \\ \\textbf{x} )^{-1} \\textbf{x}^T)\n\\end{align}\\]\nđồng thời\n\\[\\begin{align}\nTrace(\\textbf{x} (\\textbf{x}^T \\ \\textbf{x} )^{-1} \\textbf{x}^T) &= Trace( \\textbf{x}^T \\textbf{x} (\\textbf{x}^T \\ \\textbf{x} )^{-1} ) \\\\\n& = Trace(I_{p+1}) = (p+1)\n\\end{align}\\]\nđó \\(Trace(Q) = n - (p+1)\\)Thứ nhất là Q có vết (\\(Trace\\)) bằng \\(n - (p+1)\\), thật vậy\n\\[\\begin{align}\nTrace(Q) = Trace(\\textbf{}_n) - Trace(\\textbf{x} (\\textbf{x}^T \\ \\textbf{x} )^{-1} \\textbf{x}^T)\n\\end{align}\\]\nđồng thời\n\\[\\begin{align}\nTrace(\\textbf{x} (\\textbf{x}^T \\ \\textbf{x} )^{-1} \\textbf{x}^T) &= Trace( \\textbf{x}^T \\textbf{x} (\\textbf{x}^T \\ \\textbf{x} )^{-1} ) \\\\\n& = Trace(I_{p+1}) = (p+1)\n\\end{align}\\]\nđó \\(Trace(Q) = n - (p+1)\\)Thứ hai, có thể thấy rằng \\(Q = Q^{'}\\) và \\(Q = Q^2\\) đó các giá trị riêng của Q chỉ có thể nhận giá trị là 0 hoặc 1. Như vậy, nếu gọi \\(V\\) là ma trận có các cột là các véc-tơ riêng của Q thì chúng ta có’\n\\[\\begin{align}\nV Q V^{'} = \\Delta = diag(1,1,\\cdots,1,0,0,\\cdots,0)\n\\tag{10.38}\n\\end{align}\\]\ntrong đó \\(\\Delta\\) là ma trận đường chéo chỉ chứa 0 và 1 trên đường chéo chính. Kết hợp với kết quả \\(Trace(Q)\\) \\(=\\) \\(n - (p+1)\\) có thể kết luận rằng \\(Q\\) có \\(n - (p+1)\\) giá trị riêng bằng 1 và \\((p+1)\\) giá trị riêng bằng 0.Thứ hai, có thể thấy rằng \\(Q = Q^{'}\\) và \\(Q = Q^2\\) đó các giá trị riêng của Q chỉ có thể nhận giá trị là 0 hoặc 1. Như vậy, nếu gọi \\(V\\) là ma trận có các cột là các véc-tơ riêng của Q thì chúng ta có’\n\\[\\begin{align}\nV Q V^{'} = \\Delta = diag(1,1,\\cdots,1,0,0,\\cdots,0)\n\\tag{10.38}\n\\end{align}\\]\ntrong đó \\(\\Delta\\) là ma trận đường chéo chỉ chứa 0 và 1 trên đường chéo chính. Kết hợp với kết quả \\(Trace(Q)\\) \\(=\\) \\(n - (p+1)\\) có thể kết luận rằng \\(Q\\) có \\(n - (p+1)\\) giá trị riêng bằng 1 và \\((p+1)\\) giá trị riêng bằng 0.Ta có \\(\\boldsymbol{\\epsilon} \\sim \\mathcal{N}(\\textbf{0}, \\sigma^2 )\\) và \\(\\hat{\\boldsymbol{\\epsilon}} \\sim \\mathcal{N}(\\textbf{0}, \\sigma^2 Q)\\). Từ (10.38) ta có \\(V \\hat{\\boldsymbol{\\epsilon}} ~ \\sim \\mathcal{N}(\\textbf{0}, \\sigma^2 \\Delta)\\). Nói cách khác, nếu cho \\(\\textbf{z} = V \\hat{\\epsilon}\\) thì \\(\\textbf{z}\\) là véc-tơ phân phối chuẩn với trung bình là \\(\\textbf{0}\\) và ma trận hiệp phương sai \\(\\sigma^2 \\Delta\\). Ma trận đường chéo \\(\\Delta\\) có \\(n-(p+1)\\) phần tử nằm trên đường chéo chính bằng 1 và (p+1) phần tử nằm trên đường chéo chính bằng 0. Nói một cách khác, các phần tử từ vị trí thứ \\(1\\) đến \\(n-(p+1)\\) trong \\(\\textbf{z}\\) có phương sai bằng \\(\\sigma^2\\) và \\(p+1\\) phần tử còn lại trong \\(\\textbf{z}\\) có phương sai bằng 0.\\(V\\) là ma trận các giá trị riêng thỏa mãn \\(V V' = I_n\\) nên\n\\[\\begin{align}\n\\hat{\\boldsymbol{\\epsilon}}^{T} \\ \\hat{\\boldsymbol{\\epsilon}} = (V \\hat{\\boldsymbol{\\epsilon}})^T V \\hat{\\boldsymbol{\\epsilon}} = \\textbf{z}^T \\textbf{z} = z_1^2 + z_2^2 + \\cdots + z_{n-(p+1)}^2 \\sim \\sigma^2 \\cdot \\chi^2_{n-(p+1)}\n\\tag{10.39}\n\\end{align}\\]Như vậy, từ phương trình (10.39) chúng ta có\n\\[\\begin{align}\n\\left(n-(p+1)\\right) \\times \\hat{\\sigma}^2 = \\hat{\\boldsymbol{\\epsilon}}^{T} \\ \\hat{\\boldsymbol{\\epsilon}} \\sim \\sigma^2 \\times \\chi^2_{n-(p+1)}\n\\tag{10.40}\n\\end{align}\\]\\(\\hat{\\sigma}^2\\) là ước lượng không chệch của \\(\\sigma^2\\) vì\n\\[\\begin{align}\n\\mathbb{E}\\left(\\hat{\\sigma}^2\\right) & = \\cfrac{1}{(n-(p+1))} \\mathbb{E}\\left(\\sigma^2 \\cdot \\chi^2_{n-(p+1)} \\right) \\\\\n& = \\sigma^2 \\times \\cfrac{\\mathbb{E}\\left(\\chi^2_{n-(p+1)} \\right)}{(n-(p+1))} \\\\\n& = \\sigma^2\n\\end{align}\\]\nvì giá trị trung bình của biến ngẫu nhiên \\(\\chi^2_{n-(p+1)}\\) là \\(n-(p+1)\\).Có thể tóm tắt các ước lượng tham số của mô hình hồi quy tuyến tính, bao gồm các hệ số tuyến tính va phương sai của biến mục tiêu, sử dụng phương pháp bình phương nhỏ nhất như sau\\[\\begin{align}\n\\hat{\\boldsymbol{\\beta}} & = P \\cdot \\textbf{y} \\\\\n\\hat{\\sigma}^2 & = \\cfrac{1}{n-(p+1)} \\ (Q \\cdot \\textbf{y})^T (Q \\cdot \\textbf{y}) \\\\\nP & = (\\textbf{x}^T \\ \\textbf{x} )^{-1} \\textbf{x}^T \\\\\nQ & = \\textbf{}_n - \\textbf{x} \\ (\\textbf{x}^T \\ \\textbf{x} )^{-1} \\textbf{x}^T\n\\tag{10.41}\n\\end{align}\\]Lưu ý rằng \\(P \\cdot Q^T\\) là một ma trận kích thước \\((p+1) \\times n\\) có tất cả các phần tử bằng 0, đó \\(\\hat{\\boldsymbol{\\beta}}\\) và \\(\\hat{\\sigma}^2\\) là các biến ngẫu nhiên độc lập.","code":""},{"path":"mô-hình-hồi-quy-tuyến-tính.html","id":"kiểm-định-các-hệ-số-ước-lượng","chapter":"Chương 10 Mô hình hồi quy tuyến tính","heading":"10.3.3 Kiểm định các hệ số ước lượng","text":"Từ các phương trình (10.36), (10.37), và (10.40), chúng ta có phân phối xác suất của các tham số của mô hình hồi quy tuyến tính:\n\\[\\begin{align}\n& \\hat{\\beta}_j \\sim \\mathcal{N}(\\beta_j, \\sigma^2 a_{jj}) \\ \\ \\forall 1 \\leq j \\leq (p+1) \\\\\n& \\hat{\\sigma}^2 \\sim \\cfrac{\\sigma^2}{n-(p+1)} \\ \\chi^2_{n-(p+1)}\n\\tag{10.42}\n\\end{align}\\]\nvới \\(a_{jj}\\) là phần tử nằm ở hàng \\(j\\) cột \\(j\\) của ma trận \\((\\textbf{x}^T \\ \\textbf{x} )^{-1}\\).Sau khi ước lượng hệ số tuyến tính từ phương trình (10.41), chúng ta thường quan tâm đến sự kiện \\(\\beta_j \\neq 0\\) ở một mức độ tin cậy nào đó, nghĩa là biến độc lập \\(X_j\\) có tác động tuyến tính lên biến mục tiêu \\(Y\\) một cách có ý nghĩa. Để trả lời câu hỏi này, chúng ta cần kiểm định giả thuyết \\(H_0: \\beta_j = 0\\). Dưới giả thuyết \\(H_0\\), \\(\\hat{\\beta}_j\\) là biến ngẫu nhiên phân phối chuẩn có giá trị trung bình bằng 0, tuy nhiên phương sai của \\(\\hat{\\beta}_j\\) phụ thuộc vào giá trị không biết là \\(\\sigma^2\\). Cho biến ngẫu nhiên \\(T_j = \\hat{\\beta}_j/\\left(\\sqrt{a_{jj}} \\cdot \\hat{\\sigma}\\right)\\) thì cùng chia cả tử và mẫu của \\(T_j\\) cho \\(\\sqrt{a_{jj}} \\cdot \\sigma\\) ta có\n\\[\\begin{align}\nT_j = \\hat{\\beta}_j/(\\hat{\\sigma} \\cdot  \\sqrt{a_{jj}})  = \\cfrac{\\hat{\\beta}_j/(\\sigma \\cdot  \\sqrt{a_{jj}})}{\\hat{\\sigma}/\\sigma}\n\\tag{10.43}\n\\end{align}\\]Dưới giả thuyết \\(H_0\\) ta có\n\\[\\begin{align}\n& \\hat{\\beta}_j/(\\sigma \\cdot  \\sqrt{a_{jj}}) \\sim \\mathcal{N}(0,1) \\\\\n& \\hat{\\sigma}/\\sigma \\sim \\sqrt{\\cfrac{\\chi^2_{n-(p+1)}}{n-(p+1)}}\n\\tag{10.44}\n\\end{align}\\]\nđó \\(T_j\\) là biến ngẫu nhiên phân phối Student với bậc tự \\(n-(p+1)\\). Lưu ý rằng khi bậc tự đủ lớn, biến ngẫu nhiên phân phối Student sẽ hội tụ đến phân phối chuẩn với trung bình bằng 0 và phương sai bằng 1. đó trong nhiều trường hợp, người xây dựng mô hình sử dụng phân phối \\(\\mathcal{N}(0,1)\\) để kiểm định giả thuyết \\(H_0: \\beta_j = 0\\).Giá trị của \\(T_j\\) tính từ công thức (10.43) là cơ sở để bác bỏ hoặc không bác bỏ giả thuyết \\(H_0\\). Một cách tự nhiên, nếu giá trị tuyệt đối của \\(T_j\\) lớn, nghĩa là \\(T_j\\) nằm xa giá trị 0, xác suất để bác bỏ giả thuyết \\(H_0\\) là lớn hơn với khi \\(T_j\\) gần 0.Giá trị p-value được tính bởi công thức dưới đây\n\\[\\begin{align}\np-value = 2 \\mathbb{P}(T_{n-(p+1)} > |T_j|)\n\\end{align}\\]\nKhi p-value nhỏ hơn một mức ý nghĩa \\(\\alpha\\) thì có thể kết luận rằng với độ tin cậy \\((1-\\alpha)\\), hệ số \\(\\beta_j\\) là khác 0 một cách có ý nghĩa thống kê.Trong nhiều trường hợp chúng ta cần phải thực hiện kiểm định giả thuyết mà nhiều hệ số tuyến tính đồng thời bằng 0. Chẳng hạn như các hệ số tuyến tính của một nhóm các biến liên tục, hoặc hệ số tuyến tính của 1 biến rời rạc nhận từ ba giá trị trở lên. Giả sử các hệ số cần được kiểm định đồng thời là \\(\\beta_{j_1}, \\beta_{j_2}, \\cdots, \\beta_{j_h}\\), khi đó cặp giả thuyết \\(H_0\\) - \\(H_1\\) sẽ là\n\\[\\begin{align}\n& H_0: \\beta_{j_i} = 0 \\ \\ \\forall = 1,2, \\cdots, h \\\\\n& H_1: \\text{Tồn tại ít nhất } { sao cho} \\ \\beta_{j_i} \\neq 0\n\\end{align}\\]Dưới giả thuyết \\(H_0\\) ta có\n\\[\\begin{align}\n\\cfrac{1}{h} \\sum\\limits_{= 1}^h T^2_{j_i} & =  \\cfrac{ \\cfrac{1}{h}  \\sum\\limits_{= 1}^h \\left( \\hat{\\beta}_{j_i}/(\\sigma \\cdot  \\sqrt{a_{jj}}) \\right)^2}{{\\hat{\\sigma}^2/\\sigma^2}}\n= \\cfrac{\\chi^2_{h}/h}{\\chi^2_{n-(p+1)}/(n-(p+1))} \\sim \\mathcal{F}(h,n-(p+1))\n\\tag{10.45}\n\\end{align}\\]Trong đó \\(\\mathcal{F}(h,n-(p+1))\\) là phân phối \\(\\mathcal{F}\\) với các tham số \\(h\\) và \\(n - (p+1)\\). Một cách tự nhiên, nếu giá trị của \\(\\sum\\limits_{= 1}^h T^2_{j_i}/h\\) đủ lớn, chúng ta sẽ bác bỏ giả thuyết \\(H_0\\), nghĩa là tồn tại ít nhất một \\(\\) sao cho \\(\\beta_{j_i} \\neq 0\\). Nhắc lại rằng chúng tôi đã đề cập đến phân phối \\(\\mathcal{F}\\) khi kiểm định mô hình tuyến tính đa biến. Chúng tôi đã sử dụng thống kê \\(F\\) được tính toán bằng công thức (10.16). Trong trường hợp tổng quát, nếu \\(RSS_0\\) là tổng bình phương sai số của mô hình tuyến tính bao gồm đầy đủ \\(p+1\\) biến giải thích trong khi \\(RSS_1\\) là tổng bình phương sai số của mô hình tuyến tính không bao gồm các biến \\(X_{j_1}, X_{j_2}, \\cdots, X_{j_h}\\), thống kê \\(F\\) được tính toán bằng công thức sau\n\\[\\begin{align}\nF = \\cfrac{(RSS_1 - RSS_0)/h}{RSS_0/\\left(n-(p+1)\\right)}\n\\tag{10.46}\n\\end{align}\\]Nếu giả thuyết \\(H_0\\) là đúng, nghĩa là các hệ số \\(\\beta_{j_i}\\) đều nhận giá trị bằng 0, có thể chứng minh được rằng thống kê \\(F\\) sẽ có phân phối \\(\\mathcal{F}\\) với tham số \\(h\\) và \\(n - (p+1)\\).\nxxxxxxxxxxxxxxx","code":""},{"path":"mô-hình-hồi-quy-tuyến-tính.html","id":"linearmodelselection","chapter":"Chương 10 Mô hình hồi quy tuyến tính","heading":"10.4 Các phương pháp chọn biến trong mô hình hồi quy tuyến tính.","text":"Có hai lý khiến những người xây dựng mô hình thường không hài lòng với kết quả từ phương pháp bình phương nhỏ nhấtThứ nhất là khả năng dự đoán của mô hình: ước lượng tham số bằng phương pháp bình phương nhỏ nhất thường có sai lệch thấp nhưng phương sai lớn. Độ chính xác của dự đoán đôi khi có thể được cải thiện bằng các phương pháp như thu nhỏ số lượng biến giải thích hoặc các phương pháp shinkage. Bằng cách tiếp cận như vậy, người xây dựng mô hình chấp nhận tăng sai lệch để giảm phương sai của các giá trị dự đoán để có thể cải thiện độ chính xác trong dự đoán.Lý thứ hai là khả năng diễn giải của mô hình. Khi số lượng biến giải thích là quá nhiều, chúng ta thường muốn xác định một tập hợp nhỏ hơn những biến có tác động mạnh đáng kể nhất nhằm diễn giải mô hình một cách tốt nhất.Trong phần này, trước tiên chúng ta sẽ thảo luận một số cách tiếp cận để lựa chọn biến giải thích để đưa vào trong mô hình hồi quy tuyến tính bao gồm các phương pháp như lựa chọn tập hợp con tốt nhất hay best subset selection, forward stepwise selection, backward stepwise selection. Trong phần tiếp theo, chúng ta thảo luận về các phương pháp rút gọn tham số với mục tiêu kiểm soát phương sai của dự đoán. không tìm được từ Tiếng Việt có ý nghĩa hợp lý nên chúng tôi sẽ giữ nguyên tên gọi của hai phương pháp forward stepwise selection và backward stepwise selection.Nhìn chung, khi lựa chọn biến giải thích đưa vào mô hình, chúng ta chỉ giữ lại một tập hợp con của các biến và loại bỏ phần còn lại khỏi mô hình. Phương pháp bình phương nhỏ nhất được sử dụng để ước tính các hệ số tuyến tính. Các tiêu chí đánh giá mô hình sẽ được đưa ra nhằm sánh các tập hợp biến khác nhau.","code":""},{"path":"mô-hình-hồi-quy-tuyến-tính.html","id":"phương-pháp-lựa-chọn-tập-hợp-con-tốt-nhất","chapter":"Chương 10 Mô hình hồi quy tuyến tính","heading":"10.4.1 Phương pháp lựa chọn tập hợp con tốt nhất","text":"Đúng như tên gọi, lựa chọn biến bằng phương pháp lựa chọn tập hợp con tốt nhất có nghĩa là với mỗi giá trị của \\(k \\\\{0,1,2,... ,p\\}\\) người xây dựng mô hình cố gắng tìm tập hợp con có bao gồm đúng \\(k\\) biến giải thích sao cho tổng bình phương phần dư đạt giá trị nhỏ nhất. Điểm bất lợi của phương pháp này là khối lượng tính toán quá lớn bởi vì số lượng mô hình cần ước lượng là \\(2^p\\). Phương pháp tiếp cận của Furnival và Wilson (1974) giúp cho thuật toán này có thể thực hiện được với \\(p\\) lên đến 40. Tuy nhiên, thời gian tính toán chậm vẫn là điểm bất lợi nhất của phương pháp này.Dữ liệu được sử dụng để xây dựng mô hình tuyến tính là dữ liệu Boston trong thư viện MASS. Đây là dữ liệu về giá nhà tại 506 vùng ngoại ô tại Boston. Biến mục tiêu trong xây dựng mô hình là biến medv là giá trị trung vị của giá nhà tại vùng đó, đơn vị là nghìn USD. Mô hình có 13 biến phụ thuộc trong đó có 11 biến kiểu số và 2 biến kiểu rời rạc là rad và chas. Mục tiêu của chúng ta là lựa chọn được \\(k\\) biến trong số 13 biến phụ thuộc sao cho RSE là nhỏ nhất.\nHình 10.12: Phương pháp lựa chọn tập hợp con tốt nhất được thực hiện trên dữ liệu Boston. Đường ranh giới dưới là các mô hình có tổng bình phương phần dư nhỏ nhất\nHình 10.12 hiển thị tất cả các mô hình có tập hợp con \\(k\\) biến, \\(k = 0, 1, \\cdots, 13\\) cho ví dụ về giá nhà ở Boston. Đường ranh giới dưới biểu thị các mô hình đủ điều kiện để lựa chọn theo cách tiếp cận tập hợp con tốt nhất. Lưu ý rằng tập hợp con tốt nhất có kích thước bằng 2 không nhất thiết bao gồm biến nằm trong tập con tốt nhất có kích thước 1. Đường cong tập hợp con tốt nhất tính theo tiêu chí RSE thường giảm theo \\(k\\), đó thường không tối ưu khi sử dụng để chọn kích thước tập hợp con tối ưu.Có một số tiêu chí mà chúng ta có thể cân nhắc khi sánh các mô hình tuyến tính có số lượng biến khác nhau ngoài RSE. Nếu khối lượng tính toán cho phép thì chúng tôi thường sử dụng sai số tính bằng RSE trung bình tính trên xác thực chéo để làm tiêu chí lựa chọn mô hình. Khi khối lượng tính toán cho xác thực chéo quá lớn thì bạn đọc cũng có thể sử dụng tiêu chí AIC là một lựa chọn thay thế. Cách sử dụng sai số xác thực chéo làm tiêu chí lựa chọn sẽ được trình bày trong phần thực hành của chương.","code":""},{"path":"mô-hình-hồi-quy-tuyến-tính.html","id":"forward--and-backward-stepwise-selection","chapter":"Chương 10 Mô hình hồi quy tuyến tính","heading":"10.4.2 Forward- and Backward-Stepwise Selection","text":"Thay vì tìm kiếm qua tất cả các tập hợp con, điều mà này trở nên không khả thi khi số lượng biến \\(p\\) lớn, chúng ta có thể tìm kiếm tập hợp con một cách tuần tự theo từng bước, mà ở đó, kết quả của bước tiếp theo phụ thuộc vào bước trước đó.Forward stepwise selection bắt đầu bằng cách lựa chọn mô hình có 1 biến tốt nhất. Sau đó, phương pháp này tìm kiểm mô hình có \\(k\\) biến bằng cách lấy \\((k-1)\\) biến đã được lựa chọn ở bước trước đó để thêm vào một biến. Như vậy, tại bước thứ \\(k\\), sẽ có \\((p-k+1)\\) mô hình tuyến tính cần được ước lượng. Số mô hình cần được ước lượng trong phương pháp forward stepwise selection là\n\\[\\begin{align}\np + (p-1) + \\cdots + 1 = \\cfrac{p(p+1)}{2}\n\\end{align}\\]Forward stepwise selection bắt đầu bằng cách lựa chọn mô hình có 1 biến tốt nhất. Sau đó, phương pháp này tìm kiểm mô hình có \\(k\\) biến bằng cách lấy \\((k-1)\\) biến đã được lựa chọn ở bước trước đó để thêm vào một biến. Như vậy, tại bước thứ \\(k\\), sẽ có \\((p-k+1)\\) mô hình tuyến tính cần được ước lượng. Số mô hình cần được ước lượng trong phương pháp forward stepwise selection là\n\\[\\begin{align}\np + (p-1) + \\cdots + 1 = \\cfrac{p(p+1)}{2}\n\\end{align}\\]Bước thứ nhất của phương pháp backward stepwise selection bắt đầu bằng ước lượng mô hình có đầy đủ \\(p\\) biến. Sau đó, tại bước thứ \\(k\\), mô hình tìm cách loại bỏ đi 1 biến trong \\((p+2-k)\\) biến trong mô hình được ước lượng tại bước thứ \\((k-1)\\). Như vậy, tại bước thứ \\(k\\) với \\(k >1\\) sẽ có \\((p+2-k)\\) mô hình cần được ước lượng. Tổng số mô hình cần được ước lượng trong phương pháp là\n\\[\\begin{align}\n1 + p + (p-1) + \\cdots + 2 = \\cfrac{p(p+1)}{2}\n\\end{align}\\]Bước thứ nhất của phương pháp backward stepwise selection bắt đầu bằng ước lượng mô hình có đầy đủ \\(p\\) biến. Sau đó, tại bước thứ \\(k\\), mô hình tìm cách loại bỏ đi 1 biến trong \\((p+2-k)\\) biến trong mô hình được ước lượng tại bước thứ \\((k-1)\\). Như vậy, tại bước thứ \\(k\\) với \\(k >1\\) sẽ có \\((p+2-k)\\) mô hình cần được ước lượng. Tổng số mô hình cần được ước lượng trong phương pháp là\n\\[\\begin{align}\n1 + p + (p-1) + \\cdots + 2 = \\cfrac{p(p+1)}{2}\n\\end{align}\\]Hai phương pháp mô tả ở trên có bất lợi với phương pháp lựa chọn tập hợp con tốt nhất là không chắc chắn tìm thấy được mô hình có \\(k\\) biến tốt nhất kết quả của các bước phụ thuộc vào các bước trước đó. Tuy nhiên, lợi thế của các phương pháp này có thể kể đến làNguồn lực tính toán: với \\(p\\) lớn thì thời gian tính toán của hai phương pháp kể trên nhanh hơn nhiều với phương pháp tập hợp con tốt nhất.Về mặt ý nghĩa thống kê: phương pháp lựa chọn tập hợp con tốt nhất tìm kiếm mô hình có \\(k\\) biến tốt nhất trong tất cả các lựa chọn có thể, đó kết quả thường thu được mô hình có phương sai cao hơn. Ngược lại, các phương pháp forward và backward stepwise selection chỉ tìm kiếm mô hình \\(k\\) biến trong một không gian nhỏ hơn (phụ thuộc vào các biến đã được lựa chọn trong các bước trước đó) nên thường có phương sai nhỏ hơn.Hình vẽ 10.13 mô tả các mô hình có \\(k\\) biến tốt nhất với \\(k = 0, 1, \\cdots, 13\\) sử dụng phương pháp forward stepwise selection. Trong trường hợp dữ liệu Boston, ba phương pháp lựa chọn biến chúng tôi mô tả ở trên cho cùng một kết quả!\nHình 10.13: Phương pháp forward stepwise selection được thực hiện trên dữ liệu Boston. Đường ranh giới dưới là các mô hình có sai số trung bình nhỏ nhất.\n","code":""},{"path":"mô-hình-hồi-quy-tuyến-tính.html","id":"hồi-quy-tuyến-tính-có-ràng-buộc-tham-số","chapter":"Chương 10 Mô hình hồi quy tuyến tính","heading":"10.5 Hồi quy tuyến tính có ràng buộc tham số","text":"Bằng cách tìm kiếm một tập hợp con các biến giải thích, các phương pháp lựa chọn biến trình bày ở phần trên có thể giúp bạn đọc tìm ra các mô hình có khả năng giải thích tốt và có khả năng có sai lệch thấp hơn với mô hình gồm đầy đủ tất cả các biến. Hạn chế của các mô hình này ở chỗ, mỗi biến giải thích chỉ có một trong hai khả năng là có xuất hiện hoặc không xuất hiện, nên các mô hình kết quả vẫn sẽ có phương sai lớn. Các phương pháp ràng buộc tham số được trình bày trong phần này là các phương pháp thường được sử dụng để cải thiện các mô hình có phương sai lớn với mục đích giảm phương sai của các mô hình và chấp nhận đánh đổi khả năng sai lệch có thể tăng.","code":""},{"path":"mô-hình-hồi-quy-tuyến-tính.html","id":"hồi-quy-ridge","chapter":"Chương 10 Mô hình hồi quy tuyến tính","heading":"10.5.1 Hồi quy ridge","text":"Hồi quy ridge hạn chế ảnh hưởng của biến giải thích lên biến mục tiêu bằng cách thêm vào một hàm phạt vào RSS nếu giá trị tuyệt đối của các hệ số tuyến tính tăng lên. Thay vì tìm các hệ số \\(\\hat{\\boldsymbol{\\beta}}\\) để tối thiểu hóa \\(RSS(\\beta)\\), hồi quy ridge tìm các hệ số \\(\\hat{\\boldsymbol{\\beta}}^{ridge}\\) để tối thiểu hóa tổng của \\(RSS(\\beta)\\) với một hàm phạt như sau\n\\[\\begin{align}\n\\hat{\\boldsymbol{\\beta}}^{ridge} = \\underset{\\boldsymbol{\\beta}}{\\operatorname{argmin}} \\sum\\limits_{=1}^n \\left( y_i - \\beta_0 - \\sum\\limits_{j = 1}^p \\beta_j \\cdot x_{,j} \\right)^2 + \\lambda \\cdot \\sum\\limits_{j=1}^p \\beta_j^2\n\\tag{10.47}\n\\end{align}\\]hồi quy ridge sử dụng hàm phạt là \\(PF(\\boldsymbol{\\beta}) = \\sum\\limits_{j=1}^p \\beta_j^2\\). Lưu ý rằng hệ số chặn \\(\\beta_0\\) không có trong hàm phạt bởi vì hệ số này không bị tác động bởi các biến giải thích. Hệ số \\(\\lambda \\geq 0\\) điều khiển mức độ ảnh hưởng của hàm phạt lên giá trị tối ưu. Khi \\(\\lambda\\) nhỏ thì ảnh hưởng của hàm phạt lên giá trị tối ưu không đáng kể và nếu \\(\\lambda\\) lớn thì hàm phạt sẽ chiếm ưu thế trong bài toán tối ưu và làm cho tất cả các hệ số \\(\\hat{\\boldsymbol{\\beta}}^{ridge}\\) gần tới 0.Bài toán tối ưu trong phương trình (10.47) có thể phát biểu dưới dạng bài toán có ràng buộc như sau:\n\\[\\begin{align}\n& \\hat{\\boldsymbol{\\beta}}^{ridge} = \\underset{\\boldsymbol{\\beta}}{\\operatorname{argmin}} \\sum\\limits_{=1}^n \\left( y_i - \\beta_0 - \\sum\\limits_{j = 1}^p \\beta_j \\cdot x_{,j} \\right)^2, \\\\\n& \\text{ với ràng buộc: }  \\sum\\limits_{j=1}^p \\beta_j^2 \\leq c\n\\tag{10.48}\n\\end{align}\\]Trong trường hợp mô hình tuyến tính có các biến độc lập có tương quan cao với nhau ước lượng của các hệ số tuyến tính có độ biến động lớn khiến mô hình tuyến tính kém hiệu quả. Bằng cách sử dụng ràng buộc trên cho tổng bình phương các hệ số, hồi quy ridge kiểm soát được vấn đề các biến giải thích tương quan cao. Ngoài ra, khi thực hiện hồi quy ridge các hệ số tuyến tính sẽ phụ thuộc vào việc có hay không thực hiện biến đổi tuyến tính các biến giải thích, đó trước khi thực hiện hồi quy người xây dựng mô hình thường chuẩn hóa các biến giải thích. Sau khi các biến giải thích được chuẩn hóa, ước lượng cho hệ số chặn là giá trị trung bình của biến mục tiêu, và phương pháp bình phương nhỏ nhất được thực hiện để ước lượng \\(\\beta_1, \\beta_2, \\cdots, \\beta_p\\).Tổng bình phương sai số dưới trong hồi quy ridge được viết như sau\n\\[\\begin{align}\nRSS(\\boldsymbol{\\beta}) = (\\textbf{y} - \\textbf{x} \\boldsymbol{\\beta})^T \\ (\\textbf{y} - \\textbf{x} \\boldsymbol{\\beta}) + \\lambda \\boldsymbol{\\beta}^T \\boldsymbol{\\beta}\n\\end{align}\\]Lưu ý rằng ma trận biến giải thích \\(\\textbf{x}\\) sau khi chuẩn hóa có kích thước \\(n \\times p\\). Tương tự như mô hình tuyến tính, hệ số hồi quy được ước lượng bằng phương pháp bình phương nhỏ nhất\n\\[\\begin{align}\n\\hat{\\boldsymbol{\\beta}}^{ridge} = (\\textbf{x}^T \\ \\textbf{x} + \\lambda I_{p} )^{-1} \\textbf{x}^T \\ \\textbf{y}\n\\end{align}\\]\nvới \\(I_p\\) là ma trận đơn vị có kích thước \\(p \\times p\\). Như vậy, tương tự như hồi quy tuyến tính thông thường, véc-tơ hệ số \\(\\hat{\\boldsymbol{\\beta}}^{ridge}\\) vẫn là tổ hợp tuyến tính của véc-tơ biến mục tiêu \\(\\textbf{y}\\). Sự khác nhau của các hệ số đến ở chỗ hồi quy ridge thêm vào đường chéo chính của ma trận \\(\\textbf{x}^T \\ \\textbf{x}\\) giá trị \\(\\lambda\\) trước khi lấy nghịch đảo.Trong trường hợp các biến giải thích đôi một độc lập, có thể chứng minh được rằng\n\\[\\begin{align}\n\\hat{\\boldsymbol{\\beta}} = \\cfrac{\\hat{\\boldsymbol{\\beta}}^{ridge}}{1 + \\lambda}\n\\end{align}\\]\ntrong đó \\(\\hat{\\boldsymbol{\\beta}}\\) là ước lượng của các hệ số tuyến tính khi không sử dụng hàm phạt. Khi \\(\\lambda\\) đủ lớn sẽ làm cho các giá trị hệ số tuyến tính giảm dần về 0. Điều đó có nghĩa là khi \\(\\lambda\\) càng lớn, bậc tự của mô hình càng nhỏ. Trong mô hình tuyến tính thông thường, bậc tự của mô hình có thể hiểu một cách đơn giản là số lượng tham số và bằng \\((p+1)\\). Với \\(\\lambda > 0\\), vẫn có \\((p+1)\\) hệ số tuyến tính trong hồi quy ridge được ước lượng, tuy nhiên các hệ số bị ràng buộc với nhau làm cho bậc tự của mô hình giảm. Điều này cũng đồng nghĩa với việc mô hình ít bị phụ thuộc vào dữ liệu hơn (phương sai giảm) nhưng đánh đổi lại là sai lệch sẽ tăng.Khái niệm bậc tự trong các mô hình có ràng buộc về tham số thường được gọi là bậc tự hiệu quả thay vì bậc tự thông thường. Bậc tự hiệu quả được định nghĩa bằng tổng độ nhạy (đạo hàm) của các giá trị dự báo \\(\\hat{y}_i = \\hat{f}(x_i)\\) theo các giá trị quan sát của biến mục tiêu\n\\[\\begin{align}\n\\text{Bậc tự hiệu quả} = \\sum\\limits_{=1}^n \\cfrac{\\partial \\hat{y}_i }{\\partial y_i}\n\\tag{10.49}\n\\end{align}\\]Bậc tự hiệu quả lớn có nghĩa là giá trị dự đoán \\(\\hat{y}_i\\) sẽ thay đổi lớn khi các giá trị quan sát \\(y_i\\) thay đổi, ngược lại, bậc tự hiệu quả nhỏ có nghĩa là các giá trị dự đoán \\(\\hat{y}_i\\) sẽ thay đổi nhỏ khi các giá trị quan sát được \\(y_i\\) thay đổi. Tương tự như khái niệm bậc tự trong mô hình tuyến tính thông thường, mô hình có bậc tự hiệu quả lớn nghĩa là mô hình có phương sai lớn.Trong hồi quy tuyến tính thông thường hoặc hồi quy tuyến tính ridge, giá trị dự báo của mô hình có dạng\n\\[\\begin{align}\n\\hat{\\textbf{y}} = \\textbf{y}\n\\end{align}\\]\nvới \\(= \\textbf{x} (\\textbf{x}^T \\ \\textbf{x} )^{-1} \\textbf{x}^T\\) trong mô hình tuyến tính thông thường, và \\(= \\textbf{x} (\\textbf{x}^T \\ \\textbf{x} + \\lambda I_{p} )^{-1} \\textbf{x}^T\\) trong hồi quy tuyến tính ridge.Bậc tự hiệu quả của các mô hình trên chính là vết (trace) của ma trận \\(\\)\n\\[\\begin{align}\n\\sum\\limits_{=1}^n \\cfrac{\\partial \\hat{y}_i }{\\partial y_i} = Trace() = \\sum\\limits_{=1}^n A_{,}\n\\tag{10.50}\n\\end{align}\\]\ntrong đó \\(A_{,}\\) là các phần tử thứ \\(\\) nằm trên đường chéo chính của ma trận \\(\\). Trong ước lượng mô hình tuyến tính thông thường ma trận \\(\\textbf{x}^T \\ \\textbf{x}\\) có hạng bằng \\((p+1)\\), chúng ta đã có \\(trace() = (p+1)\\), nghĩa là bậc tự hiệu quả của mô hình tuyến tính thông thường bằng số tham số trong mô hình.Để trả lời câu hỏi bậc tự hiệu quả của hồi quy ridge phụ thuộc vào \\(\\lambda\\) như thế nào, chúng ta giả sử ma trận \\(\\textbf{x}^T \\ \\textbf{x}\\) có hạng bằng \\(p\\). Khi đó, ma trận \\(\\textbf{x}^T \\ \\textbf{x}\\) có thể được viêt dưới dạng sau\n\\[\\begin{align}\n\\textbf{x}^T \\ \\textbf{x} = U D U^{T}\n\\end{align}\\]\nvới \\(U\\) là ma trận các véc-tơ riêng chuẩn hóa của \\(\\textbf{x}^T \\ \\textbf{x}\\) và \\(D\\) là ma trận đường chéo có các phần tử nằm trên đường chéo chính là các giá trị riêng của ma trận \\(\\textbf{x}^T \\ \\textbf{x}\\).\n\\[\\begin{align}\nD = Diag(\\lambda_1, \\lambda_2, \\cdots, \\lambda_p)\n\\end{align}\\]\nvới \\(\\lambda_i\\) là các giá trị riêng của ma trận \\(\\textbf{x}^T \\ \\textbf{x}\\). Lưu ý rằng ma trận \\(U\\) là ma trận unitary. Hơn thế nữa, ma trận \\(\\textbf{x}^T \\ \\textbf{x}\\) xác định dương nên tất cả các giá trị riêng \\(\\lambda_i\\) đều là các số dương. Chúng ta có bậc tự hiệu quả của hồi quy ridge được xác định như sau:\n\\[\\begin{align}\nTrace\\left(\\textbf{x} (\\textbf{x}^T \\ \\textbf{x} + \\lambda \\textbf{}_p )^{-1}\\right) & = Trace(\\textbf{x}^T \\textbf{x} (\\textbf{x}^T \\ \\textbf{x} + \\lambda \\textbf{}_p )^{-1}) \\\\\n& = Trace\\left[ Diag \\left( \\cfrac{\\lambda_1}{\\lambda_1 + \\lambda}, \\cfrac{\\lambda_2}{\\lambda_2 + \\lambda}, \\cdots, \\cfrac{\\lambda_{p}}{\\lambda_{p} + \\lambda} \\right) \\right] \\\\\n& = \\sum\\limits_{= 1}^{p} \\cfrac{\\lambda_i}{\\lambda_i + \\lambda}\n\\tag{10.51}\n\\end{align}\\]Có thể thấy rằng bậc tự hiệu quả của hồi quy ridge là hàm số giảm theo \\(\\lambda\\). Trong mô hình tuyến tính với \\(p\\) biến giải thích và không có hệ số chặn, bậc tự là \\(p\\). Khi sử dụng ràng buộc trên các hệ số tuyến tính, kể cả khi các hệ số khác 0 một cách có ý nghĩa, các hệ số vẫn bị ràng buộc bởi \\(\\lambda\\). Bậc tự hiệu quả nhận giá trị bằng \\(p\\) khi \\(\\lambda = 0\\), tương đương với bài toán tối ưu không có ràng buộc, trong khi bậc tự hiệu quả sẽ xấp xỉ 0 nếu chúng ta chọn \\(\\lambda\\) đủ lớn.\nHình 10.14: hồi quy ridge tương đương với bài toán tìm RSS nhỏ nhất với ràng buộc các hệ số tuyến tính.\nHình 10.14 mô tả quá trình ước lượng tham số của hồi quy ridge trên một dữ liệu được mô phỏng. Dữ liệu có 500 quan sát và các hệ số \\(\\beta_1\\) và \\(\\beta_2\\) được lựa chọn để sinh dữ liệu là \\(\\beta_1 = 2\\) và \\(\\beta_2 = -1\\). Bạn đọc có thể thấy rằng lời giải của bài toán tối ưu không có ràng buộc là điểm xấp xỉ \\((2,-1)\\). Giá trị \\(RSS(\\boldsymbol{\\beta})\\) tương ứng với điểm tối ưu này là 80. Khi chúng ta thêm ràng buộc với hệ số \\(\\beta_1\\) và \\(\\beta_2\\) như phương trình (10.48) với \\(c = 1\\) thì miền giá trị của các hệ số phải nằm trong hình tròn có tâm tại (0,0) và bán kính bằng 1 giống như trong hình vẽ. Để thỏa mãn được ràng buộc này, chúng ta phải chấp nhận giá trị của \\(RSS(\\boldsymbol{\\beta})\\) tăng lên. Tại mỗi giá trị của \\(RSS(\\boldsymbol{\\beta})\\) lớn hơn giá trị tối thiểu là 80, tập hợp các điểm (\\(\\beta_1\\), \\(\\beta_2\\)) sao cho giá trị của RSS không thay đổi là một hình ellipse khai triển công thức của RSS sẽ thu được phương trình của một ellipse trên các biến (\\(\\beta_1\\), \\(\\beta_2\\)). Hình 10.14 mô tả ba ellipse đồng tâm, có độ lớn tăng dần tương ứng với giá trị của RSS là 100, 230 và 480. Tại giá trị 480, ellipse tiếp xúc với hình tròn mô tả miền ràng buộc tham số tại điểm chính là lời giải của bài toán tối ưu có ràng buộc hay chính là giá trị ước lượng tham số của hồi quy ridge. Bạn đọc có thể thấy rằng nếu hình tròn ràng buộc tham số không chứa điểm \\((2,-1)\\), hồi quy ridge luôn luôn đẩy các hệ số tuyến tính về gần 0 hơn và làm cho mô hình ít bị phụ thuộc hơn vào các tham số tuyến tính.","code":""},{"path":"mô-hình-hồi-quy-tuyến-tính.html","id":"phương-pháp-lasso","chapter":"Chương 10 Mô hình hồi quy tuyến tính","heading":"10.5.2 Phương pháp Lasso","text":"Lasso cũng là một phương pháp để tạo ràng buộc cho các hệ số của mô hình hồi quy tuyến tính. Lasso sử dụng hàm phạt dưới dạng tổng giá trị tuyệt đối của các hệ số thay vì tổng bình phương các hệ số tuyến tính. Lasso có thể được phát biểu dưới dạng bài toán tối ưu với ràng buộc như sau\n\\[\\begin{align}\n& \\hat{\\boldsymbol{\\beta}}^{ridge} = \\underset{\\boldsymbol{\\beta}}{\\operatorname{argmin}} \\sum\\limits_{=1}^n \\left( y_i - \\beta_0 - \\sum\\limits_{j = 1}^p \\beta_j \\cdot x_{,j} \\right)^2, \\\\\n& \\text{ với ràng buộc: }  \\sum\\limits_{j=1}^p |\\beta_j| \\leq c\n\\tag{10.52}\n\\end{align}\\]Tương tự như hồi quy ridge, các biến độc lập cũng sẽ được chuẩn hóa để có giá trị trung bình bằng 0. Khi đó, ước lượng cho \\(\\beta_0\\) là giá trị trung bình của biến phụ thuộc. Ước lượng tham số cho Lasso là quá trình tìm hệ số tuyến tính \\(\\hat{\\boldsymbol{\\beta}}^{Lasso}\\) để tối thiểu hóa tổng bình phương sai số cộng thêm một hàm phạt\\[\\begin{align}\n\\hat{\\boldsymbol{\\beta}}^{Lasso} = \\underset{\\boldsymbol{\\beta}}{\\operatorname{argmin}} \\sum\\limits_{=1}^n \\left( y_i - \\beta_0 - \\sum\\limits_{j = 1}^p \\beta_j \\cdot x_{,j} \\right)^2 + \\lambda \\cdot \\sum\\limits_{j=1}^p |\\beta_j|\n\\tag{10.53}\n\\end{align}\\]Không giống như hồi quy ridge, sử dụng hàm phạt là tổng giá trị tuyệt đối của các hệ số sẽ dẫn đến bài toán tìm \\(\\hat{\\boldsymbol{\\beta}}^{Lasso}\\) không có lời giải chính xác. Các phương pháp giải số thường được áp dụng để ước lượng tham số. Khi hằng số \\(c\\) trong ràng buộc của bài toán tối ưu (10.52) xấp xỉ 0, các hệ số tuyến tính cũng sẽ xấp xỉ 0. Ngược lại khi \\(c\\) đủ lớn, lời giải của bài toán tối ưu sẽ là hệ số của mô hình tuyến tính thông thường.\nHình 10.15: Hồi quy ridge tương đương với bài toán tìm RSS nhỏ nhất với ràng buộc các hệ số tuyến tính.\nHình 10.15 mô tả quá trình ước lượng tham số của hồi quy lasso trên một dữ liệu mô phỏng mà chúng tôi đã sử dụng trong mô tả hồi quy ridge. Các hệ số \\(\\beta_1\\) và \\(\\beta_2\\) được lựa chọn để sinh dữ liệu là \\(\\beta_1 = 2\\) và \\(\\beta_2 = -1\\) và lời giải của bài toán tối ưu không có ràng buộc là điểm xấp xỉ \\((2,-1)\\). Giá trị \\(RSS(\\boldsymbol{\\beta})\\) tương ứng với điểm tối ưu này là 80. Khi chúng ta thêm ràng buộc với hệ số \\(\\beta_1\\) và \\(\\beta_2\\) như phương trình (10.52) với \\(c = 1\\) thì miền giá trị của các hệ số phải nằm trong hình kim cương 4 cạnh được mô tả như trong hình vẽ. Cũng giống như trong hồi quy ridge, để thỏa mãn được ràng buộc chúng ta phải chấp nhận giá trị của \\(RSS(\\boldsymbol{\\beta})\\) tăng lên. Hình 10.14 mô tả ba ellipse đồng tâm, có độ lớn tăng dần tương ứng với giá trị của RSS là 100, 240 và 550. Tại giá trị 550, ellipse tiếp xúc với miền ràng buộc tham số tại một điểm và điểm đó chính là lời giải của phương pháp lasso. Tương tự như hồi quy ridge, phương pháp lasso luôn kéo các hệ số tuyến tính về gần 0 và làm cho mô hình ít bị phụ thuộc hơn vào các tham số tuyến tính. đặc điểm của miền ràng buộc, phương pháp lasso nhiều khi còn hiệu quả hơn hồi quy ridge trong ràng buộc tham số bởi vì điểm tiếp xúc của ellipse với hình kim cương sẽ luôn khiến cho một trong hai hệ số nhận giá trị gần 0 hơn.","code":""},{"path":"mô-hình-hồi-quy-tuyến-tính.html","id":"thực-hành-xây-dựng-mô-hình-tuyến-tính","chapter":"Chương 10 Mô hình hồi quy tuyến tính","heading":"10.6 Thực hành xây dựng mô hình tuyến tính","text":"Chúng ta sẽ thực hành xây dựng mô hình tuyến tính trên dữ liệu Boston trong thư viện MASS là dữ liệu về giá nhà tại các vùng ngoại ô của thành phố Boston. Biến mục tiêu có tên là medv là giá trị trung vị của giá nhà tại mỗi vùng. Có 13 biến giải thích bao gồm 11 biến giải thích là các biến liên tục và hai biến giải thích rời rạc bao gồmchas nhận hai giá trị là 1 nếu vùng ngoại ô nằm trên đường bờ sông và nhận giá trị bằng 0 nếu vùng đó không nằm trên đường bờ sông.rad là chỉ số cho biết khả năng tiếp cận đường cao tốc của mỗi vùng. Chỉ số này càng cao thì vùng càng có khả năng tiếp cận đường cao tốc.Về nguyên tắc, dữ liệu trước khi sử dụng để xây dựng mô hình cần được làm sạch, xử lý giá trị không quan sát được, loại bỏ các giá trị ngoại lai, loại bỏ các biến không cần thiết,… Tuy nhiên, để không lặp lại các kiến thức đã trình bày trong các chương trước, chúng tôi sẽ bỏ qua phần này và trực tiếp đi vào phần xây dựng mô hình.Một lưu ý khác là đa số các hàm số dùng để xây dựng mô hình trên dữ liệu đều đã được phát triển dưới dạng các hàm số có sẵn trên R. Người xây dựng mô hình chỉ cần gọi đúng tên hàm số, khai báo chính xác tham số, và đọc được kết quả trả ra mà không cần phải hiểu chính xác cách viết các hàm số đó như thế nào. Đây là ưu điểm lớn nhất đồng thời cũng là nhược điểm lớn nhất khi sử dụng R để xây dựng mô hình. Là ưu điểm vì bạn đọc chỉ cần một dòng lệnh là đã có thể xây dựng được mô hình phức tạp trên dữ liệu mà không cần hiểu một cách chính xác về mô hình đó. Là nhược điểm bởi vì khi người xây dựng mô hình không hiểu rõ về bản chất có thể dẫn tới sử dụng mô hình không đúng mục đích và dẫn đến các nhận định sai lầm. Để hạn chế được nhược điểm này, trong một số trường hợp, chúng tôi sẽ yêu cầu bạn đọc tự viết các câu lệnh tính toán tham số trước khi gọi các hàm có sẵn.","code":""},{"path":"mô-hình-hồi-quy-tuyến-tính.html","id":"xây-dựng-mô-hình-hồi-quy-đa-biến","chapter":"Chương 10 Mô hình hồi quy tuyến tính","heading":"10.6.1 Xây dựng mô hình hồi quy đa biến","text":"Trước hết chúng ta sẽ xây dựng mô hình tuyến tính đơn biến mà giá nhà phụ thuộc vào một biến có tên là lstat. Các hệ số tuyến tính trong mô hình hồi quy đơn \\(Y \\sim \\beta_0 + \\beta_1 \\cdot X\\) được ước lượng bằng phương pháp bình phương nhỏ nhất được tính toán như sau:\n\\[\\begin{align}\n\\hat{\\beta}_1 = \\cfrac{cov(X,Y)}{var(X)} \\ \\ \\ ; \\ \\ \\ \\hat{\\beta}_0 = \\bar{Y} - \\hat{\\beta}_1 \\cdot \\bar{X}\n\\end{align}\\]Như vậy, chúng ta có mô hình tuyến tính đơn biến mà medv phụ thuộc vào lstat\n\\[\\begin{align}\n\\hat{medv} = 34.5538409 - 0.9500494 \\times \\text{lstat}\n\\end{align}\\]Kết quả mô hình cho thấy khi lstat tăng thêm 1 (%) thì giá nhà trung bình sẽ giảm đi khoảng 0.950 (nghìn USD). Sai số (phần dư) của mô hình là hiệu số giữa giá trị của biến medv được ước lượng từ mô hình và giá trị của medv trong dữ liệu. Chúng ta có đồ thị phần dư như sau\nHình 10.16: Đồ thị phần dư trong mô hình hồi quy đơn medv phụ thuộc vào lstat\nNhìn vào đồ thị phần dư, bạn đọc có thể dễ dàng nhận thấy rằng giả thiết phân phối chuẩn của phần dư và giả thiết các phần dư không tương quan với nhau là không đúng. Điều này có thể cải thiện bằng cách thêm các biến giải thích khác vào mô hình. Chúng ta ước lượng được phương sai (RSE) của phần dư như sau\n\\[\\begin{align}\n\\hat{\\sigma} = RSE = \\sqrt{\\cfrac{RSS}{n-2}}\n\\end{align}\\]\ntrong đó RSS là tổng sai số bình phương và \\(n\\) là số quan sátHệ số R-squared của mô hình được tính toán như sauPhương sai của các hệ số \\(\\beta_0\\), \\(\\beta_1\\), các giá trị của phân phối Student, và p-value được tính toán như sauCó thể thấy rằng các giá trị p-value đều nhỏ, cho thấy các hệ số tuyến tính đều khác không một cách có ý nghĩa.Tất cả các tính toán ở trên đều có thể được thực hiện thông qua hàm có sẵn là lm(). Cách sử dụng hàm lm() xây dựng mô hình tuyến tính mà biến medv phụ thuộc vào biến lstat như sauBạn đọc có thể kiểm tra các tính toán ở trên với kết quả ước lượng từ hàm lm() là hoàn toàn tương tự nhau. Đối tượng lm1 là một list có 12 phần tử trong đó có các phần tử như coefficient chứa giá trị các hệ số ước lượng hay residuals là véc-tơ phần dư.Mô hình tuyến tính đa biến cũng có thể được ước lượng bằng hàm lm() giống như mô hình đơn biến. Chúng ta xây dựng mô hình tuyến tính đa biến mà trong đó biến medv phụ thuộc vào tất cả các biến còn lại như sauTừ giá trị p-value của từng hệ số tuyến tính, có thể nhận thấy rằng hầu hết các biến trong mô hình đều có ý nghĩa ngoại trừ hai biến là indus và age.","code":"\ndat <- Boston\ny <- Boston$medv\nx <- Boston$lstat\nbeta1 <- cov(x,y)/var(x); beta0 <- mean(y) - beta1 * mean(x)\nprint(c(beta0,beta1))## [1] 34.5538409 -0.9500494\nphandu <- (y -  beta0 - beta1 * x)\ndata.frame(x = 1:length(phandu), phandu = phandu)%>%\n  ggplot(aes(x,phandu))+\n  geom_line(color = \"#640514\",alpha = 0.3)+\n  geom_hline(yintercept = 0, col = \"grey30\")+\n  theme_minimal()+\n  xlab(\"Số quan sát\")+ ylab(\"Phần dư\")\nRSS <- sum(phandu^2)\nRSE <- sqrt(RSS/(nrow(Boston)-2))\nprint(paste(\"RSE =\", RSE))## [1] \"RSE = 6.21576040539807\"\nTSS <- sum((y - mean(y))^2)\nR_squared <- 1 - RSS/TSS\nprint(paste(\"Hệ số R-squared: \", R_squared))## [1] \"Hệ số R-squared:  0.54414629758648\"\nn<-nrow(Boston)\nSE_beta0 <- RSE * sqrt(1/n+ mean(x)^2/sum(((x-mean(x))^2)))\nSE_beta1 <- RSE * sqrt(1/sum(((x-mean(x))^2)))\nt_beta0 <- beta0/SE_beta0\nt_beta1 <- beta1/SE_beta1\np_value_beta0 <- 2 * (1-pt(abs(t_beta0), df = n-2))\np_value_beta1 <- 2 * (1-pt(abs(t_beta1), df = n-2))\nprint(paste(\"Các giá trị phân phối student: \", t_beta0, \" - \", t_beta1))## [1] \"Các giá trị phân phối student:  61.4151455186417  -  -24.5278998511877\"\nprint(paste(\"Các giá trị p-value: \", p_value_beta0, \" - \",p_value_beta1))## [1] \"Các giá trị p-value:  0  -  0\"\nlm1<-lm(medv~lstat, data = Boston)\nsummary(lm1)## \n## Call:\n## lm(formula = medv ~ lstat, data = Boston)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -15.168  -3.990  -1.318   2.034  24.500 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(>|t|)    \n## (Intercept) 34.55384    0.56263   61.41   <2e-16 ***\n## lstat       -0.95005    0.03873  -24.53   <2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 6.216 on 504 degrees of freedom\n## Multiple R-squared:  0.5441, Adjusted R-squared:  0.5432 \n## F-statistic: 601.6 on 1 and 504 DF,  p-value: < 2.2e-16\nBoston$chas<-as.factor(Boston$chas)\nlm.all<-lm(medv~., data = Boston)\nsummary(lm.all)## \n## Call:\n## lm(formula = medv ~ ., data = Boston)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -15.595  -2.730  -0.518   1.777  26.199 \n## \n## Coefficients:\n##               Estimate Std. Error t value Pr(>|t|)    \n## (Intercept)  3.646e+01  5.103e+00   7.144 3.28e-12 ***\n## crim        -1.080e-01  3.286e-02  -3.287 0.001087 ** \n## zn           4.642e-02  1.373e-02   3.382 0.000778 ***\n## indus        2.056e-02  6.150e-02   0.334 0.738288    \n## chas1        2.687e+00  8.616e-01   3.118 0.001925 ** \n## nox         -1.777e+01  3.820e+00  -4.651 4.25e-06 ***\n## rm           3.810e+00  4.179e-01   9.116  < 2e-16 ***\n## age          6.922e-04  1.321e-02   0.052 0.958229    \n## dis         -1.476e+00  1.995e-01  -7.398 6.01e-13 ***\n## rad          3.060e-01  6.635e-02   4.613 5.07e-06 ***\n## tax         -1.233e-02  3.760e-03  -3.280 0.001112 ** \n## ptratio     -9.527e-01  1.308e-01  -7.283 1.31e-12 ***\n## black        9.312e-03  2.686e-03   3.467 0.000573 ***\n## lstat       -5.248e-01  5.072e-02 -10.347  < 2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 4.745 on 492 degrees of freedom\n## Multiple R-squared:  0.7406, Adjusted R-squared:  0.7338 \n## F-statistic: 108.1 on 13 and 492 DF,  p-value: < 2.2e-16"},{"path":"mô-hình-hồi-quy-tuyến-tính.html","id":"lựa-chọn-biến-trong-mô-hình-hồi-quy-tuyến-tính","chapter":"Chương 10 Mô hình hồi quy tuyến tính","heading":"10.6.2 Lựa chọn biến trong mô hình hồi quy tuyến tính","text":"Chúng ta sẽ sử dụng RSE trung bình khi thực hiện xác thực chéo làm tiêu chí để lựa chọn biến trong mô hình hồi quy tuyến tính. Hàm số dùng để tạo chỉ số tập xác thực trong xác thực chéo là hàm createFolds() của thư viện caret. Dữ liệu sẽ được chia ngẫu nhiên thành \\(k = 5\\) phần theo biến mục tiêu để đảm bảo phân phối của biến mục tiêu trong từng tập xác thực tương tự nhau.Để tính sai số xác thực chéo, với mỗi \\(\\\\{1, 2, 3, 4, 5\\}\\) chúng ta xây dựng mô hình trên dữ liệu Boston loại trừ đi các quan sát thuộc tập xác thực \\(\\). Sau đó tính toán sai số của mô hình trên tập xác thực thứ \\(\\). Sai số trong bài toán hồi quy được tính bằng RMSE. Sai số xác thực chéo là giá trị trung bình của sai số trên các tập xác thực \\(\\), \\(1 \\leq \\leq 5\\). Ví dụ, chúng ta xác định sai số của mô hình đơn biến gồm một biến giải thích lstat và mô hình tuyến tính đa biến bao gồm tất cả các biến như sau:Dữ liệu Boston bao gồm 13 biến độc lập đó thực hiện lựa chọn biến bằng phương pháp lựa chọn tập con tốt nhất là có thể thực hiện được. Nguyên tắc lựa chọn mô hình là dựa trên sai số xác thực chéo, mô hình có sai số xác thực chéo nhỏ nhất sẽ được lựa chọn. Có \\((2^{13} - 1)\\) mô hình cần được xây dựng và tính toán.Như vậy mô hình tuyến tính có sai số xác thực chéo nhỏ nhất là mô hình có 11 biến liệt kê như trên. Lưu ý rằng, dữ liệu có kích thước không lớn nên kết quả của mô hình sẽ phụ thuộc vào việc chia dữ liệu thành các tập xác thực.Lựa chọn biến cho mô hình tuyến tính bằng phương pháp forward stepwise selection được thực hiện như sauBạn đọc có thể dễ dàng nhận thấy rằng thời gian để thực hiện lựa chọn biến bằng phương pháp forward stepwise selection là nhanh hơn rất nhiều với phương pháp lựa chọn tập hợp con tốt nhất.Tương tự, chúng ta có thể thực hiện lựa chọn biến bằng phương pháp backward stepwise selection được thực hiện như sau","code":"\n# Chia dữ liệu ngẫu nhiên thành 5 phần theo biến medv\nset.seed(10)\nfold_number = 5\nindex <- createFolds(Boston$medv, k = fold_number)\n# Sai số giữa hai véc-tơ số tính bằng RMSE\nRMSE <- function(y,y.hat) sqrt(mean((y - y.hat)^2))\n\n# Hàm số tính sai số xác thực chéo hồi quy tuyến tính\ncv.lm<-function(seed = 10, fold_number = 5, dat, target){\n  # seed: khởi tạo ngẫu nhiên\n  # fold_number: số lượng folds\n  # dat: dữ liệu xây dựng mô hình (X)\n  # target: biến mục tiêu (Y)\n  set.seed(seed)\n  n <- nrow(dat) ; p <- ncol(dat)\n  \n  # Tạo dữ liệu xác thực chéo\n  y <- target\n  x <- dat\n  index <- createFolds(y, k = fold_number)\n  \n  # Véc-tơ sai số xác thực chéo\n  error_reg <- rep(0, fold_number) \n  \n  for (i in 1:fold_number){\n    test.index <- index[[i]] # chỉ số tập xác thực\n    test.index <- (1:n) %in% test.index\n    \n    # Dữ liệu huấn luyện\n    x.train <- x %>% filter(!test.index)\n    y.train <- y[!test.index]\n    \n    # Dữ liệu xác thực\n    x.test <- x %>% filter(test.index)\n    y.test <- y[test.index]\n    \n    # Mô hình tuyến tính\n    lm.model <- lm(y.train~., data = x.train) # ước lượng\n    lm.pred <- predict(lm.model, x.test) # dự đoán\n    error_reg[i] <- RMSE(y.test,lm.pred)                \n  }\n  return(mean(error_reg))\n}\n\n# Sai số xác thực chéo hồi quy đơn\nmydat <- dplyr::select(Boston, lstat)\ny <- Boston$medv\ncv.lm(seed = 10, fold_number = 5, dat = mydat, target = y)## [1] 6.200729\n# Sai số xác thực chéo hồi quy bội\nmydat <- dplyr::select(Boston, -medv) # X\ncv.lm(seed = 10, fold_number = 5, dat = mydat, target = y)## [1] 4.857492\np <- ncol(Boston)-1\nnumber_model <- 2^p - 1\nvar_name <- select(Boston, - medv) %>% names() \ncv.error <- rep(0, number_model)\n\nfor (i in 1:1){\n#for (i in 1:number_model){\n  # Véc-tơ chứa tên các biến được lựa chọn\n  selected_variable <- var_name[as.logical(intToBits(i))[1:p]]\n  y <- Boston$medv\n  mydat <- select(Boston, selected_variable)\n  cv.error[i] <- cv.lm(seed = 10, fold_number = 5, dat = mydat, target = y)\n}\n\n# Mô hình có sai số xác thực chéo nhỏ nhất\nbest_model <- which.min(cv.error)\n# Danh sách các biến trong mô hình\nvar_name[as.logical(intToBits(best_model))[1:p]]## [1] \"zn\"\n# Thứ tự chạy mô hình từ ít đến nhiều biến\nM <- matrix(FALSE,number_model,p)\nfor (i in 1:number_model){ \n  M[i,] <- as.logical(intToBits(i))[1:p]\n}\nM <- M[order(apply(M,1,sum)),]\n\n# Kết quả của quá trình forward stepwise được lưu\nfws_result <- data.frame(Model_number = rep(0, p*(p-1)/2), \n                        Number_variable = rep(0, p*(p-1)/2), \n                        CV_error = rep(0, p*(p-1)/2))\ncurrent_best_model <- c()\nj <- 0\n# Lựa chọn mô hình\nfor (i in 1:number_model){\n  number_selected_variable <- sum(M[i,])\n  current_select <- var_name[M[i,]]\n  \n  if ( (i == 1) | all(current_best_model %in% current_select) ){\n    j <- j+1\n    mydat <- select(Boston, current_select)\n    cv.error <- cv.lm(seed = 10, fold_number = 5, dat = mydat, target = y)\n    fws_result[j,] <- c(i, number_selected_variable, cv.error)\n  }  \n  \n  # Lưu lại mô hình có các biến tốt nhất\n  if (i < number_model){\n    if (sum(M[i+1,])-sum(M[i,]) ==  1){\n      dat <- filter(fws_result, Number_variable == number_selected_variable)\n      ind_best_model <- dat$Model_number[which.min(dat$CV_error)]\n      current_best_model <- var_name[M[ind_best_model,]]\n    }\n  } \n}\n# Thứ tự chạy mô hình từ ít đến nhiều biến\nM <- matrix(FALSE,number_model,p)\nfor (i in 1:number_model){ \n  M[i,] <- as.logical(intToBits(i))[1:p]\n}\nM <- M[order(apply(M,1,sum),decreasing = TRUE),]\n\n# Kết quả của quá trình forward stepwise được lưu\nbws_result <- data.frame(Model_number = rep(0, p*(p-1)/2), \n                         Number_variable = rep(0, p*(p-1)/2), \n                         CV_error = rep(0, p*(p-1)/2))\ncurrent_best_model <- var_name\nj <- 0\n# Lựa chọn mô hình\nfor (i in 1:number_model){\n  number_selected_variable <- sum(M[i,])\n  current_select <- var_name[M[i,]]\n  \n  if ( (i == p) | all(current_select %in% current_best_model) ){\n    j <- j+1\n    mydat <- select(Boston, current_select)\n    cv.error <- cv.lm(seed = 10, fold_number = 5, dat = mydat, target = y)\n    bws_result[j,] <- c(i, number_selected_variable, cv.error)\n  }  \n  \n  # Lưu lại mô hình có các biến tốt nhất\n  if (number_selected_variable > 1){\n    if (sum(M[i,])-sum(M[i+1,]) ==  1){\n      dat <- filter(bws_result, Number_variable == number_selected_variable)\n      ind_best_model <- dat$Model_number[which.min(dat$CV_error)]\n      current_best_model <- var_name[M[ind_best_model,]]\n    }\n  } \n}"},{"path":"mô-hình-hồi-quy-tuyến-tính.html","id":"hồi-quy-ridge-và-lasso","chapter":"Chương 10 Mô hình hồi quy tuyến tính","heading":"10.6.3 Hồi quy ridge và lasso","text":"Hồi quy ridge và lasso được thực hiện bằng hàm số glmnet() của thư viện cùng tên glmnet. Tham số \\(\\lambda\\) của các phương pháp này được tìm kiếm bằng phương pháp xác thực chéo. Hàm số thực hiện tính toán sai số xác thực chéo của hồi quy ridge và lasso là hàm cv.glmnet().Sai số xác thực chéo tính bằng MSE được lưu trong véc-tơ con có tên là cvm. Chúng ta có sai số xác thực chéo cho mỗi giá trị của \\(\\lambda\\) của hồi quy ridge và lasso như hình (??)\nHình 10.17: Sai số xác thực chéo tương ứng với mỗi giá trị của siêu tham số lambda trong hồi quy ridge và lasso.\nCó thể thấy rằng trong hồi quy ridge và lasso, tham số \\(\\lambda\\) cho sai số xác thực chéo nhỏ nhất lần lượt là 0.14 và 0.027. Sai số xác thực chéo trong hồi quy ridge và lasso không tốt hơn với mô hình hồi quy tuyến tính bội.các phương pháp ràng buộc tham số sẽ tránh được hiện tượng khớp dữ liệu quá mức, chúng ta có thể tạo thêm các biến giải thích cho biến giá nhà bằng cách nhân chéo các biến giải thích hiện có. Tổng số biến mới được tạo thành là \\(p\\times(p+1)/2\\).Chúng ta có sai số xác thực chéo cho mỗi giá trị của \\(\\lambda\\) của hồi quy ridge và lasso với dữ liệu sau khi biến đổi như hình (??)\nHình 10.18: Sai số xác thực chéo tương ứng với mỗi giá trị của siêu tham số lambda trong hồi quy ridge và lasso.\nCó thể thấy rằng việc áp dụng hồi quy ridge và lasso trên dữ liệu có số lượng lớn biến giải thích mới được tạo thành bằng cách nhân chéo các biến hiện có không chỉ tránh được hiện tương mô hình bị khớp quá mức mà còn cải thiện khả năng dự đoán của mô hình. Sai số xác thực chéo đã giảm đáng kể với mô hình chỉ sử dụng các biến ban đầu.","code":"\nlibrary(glmnet)\n\n# Chuẩn hóa dữ liệu về dạng số\ny <- Boston$medv\nx <- model.matrix(medv~.,Boston)\n\n# Các giá trị lambda\nr.lambda<-seq(0,0.3,length=100)\nl.lambda<-seq(0,0.1,length=100)\nset.seed(50)\n\n# Xác thực chéo với ridge (tham số alpha = 0)\ncv.ridge <- cv.glmnet(x, y, alpha = 0, \n                  nfolds = 5, lambda=r.lambda)\n\n\n# Xác thực chéo với lasso (tham số alpha = 1)\ncv.lasso <- cv.glmnet(x, y, alpha = 1, \n                  nfolds = 5, lambda=l.lambda)\ndat <- select(Boston, -medv)\np <- ncol(dat)\n\ndat <- sapply(dat,  \n             function(x){\n               x <- as.numeric(x)\n               return ((x - mean(x))/sd(x))\n             })\n\ndat<-as.data.frame(dat)\n\nfor (i in 1:p){\n  for (j in i:p){\n    dat <- mutate(dat, newcol = dat[,i]*dat[,j])\n    names(dat)[length(dat)] <- paste0(\"X\",i,\"_\",j)\n  }\n}\ndat<-mutate(dat, \"medv\" = Boston$medv)\n# Chuẩn hóa dữ liệu về dạng số\ny <- dat$medv\nx <- model.matrix(medv~.,dat)\n\n# Các giá trị lambda\nr.lambda<-seq(0,0.5,length=100)\nl.lambda<-seq(0,0.2,length=100)\nset.seed(50)\n\n# Xác thực chéo với ridge (tham số alpha = 0)\ncv.ridge <- cv.glmnet(x, y, alpha = 0, \n                  nfolds = 5, lambda=r.lambda)\n\n\n# Xác thực chéo với lasso (tham số alpha = 1)\ncv.lasso <- cv.glmnet(x, y, alpha = 1, \n                  nfolds = 5, lambda=l.lambda)"},{"path":"mô-hình-hồi-quy-tuyến-tính.html","id":"bài-tập-2","chapter":"Chương 10 Mô hình hồi quy tuyến tính","heading":"10.7 Bài tập","text":"","code":""},{"path":"mô-hình-hồi-quy-tuyến-tính.html","id":"bài-tập-lý-thuyết","chapter":"Chương 10 Mô hình hồi quy tuyến tính","heading":"10.7.1 Bài tập lý thuyết","text":"","code":""},{"path":"mô-hình-hồi-quy-tuyến-tính.html","id":"bài-tập-thực-hành","chapter":"Chương 10 Mô hình hồi quy tuyến tính","heading":"10.7.2 Bài tập thực hành","text":"","code":"\nlibrary(readxl)\nlibrary(dplyr)## \n## Attaching package: 'dplyr'## The following objects are masked from 'package:stats':\n## \n##     filter, lag## The following objects are masked from 'package:base':\n## \n##     intersect, setdiff, setequal, union\nlibrary(knitr)\nlibrary(kableExtra)## \n## Attaching package: 'kableExtra'## The following object is masked from 'package:dplyr':\n## \n##     group_rows\nlibrary(ggplot2)\nlibrary(forcats)\nlibrary(ggpubr)\nlibrary(grid)\nlibrary(gridExtra)## \n## Attaching package: 'gridExtra'## The following object is masked from 'package:dplyr':\n## \n##     combine\nlibrary(forcats)\nlibrary(pryr)## \n## Attaching package: 'pryr'## The following object is masked from 'package:dplyr':\n## \n##     where\nlibrary(RColorBrewer)\nlibrary(mvtnorm)\nlibrary(caret)## Loading required package: lattice\nlibrary(latex2exp)\nlibrary(gam)## Loading required package: splines## Loading required package: foreach## Loaded gam 1.22-3\ncolorize_style = function(x, color = \"#640514\", font = \"Source Code Pro\", style = NULL) {\n  apply_style = function(text, style) {\n    if (!is.null(style)) {\n      if (style == \"bold\") {\n        text = sprintf(\"\\\\textbf{%s}\", text)\n      }else if (style == \"it\") {\n      text = sprintf(\"\\\\textit{%s}\", text)\n      }else if (style == \"under\") {\n      text = sprintf(\"\\\\underline{%s}\", text)\n      }\n    }\n    return(text)\n  }\n  if(knitr::is_latex_output()){\n    x = apply_style(x, style)\n    if (!is.null(font)) {\n      sprintf(\"\\\\textcolor{%s}{\\\\textsf{%s}{%s}}\", color, font, x)\n    }else {\n      sprintf(\"\\\\textcolor{%s}{%s}\", color, x)\n    }\n  }else if (knitr::is_html_output()){\n    if(!is.null(style)){\n      if(style == \"bold\"){\n        x = sprintf(\"<strong>%s<\/strong>\", x)\n      }else if (style == \"it\"){\n        x = sprintf(\"<em>%s<\/em>\", x)\n      }else if (style == \"under\"){\n        x = sprintf(\"<span style='text-decoration: underline;'>%s<\/span>\", x)\n      }\n    }\n    if (!is.null(font)){\n    sprintf(\"<span style='color: %s; font-family: %s;'>%s<\/span>\", color, font, x)\n    }else{\n    sprintf(\"<span style='color: %s;'>%s<\/span>\", color, x)\n    }\n  }else{\n  x\n  }\n}"},{"path":"các-mô-hình-cộng-tính-tổng-quát.html","id":"các-mô-hình-cộng-tính-tổng-quát","chapter":"Chương 11 Các mô hình cộng tính tổng quát","heading":"Chương 11 Các mô hình cộng tính tổng quát","text":"Trong chương trước, chúng ta đã nghiên cứu về mô hình hồi quy tuyến tính. Đây là lớp các mô hình tương đối đơn giản để hiểu và thực hiện, đồng thời có ưu điểm hơn các phương pháp tiếp cận khác dễ dàng diễn giải và suy luận. Tuy nhiên, các mô hình hồi quy tuyến tính thông thường có thể có những hạn chế về khả năng dự đoán. Điều này là giả định tuyến tính hiếm khi xảy ra trong dữ liệu thực tế. Chúng ta cũng đã nghiên cứu một vài phương pháp để có thể cải thiện khả năng dự báo của các mô hình tuyến tính bằng cách sử dụng hồi quy ridge, hay Lasso …, mà trong đó, khả năng dự báo được cải thiện bằng cách giảm bậc tự của mô hình tuyến tính với mục đích giảm phương sai của mô hình. Tuy nhiên, các phương pháp cải thiện mô hình đó vẫn giữ nguyên giả thuyết tuyến tính.Trong chương này, chúng tôi sẽ từng bước nới lỏng giả định tuyến tính trong xây dựng mô hình trong khi vẫn cố gắng duy trì khả năng diễn giải nhiều nhất có thể bằng cách giữ nguyên nguyên tắc cộng tính trong xây dựng mô hình. Chúng ta sẽ bắt đầu chương sách này với các mở rộng đơn giản của các mô hình tuyến tính bao gồm hồi quy đa thức, hồi quy theo hàm bậc thang, sau đó chuyển sang các phương pháp phức tạp hơn như spline, hồi quy cục bộ và sau cùng là mô hình cộng tính tổng quát.Dữ liệu chúng tôi sử dụng để minh họa các mô hình là dữ liệu về giá nhà tại Boston trong thư viện MASS. Bạn đọc tham khảo mô tả dữ liệu trên R hoặc xem lại chương mô hình tuyến tính để hiểu thêm về dữ liệu.","code":""},{"path":"các-mô-hình-cộng-tính-tổng-quát.html","id":"hồi-quy-splines","chapter":"Chương 11 Các mô hình cộng tính tổng quát","heading":"11.1 Hồi quy splines","text":"Khi thảo luận về xây dựng mô hình tuyến tính, chúng tôi đã đề cập đến vấn đề khi tồn lại mối liên hệ phi tuyến giữa biến mục tiêu và biến giải thích. Một phương pháp giải quyết vấn đề này là mở rộng hồi quy tuyến tính mà trong đó biến mục tiêu được mô tả thông qua biến giải thích và các hàm mũ của biến đó, hay nói một cách khác là được mô tả bằng một đa thức của biến giải thích. Ví dụ như chúng ta thay thế mô hình tuyến tính đơn biến\n\\[\\begin{align}\ny_i = \\beta_0 + \\beta_1 \\cdot x_i + \\epsilon_i\n\\end{align}\\]\nbằng một mô hình hồi quy đa thức\n\\[\\begin{align}\ny_i = \\beta_0 + \\beta_1 \\cdot x_i + \\beta_2 \\cdot x_i^2 + \\cdots + \\beta_d \\cdot x_i^d + \\epsilon_i\n\\tag{11.1}\n\\end{align}\\]\nvới \\(\\epsilon_i\\) là phần dư và \\(d\\) là bậc của đa thức. Khi bậc của đa thức \\(d\\) là lớn, đa thức sẽ có càng nhiều điểm uốn và độ cong đủ lớn để mô tả các mối liên hệ phi tuyến. Lưu ý rằng các hệ số trong (11.1) có thể được ước lượng dễ dàng bằng cách sử dụng phương pháp bình phương nhỏ nhất vì đây là mô hình tuyến tính thông thường với các biến giải thích \\(x_i, x_i^2, \\cdots , x_i^d\\). Khi sử dụng hồi quy đa thức cần lưu ý là khi sử dụng bậc của đa thức quá lớn, ví dụ như \\(d \\geq 4\\), thì đa thức sẽ có hình dạng khá kỳ lạ tại các điểm giới hạn của biến giải thích. Các điểm giới hạn bao gồm các điểm dữ liệu rất nhỏ và rất lớn của biến giải thích.\nHình 11.1: Hồi quy đa thức biến giá nhà (nghìn USD) theo biến tỷ lệ người sống dưới mức trung bình (%) trên dữ liệu Boston. Hình bên trái: sử dụng đa thức bậc ba. Hình ở giữa: sử dụng đa thức bậc bốn. Hình bên phải: sử dụng đa thức bậc năm\nHình 11.1 mô tả mô hình hồi quy đa thức trong đó biến mục tiêu là giá nhà (nghìn USD) theo biến tỷ lệ người sống dưới mức trung bình (%) trên dữ liệu về giá nhà tại Boston. Hình bên trái cho thấy sử dụng đa thức bậc ba mô tả khá đầy đủ hình dạng mối liên hệ tuyến tính giữa hai biến: giá nhà có xu hướng giảm tại các khu vực có tỷ lệ người sống dưới mức trung bình lớn. Tốc độ giảm của giá nhà theo biến giải thích có sự khác biệt, giá nhà giảm nhanh khi tỷ lệ sống dưới mức trung bình tăng từ 5% lên 15%, tốc độ giảm chậm dần khi biến giải thích nhận giá trị trong khoảng 15% đến 25%, sau đó tốc độ giảm của giá nhà lại tăng khi tỷ lệ sống dưới mức trung bình cao hơn 25%. Hình ở giữa trong Hình 11.1 sử dụng đa thức bậc bốn. Bạn đọc có thể nhận thấy ngay rằng đa thức bậc bốn là không phù hợp để mô tả mối liên hệ giữa biến mục tiêu và biến giải thích khi hàm số có giá trị tăng trên khoảng tỷ lệ sống dưới mức trung bình cao hơn 30%. Hình bên phải trong Hình 11.1 sử dụng đa thức bậc năm để mô tả mối liên hệ giữa giá nhà và tỷ lệ sống dưới mức trung bình. Không có sự khác biệt nhiều giữa đa thức bậc ba và đa thức bậc năm trong miền 5% đến 30%, tuy nhiên đa thức bậc năm lại cho hình dạng kỳ lạ khi biến giải thích lớn hơn 25%!Nhìn chung, kinh nghiệm cho thấy rằng sử dụng đa thức bậc lớn hơn ba trong hồi quy đa thức thường không đem lại hiệu quả trong mô tả dữ liệu. Thay vì tăng bậc của đa thức để giải thích tốt hơn mối liên hệ giữa biến mục tiêu và biến giải thích, những người xây dựng mô hình sử dụng một dạng hàm \\(f(x)\\) mà với mỗi khoảng giá trị khác nhau của \\(x\\) hàm \\(f\\) là một đa thức khác nhau. Nói một cách khác, mối liên hệ giữa \\(Y\\) và \\(X\\) được mô tả bằng một splines. Thay vì tăng bậc cho đa thức bậc ba:\n\\[\\begin{align}\ny_i = \\beta_0 + \\beta_1 \\cdot x_i + \\beta_2 \\cdot x_i^2 +\\beta_3 \\cdot x_i^3 + \\epsilon_i\n\\tag{11.2}\n\\end{align}\\]\nchúng ta có thể thay thế bằng cách hồi quy hai đa thức bậc ba trên hai miền giá trị khác nhau của \\(x_i\\)\n\\[\\begin{align}\ny_i = \\begin{cases}\n\\beta_{01} + \\beta_{11} \\cdot x_i + \\beta_{21} \\cdot x_i^2 +\\beta_{31} \\cdot x_i^3 + \\epsilon_i \\text{ nếu } x_i < c \\\\\n\\beta_{02} + \\beta_{12} \\cdot x_i + \\beta_{22} \\cdot x_i^2 +\\beta_{32} \\cdot x_i^3 + \\epsilon_i \\text{ nếu } x_i \\geq c \\\\\n\\end{cases}\n\\tag{11.3}\n\\end{align}\\]Chúng ta chia dữ liệu thành hai phần: phần dữ liệu thứ nhất bao gồm các quan sát có \\(x_i < c\\) và phần dữ liệu thứ hai bao gồm các quan sát có \\(xi \\geq c\\). Đa thức bậc ba thứ nhất có các hệ số \\(\\beta_{01}\\), \\(\\beta_{11}\\), \\(\\beta_{21}\\) và \\(\\beta_{31}\\) được ước lượng trên phần dữ liệu thứ nhất và đa thức bậc ba thứ hai có các hệ số \\(\\beta_{02}\\), \\(\\beta_{12}\\), \\(\\beta_{22}\\) và \\(\\beta_{32}\\) được ước lượng trên phần dữ liệu thứ hai. Cả hai đa thức đều có thể được ước lượng bằng cách sử dụng phương pháp bình phương nhỏ nhất giống như trong hồi quy đa biến. Điểm \\(c\\) chia dữ liệu làm hai miền được gọi là một nút hay một điểm cắt. Việc sử dụng nút sẽ giúp cho mô hình linh hoạt hơn là tăng bậc của đa thức và sử dụng càng nhiều nút sẽ càng làm cho hàm \\(f\\) trở nên linh hoạt. Nếu chúng ta sử dụng \\(k\\) nút khác nhau trên miền giá trị của biến \\(X\\) thì chúng ta có \\(k+1\\) miền dữ liệu và tương ứng là \\(k+1\\) đa thức cần ước lượng. Lưu ý rằng chúng ta không nhất thiết phải sử dụng đa thức bậc ba. Thay vào đó chúng ta có thể sử dụng các hàm tuyến tính hoặc đa thức bậc hai, hoặc thậm chí là một hằng số (đa thức bậc không) trên từng phần của dữ liệu.Hình ?? mô tả sử dụng hồi quy đa thức các đa thức bậc khác nhau trên từng phần dữ liệu. Mặc dù đã hạn chế được hình dạng kỳ lạ của tại các điểm giới hạn của biến \\(X\\), nhưng bạn đọc có thể nhận thấy ngay vấn đề: các đa thức có giá trị không liên tục tại điểm cắt và nếu chúng ta sử dụng hai đa thức bậc ba, sẽ có tổng số tám tham số hay tám bậc tự để mô tả mối liên hệ giữa biến mục tiêu và biến giải thích. Một cách tổng quát, nếu chúng ta sử dụng \\(k\\) nút, và các đa thức từng phần đều là các đa thức bậc ba, thì sẽ có tổng số \\(4 \\times (k+1)\\) tham số cần được ước lượng. Việc này rất dễ dẫn đến hiện tượng mô hình khớp quá mức.Để khắc phục vấn đề giá trị của các đa thức không liên tục tại các điểm cắt, trong quá trình ước lượng tham số chúng ta có thể thêm vào rằng buộc là giá trị của các đa thức tại các điểm cắt phải bằng nhau. Ngoài ràng buộc giá trị của đa thức bằng nhau tại nút \\(c\\), người xây dựng mô hình còn thêm các rằng buộc về sự liên tục của đạo hàm bậc một và đạo hàm bậc hai của các đa thức. Nói cách khác ước lượng các tham số \\(\\beta_{01}\\), \\(\\beta_{11}\\), \\(\\beta_{21}\\) và \\(\\beta_{31}\\) của đa thức thứ nhất và các tham số \\(\\beta_{02}\\), \\(\\beta_{12}\\), \\(\\beta_{22}\\) và \\(\\beta_{32}\\) của đa thức thứ hai trở thành bài toán tối ưu:\n\\[\\begin{align}\n\\hat{\\boldsymbol{\\beta}} = \\underset{\\boldsymbol{\\beta}}{\\operatorname{argmin}} \\sum\\limits_{=1}^n \\left[\\mathbb{}_{\\{x_i < c\\}} \\left(y_i - \\beta_{01} - \\beta_{11} \\cdot x_i - \\beta_{21} \\cdot x_i^2 - \\beta_{31} \\cdot x_i^3 \\right)^2 + \\\\\n\\ \\ \\ \\ \\ \\ \\ \\ \\  \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\mathbb{}_{\\{x_i \\geq c\\}} \\left(y_i - \\beta_{02} - \\beta_{12} \\cdot x_i - \\beta_{22} \\cdot x_i^2 - \\beta_{32} \\cdot x_i^3 \\right)^2 \\right]\n\\end{align}\\]\nvới các ràng buộc\n\\[\\begin{align}\n& \\beta_{01} + \\beta_{11} \\cdot c + \\beta_{21} \\cdot c^2 +\\beta_{31} \\cdot c^3 = \\beta_{02} + \\beta_{12} \\cdot c + \\beta_{22} \\cdot c^2 +\\beta_{32} \\cdot c^3 \\\\\n& \\beta_{11}  + 2 \\beta_{21} \\cdot c + 3 \\beta_{31} \\cdot c^2 = \\beta_{12} + 2 \\beta_{22} \\cdot c +3 \\beta_{32} \\cdot c^2 \\\\\n& 2 \\beta_{21} + 6 \\beta_{31} \\cdot c = 2 \\beta_{22}  + 6 \\beta_{32} \\cdot c\n\\tag{11.4}\n\\end{align}\\]Mỗi ràng buộc mà chúng ta áp đặt lên các đa thức bậc ba sẽ làm số lượng tham số, hay bậc tự của mô hình, giảm đi một bậc tự . Điều này cũng đồng nghĩa với việc giảm đi sự phức tạp của mô hình và tránh được hiện tượng mô hình khớp quá mức. Với ba ràng buộc trong phương trình (11.4) bao gồm ràng buộc về sự liên tục của đa thức, của đạo hàm bậc nhất và của đạo hàm bậc hai, mô hình sẽ còn năm bậc tự . Hàm số được xây dựng trên cơ sở các đa thức từng phần bậc \\(d\\) với các ràng buộc về sự liên tục của đạo hàm đến bậc \\(d-1\\) tại các nút được gọi chung là các splines bậc \\(d\\). Về lý thuyết bạn đọc có thể chọn \\(d \\geq 4\\) nhưng kinh nghiệm cho thấy bậc của splines không nên vượt quá \\(d = 3\\). Nếu muốn tăng sự phức tạp cho splines, giải pháp là tăng số nút chứ không nên tăng bậc. Trong trường hợp tổng quát, một splines bậc ba với \\(k\\) nút sẽ có \\(4 \\times (k+1)\\) tham số cần được ước lượng, đồng thời có \\(3 \\times k\\) ràng buộc tại \\(k\\) nút, đó số bậc tự sẽ là \\(k + 4\\). Để ước lượng tham số của splines, chúng ta không giải bài toán tối ưu có ràng buộc như phương trình (11.4) mà thực hiện biến đổi tham số và sau đó sử dụng phương pháp bình phương nhỏ nhất thông thường. Bạn đọc tham khảo phần 11.6.1 để hiểu về cách biến đổi tham số.\nHình 11.2: Hồi quy giá nhà (medv) theo splines bậc ba của biến giải thích là tỷ lệ người sống dưới mức trung bình (lstat) trong dữ liệu Boston. Hình bên trái: sử dụng một nút tại 15%. Hình bên phải: sử dụng hai nút tại 10% và 20%\nHình 11.2 mô tả mô hình hồi quy splines giá nhà theo tỷ lệ người sống dưới mức trung bình trên dữ liệu Boston. Hình bên trái mô tả biến mục tiêu là một splines bậc ba với một nút duy nhất là \\(c = 15\\%\\); Hình bên phải mô tả splines bậc ba với hai nút là \\(c_1 = 10\\%\\) và \\(c_2 = 20\\%\\). Với các ràng buộc về sự liên tục của các đa thức và đạo hàm đến bậc hai tại các nút, không thể nhận ra sự khác biệt của các đa thức tại các nút.Khi xây dựng một splines, chúng ta cần trả lời câu hỏi là nên đặt bao nhiêu nút hoặc đặt các nút ở đâu? Tại sao trong ví dụ kể trên chúng ta lại sử dụng một nút tại 15%, hay sử dụng hai nút tại 10% và 20% ? Và sau cùng là giữa các cách đặt nút như vậy thì cách nào tối ưu hơn. Trước hết, có thể thấy rằng đường hồi quy linh hoạt hơn ở những khoảng giá trị có nhiều nút vì ở những khoảng giá trị đó hệ số đa thức có thể thay đổi nhanh chóng. đó, một gợi ý cho việc đặt nút là nên cho nhiều nút hơn ở những nơi mà chúng ta nhận thấy hàm số có thể thay đổi nhanh nhất và đặt ít nút thắt hơn ở những nơi có vẻ ổn định hơn. Hướng tiếp cận này khá cảm tính và đòi hỏi người xây dựng mô hình cần có nhiều kinh nghiệm.Một tiếp cận khác khi đặt nút là dựa theo các quantile của biến giải thích. Khi đặt \\(k\\) nút \\(c_1 < c_2 < \\cdots < c_k\\), nút \\(c_j\\) sẽ là giá trị quantile tương ứng với mức xác suất \\(\\frac{j}{k+1}\\), hay nói một cách khác, có \\(\\frac{n \\times j}{k+1}\\) quan sát của biến giải thích nhỏ hơn \\(c_j\\) và \\(\\frac{n\\times(k+1-j)}{k+1}\\) quan sát của biến giải thích lớn hơn \\(c_j\\) với \\(n\\) là số lượng quan sát.Không có câu trả lời chính xác cho câu hỏi là cần đặt bao nhiêu nút khi xây dựng splines. Kinh nghiệm cho thấy rằng sử dụng xác thực chéo để lựa chọn số lượng nút thường cho lựa chọn tốt. Tham số \\(k\\) tương ứng với sai số xác thực chéo nhỏ nhất sẽ là số nút tối ưu. Tuy nhiên xác thực chéo chỉ có thể thực hiện khi có một hoặc một vài biến giải thích. Khi chúng ta xây dựng mô hình mà biến mục tiêu phụ thuộc vào một số lượng lớn biến giải thích, sử dụng xác thực chéo để lựa chọn số lượng nút cho từng biến giải thích sẽ yêu cầu khối lượng tính toán tăng nhanh theo hàm mũ.\nHình 11.3: Lựa chọn số nút tối ưu bằng xác thực chéo khi hồi quy giá nhà (medv) theo splines của biến giải thích là tỷ lệ số người có mức sống dưới mức trung bình (lstat). Tham số của xác thực chéo (số folds) được lựa chọn là K = 5. Hình bên trái sử dụng splines bậc hai. Hình bên phải sử dụng splines bậc ba.\nHình 11.3 mô tả cách lựa chọn số nút cho mô hình hồi quy biến mục tiêu giá nhà theo splines của biến giải thích là tỷ lệ số người có mức sống dưới trung bình bằng cách sử dụng xác thực chéo. Chúng tôi sử dụng xác thực chéo với tham số \\(K\\) bằng 5. Nếu splines có bậc hai, số lượng nút cho sai số xác thực chéo nhỏ nhất là \\(k = 8\\). Còn khi splines có bậc ba, số lượng nút cho sai số của xác thực chéo nhỏ nhất là \\(k=5\\). Có thể giải thích là khi bậc của splines nhỏ hơn, các đa thức từng phần sẽ ít linh hoạt hơn, đó cần số nút lớn hơn để mô tả tốt mối liên hệ phi tuyến giữa biến mục tiêu và biến giải thích.Khái niệm cuối cùng mà chúng tôi muốn giới thiệu đến bạn đọc trước khi chuyển sang phần chính của chương là khái niệm về natural splines. Bạn đọc có thể nhận thấy rằng với hầu hết các dữ liệu, tại các giá trị giới hạn của biến giải thích, mật độ của các điểm dữ liệu thường khá thưa thớt. đó nếu sử dụng các đa thức bậc lớn hơn hoặc bằng hai để mô tả mối liên hệ giữa biến mục tiêu và biến giải thích có thể dẫn đến các đường cong có hình dạng kỳ lạ. Để tránh gặp phải tình trạng như vậy, người xây dựng mô hình sẽ thêm vào các rằng buộc rằng đa thức phải có bậc một tại các giá trị giới hạn của biến giải thích. Nói một cách khác, hàm \\(f\\) phải là hàm tuyến tính trong các vùng biến giải thích \\(X\\) nhỏ hơn nút nhỏ nhất và \\(X\\) lớn hơn nút lớn nhất. Natural splines đơn giản là một splines với ràng buộc tuyến tính tại các giá trị giới hạn của biến giải thích. Ràng buộc bổ sung này giúp cho hàm \\(f\\) tự nhiên hơn và tạo ra các ước tính ổn định hơn ở điểm biên.Sử dụng natural splines dựa trên đa thức bậc ba thay thế cho Splines bậc ba sẽ giúp giải phóng bốn bậc tự vì ở mỗi vùng giới hạn chúng ta giảm đi hai tham số. Nói một cách khác, natural splines bậc ba sẽ có \\((k+4)-4 = k\\) tham số. Ước lượng tham số cho natural splines không được thực hiện thông qua giải bài toán tối ưu mà được thông qua phép biến đổi tham số giống như khi ước lượng tham số cho splines. Bạn đọc tham khảo chi tiết tại 11.6.2.\nHình 11.4: sánh giữa splines thông thường và natural splines khi sử dụng véc-tơ các nút giống nhau. Hình bên trái: Giá nhà tại Boston là một splines bậc ba theo tỷ lệ sống dưới mức trung bình; splines sử dụng 5 nút tại các giá trị quantile tương ứng với các mức xác suất 1/6, 2/6, 3/6, 4/6, 5/6 của biến giải thích. Hình bên phải: Giá nhà tại Boston là một natural splines; splines sử dụng 5 nút tại các giá trị quantile tương ứng với các mức xác suất 1/6, 2/6, 3/6, 4/6, 5/6 của biến giải thích.\nBạn đọc có thể nhận thấy từ Hình 11.4 rằng natural splines sẽ cho kết quả là một hàm tuyến tính tại các vùng mà biến giải thích lớn hơn nút nhỏ nhất hoặc lớn hơn nút lớn nhất. Không có sự khác biệt nhiều giữa splines thông thường và natural splines trong khoảng biến giải thích nhỏ hơn nút nhỏ nhất, tuy nhiên có sự khác biệt rõ ràng trong vùng biến giải thích lớn hơn nút lớn nhất. Trong vùng này, splines thông thường cho thấy xu thế giảm nhanh sau đó đi ngang và cuối cùng là đi lên khi biến giải thích tăng dần. Trong khi đó khi sử dụng natural splines chỉ có một xu thế duy nhất là giảm tại vùng giá trị giới hạn này. Cần dựa trên sai số khi thực hiện xác thực chéo để biết mô hình nào tốt hơn thay vì dựa trên các nhận xét cảm tính, tuy nhiên chắc chắn rằng mô hình được xây dựng từ natural spline sẽ cho dự đoán ổn định hơn, hay nói một cách khác là có phương sai nhỏ hơn với splines thông thường.Trong phần tiếp theo, chúng ta sẽ thảo luận về một hướng tiếp cận khác khi xây dựng mô hình mô tả mối liên hệ phi tuyến giữa biến mục tiêu và biến giải thích nhưng cũng cho kết quả là một splines. Kết quả này còn được biết đến với tên gọi là smoothing splines.","code":""},{"path":"các-mô-hình-cộng-tính-tổng-quát.html","id":"smoothing-splines","chapter":"Chương 11 Các mô hình cộng tính tổng quát","heading":"11.2 Smoothing splines","text":"Trong các phần trước, chúng ta đã thảo luận về cách xây dựng các đường hồi quy phi tuyến tính được tạo ra bằng cách sử dụng một tập hợp các nút của biến giải thích và các đa thức bậc \\(d\\) trên các vùng được xác định bởi các nút. Tham số của các đường hồi quy phi tuyến được ước lượng bằng phương pháp bình phương nhỏ nhất sau khi thực hiện phép biến đổi tham số. Trong phần này, chúng tôi giới thiệu một cách tiếp cận khác nhưng cũng cho kết quả là một splines. Một cách tổng quát, mục tiêu khi chúng ta muốn xây dựng hàm số mô tả mối liên hệ giữa biến mục tiêu \\(Y\\) và biến giải thích \\(X\\), tạm gọi là hàm \\(f\\), để phù hợp tối đa với dữ liệu được quan sát: nghĩa là chúng ta muốn\\[\\begin{align}\nRSS = \\sum\\limits_{=1}^n (y_i − f(x_i))^2\n\\end{align}\\]càng nhỏ càng tốt. Tuy nhiên, nếu chúng ta không đặt bất kỳ ràng buộc nào lên hàm \\(f\\) thì chúng ta luôn có thể làm cho RSS bằng 0 bằng cách chọn \\(f\\) đủ phức tạp sao cho \\(f(x_i) = y_i \\ \\forall \\). Một hàm \\(f\\) như vậy sẽ quá phù hợp với dữ liệu huấn luyện mô hình nhưng sẽ không cho kết quả tốt trên dữ liệu kiểm thử mô hình. Hàm \\(f\\) mà chúng ta thực sự cần xây dựng là một hàm làm cho RSS nhỏ nhưng cũng cần có sự ràng buộc về sự linh hoạt của hàm \\(f\\). Đường hồi quy được xây dựng trong Hình 11.5 mô tả một hàm \\(f\\) quá khớp với dữ liệu huấn luyện mô hình. Rất khó để các hàm như vậy có thể cho kết quả tốt cho kết quả tốt trên dữ liệu kiểm tra mô hình phương sai của hàm \\(f\\) là quá lớn.\nHình 11.5: Xây dựng hàm f quá linh hoạt. Sai số trên tập huấn luyện mô hình sẽ nhỏ nhưng sai số trên dữ liệu kiểm tra mô hình sẽ lớn.\nLàm thế nào chúng ta có thể đảm bảo rằng hàm \\(f\\) đạt được mức độ linh hoạt cần thiết để mô tả được mối liên hệ giữa biến mục tiêu và biến giải thích nhưng cũng không quá linh hoạt vì dễ dẫn đến mô hình khớp quá mức? Câu trả lời là cần có ràng buộc cho sự linh hoạt của hàm \\(f\\).Nếu như đạo hàm của hàm \\(f\\) tại điểm \\(x_i\\) cho biết độ dốc của hàm \\(f\\) tại điểm này, thì đạo hàm bậc hai của hàm \\(f\\) cho biết độ dốc của hàm \\(f\\) thay đổi nhanh hay chậm. Trên một khoảng giá trị \\([,b]\\) bất kỳ, nếu tổng giá trị tuyệt đối, hoặc tổng bình phương, của các đạo hàm bậc hai của một hàm càng lớn thì hàm số đó sẽ càng linh hoạt. Nói một cách khác, hàm số \\(f\\) sẽ càng linh hoạt nếu \\(\\int\\limits_a^b f^{''}(t)^2 dt\\) càng lớn và ngược lại, giá trị này càng gần 0 thì hàm càng ít linh hoạt. Giá trị \\(\\int\\limits_a^b f^{''}(t)^2 dt\\) có thể được coi như một thước đo cho sự linh hoạt của hàm \\(f\\) trên khoảng \\([,b]\\). Lưu ý rằng bất kỳ hàm tuyến tính nào trên khoảng \\([,b]\\) cũng sẽ có \\(\\int\\limits_a^b f^{''}(t)^2 dt = 0\\), nghĩa là hàm tuyến tính là hàm ít linh hoạt nhất.\nHình 11.6: Sự linh hoạt của các hàm số với hàm không linh hoạt là hàm tuyến tính. Hình bên trái: Hàm số ít linh loạt. Hình bên phải: Hàm số rất linh hoạt.\nHình bên trái của Hình 11.6 mô tả một hàm \\(f\\) có miền xác định trên đoạn \\([-1,1]\\) có độ dốc (đạo hàm cấp một) thay đổi nhưng tốc độ thay đổi của đạo hàm cấp một không quá nhanh. Hình bên phải của Hình 11.6 mô tả một hàm \\(f\\) có độ dốc thay đổi liên tục khi \\(x\\) chạy từ -1 đến 1. Kết quả là độ linh hoạt của hàm số ở hình bên phải đo bằng \\(\\int\\limits_a^b f^{''}(t)^2 dt\\) lớn gấp 40 lần với độ linh hoạt của hàm được mô tả ở hình bên trái. Để cân bằng giữa sai số RSS và độ linh hoạt của hàm \\(f\\), thay vì tìm hàm \\(f\\) để tối thiểu hóa RSS, người xây dựng mô hình sẽ tìm hàm \\(f\\) để tối thiểu hóa giá trị RSS cộng thêm một hàm phạt cho sự linh hoạt\n\\[\\begin{align}\n\\hat{f} = \\underset{f}{\\operatorname{argmin}} \\sum\\limits_{=1}^n (y_i - f(x_i))^2 + \\lambda \\cdot \\int\\limits_a^b f^{''}(t)^2 dt\n\\tag{11.5}\n\\end{align}\\]\ntrong đó \\([,b]\\) là miền giá trị mà người xây dựng mô hình muốn đặt ràng buộc cho sự linh hoạt của hàm \\(f\\). Thông thường thì cận dưới \\(\\) thường được lựa chọn là giá trị nhỏ nhất của biến giải thích trong khi cận trên \\(b\\) là giá trị lớn nhất của biến giải thích. Tham số \\(\\lambda > 0\\) đóng vai trò điều chỉnh sự linh hoạt của hàm \\(f\\): nếu \\(\\lambda\\) nhỏ thì kết quả của bài toán tối ưu (11.5) sẽ là hàm linh hoạt hơn với khi \\(\\lambda\\) nhận giá trị lớn. Khi \\(\\lambda\\) rất lớn thì hàm \\(\\hat{f}\\) sẽ xấp xỉ với hàm tuyến tính trong khi \\(\\lambda\\) xấp xỉ 0 sẽ cho kết quả là một hàm nội suy lại chính xác dữ liệu dùng để huấn luyện mô hình. Tương tự như trong hồi quy ridge hay lasso, tham số \\(\\lambda\\) được sử dụng với vai trò cân bằng giữa sự sai lệch và phương sai của mô hình.Điều thú vị là lời giải \\(\\hat{f}\\) của bài toán tối ưu (11.5) có tính chất đặc biệt:Hàm \\(\\hat{f}\\) là một splines bậc ba với các nút tại các giá trị duy nhất của \\(x_1\\), \\(x_2\\), \\(\\cdots\\), \\(x_n\\)\nHàm \\(\\hat{f}\\) là một splines bậc ba với các nút tại các giá trị duy nhất của \\(x_1\\), \\(x_2\\), \\(\\cdots\\), \\(x_n\\)Hàm \\(\\hat{f}\\) có các đạo hàm bậc nhất và bậc hai liên tục tại mỗi nút.\nHàm \\(\\hat{f}\\) có các đạo hàm bậc nhất và bậc hai liên tục tại mỗi nút.Hơn nữa, tại các khoảng giá trị nhỏ hơn nút nhỏ nhất và lớn hơn nút lớn nhất hàm số là hàm tuyến tính. Hay nói cách khác, hàm \\(\\hat{f}\\) là một natural splines bậc ba với các nút đặt tại \\(x_1\\), \\(x_2\\), \\(\\cdots\\), \\(x_n\\). Tuy nhiên, \\(\\hat{f}\\) không chính xác là một natural splines với \\(n\\) tham số tương ứng với \\(n\\) nút giống như chúng ta đã đề cập ở phần trước của chương, mà các tham số bị ràng buộc theo tham số \\(\\lambda\\). Để tham khảo nguyên nhân tại sao \\(\\hat{f}\\) lại là một natural splines, bạn đọc tham khảo phần 11.6.3\nHơn nữa, tại các khoảng giá trị nhỏ hơn nút nhỏ nhất và lớn hơn nút lớn nhất hàm số là hàm tuyến tính. Hay nói cách khác, hàm \\(\\hat{f}\\) là một natural splines bậc ba với các nút đặt tại \\(x_1\\), \\(x_2\\), \\(\\cdots\\), \\(x_n\\). Tuy nhiên, \\(\\hat{f}\\) không chính xác là một natural splines với \\(n\\) tham số tương ứng với \\(n\\) nút giống như chúng ta đã đề cập ở phần trước của chương, mà các tham số bị ràng buộc theo tham số \\(\\lambda\\). Để tham khảo nguyên nhân tại sao \\(\\hat{f}\\) lại là một natural splines, bạn đọc tham khảo phần 11.6.3Khi thảo luận về bậc tự của các hàm có ràng buộc tham số, cũng giống như trong hồi quy ridge, chúng ta cần nhắc đến khái niệm bậc tự hiệu quả. Tham số \\(\\lambda\\) kiểm soát độ linh hoạt của hàm \\(\\hat{f}\\) đó tham số này cũng quyết định bậc tự hiệu quả của mô hình.Khi \\(\\lambda \\rightarrow + \\infty\\), hàm \\(\\hat{f}\\) sẽ tiến đến một đường tuyến tính có thước đo độ linh hoạt bằng 0 và đó có bậc tự hiệu quả bằng 2, tương ứng với 2 tham số là hệ số chặn và hệ số góc.Khi \\(\\lambda = 0\\), \\(\\hat{f}\\) là một natural splines có \\(n\\) nút và không có ràng buộc, nghĩa là số bậc tự hiệu quả bằng \\(n\\). Nếu định nghĩa \\(d_{\\hat{f}}(\\lambda)\\) là bậc tự hiệu quả của hàm \\(\\hat{f}\\) thì \\(d_{\\hat{f}}(\\lambda)\\) là một hàm giảm từ \\(n\\) về 2 khi \\(\\lambda\\) nhận giá trị từ 0 đến \\(+\\infty\\).Với mỗi giá trị \\(\\lambda > 0\\), hàm \\(\\hat{f}\\) ước lượng được từ dữ liệu được gọi là một smoothing splines. Quá trình ước lượng tham số cho hàm \\(\\hat{f}\\) yêu cầu những chứng minh khá phức tạp. Bạn đọc có thể tham khảo tại 11.6.4. Tuy nhiên, điều thú vị khi ước lượng một hàm smoothing splines là chúng ta không cần phải quan tâm là cần bao nhiêu nút hoặc đặt các nút ở đâu. Tất cả các tham số cần khai báo chỉ là bậc tự hiệu quả của smoothing splines đó! Lựa chọn tham số \\(\\lambda\\) được thực hiện thông qua xác thực chéo, nghĩa là giá trị \\(\\lambda\\) được lựa chọn sao cho sai số xác thực chéo trên dữ liệu huấn luyện mô hình là nhỏ nhất.\nHình 11.7: Giá nhà tại Boston được hồi quy theo smoothing splines của biến giải thích là tỷ lệ người sống dưới mức độ trung bình với bậc tự hiệu quả khác nhau. Hình bên trái: Khi bậc tự hiệu quả bằng 2, smoothing splines là một hàm tuyến tính. Hình ở giữa: bậc tự hiệu quả bằng 10 cho kết quả một đường cong mịn và khớp với dữ liệu. Hình bên phải: Bậc tự hiệu quả quá lớn làm cho hàm số trở nên quá linh hoạt và dễ dẫn đến mô hình khớp quá mức.\nHình 11.7 mô tả mối liên hệ giữa giá nhà và tỷ lệ người có thu nhập thấp trên dữ liệu Boston sử dụng smoothing splines. Khi bậc tự hiệu quả bằng 2 tương đương với tham số \\(\\lambda = +\\infty\\), smoothing splines trở thành đường tuyến tính giống như đồ thị bên trái. Nếu chúng ta tăng bậc tự hiệu quả, hay giảm \\(\\lambda\\), smoothing splines sẽ trở nên linh hoạt hơn và khớp tốt hơn với dữ liệu huấn luyện mô hình, tuy nhiên khi bậc tự hiệu quả quá lớn thì hàm \\(f\\) sẽ trở nên quá linh hoạt như đồ thị phải của Hình 11.7.Trước khi đi vào nội dung chính của chương này là mô hình cộng tính tổng quát, chúng ta sẽ thảo luận về một phương pháp tiếp cận cũng thường được sử dụng và cho hiệu quả tương đương như khi sử dụng splines để mô tả mối liên hệ phi tuyến giữa biến mục tiêu và biến giải thích. Cách tiếp cận này được gọi là hồi quy từng đoạn, local regression hay viết tắt là loess","code":""},{"path":"các-mô-hình-cộng-tính-tổng-quát.html","id":"hồi-quy-từng-đoạn","chapter":"Chương 11 Các mô hình cộng tính tổng quát","heading":"11.3 Hồi quy từng đoạn","text":"Hồi quy từng đoạn hay hồi quy cục bộ là một cách tiếp cận khác để mô tả mối liên hệ phi tuyến tính giữa biến mục tiêu \\(Y\\) và biến giải thích \\(X\\). Khái niệm cục bộ có nghĩa là, tại một điểm \\(x_0\\) nằm trong miền giá trị của biến giải thích \\(X\\), biến mục tiêu \\(Y\\) được mô tả thông qua một hàm tuyến tính \\(\\hat{f}\\) được ước lượng từ những giá trị quan sát được của biến \\(X\\) nằm gần với giá trị \\(x_0\\).Thứ nhất, trong hồi quy cục bộ luôn luôn phải có định nghĩa rõ ràng cho khái niệm các điểm dữ liệu gần và xa với điểm \\(x_0\\). Người xây dựng mô hình phải định nghĩa một tham số \\(k\\); \\(2 \\leq k \\leq n\\), để với mỗi \\(x_0\\) chúng ta sử dụng đúng \\(k\\) điểm dữ liệu quan sát được gần với \\(x_0\\) nhất để ước lượng hàm \\(f\\). Một cách tổng quát hơn là định nghĩa tham số span được tính bằng \\(k/n\\) để mô tả cho tỷ lệ dữ liệu sử dụng trong hồi quy cục bộ. Dễ thấy rằng span nhận giá trị trong khoảng \\((0,1]\\). Đồng thời khi span rất gần 0, số lượng điểm dữ liệu để sử dụng để ước lượng hàm \\(\\hat{f}\\) là rất nhỏ. Ngược lại, khi span rất gần 1 thì số lượng điểm dữ liệu sử dụng để ước lượng hàm \\(\\hat{f}\\) là gần như toàn bộ dữ liệu dùng để huấn luyện mô hình.Thứ nhất, trong hồi quy cục bộ luôn luôn phải có định nghĩa rõ ràng cho khái niệm các điểm dữ liệu gần và xa với điểm \\(x_0\\). Người xây dựng mô hình phải định nghĩa một tham số \\(k\\); \\(2 \\leq k \\leq n\\), để với mỗi \\(x_0\\) chúng ta sử dụng đúng \\(k\\) điểm dữ liệu quan sát được gần với \\(x_0\\) nhất để ước lượng hàm \\(f\\). Một cách tổng quát hơn là định nghĩa tham số span được tính bằng \\(k/n\\) để mô tả cho tỷ lệ dữ liệu sử dụng trong hồi quy cục bộ. Dễ thấy rằng span nhận giá trị trong khoảng \\((0,1]\\). Đồng thời khi span rất gần 0, số lượng điểm dữ liệu để sử dụng để ước lượng hàm \\(\\hat{f}\\) là rất nhỏ. Ngược lại, khi span rất gần 1 thì số lượng điểm dữ liệu sử dụng để ước lượng hàm \\(\\hat{f}\\) là gần như toàn bộ dữ liệu dùng để huấn luyện mô hình.Thứ hai, hồi quy từng đoạn không chỉ loại bỏ các điểm dữ liệu cách xa \\(x_0\\), mà còn ước lượng hàm \\(\\hat{f}\\) bằng phương pháp bình phương nhỏ nhất có trọng số. Trọng số cho các điểm dữ liệu nằm gần \\(x_0\\) thường lớn hơn trọng số của các điểm nằm xa \\(x_0\\) để đảm bảo rằng các điểm nằm gần \\(x_0\\) có tác động mạnh hơn đến hình dạng của hàm \\(\\hat{f}\\). Hàm số được sử dụng để định nghĩa trọng số thường là hàm tính trên khoảng cách từ các điểm dữ liệu đến điểm \\(x_0\\). Hàm số phải đảm bảo tính chất là nhận giá trị trên tập các số thực dương và là hàm tăng. Hàm trọng số trên một điểm dữ liệu \\(x_i\\) khi ước lượng hàm hồi quy cục bộ thường được sử dụng để ước lượng hàm \\(\\hat{f}\\) làThứ hai, hồi quy từng đoạn không chỉ loại bỏ các điểm dữ liệu cách xa \\(x_0\\), mà còn ước lượng hàm \\(\\hat{f}\\) bằng phương pháp bình phương nhỏ nhất có trọng số. Trọng số cho các điểm dữ liệu nằm gần \\(x_0\\) thường lớn hơn trọng số của các điểm nằm xa \\(x_0\\) để đảm bảo rằng các điểm nằm gần \\(x_0\\) có tác động mạnh hơn đến hình dạng của hàm \\(\\hat{f}\\). Hàm số được sử dụng để định nghĩa trọng số thường là hàm tính trên khoảng cách từ các điểm dữ liệu đến điểm \\(x_0\\). Hàm số phải đảm bảo tính chất là nhận giá trị trên tập các số thực dương và là hàm tăng. Hàm trọng số trên một điểm dữ liệu \\(x_i\\) khi ước lượng hàm hồi quy cục bộ thường được sử dụng để ước lượng hàm \\(\\hat{f}\\) là\\[\\begin{align}\nw(x_i) = \\left[(1 - d^3)\\right]^3\n\\end{align}\\]\ntrong đó\\[\\begin{align}\nd = \\cfrac{|x_i - x_0|}{\\text{maxdist}}\n\\end{align}\\]\nvới maxdist là khoảng cách xa nhất từ các điểm được lựa chọn đến điểm \\(x_0\\). Các hình 11.8 và 11.9 mô tả phương pháp hồi quy cục bộ trên một dữ liệu mô phỏng từ hàm \\(f(x) = 2*(x-1)^2\\) với hai lựa chọn khác nhau của tham số span. Dữ liệu được cộng thêm nhiễu có phân phối chuẩn có độ lệch chuẩn bằng 0.5.\nHình 11.8: Hàm số phi tuyến được xây dựng bằng phương pháp hồi quy cục bộ. Đường màu xanh da trời là giá trị thật của hàm f. Tham số span được sử dụng trong hồi quy cục bộ là 0.3\n\nHình 11.9: Hàm số phi tuyến được xây dựng bằng phương pháp hồi quy cục bộ. Đường màu xanh da trời là giá trị thật của hàm f. Tham số span được sử dụng trong hồi quy cục bộ là 0.7\nHình 11.8 và 11.9 minh họa ý tưởng xây dựng hàm \\(\\hat{f}\\) bằng phương pháp hồi quy cục bộ trên một dữ liệu mô phỏng. Hình 11.8 sử dụng tham số span bằng 0.3, nghĩa là mỗi lần ước lượng hàm \\(f\\) chỉ có 30% điểm dữ liệu được đưa vào trong mô hình hồi quy. Đường màu xanh mô tả hàm số được sử dụng để tạo ra dữ liệu trong khi đường nét đứt là hàm được ước lượng bằng phương pháp hồi quy cục bộ. Tại điểm \\(x_0 = 0.25\\) hàm \\(\\hat{f}\\) là đường tiếp tuyến được ước lượng dựa trên phương pháp bình phương nhỏ nhất có trọng số dựa trên 30% điểm nằm gần \\(x_0\\) nhất (là các điểm màu cam). Trong hình 11.9, đường màu cam là ước lượng của hàm \\(f\\) với tham số span bằng 0.7. Đường tiếp tuyến tại điểm \\(x_0 = 1.5\\) là ước lượng của hàm \\(f\\) tại điểm này. Có thể thấy rằng khi tham số span càng gần 1 thì hàm \\(\\hat{f}\\) càng ít linh hoạt.Quá trình ước lượng hàm \\(\\hat{f}\\) tại điểm \\(x_0\\) bằng phương pháp hồi quy cục bộ có thể được mô tả qua các bước sau đâyLựa chọn tham số span và sau đó tính \\(k\\) là phần nguyên của span \\(\\times\\) \\(n\\) với \\(n\\) là số lượng quan sát. Lựa chọn ra \\(k\\) điểm trong số \\(n\\) điểm có khoảng cách gần với điểm \\(x_0\\) nhất.Lựa chọn tham số span và sau đó tính \\(k\\) là phần nguyên của span \\(\\times\\) \\(n\\) với \\(n\\) là số lượng quan sát. Lựa chọn ra \\(k\\) điểm trong số \\(n\\) điểm có khoảng cách gần với điểm \\(x_0\\) nhất.Lựa chọn hàm trọng số \\(w(x_i) = h(d)\\) là hàm số nhận giá trị dương và tăng theo \\(d\\), với \\(d = \\cfrac{|x_i - x_0|}{\\text{maxdist}}\\) là khoảng cách từ một điểm \\(x_i\\) trong \\(k\\) điểm được chọn trong bước (1.) và maxdist là khoảng cách từ điểm xa nhất đến \\(x_0\\) để đảm bảo \\(d\\) luôn nằm trong khoảng (0,1].Lựa chọn hàm trọng số \\(w(x_i) = h(d)\\) là hàm số nhận giá trị dương và tăng theo \\(d\\), với \\(d = \\cfrac{|x_i - x_0|}{\\text{maxdist}}\\) là khoảng cách từ một điểm \\(x_i\\) trong \\(k\\) điểm được chọn trong bước (1.) và maxdist là khoảng cách từ điểm xa nhất đến \\(x_0\\) để đảm bảo \\(d\\) luôn nằm trong khoảng (0,1].Tìm các tham số \\(\\hat{\\beta}_0\\), \\(\\hat{\\beta}_1\\) để tối thiểu hóa RSS\n\\[\\begin{align}\nRSS  = \\sum\\limits_{= 1}^k w(x_i) \\cdot (y_i - \\beta_0 - \\beta_1 \\cdot x_i)^2\n\\end{align}\\]Tìm các tham số \\(\\hat{\\beta}_0\\), \\(\\hat{\\beta}_1\\) để tối thiểu hóa RSS\n\\[\\begin{align}\nRSS  = \\sum\\limits_{= 1}^k w(x_i) \\cdot (y_i - \\beta_0 - \\beta_1 \\cdot x_i)^2\n\\end{align}\\]Trả lại giá trị \\(\\hat{f}(x_0) = \\hat{\\beta}_0 + \\hat{\\beta}_1 \\cdot x_1\\).Trả lại giá trị \\(\\hat{f}(x_0) = \\hat{\\beta}_0 + \\hat{\\beta}_1 \\cdot x_1\\).Bạn đọc có thể thấy rằng, để xây dựng được một đường hồi quy liên tục như trong Hình 11.8 và 11.9 chúng ta sẽ phải liên tục cập nhật các điểm \\(x_0\\) mới trên miền giá trị của biến giải thích, và với mỗi một điểm \\(x_0\\) mới, chúng ta lại thực hiện một ước lượng bình phương nhỏ nhất có trọng số trên một tập dữ liệu mới. Điều này khiến cho khối lượng tính toán của phương pháp này lớn hơn rất nhiều với hồi quy splines. Đây cũng là nhược điểm chính của phương pháp hồi quy từng đoạn.Tham số span được sử dụng để điều chỉnh độ sai lệch và phương sai của đường hồi quy. Khi span nhỏ, hàm \\(\\hat{f}\\) sẽ rất linh hoạt nhưng có phương sai lớn và ngược lại, nếu span lớn hàm \\(\\hat{f}\\) sẽ ít linh hoạt hơn và đánh đổi lại là phương sai sẽ nhỏ hơn. Cũng giống như tham số \\(\\lambda\\) của smoothing splines, tham số span cần được lựa chọn dựa trên xác thực chéo.\nHình 11.10: Giá nhà được hồi quy cục bộ theo biến giải thích là tỷ lệ người có thu nhập thấp. Tham số span được lựa chọn bằng xác thực chéo.\nÝ tưởng hồi quy cục bộ có thể được khái quát theo trong trường hợp có nhiều biến giải thích \\(X_1, X_2, \\cdots, X_p\\) và thường cho kết quả tốt hơn với mô hình hồi quy tuyến tính đa biến khi có các biến giải thích có tính chất cục bộ mà điển hình là biến thời gian. Khi có biến mục tiêu phụ thuộc vào biến giải thích là thời gian thì rất thường gặp phải trường hợp mà hệ số tuyến tính liên tục thay đổi, điều mà mô hình tuyến tính đa biến không thể mô tả được. Tuy nhiên, hồi quy cục bộ lại thường hoạt động không hiệu quả khi số lượng biến giải thích \\(p\\) lớn rất khó để tìm được các điểm lân cận có tính chất tương tự điểm \\(x_0\\) khi số chiều tăng lên.Một lưu ý cuối cùng cho bạn đọc về hồi quy cục bộ đó là việc lựa chọn dạng của hàm \\(\\hat{f}\\) trong hồi quy cục bộ không nhất thiết phải là hàm tuyến tính theo \\(x_i\\). Bạn đọc có thể sử dụng các dạng hàm phức tạp hơn, chẳng hạn như sử dụng các đa thức bậc hai. Khi đó, các tham số cần được ước lượng là \\(\\beta_0\\), \\(\\beta_1\\), và \\(\\beta_2\\) để tối thiểu hóa\n\\[\\begin{align}\nRSS  = \\sum\\limits_{= 1}^k w(x_i) \\cdot (y_i - \\beta_0 - \\beta_1 \\cdot x_i - \\beta_2 \\cdot x_i^2)^2\n\\end{align}\\]","code":""},{"path":"các-mô-hình-cộng-tính-tổng-quát.html","id":"gammodel","chapter":"Chương 11 Các mô hình cộng tính tổng quát","heading":"11.4 Mô hình hồi quy cộng tính tổng quát","text":"Trong các phần trước của Chương, chúng tôi trình bày một số phương pháp xây dựng các hàm phi tuyến nhằm mô tả mối liên hệ giữa biến mục tiêu \\(Y\\) dựa vào một biến giải thích \\(X\\) duy nhất. Những phương pháp này có thể được coi là sự mở rộng của hồi quy tuyến tính đơn biến. Trong mô hình cộng tính tổng quát, chúng ta thảo luận về vấn đề dự đoán hay đánh giá một biến mục tiêu Y trên cơ sở các hàm phi tuyến của \\(p\\) biến giải thích \\(X_1\\), \\(X_2\\), \\(\\cdots\\) , \\(X_p\\). Mô hình cộng tính tổng quát cũng có thể hiểu như là một sự mở rộng của hồi quy tuyến tính đa biến. Các mô hình cộng tính tổng quát (Genaralized Additive Model hay viết tắt là GAM) mở rộng mô hình tuyến tính tiêu chuẩn bằng cách mô tả biến mục tiêu thông qua tổng các hàm phi tuyến tính của từng biến giải thích. Nếu như mô hình hồi quy tuyến tính mô tả biến mục tiêu bằng phương trình\n\\[\\begin{align}\nY = \\beta_0 + \\beta_1 \\cdot X_1 + \\beta_2 \\cdot X_2 + \\cdots + \\beta_p  \\cdot X_p + \\epsilon\n\\end{align}\\]\nthì mô hình GAM mô tả biến giải thích thông qua phương trình\n\\[\\begin{align}\nY = \\beta_0 + f_1(X_1) + f_2(X_2) + \\cdots + f_p(X_p) + \\epsilon\n\\tag{11.6}\n\\end{align}\\]\ntrong đó \\(f_j\\) có thể là hàm phi tuyến với mọi \\(1 \\leq j \\leq p\\). Cụ thể hơn, \\(f_j\\) có thể là bất kỳ hàm số nào trong các hàm số mà chúng ta đã thảo luận trong khác phần trước của chương: \\(f_j(x)\\) có thể đơn giản là một đa thức bậc \\(d\\) của \\(x\\), có thể là một splines bậc \\(d\\), một natural splines, smoothing splines, hay là một hàm hồi quy cục bộ theo các miền giá trị của \\(x\\). Và điều quan trọng là, với \\(p\\) lựa chọn khác nhau cho dạng hàm \\(f_i\\), có thể sử dụng một phương pháp được gọi là backfitting để ước lượng tất cả các hàm \\(f_i\\) mà người xây dựng mô hình lựa chọn. Phương pháp backfitting được giới thiệu trong nghiên cứu của Leo Breiman (1985) với mục tiêu để ước lượng các mô hình cộng tính tổng quát được mô tả bởi phương trình (11.6). Các ràng buộc bổ sung cho các hàm \\(f_j\\) cho quá trình backfitting là\n\\[\\begin{align}\n\\sum\\limits_{=1}^n \\ f_j(X_{,j}) = 0 \\ \\ \\ \\forall{j}  \n\\tag{11.7}\n\\end{align}\\]\nđể đảm bảo nghiệm của quá trình ước lượng tham số cho kết quả duy nhất. Với ràng buộc (11.7) chúng ta có thể ước lượng các hàm \\(f_j\\) như sau:Bước thứ nhất: Tính toán ước lượng cho tham số \\(\\beta_0\\) bằng trung bình biến mục tiêu:\n\\[\\begin{align}\n\\beta_0 = \\cfrac{1}{n} \\sum\\limits_{=1}^n y_i\n\\tag{11.8}\n\\end{align}\\]\nvà cho \\(\\hat{f}_j = 0\\) với mọi \\(1 \\leq j \\leq p\\).Bước thứ nhất: Tính toán ước lượng cho tham số \\(\\beta_0\\) bằng trung bình biến mục tiêu:\n\\[\\begin{align}\n\\beta_0 = \\cfrac{1}{n} \\sum\\limits_{=1}^n y_i\n\\tag{11.8}\n\\end{align}\\]\nvà cho \\(\\hat{f}_j = 0\\) với mọi \\(1 \\leq j \\leq p\\).Bước thứ hai: Với mỗi \\(j\\) bằng \\(1, 2, \\cdots, p\\): Ước lượng hàm \\(\\hat{f}_j\\) theo dạng hàm đã lựa chọn với biến mục tiêu là\n\\[\\begin{align}\ny_i - \\hat{\\beta}_0 - \\sum\\limits_{m=1,m \\neq j}^{p} \\hat{f}_m(x_{,m})\n\\tag{11.9}\n\\end{align}\\]\nvà sau đó để đảm bảo ràng buộc (11.7) chúng ta điều chỉnh \\(\\hat{f}_j\\) như sau\n\\[\\begin{align}\n\\hat{f}_j = \\hat{f}_j - \\sum\\limits_{=1}^n \\hat{f}_j(x_{,j})\n\\tag{11.10}\n\\end{align}\\]Bước thứ hai: Với mỗi \\(j\\) bằng \\(1, 2, \\cdots, p\\): Ước lượng hàm \\(\\hat{f}_j\\) theo dạng hàm đã lựa chọn với biến mục tiêu là\n\\[\\begin{align}\ny_i - \\hat{\\beta}_0 - \\sum\\limits_{m=1,m \\neq j}^{p} \\hat{f}_m(x_{,m})\n\\tag{11.9}\n\\end{align}\\]\nvà sau đó để đảm bảo ràng buộc (11.7) chúng ta điều chỉnh \\(\\hat{f}_j\\) như sau\n\\[\\begin{align}\n\\hat{f}_j = \\hat{f}_j - \\sum\\limits_{=1}^n \\hat{f}_j(x_{,j})\n\\tag{11.10}\n\\end{align}\\]Bước thứ ba: Lặp lại bước thứ hai cho đến khi các hàm \\(\\hat{f}_j\\) không thay đổi đáng kể sau mỗi lần cập nhật.Bước thứ ba: Lặp lại bước thứ hai cho đến khi các hàm \\(\\hat{f}_j\\) không thay đổi đáng kể sau mỗi lần cập nhật.Chi tiết của phương pháp backfitting sẽ được thảo luận trong phần ??. Bạn đọc có thể hiểu quá trình backfitting một cách đơn giản như khi chúng ta ước lượng mô hình hồi quy tuyến tính thông thường biến \\(Y\\) phụ thuộc vào hai biến giải thích là \\(X_1\\) và \\(X_2\\) mà không ước lượng các hệ số \\(\\beta_1\\) của \\(X_1\\) và \\(\\beta_2\\) của \\(X_2\\) một cách đồng thời trong mô hình đa biến bằng tính toán ma trận, mà chúng ta sử dụng lặp đi lặp lại các việc ước lượng mô hình hồi quy đơn biến. Thật vậy,Bước thứ nhất: tương tự như thuật toán phát biểu ở trên, chúng ta cho \\(\\hat{\\beta_0} = \\bar{y}\\)Bước thứ nhất: tương tự như thuật toán phát biểu ở trên, chúng ta cho \\(\\hat{\\beta_0} = \\bar{y}\\)Bước thứ hai: Ước lượng mô hình tuyến tính\n\\[\\begin{align}\n(Y - \\hat{\\beta}_0) \\sim X_1\n\\end{align}\\]\nvà thu được hệ số chặn \\(\\hat{\\beta}_{10}\\) và hệ số góc \\(\\hat{\\beta}_{11}\\). biến mục tiêu \\((Y - \\hat{\\beta}_0)\\) có trung bình bằng 0 nên ta có \\(\\hat{\\beta}_{10} + \\hat{\\beta}_{11} \\cdot \\bar{x_1} = 0\\). Nói cách khác, hàm \\(\\hat{f}_1(x) = \\hat{\\beta}_{10} + \\hat{\\beta}_{11} \\cdot x\\) đã được tự động điều chỉnh để thỏa mãn ràng buộc (11.7). Với \\(\\hat{\\beta}_{10}\\) và \\(\\hat{\\beta}_{11}\\) ước lượng được chúng ta ước lượng mô hình tuyến tính\n\\[\\begin{align}\n(Y - \\hat{\\beta}_0 - \\hat{\\beta}_{10} + \\hat{\\beta}_{11} \\cdot X_1) \\sim X_2\n\\end{align}\\]\nđể thu được hệ số chặn \\(\\hat{\\beta}_{20}\\) và hệ số góc \\(\\hat{\\beta}_{21}\\). Tương tự như hàm \\(\\hat{f}_1(x)\\), hàm \\(\\hat{f}_2(x)\\) cũng tự động thỏa mãn ràng buộc (11.7).Bước thứ hai: Ước lượng mô hình tuyến tính\n\\[\\begin{align}\n(Y - \\hat{\\beta}_0) \\sim X_1\n\\end{align}\\]\nvà thu được hệ số chặn \\(\\hat{\\beta}_{10}\\) và hệ số góc \\(\\hat{\\beta}_{11}\\). biến mục tiêu \\((Y - \\hat{\\beta}_0)\\) có trung bình bằng 0 nên ta có \\(\\hat{\\beta}_{10} + \\hat{\\beta}_{11} \\cdot \\bar{x_1} = 0\\). Nói cách khác, hàm \\(\\hat{f}_1(x) = \\hat{\\beta}_{10} + \\hat{\\beta}_{11} \\cdot x\\) đã được tự động điều chỉnh để thỏa mãn ràng buộc (11.7). Với \\(\\hat{\\beta}_{10}\\) và \\(\\hat{\\beta}_{11}\\) ước lượng được chúng ta ước lượng mô hình tuyến tính\n\\[\\begin{align}\n(Y - \\hat{\\beta}_0 - \\hat{\\beta}_{10} + \\hat{\\beta}_{11} \\cdot X_1) \\sim X_2\n\\end{align}\\]\nđể thu được hệ số chặn \\(\\hat{\\beta}_{20}\\) và hệ số góc \\(\\hat{\\beta}_{21}\\). Tương tự như hàm \\(\\hat{f}_1(x)\\), hàm \\(\\hat{f}_2(x)\\) cũng tự động thỏa mãn ràng buộc (11.7).Bước thứ ba: Lặp lại bước thứ hai cho đến khi các hệ số \\(\\hat{\\beta}_{10}\\), \\(\\hat{\\beta}_{11}\\), \\(\\hat{\\beta}_{20}\\) và \\(\\hat{\\beta}_{21}\\) không thay đổi đáng kể sau mỗi bước lặp.Bước thứ ba: Lặp lại bước thứ hai cho đến khi các hệ số \\(\\hat{\\beta}_{10}\\), \\(\\hat{\\beta}_{11}\\), \\(\\hat{\\beta}_{20}\\) và \\(\\hat{\\beta}_{21}\\) không thay đổi đáng kể sau mỗi bước lặp.\nHình 11.11: Tốc độ hội tụ khi sử dụng phương pháp backfitting trong ước lượng tham số của mô hình hồi quy đa biến với biến giải thích là medv và các biến phụ thuộc là rm và lstat. Hình trên bên trái: Hệ số chặn của hàm tuyến tính của biến rm. Hình trên bên phải: Hệ số góc của hàm tuyến tính của biến rm. Hình dưới bên trái: Hệ số chặn của hàm tuyến tính của biến lstat. Hình dưới bên phải: Hệ số góc của hàm tuyến tính của biến lstat.\nHình 11.11 mô tả sự hội tụ của các hệ số chặn và hệ số góc của các hàm \\(f_1\\) và \\(f_2\\) lần lượt là các hàm tuyến tính của biến rm và biến lstat trong mô hình hồi quy đa biến với biến mục tiêu là biến medv trên dữ liệu Boston. Trước hết bạn đọc có thể thấy rằng các hệ số tuyến tính hội tụ rất nhanh chỉ sau hơn 10 bước lặp. Thứ hai, các hàm ước lượng được từ backfitting là\n\\[\\begin{align}\n\\hat{\\beta}_0 &= \\bar{y} = 22.533 \\\\\n\\hat{f}_1(rm) &= \\hat{\\beta}_{10} + \\hat{\\beta}_{11} \\cdot rm = -32.019 + 5.095 \\cdot rm \\\\\n\\hat{f}_2(lstat) &= \\hat{\\beta}_{20} + \\hat{\\beta}_{21} \\cdot lstat = 8.127 - 0.642 \\cdot lstat\n\\tag{11.11}\n\\end{align}\\]\nsẽ cho ước lượng cộng tính cho hàm \\(f\\) là\n\\[\\begin{align}\n\\hat{f}(rm, lstat) & = \\hat{\\beta}_0 + \\hat{f}_1(rm) + \\hat{f}_2(lstat) \\\\\n& = - 1.358 +  5.095 \\cdot rm - 0.642 \\cdot lstat\n\\tag{11.12}\n\\end{align}\\]Bạn đọc có thể sử dụng ước lượng hồi quy đa biến thông thường để xác nhận rằng kết quả của hồi quy đa biến băng phương pháp bình phương nhỏ nhất cho kết quả không khác với phương trình (11.12).Trong thực tế, bạn đọc không cần phải viết vòng lặp để ước lượng mô hình cộng tính tổng quát mà sử dụng thư viện gam. Hàm số được sử dụng để xây dựng và ước lượng mô hình cộng tính tổng quát trên R là hàm cùng tên với thư viện, hàm gam(). Chúng ta sẽ sử dụng hàm số này nhiều hơn trong phần thực hành.Đối với mô hình cộng tính tổng quát, thách thức với người xây dựng mô hình là chọn dạng hàm cho từng biến giải thích và số bậc tự tương ứng. Tiêu chí để đánh giá mô hình thường được sử dụng là sai số xác thực chéo. Tuy nhiên, cần nhắc lại với bạn đọc rằng sử dụng xác thực chéo khi số lượng biến giải thích lớn sẽ khiến cho thời gian tính toán tăng lên đáng kể.Quay trở lại ví dụ khi chúng ta xây dựng mô hình cộng tính tổng quát khi biến mục tiêu medv phụ thuộc vào hai biến giải thích là lstat và rm. Giả sử chúng ta lựa chọn dạng hàm cho lstat và rm đều là các smoothing spline. Tham số duy nhất của smoothing spline là bậc tự hiệu quả. Tìm bậc tự hiệu quả cho lstat và rm phải thực hiện đồng thời và dựa trên sai số xác thực chéo\nHình 11.12: Sai số xác thực chéo khi lựa chọn tham số bậc tự hiệu quả cho smoothing spline của biến lstat và bậc tự hiệu quả cho smoothing spline của biến rm.\nHình 11.12 mô tả quá trình tìm kiếm tham số bậc tự hiệu quả cho biến lstat và tham số bậc tự hiệu quả cho biến rm dựa trên sai số xác thực chéo. Có thể thấy rằng tham số cho sai số xác thực chéo nhỏ nhất là \\(df = 7.9\\) đối với biến lstat và \\(df = 5.9\\) đối với biến rm.Cũng giống như mô hình tuyến tính thông thường, mô hình cộng tính tổng quát hoàn toàn có thể sử dụng trong các bài toán phân loại. Mô hình tuyến tính thông thường không thể sử dụng trực tiếp cho bài toán phân loại mà cần có sự biến đổi cho phù hợp với phân phối xác suất của biến mục tiêu. Các mô hình tuyến tính dùng cho mục đích phân loại là lớp các mô hình thường được gọi là mô hình tuyến tính tổng quát mà ở đó chúng ta có thể bỏ qua giả thuyết về phân phối chuẩn của biến mục tiêu. Các mô hình này sẽ được trình bày trong phần sau của cuốn sách. Mô hình GAM cho mục đích phân loại cũng cần xây dựng dựa trên nền tảng của mô hình tuyến tính tổng quát đó sẽ được đề cập trong các chương tiếp theo.","code":""},{"path":"các-mô-hình-cộng-tính-tổng-quát.html","id":"thực-hành","chapter":"Chương 11 Các mô hình cộng tính tổng quát","heading":"11.5 Thực hành","text":"","code":""},{"path":"các-mô-hình-cộng-tính-tổng-quát.html","id":"hồi-quy-spline-trên-dữ-liệu-boston","chapter":"Chương 11 Các mô hình cộng tính tổng quát","heading":"11.5.1 Hồi quy spline trên dữ liệu Boston","text":"","code":""},{"path":"các-mô-hình-cộng-tính-tổng-quát.html","id":"hồi-quy-cục-bộ-trên-dữ-liệu-boston","chapter":"Chương 11 Các mô hình cộng tính tổng quát","heading":"11.5.2 Hồi quy cục bộ trên dữ liệu Boston","text":"","code":""},{"path":"các-mô-hình-cộng-tính-tổng-quát.html","id":"mô-hình-công-tính-tổng-quát-trên-dữ-liệu-boston","chapter":"Chương 11 Các mô hình cộng tính tổng quát","heading":"11.5.3 Mô hình công tính tổng quát trên dữ liệu Boston","text":"","code":""},{"path":"các-mô-hình-cộng-tính-tổng-quát.html","id":"phụ-lục-4","chapter":"Chương 11 Các mô hình cộng tính tổng quát","heading":"11.6 Phụ lục","text":"","code":""},{"path":"các-mô-hình-cộng-tính-tổng-quát.html","id":"gamapen1","chapter":"Chương 11 Các mô hình cộng tính tổng quát","heading":"11.6.1 Ước lượng tham số cho splines bậc ba và \\(k\\) nút","text":"Ước lượng tham số cho splines bậc ba không được thực hiện thông qua bài toán tối ưu như phương trình (11.4) mà cần có sự biến đổi tham số. Giả sử \\(k=1\\) và nút duy nhất được lựa chọn là \\(c\\). Các tham số (\\(\\beta_{01}\\), \\(\\beta_{11}\\), \\(\\beta_{21}\\), \\(\\beta_{31}\\)) của đa thức thứ nhất và các tham số (\\(\\beta_{02}\\), \\(\\beta_{12}\\), \\(\\beta_{22}\\), \\(\\beta_{32}\\)) của đa thức thứ hai thỏa mãn các ràng buộc trong bài toán tối ưu như phương trình (11.4), có thể chứng minh được rằng tồn tại các tham số \\(\\beta_0\\), \\(\\beta_1\\), \\(\\cdots\\), \\(\\beta_4\\) sao cho hàm số \\(f\\) là nghiệm của bài tối ưu thỏa mãn\n\\[\\begin{align}\nf(x) = \\beta_0 + \\beta_1 \\cdot x + \\beta_2 \\cdot x^2 + \\beta_3 \\cdot x^3 + \\beta_4 \\cdot \\left[(x-c)^3\\right]^+\n\\end{align}\\]\nNói một cách khác, thay vì giải bài toán tối ưu có điều kiện ràng buộc, chúng ta chỉ cần sử dụng phương pháp bình phương nhỏ nhất thông thường để tìm các hệ số tuyến tính \\(\\beta_0\\), \\(\\beta_1\\), \\(\\cdots\\), \\(\\beta_4\\) tương ứng với các biến giải thích lần lượt là \\(1\\), \\(x_i\\), \\(x_i^2\\), \\(x_i^3\\), \\(\\left[(x_i-c)^3\\right]^+\\).Để tránh sự nhầm lẫn khi viết các hệ số \\(\\beta\\) với chỉ số, chúng ta viết lại bài toán hai đa thức như sau: Cho hai đa thức bậc ba \\(P_1\\) và \\(P_2\\) như sau\n\\[\\begin{align}\n& P_1(x) = a_0 + a_1 x + a_2 x^2 + a_3 x^3 \\\\\n& P_2(x) = b_0 + b_1 x + b_2 x^2 + b_3 x^3\n\\end{align}\\]\nvà nút \\(c\\) sao cho\n\\[\\begin{align}\nP_1(c) = P_2(c); \\ \\ \\ P^{'}_1(c) = P{'}_2(c); \\ \\ \\ P{''}_1(c) = P{''}_2(c)\n\\end{align}\\]\nHàm số \\(f\\) được xác định bởi\n\\[\\begin{align}\nf(x) = \\mathbb{}_{\\{x_i < c\\}} \\cdot P_1(x) + \\mathbb{}_{\\{x_i \\geq c\\}} \\cdot P_2(x)\n\\end{align}\\]\nChúng ta sẽ chứng minh rằng \\(f(x)\\) có thể được viết dưới dạng\n\\[\\begin{align}\nf(x) = a_0 + a_1 x + a_2 x^2 + a_3 x^3 + (b_3 - a_3) \\cdot \\left[(x-c)^3\\right]^{+}\n\\end{align}\\]Trước hết, chúng ta viết lại đa thức \\(P_2(x)\\) như sau\n\\[\\begin{align}\nP_2(x) = d_0 + d_1 (x-c) + d_2 (x-c)^2 + b_3 (x-c)^3\n\\end{align}\\]\n\\(P_1(c) = P_2(c)\\), \\(P^{'}_1(c) = P{'}_2(c)\\), và \\(P{''}_1(c) = P{''}_2(c)\\) nên chúng ta có \\(d_0 = P_1(c)\\); \\(d_1 = P^{'}_1(c)\\), và \\(d_2 = P{''}_1(c)/2\\).Khi \\(x < c\\) có thể dễ dàng thấy rằng \\(f(x) = P_1(x)\\). Với \\(x > c\\), chúng ta có\n\\[\\begin{align}\nf(x) & = a_0 + a_1 x + a_2 x^2 + a_3 x^3 + (b_3 - a_3) \\cdot (x-c)^3 \\\\\n& = (a_0 + a_3 c^3) + (a_1 - 3 a_3 c^2) x + (a_2 + 3 a_3 c) x^2 + b_3 (x-c)^3\n\\end{align}\\]Với \\(t = x - c\\),\n\\[\\begin{align}\nf(x) & = (a_0 + a_3 c^3) + (a_1 - 3 a_3 c^2) (y + c) + (a_2 + 3 a_3 c) (y + c)^2 + b_3 y^3 \\\\\n& = (a_0 + a_3 c^3 + a_1 c + a_2 c^2) + (a_1 + 3 a_3 c^2 + 2 a_2) y + (a_2 + 3 a_3 c) y^2 + b_3 y^3 \\\\\n& = d_0 + d_1 y + d_2 y^2 + b_3 y^3\n\\end{align}\\]\nhay nói cách khác \\(f(x) = P_2(x)\\) khi \\(x > c\\)!Trong trường hợp tổng quát, ước lượng một splines bậc ba với \\(k\\) nút \\(c_1 < c_2 < \\cdots < c_k\\) sẽ tương đương với ước lượng \\(k+4\\) tham số \\(\\beta_0\\), \\(\\beta_1\\), \\(\\cdots\\), \\(\\beta_{(k+3)}\\) với các biến giải thích \\(1\\), \\(x_i\\), \\(x_i^2\\), \\(x_i^3\\), \\(\\left[(x_i-c_1)^3\\right]^+\\), \\(\\left[(x_i-c_2)^3\\right]^+\\), \\(\\cdots\\), \\(\\left[(x_i-c_k)^3\\right]^+\\).","code":""},{"path":"các-mô-hình-cộng-tính-tổng-quát.html","id":"gamapen2","chapter":"Chương 11 Các mô hình cộng tính tổng quát","heading":"11.6.2 Ước lượng tham số cho natural splines bậc ba có \\(k\\) nút","text":"Ước lượng tham số cho một \\(natural\\) \\(splines\\) không được thực hiện thông qua bài toán tối ưu có ràng buộc tại \\(k\\) nút, mà được thực hiện thông qua giải bài toán bình phương nhỏ nhất thông thường của biến mục tiêu \\(Y\\) với \\(k\\) biến giải thích được tính toán từ biến giải thích \\(X\\). Giả sử \\(k\\) nút được xắp xếp theo thứ tự tăng dần \\(c_1 < c_2 < \\cdots < c_k\\), ta có \\(k\\) biến giải thích: \\(X_1 = 1\\), \\(X_2 = x\\) và với \\(j = 3, \\cdots, k\\)\n\\[\\begin{align}\nX_j = \\cfrac{\\left[(x-c_{j-2})^3\\right]^+ - \\left[(x-c_k)^3\\right]^+ }{c_k - c_{j-2}} - \\cfrac{\\left[(x-c_{k-1})^3\\right]^+ - \\left[(x-c_k)^3\\right]^+}{c_k - c_{k-1}}\n\\end{align}\\]Hàm \\(\\hat{f}\\) thu được bằng cách hồi quy tuyến tính \\(Y\\) theo \\(X_1\\), \\(X_2\\), \\(\\cdots\\), \\(X_k\\) có dạng\n\\[\\begin{align}\n\\hat{f}(x) = \\hat{\\beta}_1 \\cdot X_1(x) + \\hat{\\beta}_2 \\cdot X_2(x) + \\cdots + \\hat{\\beta}_k \\cdot X_k(x)\n\\end{align}\\]\nlà một natural splines vì:Khi \\(x\\) nhỏ hơn \\(c_1\\) hoặc \\(x\\) lớn hơn \\(c_k\\) hàm \\(f\\) là một hàm tuyến tính. Thật vậyVới \\(x < c_1\\) ta có \\(X_j(x) = 0\\) với mọi \\(j \\geq 3\\), đó\n\\[\\begin{align}\n\\hat{f}(x) = \\hat{\\beta}_1 \\cdot X_1(x) + \\hat{\\beta}_2 \\cdot X_2(x) = \\hat{\\beta}_1 + \\hat{\\beta}_2 \\cdot x\n\\end{align}\\]\nlà một hàm tuyến tính.Với \\(x < c_1\\) ta có \\(X_j(x) = 0\\) với mọi \\(j \\geq 3\\), đó\n\\[\\begin{align}\n\\hat{f}(x) = \\hat{\\beta}_1 \\cdot X_1(x) + \\hat{\\beta}_2 \\cdot X_2(x) = \\hat{\\beta}_1 + \\hat{\\beta}_2 \\cdot x\n\\end{align}\\]\nlà một hàm tuyến tính.Với \\(x > c_k\\) ta có: hệ số của \\(x^3\\) của \\(X_j\\) là\n\\[\\begin{align}\n\\cfrac{1 - 1}{c_k - c_{j-2}} - \\cfrac{1 - 1}{c_k - c_{k-1}} = 0 - 0 = 0\n\\end{align}\\]\nvà hệ số của \\(x^2\\) của \\(X_j\\) là\n\\[\\begin{align}\n\\cfrac{-3c_{j-2} + 3c_k}{c_k - c_{j-2}} - \\cfrac{-3c_{k-1} + 3c_k}{c_k - c_{k-1}} = 3 - 3 = 0\n\\end{align}\\]Với \\(x > c_k\\) ta có: hệ số của \\(x^3\\) của \\(X_j\\) là\n\\[\\begin{align}\n\\cfrac{1 - 1}{c_k - c_{j-2}} - \\cfrac{1 - 1}{c_k - c_{k-1}} = 0 - 0 = 0\n\\end{align}\\]\nvà hệ số của \\(x^2\\) của \\(X_j\\) là\n\\[\\begin{align}\n\\cfrac{-3c_{j-2} + 3c_k}{c_k - c_{j-2}} - \\cfrac{-3c_{k-1} + 3c_k}{c_k - c_{k-1}} = 3 - 3 = 0\n\\end{align}\\]Nói cách khác, hàm \\(\\hat{f}(x)\\) là một hàm tuyến tính theo \\(x\\) khi \\(x\\) nhỏ hơn \\(c_1\\) hoặc \\(x\\) lớn hơn \\(c_k\\).Khi \\(x\\) nhận giá trị bất kỳ giữa hai nút \\(c_{j-1}\\) và \\(c_j\\), hàm \\(f\\) là một đa thức bậc ba.Khi \\(x\\) nhận giá trị bất kỳ giữa hai nút \\(c_{j-1}\\) và \\(c_j\\), hàm \\(f\\) là một đa thức bậc ba.Hàm \\(f\\) có liên tục, và có các đạo hàm đến bậc hai liên tục tại nút \\(c_j\\) bất kỳ. Điều này hiển nhiên vì tất cả các hàm \\(X_j(x)\\) đều liên tục và có đạo hàm đến bậc hai liên tục tại nút \\(c_j\\) bất kỳ, đó tổ hợp tuyến tính của các hàm \\(X_j(x)\\) cũng có tính chất này.Hàm \\(f\\) có liên tục, và có các đạo hàm đến bậc hai liên tục tại nút \\(c_j\\) bất kỳ. Điều này hiển nhiên vì tất cả các hàm \\(X_j(x)\\) đều liên tục và có đạo hàm đến bậc hai liên tục tại nút \\(c_j\\) bất kỳ, đó tổ hợp tuyến tính của các hàm \\(X_j(x)\\) cũng có tính chất này.","code":""},{"path":"các-mô-hình-cộng-tính-tổng-quát.html","id":"gamapen3","chapter":"Chương 11 Các mô hình cộng tính tổng quát","heading":"11.6.3 Tại sao smoothing splines lại là một natural spline","text":"Trước hết, nếu \\(f(x)\\) là một natural spline trên đoạn \\([,b]\\) đi qua \\(n\\) điểm \\((x_i, z_i)\\) với \\(n\\) nút thắt tại \\(x_1, x_2, \\cdots, x_n\\) thì với mọi hàm số \\(g(x)\\) có đạo hàm bậc hai liên tục và cũng đi qua \\(n\\) điểm \\((x_i, z_i)\\), ta sẽ có\n\\[\\begin{align}\n\\int\\limits_{}^b \\ \\left[g^{''}(x)\\right]^2 dx \\geq \\int\\limits_{}^b \\ \\left[f^{''}(x)\\right]^2 dx\n\\end{align}\\]\nnói một cách khác, trong tất cả các hàm có đạo hàm bậc hai liên tục đi qua \\(n\\) điểm cho trước \\((x_i, z_i)\\), natural spline là hàm số ít linh hoạt nhất.Thật vậy, cho \\(h(x) = g(x) - f(x)\\), ta có \\(h^{''}(x) = g^{''}(x) - f(x)^{''}\\) và\n\\[\\begin{align}\n\\int\\limits_{}^b f^{''}(x) h^{''}(x) dx = \\left[f^{''}(x) h'(x) \\right]^b_a - \\int\\limits_{}^b f^{'''}(x) h^{'}(x) dx\n\\end{align}\\]Dễ thấy\n\\[\\begin{align}\n\\left[f^{''}(x) h'(x) \\right]^b_a = f^{''}(b) h'(b) - f^{''}() h'() = 0\n\\end{align}\\]\n\\(f^{''}(b) = f^{''}() = 0\\) vì \\(f\\) là hàm tuyến tính khi \\(x < x_1\\) và \\(x > x_n\\). Thêm vào đó\n\\[\\begin{align}\n\\int\\limits_{}^b f^{'''}(x) h^{'}(x) dx & = \\int\\limits_{x_1}^{x_n} f^{'''}(x) h^{'}(x) dx \\\\\n& = \\sum\\limits_{=1}^n \\int\\limits_{x_1}^{x_{+1}} f^{'''}(x) h^{'}(x) dx \\\\\n& = \\sum\\limits_{=1}^n \\left[f^{'''}(x) h(x) \\right]^{x_{+1}}_{x_i} -  \\sum\\limits_{=1}^n \\int\\limits_{x_1}^{x_{+1}} f^{(4)}(x) h(x) dx\n\\end{align}\\]Ta có \\(h(x_i) = g(x_i) - f(x_i) = z_i - z_i = 0\\) cả hai hàm \\(g\\) và \\(f\\) đều đi qua điểm \\((x_i, z_i)\\). Đồng thời \\(f^{(4)}(x) = 0\\) với mọi \\(x\\) \\(f\\) là một hàm bậc 3. Nói cách khác, giá trị của biểu thức \\(\\int\\limits_{}^b f^{''}(x) h^{''}(x) dx\\) cũng bằng 0.Dựa vào kết quả trên, có thể thấy rằng\n\\[\\begin{align}\n\\int\\limits_{}^b \\ \\left[g^{''}(x)\\right]^2 dx & = \\int\\limits_{}^b \\ \\left[f^{''}(x) + h^{''}(x)\\right]^2 dx \\\\\n& = \\int\\limits_{}^b \\ \\left[f^{''}(x)\\right]^2 dx + 2 \\cdot \\int\\limits_{}^b f^{''}(x) h^{''}(x) dx + \\int\\limits_{}^b \\ \\left[h^{''}(x)\\right]^2 dx \\\\\n& = \\int\\limits_{}^b \\ \\left[f^{''}(x)\\right]^2 dx + \\int\\limits_{}^b \\ \\left[h^{''}(x)\\right]^2 dx \\\\\n& \\geq \\int\\limits_{}^b \\ \\left[f^{''}(x)\\right]^2 dx\n\\end{align}\\]\nvà dấu bằng xảy ra chỉ khi \\(h^{''}(x) = 0\\) với mọi \\(x\\). Điều này chỉ xảy ra khi \\(h\\) là một hàm tuyến tính. Tuy nhiên, ta lại có \\(h(x_i) = 0\\) với mọi \\(\\), đó \\(h(x) = 0\\) với mọi \\(x\\).Quay trở lại với Smoothing spline, giả sử \\(\\hat{f}\\) là lời giải của bài toán tối ưu\n\\[\\begin{align}\n\\hat{f} = \\underset{f}{\\operatorname{argmin}} \\sum\\limits_{=1}^n (y_i - f(x_i))^2 + \\lambda \\cdot \\int\\limits_a^b f^{''}(t)^2 dt\n\\end{align}\\]Gọi \\(\\tilde{f}\\) là natural spline đi qua các điểm \\((x_i, \\hat{f}(x_i))\\) và có nút tại tất cả các \\(x_i\\) với \\(1 \\leq \\leq n\\) thì theo kết quả ở trên ta có\n\\[\\begin{align}\n\\int\\limits_{}^b \\ \\left[\\hat{f}^{''}(x)\\right]^2 dx \\geq \\int\\limits_{}^b \\ \\left[\\tilde{f}^{''}(x)\\right]^2 dx\n\\end{align}\\]\nngoài ra\n\\[\\begin{align}\n\\sum\\limits_{=1}^n (y_i - \\hat{f}(x_i))^2 = \\sum\\limits_{=1}^n (y_i - \\tilde{f}(x_i))^2\n\\end{align}\\]\nhay nói một cách khác\n\\[\\begin{align}\n\\sum\\limits_{=1}^n (y_i - \\hat{f}(x_i))^2 + \\lambda \\cdot \\int\\limits_a^b \\hat{f}^{''}(t)^2 dt \\geq \\sum\\limits_{=1}^n (y_i - \\tilde{f}(x_i))^2 + \\lambda \\cdot \\int\\limits_a^b \\tilde{f}^{''}(t)^2 dt\n\\end{align}\\]\nđiều này chỉ có thể xảy ra khi \\(\\hat{f}(x) = \\tilde{f}(x)\\) với mọi \\(x\\)","code":""},{"path":"các-mô-hình-cộng-tính-tổng-quát.html","id":"gamapen4","chapter":"Chương 11 Các mô hình cộng tính tổng quát","heading":"11.6.4 Ước lượng smoothing splines","text":"smoothing splines là một natural spline nên dạng hàm của smoothing spline có thể viết dưới dạng\n\\[\\begin{align}\n\\hat{f}(x) = \\hat{\\beta}_1 \\cdot X_1(x) + \\hat{\\beta}_2 \\cdot X_2(x) + \\cdots + \\hat{\\beta}_k \\cdot X_k(x)\n\\end{align}\\]\nvới\n\\[\\begin{align}\nX_j = \\cfrac{\\left[(x-c_{j-2})^3\\right]^+ - \\left[(x-c_k)^3\\right]^+ }{c_k - c_{j-2}} - \\cfrac{\\left[(x-c_{k-1})^3\\right]^+ - \\left[(x-c_k)^3\\right]^+}{c_k - c_{k-1}}\n\\end{align}\\]Có thể chứng minh được rằng các hệ số \\(\\boldsymbol{\\hat\\beta}\\) để hàm \\(\\hat{f}\\) là nghiệm của bài toán tối ưu\n\\[\\begin{align}\n\\hat{f} = \\underset{f}{\\operatorname{argmin}} \\sum\\limits_{=1}^n (y_i - f(x_i))^2 + \\lambda \\cdot \\int\\limits_a^b f^{''}(t)^2 dt\n\\end{align}\\]\nlà nghiệm của bài toán tối ưu tương ứng\n\\[\\begin{align}\n\\boldsymbol{\\hat\\beta} = \\underset{\\boldsymbol{\\beta}}{\\operatorname{argmin}} \\left(\\textbf{y} - \\textbf{x} \\boldsymbol{\\beta}\\right)^T \\left(\\textbf{y} - \\textbf{x} \\boldsymbol{\\beta}\\right) + \\lambda \\cdot \\boldsymbol{\\beta}^T \\Omega \\boldsymbol{\\beta}\n\\end{align}\\]\nvới \\(\\lambda > 0\\), \\(\\textbf{x}\\) là ma trận kích thước \\(n \\times k\\) các biến giải thích có phần tử nằm ở hàng thứ \\(\\) cột thứ \\(j\\) là \\(X_j(x_i)\\) và \\(\\Omega\\) là ma trận kích thước \\(k \\times k\\) và\n\\[\\begin{align}\n\\Omega_{j,l} = \\int\\limits_{}^b X_j(t) X_l(t) dt\n\\end{align}\\]\nvới \\(1 \\leq j,l \\leq n\\). Có thể thấy rằng ước lượng smoothing spline cũng tương tự như ước lượng tham số của hồi quy ridge. Ta có lời giải chính xác cho các hệ số tuyến tính:\n\\[\\begin{align}\n\\boldsymbol{\\hat\\beta} = \\left( \\textbf{x}^T \\textbf{x} + \\lambda \\Omega \\right)^{-1} \\textbf{x}^T \\textbf{y}\n\\end{align}\\]Tương tự như hồi quy ridge, giá trị \\(\\lambda\\) sẽ quyết định mức độ linh hoạt của smoothing spline: nếu \\(\\lambda\\) lớn thì sự linh hoạt của các smoothing spline sẽ giảm và ngược lại, nếu \\(\\lambda\\) giảm thì mức độ linh hoạt của hàm sẽ tăng. Mức độ linh hoạt thường được đo lường bằng khái niệm bậc tự hiệu quả. Với hệ số \\(\\boldsymbol{\\hat\\beta}\\) ước lượng được như trên, chúng ta có giá trị ước lượng cho biến mục tiêu \\(\\hat{y}\\) như sau\n\\[\\begin{align}\n\\hat{y} & = \\textbf{x} \\boldsymbol{\\hat\\beta} \\\\\n& = \\textbf{x} \\left( \\textbf{x}^T \\textbf{x} + \\lambda \\Omega \\right)^{-1} \\textbf{x}^T \\textbf{y} \\\\\n& = \\textbf{S}_{\\lambda} \\textbf{y}\n\\end{align}\\]\nvới\n\\[\\begin{align}\n\\textbf{S}_{\\lambda} = \\textbf{x} \\left( \\textbf{x}^T \\textbf{x} + \\lambda \\Omega \\right)^{-1} \\textbf{x}^T\n\\end{align}\\]Bậc tự hiệu quả của smoothing spline được xác định cũng giống như trong hồi quy ridge, là vết của ma trận \\(\\textbf{S}_{\\lambda}\\), được ký hiệu là \\(trace\\left(\\textbf{S}_{\\lambda}\\right)\\).","code":"## \n## Attaching package: 'dplyr'## The following objects are masked from 'package:stats':\n## \n##     filter, lag## The following objects are masked from 'package:base':\n## \n##     intersect, setdiff, setequal, union## \n## Attaching package: 'kableExtra'## The following object is masked from 'package:dplyr':\n## \n##     group_rows## \n## Attaching package: 'gridExtra'## The following object is masked from 'package:dplyr':\n## \n##     combine## \n## Attaching package: 'pryr'## The following object is masked from 'package:dplyr':\n## \n##     where## Classes and Methods for R developed in the\n## Political Science Computational Laboratory\n## Department of Political Science\n## Stanford University\n## Simon Jackman\n## hurdle and zeroinfl functions by Achim Zeileis"},{"path":"mô-hình-tuyến-tính-tổng-quát..html","id":"mô-hình-tuyến-tính-tổng-quát.","chapter":"Chương 12 Mô hình tuyến tính tổng quát.","heading":"Chương 12 Mô hình tuyến tính tổng quát.","text":"Mô hình tuyến tính tổng quát, Generalized Linear Model hay viết tắt là GLM, là một mô hình được ứng dụng rộng rãi trong các doanh nghiệp, các cơ quan tổ chức hoạt động trong lĩnh vực tài chính, ngân hàng, và bảo hiểm. Các chuyên gia quản trị rủi ro trong các ngân hàng sử dụng mô hình GLM để chấm điểm tín dụng khách hàng và quyết định phê duyệt tín dụng. Các chuyên gia tính toán thường xuyên sử dụng mô hình GLM để xác định phí thuần của các sản phẩm bảo hiểm, để xác định dự phòng nghiệp vụ, hoặc để phân loại rủi ro mà công ty phải đối mặt. Khái niệm GLM lần đầu tiên được sử dụng trong nghiên cứu của Nelder và Wedderburn (1972) và từ đó đến nay đã có nhiều sách tham khảo tin cậy cho mô hình này như Alan Agresti (2015) hay Annette J. Dobson Adrian G. Barnett (2018). Đa số các tài liệu tham khảo giới thiệu GLM dưới góc độ toán học và mang nhiều tính lý thuyết. Chương sách này sẽ cố gắng giải thích và tiếp cận GLM từ một góc nhìn mang tính thực hành nhiều hơn. Chúng tôi sẽ không quá đi sâu vào các khía cạnh như giả thiết hay phương pháp ước lượng của mô hình GLM, mà sẽ tập trung vào hướng dẫn bạn đọc ứng dụng GLM trên nhiều kiểu dữ liệu nhất có thể.Mô hình tuyến tính tổng quát là các mô hình được xây dựng trên các dữ liệu kiểu truyền thống, với biến mục tiêu \\(Y\\) và ma trận biến giải thích \\(\\textbf{X} = \\left(X_1, X_2, \\cdots, X_p\\right)\\). Dữ liệu có \\(n\\) quan sát tương ứng với \\(n\\) dòng của ma trận \\(\\textbf{X}\\). Mô hình tuyến tính tổng quát có thể được sử dụng trong trường hợp có nhiều biến mục tiêu, tuy nhiên trong cuốn sách này chúng tôi chỉ tập trung vào trường hợp có một biến mục tiêu duy nhất.Mô hình tuyến tính tổng quát được phát biểu như sau:Biến mục tiêu \\(Y\\) có hàm phân phối xác suất \\(F_{\\boldsymbol{\\theta}}\\) với tham số là một véc-tơ \\(\\boldsymbol{\\theta}\\).\n\\[\\begin{align}\nY \\sim  F_{\\boldsymbol{\\theta}} \\text{    hay    } \\mathbb{P}(Y \\leq y) =  F_{\\boldsymbol{\\theta}}(y)\n\\end{align}\\]Biến mục tiêu \\(Y\\) có hàm phân phối xác suất \\(F_{\\boldsymbol{\\theta}}\\) với tham số là một véc-tơ \\(\\boldsymbol{\\theta}\\).\n\\[\\begin{align}\nY \\sim  F_{\\boldsymbol{\\theta}} \\text{    hay    } \\mathbb{P}(Y \\leq y) =  F_{\\boldsymbol{\\theta}}(y)\n\\end{align}\\]Giá trị trung bình của biến mục tiêu \\(Y\\) phụ thuộc vào giá trị của biến độc lập. Với điều kiện véc-tơ biến độc lập \\(\\textbf{X} = (X_1, X_2, \\cdots, X_p)\\) nhận giá trị \\(\\textbf{x_i} = (x_{,1}, x_{,2}, \\cdots, x_{,p})\\), giá trị trung bình của biến mục tiêu \\(Y\\) được xác định bằng một hàm số tính trên một tổ hợp tuyến tính của các biến độc lập:\n\\[\\begin{align}\n& \\mathbb{E}(Y|\\textbf{X} = \\textbf{x}_i) = \\mu_i \\\\\n& g(\\mu_i) = \\beta_0 + \\beta_1 \\cdot x_{,1} + \\beta_2 \\cdot x_{,2} + \\cdots + \\beta_p \\cdot x_{,p} \\\\\n& \\mu_i = g^{-1}\\left(\\beta_0 + \\beta_1 \\cdot x_{,1} + \\beta_2 \\cdot x_{,2} + \\cdots + \\beta_p \\cdot x_{,p} \\right)\n\\tag{12.1}\n\\end{align}\\]Giá trị trung bình của biến mục tiêu \\(Y\\) phụ thuộc vào giá trị của biến độc lập. Với điều kiện véc-tơ biến độc lập \\(\\textbf{X} = (X_1, X_2, \\cdots, X_p)\\) nhận giá trị \\(\\textbf{x_i} = (x_{,1}, x_{,2}, \\cdots, x_{,p})\\), giá trị trung bình của biến mục tiêu \\(Y\\) được xác định bằng một hàm số tính trên một tổ hợp tuyến tính của các biến độc lập:\n\\[\\begin{align}\n& \\mathbb{E}(Y|\\textbf{X} = \\textbf{x}_i) = \\mu_i \\\\\n& g(\\mu_i) = \\beta_0 + \\beta_1 \\cdot x_{,1} + \\beta_2 \\cdot x_{,2} + \\cdots + \\beta_p \\cdot x_{,p} \\\\\n& \\mu_i = g^{-1}\\left(\\beta_0 + \\beta_1 \\cdot x_{,1} + \\beta_2 \\cdot x_{,2} + \\cdots + \\beta_p \\cdot x_{,p} \\right)\n\\tag{12.1}\n\\end{align}\\]Chúng tôi ký hiệu biến ngẫu nhiên có điều kiện \\((Y|\\textbf{X} = \\textbf{x}_i)\\) một cách ngắn gọn là \\(Y_i\\). Phương trình thứ ba trong hệ phương trình (12.1) có nghĩa là giá trị trung bình của biến \\(Y_i\\), ký hiệu \\(\\mu_i\\), được xác định bằng giá trị một hàm số ngược của một hàm số thực \\(g\\), ký hiệu là hàm \\(g^{-1}\\), tính trên một tổ hợp tuyến tính của các giá trị \\((x_{,1}, x_{,2}, \\cdots, x_{,p})\\).Thay vì cố gắng hiểu các khái niệm toán học ở trên, chúng ta hãy thử áp dụng mô hình kể trên trong một trường hợp cụ thể. Chúng ta sẽ xây dựng một mô hình tuyến tính tổng quát mà trong đó biến mục tiêu \\(Y\\) cho biết người mua bảo hiểm xe có hay không lựa chọn các quyền lợi bảo hiểm bổ sung khi ký hợp đồng mua bảo hiểm trách nhiệm dân sự bắt buộc. Dữ liệu được sử dụng có tên là MotoInsurance bao gồm có 6 biến độc lập là:Độ tuổi của người lái xe, biến age, nhận giá trị là các số nguyên dương từ 15 đến 92 tuổi.\nĐộ tuổi của người lái xe, biến age, nhận giá trị là các số nguyên dương từ 15 đến 92 tuổi.Kinh nghiệm lái xe, biến seniority, cho biết số năm kinh nghiệm lái xe, giá trị là các số nguyên từ 2 đến 40 năm.\nKinh nghiệm lái xe, biến seniority, cho biết số năm kinh nghiệm lái xe, giá trị là các số nguyên từ 2 đến 40 năm.Giới tính của người lái xe, biến sex, nhận giá trị 'M' nếu người lái xe là nam giới và 'F' nếu người lái xe là nữ giới.\nGiới tính của người lái xe, biến sex, nhận giá trị 'M' nếu người lái xe là nam giới và 'F' nếu người lái xe là nữ giới.Nơi xe được đăng ký, biến urban, nhận giá trị là 1 nếu xe được đăng ký tại khu vực thành phố và nhận giá trị bằng 0 trong các trường hợp còn lại.\nNơi xe được đăng ký, biến urban, nhận giá trị là 1 nếu xe được đăng ký tại khu vực thành phố và nhận giá trị bằng 0 trong các trường hợp còn lại.Loại hình đăng ký xe, biến private, nhận giá trị là 1 nếu xe mua bảo hiểm là xe đăng ký theo cá nhân và nhận giá trị 0 trong các trường hợp còn lại.\nLoại hình đăng ký xe, biến private, nhận giá trị là 1 nếu xe mua bảo hiểm là xe đăng ký theo cá nhân và nhận giá trị 0 trong các trường hợp còn lại.Tình trạng hôn nhân của người lái xe, biến marital, nhận giá trị 'C' nếu đã kết hôn, 'S' tương ứng với chưa kết hôn, và 'O' tương ứng với đã ly dị.\nTình trạng hôn nhân của người lái xe, biến marital, nhận giá trị 'C' nếu đã kết hôn, 'S' tương ứng với chưa kết hôn, và 'O' tương ứng với đã ly dị.Biến mục tiêu hay biến phụ thuộc là biến \\(Y\\) nhận một trong hai giá trị, 'Yes' nếu người mua bảo hiểm trách nhiệm dân sự đồng ý lựa thêm quyền lợi bảo hiểm bổ sung và '' nếu người mua bảo hiểm trách nhiệm dân sự không lựa chọn mua quyền lợi bổ sung. Để mô hình ở dạng đơn giản nhất có thể, chúng tôi lựa chọn hai biến độc lập để xây dựng mô hình là biến age và biến sex. Chúng ta sẽ sử dụng biến age như một biến kiểu số, trong khi biến sex là một biến kiểu phân loại/rời rạc nhận một trong hai giá trị là 'M' tương ứng với nam giới và 'F' tương ứng với nữ giới.Đoạn lệnh R dưới đây được sử dụng để lấy dữ liệu và phân tích nhanh ảnh hưởng của các biến age và sex lên quyết định mua bảo hiểm bổ sung của người sở hữu xe ô tô.\nHình 12.1: Mối liên hệ giữa các biến giải thích là độ tuổi và giới tính lên biến mục tiêu quyết định mua bảo hiểm bổ sung trong dữ liệu MotoInsurance. Hình bên trái là đồ thị hình hộp cho biến phân phối xác suất của độ tuổi cho nhóm người có mua và không mua. Hình bên phải là đồ thị dạng cột cho biết mối liên hệ giữa giới tính đến quyết định mua bảo hiểm bổ sung\nTừ Hình 12.1 cho thấy cả hai biến age và sex có ảnh hưởng đến quyết định mua bảo hiểm bổ sung. Đồ thị bên trái cho thấy phân phối xác suất của những người có mua bảo hiểm bổ sung thấp hơn, điều này có nghĩa là những người trẻ hơn có xu hướng đồng ý mua bảo hiểm bổ sung hơn. Đồ thị bên phải cho thấy tỷ lệ nữ giới có mua bảo hiểm cao hơn với tỷ lệ nam giới, hay nói cách khác giới tính nữ có khả năng đồng ý mua bảo hiểm bổ sung cao hơn với nam giới.Chúng ta có thể khẳng định mối liên hệ giữa biến age đến biến mục tiêu \\(Y\\) qua phân tích phương sai:Mối liên hệ giữa biến giới tính đến biến mục tiêu được khẳng định qua kiểm định chisq.testChúng ta sẽ xây dựng một mô hình tuyến tính tổng quát để xác nhận lại các phân tích ở trên và lượng hóa được ảnh hưởng của các biến age và sex lên quyết định mua bảo hiểm bổ sung. Các thành phần của mô hình tuyến tính tổng quát mà chúng ta đã đề cập trong phần trên có thể được cụ thể hóa trên dữ liệu MotoInsurance như sauThứ nhất: biến mục tiêu \\(Y\\) chỉ nhận một trong hai giá trị là 'Yes' hoặc '' đó phân phối xác suất cho biến mục tiêu được lựa chọn để xây dựng mô hình là phân phối nhị thức với trung bình là \\(\\rho\\), ký hiệu \\(Y \\sim \\mathcal{B}(\\rho)\\). Giá trị trung bình \\(\\rho\\) phụ thuộc vào các biến giải thích.Thứ nhất: biến mục tiêu \\(Y\\) chỉ nhận một trong hai giá trị là 'Yes' hoặc '' đó phân phối xác suất cho biến mục tiêu được lựa chọn để xây dựng mô hình là phân phối nhị thức với trung bình là \\(\\rho\\), ký hiệu \\(Y \\sim \\mathcal{B}(\\rho)\\). Giá trị trung bình \\(\\rho\\) phụ thuộc vào các biến giải thích.Thứ hai: với mỗi quan sát của các biến giải thích age và sex, giả sử là \\(age_i\\) và \\(sex_i\\), chúng ta có giá trị trung bình của phân phối nhị thức là \\(\\rho_i\\). Đồng thời mối liên hệ giữa giá trị trung bình \\(\\rho_i\\) với các biến giải thích được mô tả thông qua một hàm số được gọi là hàm probit. Hàm probit là một hàm số đơn điệu tăng có miền xác định là khoảng (0,1) và nhận giá trị trên tập các số thực.Thứ hai: với mỗi quan sát của các biến giải thích age và sex, giả sử là \\(age_i\\) và \\(sex_i\\), chúng ta có giá trị trung bình của phân phối nhị thức là \\(\\rho_i\\). Đồng thời mối liên hệ giữa giá trị trung bình \\(\\rho_i\\) với các biến giải thích được mô tả thông qua một hàm số được gọi là hàm probit. Hàm probit là một hàm số đơn điệu tăng có miền xác định là khoảng (0,1) và nhận giá trị trên tập các số thực.\\[\\begin{align}\n& Y_i =  \\left(Y|\\text{age} = \\text{age}_i, \\text{sex} = \\text{sex}_i\\right) \\sim \\mathcal{B}(\\rho_i) \\\\\n& \\mathbb{E}(Y_i) =  \\rho_i \\\\\n& \\text{Probit}\\left(\\rho_i\\right) = \\beta_0 + \\beta_1 \\times \\text{age}_i + \\beta_2 \\times \\text{sex}_i \\\\\n\\tag{12.2}\n\\end{align}\\]Giá trị \\(\\rho_i\\) là vừa là giá trị trung bình, vừa là xác suất mà người \\(\\) có tuổi bằng \\(age_i\\), giới tính \\(sex_i\\), có mua bảo hiểm bổ sung. Hàm probit ở trên là hàm ngược của hàm phân phối xác suất của biến ngẫu nhiên phân phối chuẩn có trung bình 0 và phương sai bằng 1. Hàm số này dùng để kết nối giữa giá trị trung bình \\(\\rho_i\\) với các giá trị tương ứng của biến giải thích \\(age_i\\) và \\(sex_i\\) và còn được gọi là hàm liên kết.Trong mô hình tuyến tính tổng quát được mô tả trong hệ phương trình (12.2) có ba tham số cần được ước lượng từ dữ liệu là \\(\\beta_0\\), \\(\\beta_1\\) và \\(\\beta_2\\). Hàm số dùng để xây dựng và ước lượng mô hình tuyến tính tổng quát trong R là hàm glm() của thư viện stats. Chúng ta ước lượng các hệ số tuyến tính như sau:Có thể thấy rằng các hệ số của biến age và sex đều có ý nghĩa thống kê khi các giá trị p-value đều rất nhỏ. Đúng như chúng nhận định từ phần phân tích khám phá dữ liệu, hệ số tuyến tính của biến age là số âm, bằng -0.016, cho biết người trẻ tuổi hơn có khả năng đồng ý mua bảo hiểm bổ sung cao hơn. Hệ số tương ứng với biến giới tính nam là số âm, bằng -0.447, điều này cho biết khả năng nam giới đồng ý mua bảo hiểm bổ sung là thấp hơn với nữ giới. Các kết luận có thể đưa ra từ kết quả của mô hình tuyến tính tổng quát ở trên như sau:Xác suất mà một người mua thêm bảo hiểm bổ sung giảm nếu tuổi của người tham gia bảo hiểm tăng, điều này có nghĩa là những người trẻ tuổi hơn thường có nhu cầu mua đầy đủ các quyền lợi bảo hiểm hơn những người lớn tuổi.Xác suất mà một người mua thêm bảo hiểm bổ sung giảm nếu tuổi của người tham gia bảo hiểm tăng, điều này có nghĩa là những người trẻ tuổi hơn thường có nhu cầu mua đầy đủ các quyền lợi bảo hiểm hơn những người lớn tuổi.Xác suất mà nam giới mua thêm bảo hiểm bổ sung thấp hơn xác suất mà nữ giới mua thêm bảo hiểm bổ sung.Xác suất mà nam giới mua thêm bảo hiểm bổ sung thấp hơn xác suất mà nữ giới mua thêm bảo hiểm bổ sung.Mối liên hệ giữa xác suất mua bảo hiểm đầy đủ và các thuộc tính của người thứ \\(\\) được mô tả một cách định lượng thông qua phương trìnhMối liên hệ giữa xác suất mua bảo hiểm đầy đủ và các thuộc tính của người thứ \\(\\) được mô tả một cách định lượng thông qua phương trình\\[\\begin{align}\n\\text{Probit}(\\rho_i) = 0.678 - 0.016 \\times \\text{age}_i - 0.447 \\times \\text{sex}_i\n\\tag{12.3}\n\\end{align}\\]trong đó \\(age_i\\) là tuổi của người \\(\\); \\(sex_i\\) là giới tính của người \\(\\), nhận giá trị bằng 1 nếu người đó là nam giới và 0 nếu người đó là nữ giới. Hàm probit được lựa chọn trong công thức ở trên là hàm số ngược của hàm phân phối của biến ngẫu nhiên phân phối chuẩn \\(\\mathcal{N}(0,1)\\) nên có thể viết mối liên hệ giữa \\(\\rho_i\\) và các biến giải thích trong phương trình (12.3) như sau:\n\\[\\begin{align}\n\\rho_i & = \\text{Probit}^{-1}\\left(0.678 - 0.016 \\times \\text{age}_i - 0.447 \\times \\text{sex}_i \\right) \\\\\n& = \\Phi(0.678 - 0.016 \\times \\text{age}_i - 0.447 \\times \\text{sex}_i)\n\\tag{12.4}\n\\end{align}\\]trong đó \\(\\Phi\\) là hàm phân phối xác suất của biến ngẫu nhiên phân phối chuẩn \\(\\mathcal{N}(0,1)\\)\n\\[\\begin{align}\n\\Phi(x) = \\cfrac{1}{\\sqrt{2 \\pi}} \\int\\limits_{-\\infty}^x \\ exp\\left(-t^2/2\\right) \\ dt\n\\tag{12.5}\n\\end{align}\\]Hàm probit được lựa chọn làm hàm liên kết vì miền xác định của hàm số này là khoảng (0,1) phù hợp với việc mô tả xác suất \\(\\rho_i\\) trong khi miền giá trị của hàm số này là tập các số thực phù hợp với tổ hợp tuyến tính của các biến giải thích.Bạn đọc có thể nhận thấy được sự khác biệt giữa mô hình tuyến tính tổng quát ở trên với mô hình tuyến tính thông thường được trình bày trong các chương trước ở hai điểm:Thứ nhất: phân phối xác suất của biến mục tiêu \\(Y\\) là phân phối nhị thức chứ không phải là phân phối chuẩn.Thứ nhất: phân phối xác suất của biến mục tiêu \\(Y\\) là phân phối nhị thức chứ không phải là phân phối chuẩn.Thứ hai: mối liên hệ giữa giá trị trung bình của biến mục tiêu \\(Y\\) và tổ hợp tuyến tính của các biến độc lập được thể hiện thông qua một hàm số, trong trường hợp này là hàm probit. Trong mô hình tuyến tính thông thường, giá trị trung bình của biến mục tiêu bằng tổ hợp tuyến tính của các biến độc lập.Thứ hai: mối liên hệ giữa giá trị trung bình của biến mục tiêu \\(Y\\) và tổ hợp tuyến tính của các biến độc lập được thể hiện thông qua một hàm số, trong trường hợp này là hàm probit. Trong mô hình tuyến tính thông thường, giá trị trung bình của biến mục tiêu bằng tổ hợp tuyến tính của các biến độc lập.Hai điểm nêu trên cũng chính là hai cải tiến quan trọng của mô hình tuyến tính tổng quát với mô hình hồi quy tuyến tính thông thường. Việc tổng quát hóa phân phối của biến phụ thuộc và thiết lập một hàm liên kết giữa giá trị trung bình của biến phụ thuộc và với các biến độc lập giúp cho mô hình tuyến tính tổng quát linh hoạt hơn rất nhiều khi làm việc với các dữ liệu cụ thể và vẫn giữ được khả năng suy diễn giống như mô hình hồi quy tuyến tính thông thường. Trong phần tiếp theo của chương chúng ta sẽ thảo luận kỹ hơn về các vấn đề này.","code":"\ndat<-read.csv(\"../KHDL_KTKD Final/Dataset/MotoInsurance.csv\")\n\n# Đổi các biến Y và sex sang kiểu factor\ndat$Y<-as.factor(dat$Y)\ndat$sex<-as.factor(dat$sex)\n\n# Thực hiện các phân tích khai phá\np1<-dat%>%ggplot()+geom_boxplot(aes(x = Y, y = age, fill = Y), color = \"#640514\", alpha = 0.75)+\n  ggtitle(\"Mối liên hệ giữa age và biến Y\")+ \n  ylab(\"Độ tuổi\") + xlab(\"Quyết định mua bảo hiểm bổ sung\")+\n  theme_minimal() + theme(legend.position = \"none\")\n\np2<-dat%>%ggplot()+geom_bar(aes(x = sex, fill = Y),col = \"#640514\", alpha = 0.75)+\n  ggtitle(\"Mối liên hệ giữa sex và biến Y\")+\n  ylab(\"Số lượng người mua\") + xlab(\"Giới tính\")+\n  theme_minimal()+ guides(fill=guide_legend(title=\"Quyết định mua\"))\n\ngrid.arrange(p1,p2, ncol = 2)\nsummary(aov(age~Y,data = dat))##               Df Sum Sq Mean Sq F value Pr(>F)    \n## Y              1  24354   24354   139.9 <2e-16 ***\n## Residuals   3998 696008     174                   \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nchisq.test(dat$Y, dat$sex)## \n##  Pearson's Chi-squared test with Yates' continuity correction\n## \n## data:  dat$Y and dat$sex\n## X-squared = 130.19, df = 1, p-value < 2.2e-16\n# Biến Y có phân phối nhị thức\n# Hàm g là hàm probit\nglm1<-glm(Y ~ age + sex, data=dat, \n          family = binomial(link = \"probit\")) # khai báo hàm g\nsummary(glm1)## \n## Call:\n## glm(formula = Y ~ age + sex, family = binomial(link = \"probit\"), \n##     data = dat)\n## \n## Deviance Residuals: \n##     Min       1Q   Median       3Q      Max  \n## -1.4495  -0.9278  -0.7382   1.2584   2.0391  \n## \n## Coefficients:\n##              Estimate Std. Error z value Pr(>|z|)    \n## (Intercept)  0.678644   0.079138   8.575   <2e-16 ***\n## age         -0.016257   0.001619 -10.041   <2e-16 ***\n## sexM        -0.446783   0.047421  -9.422   <2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## (Dispersion parameter for binomial family taken to be 1)\n## \n##     Null deviance: 5163.3  on 3999  degrees of freedom\n## Residual deviance: 4933.9  on 3997  degrees of freedom\n## AIC: 4939.9\n## \n## Number of Fisher Scoring iterations: 4"},{"path":"mô-hình-tuyến-tính-tổng-quát..html","id":"các-nhược-điểm-của-mô-hình-hồi-quy-tuyến-tính.","chapter":"Chương 12 Mô hình tuyến tính tổng quát.","heading":"12.1 Các nhược điểm của mô hình hồi quy tuyến tính.","text":"Mô hình hồi quy tuyến tính là nền tảng quan trọng cho hầu hết các mô hình học máy và các mô hình trí tuệ nhân tạo hiện tại. Trước khi những mô hình học máy được nghiên cứu và phát triển mạnh mẽ như hiện nay, những người xây dựng mô hình luôn gặp khó khăn khi sử dụng mô hình hồi quy tuyến tính trong nhiều hoàn cảnh. Nguyên nhân là giả thiết về phân phối xác suất của biến mục tiêu và miền giá trị trung bình của biến mục tiêu của mô hình hồi quy tuyến tính thông thường là không phù hợp với đa số dữ liệu thực tế.Thật vậy, mô hình hồi quy tuyến tính được thảo luận trong phần trước của cuốn sách có thể được tóm tắt như sau: người xây dựng mô hình cố gắng nghiên cứu mối quan hệ giữa một biến mục tiêu \\(Y\\) với véc-tơ biến độc lập \\(\\textbf{X} = (X_1, X_2, \\cdots, X_p)\\) bằng cách cho rằng mối liên hệ giữa \\(Y\\) và \\(\\textbf{X}\\) là một hàm tuyến tính. Mối liên hệ đó không đồng nhất nên sai số sẽ tồn tại và những người xây dựng mô hình cho rằng sai số có phân phối chuẩn với trung bình bằng 0 và độ lệch chuẩn là một hằng số \\(\\sigma > 0\\). Chúng ta biểu diễn mô hình tuyến tính thông thường như sau:\\[\\begin{align}\n& Y = \\beta_0 + \\beta_1 \\cdot X_1 + \\beta_2 \\cdot X_2 + \\cdots + \\beta_p \\cdot X_p + \\epsilon \\\\\n& \\epsilon \\sim \\mathcal{N}(0, \\sigma^2)\n\\tag{12.6}\n\\end{align}\\]Bạn đọc có thể thấy rằng trong mô hình hồi quy tuyến tính, biến phụ thuộc \\(Y\\) là biến ngẫu nhiên có phân phối chuẩn có phương sai là \\(\\sigma^2\\) và giá trị trung bình phụ thuộc vào véc-tơ biến độc lập \\(\\textbf{X}_i\\). Với điều kiện biến độc lập nhận giá trị là \\(\\textbf{x}_i = (x_{,1}, x_{,2}, \\cdots, x_{,p})\\); chúng ta có mô hình hồi quy tuyến tính như sau:\\[\\begin{align}\n& Y_i \\sim  \\mathcal{N}(\\mu_i, \\sigma^2) \\\\\n& \\mu_i = \\beta_0 + \\beta_1 \\cdot x_{,1} + \\beta_2 \\cdot x_{,2} + \\cdots + \\beta_p \\cdot x_{,p}\n\\tag{12.7}\n\\end{align}\\]Ngoài giả thiết về phân phối chuẩn của \\(Y\\), mô hình hồi quy tuyến tính thông thường còn cho rằng giá trị trung bình của biến ngẫu nhiên \\(Y\\) với điều kiện các biến độc lập nhận giá trị \\(\\textbf{x}_i\\), ký hiệu \\(\\mu_i\\), là một tổ hợp tuyến tính của các biến độc lập. Với giả thiết này, mô hình tuyến tính luôn cho rằng miền giá trị của \\(\\mu_i\\) sẽ là toàn bộ tập các số thực \\(\\mathbb{R}\\).Câu hỏi đặt ra là: làm như thế nào để áp dụng mô hình hồi quy tuyến tính trong các trường hợp như sau?Thứ nhất. Biến mục tiêu \\(Y\\) chỉ nhận hai giá trị là 0 hoặc 1. Đây là trường hợp rất thường gặp phải trong nhiều lĩnh vực khi thực hiện phân tích dữ liệu. Có thể kể đến như: khi biến \\(Y\\) đại diện cho sự kiện một người có hay không tham gia bảo hiểm xã hội; một người có hay không thực hiện rút bảo hiểm xã hội một lần trong thời gian một khoảng thời gian; một khách hàng có hay không gửi yêu cầu thanh toán bảo hiểm; hay tương tự như ví dụ trong phần đầu của cuốn sách, một khách hàng đã mua bảo hiểm bắt buộc có hay không mua thêm các quyền lợi bảo hiểm bổ sung. Ngoài lĩnh vực bảo hiểm, biến mục tiêu \\(Y\\) chỉ nhận giá trị 0 hoặc 1 còn xuất hiện trong lĩnh vực ngân hàng-tài chính như: biến \\(Y\\) cho biết một giao dịch trực tuyến có phải là một giao dịch gian lận hay không; một khách hàng có hay không tiếp nhận sản phẩm dịch vụ tài chính; một khách hàng có hay không hoàn trả khoản nợ thẻ tín dụng trong thời gian tới; … Trong tất cả các trường hợp kể trên không thể giả thiết phân phối xác suất của biến mục tiêu là phân phối chuẩn. Hơn thế nữa, giá trị trung bình của biến mục tiêu sẽ luôn nằm trong khoảng 0 đến 1, chứ không phải toàn bộ tập số thực. Nếu sử dụng một tổ hợp tuyến tính của các biến độc lập để tính toán giá trị trung bình của biến mục tiêu, chúng ta sẽ có thể gặp các giá trị nhỏ hơn 0 hoặc các giá trị lớn hơn 1.Biến mục tiêu \\(Y\\) là biến dạng rời rạc có thứ tự. Chẳng hạn như \\(Y\\) cho biết một người tham gia bảo hiểm xã hội gửi yêu cầu bồi thường bao nhiêu lần trong một năm; biến \\(Y\\) cho biết một khách hàng mua bảo hiểm xe ô tô gây ra bao nhiêu tai nạn trong thời gian được bảo hiểm,… Trong trường hợp này, \\(Y\\) sẽ nhận giá trị kiểu số đếm: \\(0, 1, 2, \\cdots\\) tương ứng với số lần khách hàng gửi yêu cầu bảo hiểm. Không thể sử dụng mô hình hồi quy tuyến tính thông thường với giả thiết biến \\(Y\\) phân phối chuẩn trên các quan sát là các giá trị số đếm. Đồng thời, giá trị trung bình của \\(Y\\) là một số lớn hơn 0 chỉ là một tập hợp con của tập các số thực.Biến mục tiêu \\(Y\\) là biến dạng rời rạc có thứ tự. Chẳng hạn như \\(Y\\) cho biết một người tham gia bảo hiểm xã hội gửi yêu cầu bồi thường bao nhiêu lần trong một năm; biến \\(Y\\) cho biết một khách hàng mua bảo hiểm xe ô tô gây ra bao nhiêu tai nạn trong thời gian được bảo hiểm,… Trong trường hợp này, \\(Y\\) sẽ nhận giá trị kiểu số đếm: \\(0, 1, 2, \\cdots\\) tương ứng với số lần khách hàng gửi yêu cầu bảo hiểm. Không thể sử dụng mô hình hồi quy tuyến tính thông thường với giả thiết biến \\(Y\\) phân phối chuẩn trên các quan sát là các giá trị số đếm. Đồng thời, giá trị trung bình của \\(Y\\) là một số lớn hơn 0 chỉ là một tập hợp con của tập các số thực.Ngay cả khi trong các trường hợp biến phụ thuộc \\(Y\\) là biến liên tục, sử dụng mô hình tuyến tính thông thường cũng sẽ gặp phải vấn đề. Chẳng hạn như khi \\(Y\\) là số tiền khách hàng yêu cầu bồi thường cho xe ô tô trong trường hợp xảy ra tai nạn. Biến \\(Y\\) chỉ nhận giá trị là số dương và thường có phân phối xác suất lệch phải với đuôi dài. Sử dụng giả thiết phân phối chuẩn cho biến \\(Y\\) sẽ làm cho mô hình không đánh giá đúng khả mức độ nghiêm trọng của yêu cầu bồi thường phân phối chuẩn không có khả năng mô tả các rủi ro có đuôi dài. Đồng thời, giá trị trung bình của số tiền yêu cầu bồi thường luôn là số dương, đó cũng không thể sử dụng tổ hợp tuyến tính của các biến độc lập để trực tiếp mô tả.Ngay cả khi trong các trường hợp biến phụ thuộc \\(Y\\) là biến liên tục, sử dụng mô hình tuyến tính thông thường cũng sẽ gặp phải vấn đề. Chẳng hạn như khi \\(Y\\) là số tiền khách hàng yêu cầu bồi thường cho xe ô tô trong trường hợp xảy ra tai nạn. Biến \\(Y\\) chỉ nhận giá trị là số dương và thường có phân phối xác suất lệch phải với đuôi dài. Sử dụng giả thiết phân phối chuẩn cho biến \\(Y\\) sẽ làm cho mô hình không đánh giá đúng khả mức độ nghiêm trọng của yêu cầu bồi thường phân phối chuẩn không có khả năng mô tả các rủi ro có đuôi dài. Đồng thời, giá trị trung bình của số tiền yêu cầu bồi thường luôn là số dương, đó cũng không thể sử dụng tổ hợp tuyến tính của các biến độc lập để trực tiếp mô tả.Có thể tổng kết rằng hai vấn đề thường gặp phải khi sử dụng mô hình hồi quy tuyến tính thông thường trên dữ liệu thực tế làThứ nhất: sự không phù hợp của giả thiết phân phối chuẩn đối với biến mục tiêu \\(Y\\); vàThứ nhất: sự không phù hợp của giả thiết phân phối chuẩn đối với biến mục tiêu \\(Y\\); vàThứ hai: miền giá trị trung bình của biến mục tiêu \\(Y\\) không phù hợp với miền giá trị của tổ hợp tuyến tính của biến độc lập. Giá trị \\(\\beta_0 + \\beta_1 \\ x_{,1} + \\beta_2 \\ x_{,2} + \\cdots + \\beta_p \\ x_{,p}\\) có thể nhận bất kỳ giá trị nào trong tập các số thực \\(\\mathbb{R}\\), trong khi giá trị trung bình của biến mục tiêu \\(Y\\) trong các dữ liệu thực tế lại thường chỉ là một tập con của \\(\\mathbb{R}\\).Thứ hai: miền giá trị trung bình của biến mục tiêu \\(Y\\) không phù hợp với miền giá trị của tổ hợp tuyến tính của biến độc lập. Giá trị \\(\\beta_0 + \\beta_1 \\ x_{,1} + \\beta_2 \\ x_{,2} + \\cdots + \\beta_p \\ x_{,p}\\) có thể nhận bất kỳ giá trị nào trong tập các số thực \\(\\mathbb{R}\\), trong khi giá trị trung bình của biến mục tiêu \\(Y\\) trong các dữ liệu thực tế lại thường chỉ là một tập con của \\(\\mathbb{R}\\).Mô hình tuyến tính tổng quát được xây dựng trên cơ sở của mô hình hồi quy tuyến tính thông thường với mục đích khắc phục hai nhược điểm kể trên:Trước hết, mô hình tuyến tính tổng quát giả thiết một phân phối phù hợp cho biến phụ thuộc \\(Y\\), tùy vào dữ liệu sử dụng để phân tích, tạm gọi là phân phối \\(F\\) với tham số \\(\\boldsymbol{\\theta}\\), ký hiệu là \\(F_\\boldsymbol{\\theta}\\).Tiếp theo, để đảm bảo miền giá trị của giá trị trung bình của \\(Y\\) với điều kiện biến độc lập bằng \\(\\textbf{x}_i\\), \\(\\mu_i = E(Y|\\textbf{X} = \\textbf{x}_i)\\), phù hợp với miền giá trị của \\(\\textbf{x}_i^{'} \\boldsymbol{\\beta}\\), mô hình tuyển tính tổng quát sử dụng một hàm số đơn điệu \\(g\\), được gọi là hàm liên kết, để biến đổi miền giá trị của \\(\\mu_i\\). Chúng ta phát biểu mô hình tuyến tính tổng quát như sau\\[\\begin{align}\n& Y \\sim  F_\\theta \\\\\n& \\mathbb{E}(Y|\\textbf{X} = \\textbf{x}_i) = \\mu_i \\\\\n& g(\\mu_i) = \\beta_0 + \\beta_1 \\cdot x_{,1} + \\beta_2 \\cdot x_{,2} + \\cdots + \\beta_p \\cdot x_{,p}\n\\tag{12.8}\n\\end{align}\\]Trước hết, có thể thấy rằng mô hình hồi quy tuyến tính thông thường là trường hợp đặc biệt của mô hình tuyến tính tổng quát khi phân phối \\(F\\) là phân phối chuẩn; tham số \\(\\boldsymbol{\\theta}\\) là \\(\\sigma^2\\); và hàm \\(g\\) là hàm số đồng nhất \\(g(x) = x\\).Giả thiết đơn điệu của hàm liên kết \\(g\\) đảm bảo sự tồn tại của hàm số ngược \\(g^{-1}\\). Mối liên hệ của \\(\\mu_i\\) và \\(\\textbf{x}_i^{'} \\boldsymbol{\\beta}\\) có thể được viết lại dưới dạng hàm ngược của hàm liên kết như sau\n\\[\\begin{align}\n\\mu_i = g^{-1}\\left(\\beta_0 + \\beta_1 \\cdot x_{,1} + \\beta_2 \\cdot x_{,2} + \\cdots + \\beta_p \\cdot x_{,p}\\right)\n\\tag{12.9}\n\\end{align}\\]Quay trở lại ví dụ ở phần trước của chương sách, chúng ta phân tích về tác động của độ tuổi và giới tính lên quyết định có tham gia quyền lợi bảo hiểm bổ sung hay không. Mô hình tuyến tính tổng quát được xây dựng như sau:\\[\\begin{align}\n& Y \\sim  \\mathcal{B}(\\rho) \\\\\n& \\mathbb{E}(Y|\\text{age}_i, \\text{sex}_i) = \\rho_i \\\\\n& \\Phi^{-1}(\\rho_i) = 0.678 - 0.016 \\times \\text{age}_i - 0.446 \\times \\text{sex}_i \\\\\n& \\rho_i = \\Phi\\left(0.678 - 0.016 \\times \\text{age}_i - 0.446 \\times \\text{sex}_i\\right)\n\\end{align}\\]Phân phối xác suất của biến \\(Y\\) là phân phối nhị thức \\(Y\\) chỉ có thể nhận là 0 hoặc 1. Đồng thời, giá trị trung bình của \\(Y\\) nằm trong khoảng \\((0,1)\\) chúng ta có thể chọn hàm liên kết \\(g\\) là hàm \\(\\Phi^{-1}\\). Nếu không lựa chọn \\(\\Phi^{-1}\\) làm hàm liên kết, mọi hàm số đơn điệu, có miền xác định là khoảng \\((0,1)\\), và miền giá trị là tập số thực \\(\\mathbb{R}\\) đều có thể được lựa chọn thay thế.","code":""},{"path":"mô-hình-tuyến-tính-tổng-quát..html","id":"xây-dựng-mô-hình-tuyến-tính-tổng-quát","chapter":"Chương 12 Mô hình tuyến tính tổng quát.","heading":"12.2 Xây dựng mô hình tuyến tính tổng quát","text":"Trong phần này của cuốn sách, chúng ta sẽ thảo luận về cách xây dựng mô hình tuyến tổng quát với các kiểu giá trị khác nhau của biến phụ thuộc \\(Y\\). Xin nhắc lại rằng đây là cuốn sách dành cho cả các bạn đọc không có nền tảng toán học nâng cao. đó, những thảo luận phức tạp liên quan đến các giả thiết của mô hình hay phương pháp ước lượng tham số sẽ được trình bày ở phần sau của chương sách. Chúng ta sẽ hiểu về mô hình tuyến tính tổng quát thông qua việc ứng dụng mô hình cho các các dữ liệu thực tế trước khi đi sâu vào bản chất toán học của mô hình.","code":""},{"path":"mô-hình-tuyến-tính-tổng-quát..html","id":"biến-phụ-thuộc-là-biến-dạng-nhị-phân.","chapter":"Chương 12 Mô hình tuyến tính tổng quát.","heading":"12.2.1 Biến phụ thuộc là biến dạng nhị phân.","text":"Đây là các trường hợp mà biến phụ thuộc chỉ nhận một trong hai giá trị. Trong phần trước chúng ta đã nêu một số ví dụ cho trường hợp này: khách hàng có hay không hoàn trả nợ thẻ tín dụng, khách hàng phản hồi tích cực hay tiêu cực về sản phầm, một yêu cầu bồi thường bảo hiểm là trục lợi hay bình thường, một giao dịch rút tiền ngân hàng có hay không phải là giao dịch gian lận, … Mặc dù đây chỉ là một trường hợp đặc biệt của biến phụ thuộc nhận giá trị rời rạc nhưng qua các ví dụ thực tế lại thấy rằng phần lớn các dữ liệu gặp phải lại có biến phụ thuộc ở dạng nhị phân. Khi \\(Y\\) chỉ nhận hai giá trị, chúng ta sẽ luôn mã hóa giá trị của \\(Y\\) thành hai số là 0 và 1. Một vài cuốn sách khác, hoặc trong một vài lĩnh vực nghiên cứu khác, người xây dựng mô hình có thể mã hóa \\(Y\\) thành -1 và 1. Tuy nhiên hai cách mã hóa này chỉ khác nhau ở hình thức chứ không làm ảnh hưởng đến kết quả của mô hình.","code":""},{"path":"mô-hình-tuyến-tính-tổng-quát..html","id":"phân-phối-xác-suất-của-biến-phụ-thuộc.","chapter":"Chương 12 Mô hình tuyến tính tổng quát.","heading":"12.2.1.1 Phân phối xác suất của biến phụ thuộc.","text":"Khi biến phụ thuộc là biến dạng nhị phân, một cách tự nhiên, chúng ta sẽ sử dụng phân phối nhị thức để mô tả biến phụ thuộc. Biến ngẫu nhiên \\(B\\) có phân phối nhị thức với tham số \\(\\rho\\), \\(0 < \\rho < 1\\), ký hiệu \\(\\mathcal{B}(\\rho)\\), là biến ngẫu nhiên chỉ nhận hai giá trị là 0 và 1 với hàm khối lượng xác suất như sau\\[\\begin{align}\n\\mathbb{P}(B = x) = \\rho^x \\times (1-\\rho)^{(1-x)} \\text{ với } x \\\\{0;1\\}\n\\tag{12.10}\n\\end{align}\\]Chúng ta có giá trị trung bình và phương sai của \\(\\mathcal{B}(\\rho)\\).\n\\[\\begin{align}\n& \\mathbb{E}(B) = \\rho \\\\\n& \\mathbb{V}(B) = \\rho \\ (1-\\rho)\n\\tag{12.11}\n\\end{align}\\]Có thể thấy rằng phân phối nhị thức có duy nhất một tham số \\(\\rho\\) và tham số này cũng chính là giá trị trung bình của biến đó. Phương sai của \\(\\mathcal{B}(\\rho)\\) nhỏ hơn giá trị trung bình.","code":""},{"path":"mô-hình-tuyến-tính-tổng-quát..html","id":"lựa-chọn-hàm-liên-kết.","chapter":"Chương 12 Mô hình tuyến tính tổng quát.","heading":"12.2.1.2 Lựa chọn hàm liên kết.","text":"Khi biến phụ thuộc \\(Y\\) có phân phối nhị thức, giá trị trung bình của biến phụ thuộc sẽ nằm trong khoảng \\((0,1)\\). Từ công thức (12.8), nếu cho \\(\\rho_i\\) là giá trị trung bình của biến phụ thuộc với điều kiện các biến độc lập \\(\\textbf{X} = \\textbf{x}_i\\), ta có\\[\\begin{align}\n& \\mathbb{E}(Y|\\textbf{X} = \\textbf{x}_i) = \\rho_i \\\\\n& g(\\rho_i) = \\beta_0 + \\beta_1 \\cdot x_{,1} + \\beta_2 \\cdot x_{,2} + \\cdots + \\beta_p \\cdot x_{,p}\n\\tag{12.12}\n\\end{align}\\]Như vậy, mọi hàm số đơn điệu có miền xác định là khoảng \\((0,1)\\) và miền giá trị là tập số thực \\(\\mathbb{R}\\) đều có thể được lựa chọn để làm hàm ngược của hàm liên kết. Làm thế nào để có được những hàm số có tính chất như vậy? Chúng ta đều biết rằng các hàm phân phối xác suất của một biến ngẫu nhiên liên tục bất kỳ là các hàm số đơn điệu tăng, có miền xác định là \\(\\mathbb{R}\\) và miền giá trị là khoảng \\((0,1)\\). đó, hàm số ngược của các hàm phân phối xác suất, sẽ là các hàm số tăng, có miền xác định là khoảng \\((0,1)\\) và miền giá trị là \\(\\mathbb{R}\\). Nói một cách khác, hàm số ngược của các hàm phân phối xác suất liên tục bất kỳ thỏa mãn đầy đủ tính chất của hàm liên kết trong trường hợp \\(Y\\) có phân phối nhị thức.Trong thực tế, việc lựa chọn hàm liên kết còn có mục tiêu là để mô hình dễ giải thích và quá trình ước lượng mô hình đơn giản nhất có thể. Các hàm phân phối xác suất thường được lựa chọn làm hàm ngược của hàm liên kết bao gồm:Hàm phân phối của biến ngẫu nhiên logistic;\nHàm phân phối của biến ngẫu nhiên logistic;Hàm phân phối của biến ngẫu nhiên phân phối chuẩn;\nHàm phân phối của biến ngẫu nhiên phân phối chuẩn;Hàm phân phối của biến ngẫu nhiên Cauchy.\nHàm phân phối của biến ngẫu nhiên Cauchy.Hàm phân phối của biến ngẫu nhiên logistic và hàm ngược được cho bởi công thức sau\n\\[\\begin{align}\n& \\text{Hàm phân phối xác suất: } \\ g^{-1}(x) = \\cfrac{1}{1 + e^{-x}} \\\\\n& \\text{Hàm ngược: } \\ g(x) = ln(\\cfrac{x}{1- x})\n\\end{align}\\]Với biến phụ thuộc \\(Y\\) là có phân phối nhị thức và hàm \\(g\\) là hàm số ngược của hàm phân phối của biến ngẫu nhiên logistic, chúng ta có mô hình tuyến tính tổng quát như sau:\\[\\begin{align}\n& Y_i \\sim \\mathcal{B}(\\rho_i) \\\\\n& \\rho_i = \\cfrac{1}{1 + e^{-(\\beta_0 + \\beta_1 \\cdot x_{,1} + \\beta_2 \\cdot x_{,2} + \\cdots + \\beta_p \\cdot x_{,p})}} \\\\\n& ln(\\cfrac{\\rho_i}{1 - \\rho_i}) = \\beta_0 + \\beta_1 \\cdot x_{,1} + \\beta_2 \\cdot x_{,2} + \\cdots + \\beta_p \\cdot x_{,p}\n\\tag{12.13}\n\\end{align}\\]Mô hình (12.13) được biết đến rộng rãi trong học thuật và ứng dụng với tên gọi là hồi quy logistic và cũng là mô hình thường được sử dụng nhất trong khi biến phụ thuộc là biến nhị phân. Mô hình có ưu điểm là sự dễ hiểu khi diễn giải kết quả:\\(\\rho_i\\) ngoài ý nghĩa là trung bình của biến ngẫu nhiên \\(Y_i\\), còn có ý nghĩa là xác suất xảy ra sự kiện \\(Y_i = 1\\).\\(\\rho_i\\) ngoài ý nghĩa là trung bình của biến ngẫu nhiên \\(Y_i\\), còn có ý nghĩa là xác suất xảy ra sự kiện \\(Y_i = 1\\).Giá trị \\(\\cfrac{\\rho_i}{1 - \\rho_i}\\) được gọi là odds của sự kiện \\(Y_i = 1\\). Mối liên hệ giữa quan sát thứ \\(\\) của biến độc lập \\(X_j\\) là \\(x_{,j}\\) và obbs của sự kiện \\(Y_i = 1\\) có thể được diễn giải trực tiếp qua hệ số \\(\\beta_j\\).Giá trị \\(\\cfrac{\\rho_i}{1 - \\rho_i}\\) được gọi là odds của sự kiện \\(Y_i = 1\\). Mối liên hệ giữa quan sát thứ \\(\\) của biến độc lập \\(X_j\\) là \\(x_{,j}\\) và obbs của sự kiện \\(Y_i = 1\\) có thể được diễn giải trực tiếp qua hệ số \\(\\beta_j\\).Hàm phân phối của biến ngẫu nhiên chuẩn \\(\\mathcal{N}(0,1)\\) và hàm ngược của hàm phân phối được cho bởi công thức sau\n\\[\\begin{align}\n& g^{-1}(x) = \\Phi(x) \\\\\n& g(x) = \\Phi^{-1}(x) \\\\\n& \\Phi(x) = \\cfrac{1}{\\sqrt{2 \\pi}} \\  \\int\\limits_{-\\infty}^x \\ exp(-t^2/2) \\ dt  \\\\\n\\end{align}\\]Chúng ta có mô hình tuyến tính tổng quát như sau\\[\\begin{align}\n& Y_i \\sim \\mathcal{B}(\\rho_i) \\\\\n& \\rho_i = \\Phi(\\beta_0 + \\beta_1 \\cdot x_{,1} + \\beta_2 \\cdot x_{,2} + \\cdots + \\beta_p \\cdot x_{,p}) \\\\\n& \\Phi^{-1}(\\rho_i) = \\beta_0 + \\beta_1 \\cdot x_{,1} + \\beta_2 \\cdot x_{,2} + \\cdots + \\beta_p \\cdot x_{,p}\n\\tag{12.14}\n\\end{align}\\]Hàm số ngược của biến ngẫu nhiên phân phối chuẩn được gọi là hàm probit đó mô hình tuyến tính tổng quát trong trường hợp này còn được biết đến với tên gọi là mô hình probit.Hàm phân phối của biến ngẫu nhiên Cauchy và hàm ngược của hàm phân phối được cho bởi công thức sau\n\\[\\begin{align}\n& g^{-1}(x) = 0.5 + \\cfrac{\\text{arctan}(x)}{\\pi} \\\\\n& g(x) = \\text{tan}\\left( \\pi \\left( x - 0.5 \\right) \\right)\n\\tag{12.15}\n\\end{align}\\]Chúng ta có mô hình tuyến tính tổng quát như sau\\[\\begin{align}\n& Y_i \\sim \\mathcal{B}(\\rho_i) \\\\\n& p_i = 0.5+ \\cfrac{\\text{arctan}(\\beta_0 + \\beta_1 \\cdot x_{,1} + \\beta_2 \\cdot x_{,2} + \\cdots + \\beta_p \\cdot x_{,p})}{\\pi}  \\\\\n& \\text{tan}\\left( \\pi \\left( \\rho_i - 0.5 \\right) \\right) = \\beta_0 + \\beta_1 \\cdot x_{,1} + \\beta_2 \\cdot x_{,2} + \\cdots + \\beta_p \\cdot x_{,p}\n\\tag{12.16}\n\\end{align}\\]Hàm số ngược của phân phối cauchy còn được gọi là hàm cauchit, đó mô hình tuyến tính tổng quát trong trường hợp này còn được biết đến với tên là mô hình cauchit.Hình 12.2 mô tả hình dạng của ba phân phối xác suất và ba hàm liên kết là các hàm ngược của hàm phân phối xác suất\nHình 12.2: Các hàm phân phối xác suất thường được sử dụng làm hàm ngược của hàm liên kết khi biến phụ thuộc có phân phối nhị thức. Hình bên trái: các hàm phân phối xác suất. Hình bên phải: các hàm liên kết\nMô hình tuyến tính tổng quát trong trường hợp \\(Y\\) là biến nhị phân có các tham số là các hệ số tuyến tính \\(\\beta_0\\), \\(\\beta_1\\), \\(\\cdots\\), \\(\\beta_p\\). Các tham số này được ước lượng từ dữ liệu bằng phương pháp tối đa hóa hàm hợp lý, hay gọi tắt là phương pháp MLE. Chúng tôi sẽ trình bày phần ước lượng mô hình trong phần sau của chương sách để tránh nhắc đến các khái niệm toán học phức tạp.Hàm số glm() của thư viện stats được sử dụng để xây dựng mô hình tuyến tính tổng quát. Tham số family trong hàm glm() dùng để khai báo phân phối xác suất cho biến phụ thuộc \\(Y\\) và để lựa chọn hàm liên kết phù hợp. Trở lại với ví dụ khi \\(Y\\) là biến nhị phân mô tả khách hàng có hay không lựa chọn các quyền lợi đầy đủ khi tham gia bảo hiểm bổ sung, chúng ra thực hiện xây dựng mô hình như sauCả ba mô hình tuyến tính tổng quát ở trên đều cho \\(Y\\) là một biến ngẫu nhiên phân phối nhị thức, nhưng mối liên hệ giữa độ tuổi và giới tính đến giá trị trung bình của \\(Y\\) lại được mô tả bằng các công thức khác nhau. Từ kết quả ước lượng các mô hình, chúng ta cóTrong mô hình logistic:\n\\[\\begin{align}\n\\mathbb{P}(Y_i = 1| \\text{age}_i, \\text{sex}_i) = \\cfrac{1}{1 + exp(-(1.109 - 0.026 \\times \\text{age}_i - 0.723 \\times \\text{sex}_i))}\n\\end{align}\\]Trong mô hình logistic:\n\\[\\begin{align}\n\\mathbb{P}(Y_i = 1| \\text{age}_i, \\text{sex}_i) = \\cfrac{1}{1 + exp(-(1.109 - 0.026 \\times \\text{age}_i - 0.723 \\times \\text{sex}_i))}\n\\end{align}\\]Trong mô hình probit:\n\\[\\begin{align}\n\\mathbb{P}(Y_i = 1| \\text{age}_i, \\text{sex}_i) = \\Phi\\left(0.678 - 0.016 \\times \\text{age}_i - 0.446 \\times \\text{sex}_i\\right)\n\\end{align}\\]Trong mô hình probit:\n\\[\\begin{align}\n\\mathbb{P}(Y_i = 1| \\text{age}_i, \\text{sex}_i) = \\Phi\\left(0.678 - 0.016 \\times \\text{age}_i - 0.446 \\times \\text{sex}_i\\right)\n\\end{align}\\]Trong mô hình cauchit:\n\\[\\begin{align}\n\\mathbb{P}(Y_i = 1| \\text{age}_i, \\text{sex}_i) = \\cfrac{1}{2} + \\cfrac{\\text{arctan}\\left(0.997 - 0.024 \\times \\text{age}_i - 0.624 \\times \\text{sex}_i\\right)} {\\pi}\n\\end{align}\\]Trong mô hình cauchit:\n\\[\\begin{align}\n\\mathbb{P}(Y_i = 1| \\text{age}_i, \\text{sex}_i) = \\cfrac{1}{2} + \\cfrac{\\text{arctan}\\left(0.997 - 0.024 \\times \\text{age}_i - 0.624 \\times \\text{sex}_i\\right)} {\\pi}\n\\end{align}\\]Cả ba mô hình đều cho cùng một kết quả: người có độ tuổi càng cao thì càng ít có khả năng lựa chọn quyền lợi bảo hiểm bổ sung và xác suất nam giới lựa chọn quyền lợi bảo hiểm bổ sung là ít hơn với nữ giới. Bảng ?? tổng hợp kết quả là xác suất chấp nhận mua thêm quyền lợi bảo hiểm bổ sung theo ba mô hình với hai biến độc lập là độ tuổi và giới tính:Từ Bảng ?? có thể thấy rằng không có sự khác biệt lớn trong tính toán xác suất của \\(Y\\) khi sử dụng hàm liên kết khác nhau. Thực tế cho thấy việc lựa chọn hàm liên kết sẽ không ảnh hưởng lớn đến kết quả của mô hình. Các lựa chọn quan trọng hơn trong mô hình tuyến tính bao gồm lựa chọn phân phối cho biến phụ thuộc và lựa chọn biến độc lập để đưa vào trong mô hình.","code":"\n# Phân phối Y là nhị thức và hàm liên kết là hàm logit\nglm.binomial.logit<-glm(Y ~ age + sex, data=dat, \n          family = binomial(link = \"logit\"))\n\n# Phân phối Y là nhị thức và hàm liên kết là hàm probit\nglm.binomial.probit<-glm(Y ~ age + sex, data=dat, \n          family = binomial(link = \"probit\"))\n\n# Phân phối Y là nhị thức và hàm liên kết là hàm cauchit\nglm.binomial.cauchy<-glm(Y ~ age + sex, data=dat, \n          family = binomial(link = \"cauchit\"))"},{"path":"mô-hình-tuyến-tính-tổng-quát..html","id":"biến-phụ-thuộc-là-biến-rời-rạc-không-có-thứ-tự.","chapter":"Chương 12 Mô hình tuyến tính tổng quát.","heading":"12.2.2 Biến phụ thuộc là biến rời rạc không có thứ tự.","text":"Biến rời rạc không có thứ tự, hay còn gọi là biến rời rạc danh nghĩa (non-ordinal variable), là biến ngẫu nhiên mà các giá trị có thể nhận không có ý nghĩa sánh với nhau. Khi nói đến biến rời rạc không có thứ tự, chúng ta luôn hiểu rằng biến nhận từ ba giá trị trở lên bởi vì trường hợp biến mục tiêu chỉ có hai giá trị sẽ được xử lý như biến phân phối nhị phân. Một ví dụ đơn giản cho biến rời rạc không có thứ tự là màu sắc được chọn khi mua xe ô tô, hoặc loại hình bảo hiểm được lựa chọn bởi các khách hàng của một công ty bảo hiểm.Giả sử biến rời rạc danh nghĩa \\(Y\\) có thể nhận \\(J\\) giá trị khác nhau lần lượt là \\(1, 2, \\cdots, J\\). Với biến danh nghĩa, chúng ta khó có thể mô tả được mối liên hệ giữa xác suất của hai sự kiện \\((Y=)\\) và \\((Y=j)\\) với \\(1 \\leq < j \\leq J\\) dưới dạng tham số. Chính vì thế cấu trúc dạng tham số của mô hình tuyến tính tổng quát như phương trình (12.1) là khó áp dụng trong trường hợp này.Một phương pháp tiếp cận cho trường hợp biến mục tiêu \\(Y\\) là biến rời rạc danh nghĩa là mở rộng mô hình tuyến tính tổng quát với biến nhị phân: với mỗi \\(j \\{1,2,\\cdots,J}\\), xác suất biến mục tiêu nhận giá trị bằng \\(j\\) với điều kiện các biến độc lập nhận giá trị \\(\\textbf{x}_i\\) được mô tả dưới dạng tham số như sau:\n\\[\\begin{align}\n\\mathbb{P}\\left(Y = j\\right|\\textbf{x}_i) = \\cfrac{h(\\beta_{j,0} + \\beta_{j,1} \\cdot x_{,1} + \\cdots + \\beta_{j,p} \\cdot x_{,p} ) }{ \\sum\\limits_{j=1}^J h(\\beta_{j,0} + \\beta_{j,1} \\cdot x_{,1} + \\cdots + \\beta_{j,p} \\cdot x_{,p} ) }\n\\tag{12.17}\n\\end{align}\\]với hàm số \\(h\\) có miền xác định là tập số thực \\(\\mathbb{R}\\) và miền giá trị là tập các số thực dương \\(\\mathbb{R}^+\\). Cũng giống như khi lựa chọn hàm liên kết, việc lựa chọn hàm số \\(h\\) thường thường được lựa chọn sao cho quá trình ước lượng các hệ số tuyến tính không quá phức tạp. đó, hàm lũy thừa cơ số tự nhiên exp() thường được lựa chọn. Trong trường hợp này, hàm số xác định xác suất xảy ra các sự kiện \\((Y=j)\\) trong phương trình (12.17) được gọi là hàm Softmax:\n\\[\\begin{align}\n\\text{softmax}(\\textbf{z})  & = \\left(\\cfrac{e^{z_1}}{\\sum\\limits_{j=1}^p e^{z_j}}, \\cfrac{e^{z_2}}{\\sum\\limits_{j=1}^p e^{z_1}}, \\cdots, \\cfrac{e^{z_p}}{\\sum\\limits_{=1}^p e^{z_j}}  \\right) \\\\\n\\textbf{z} & = (z_1, z_2, \\cdots, z_p)\n\\end{align}\\]Các tham số \\(\\beta_{j,k}\\) được ước lượng để tối thiểu hóa hàm cross-entropy:\n\\[\\begin{align}\n\\sum\\limits_{=1}^n \\sum\\limits_{j=1}^J y_{,j} \\cdot \\log\\left(\\mathbb{P}\\left(Y = j|\\textbf{x}_i\\right)\\right)\n\\end{align}\\]trong đó \\(\\mathbb{P}\\left(Y = j|\\textbf{x}_i\\right)\\) được tính toán từ công thức (12.17) và \\(y_{,j}\\) nhận một trong hai giá trị:là 1 nếu giá trị quan sát thứ \\(\\) của biến mục tiêu \\(Y\\) bằng \\(j\\)là 0 nếu giá trị quan sát thứ \\(\\) của biến mục tiêu \\(Y\\) khác \\(j\\)Lưu ý rằng có \\(J\\) véc-tơ hệ số tuyến tính trong phương trình (12.17). Trong thực tế, khi biến mục tiêu \\(Y\\) có thể nhận \\(J\\) giá trị danh nghĩa, chúng ta xây dựng mô hình với \\((J-1)\\) véc-tơ hệ số tuyến tính tương ứng với các giá trị danh nghĩa \\(1, 2, \\cdots, (J-1)\\), đồng thời cố định giá trị của tất cả các hệ số tuyến tính bằng 0 với giá trị danh nghĩa \\(J\\). Khi tất cả các hệ số tuyến tính bằng 0, chúng ta có \\(exp(\\textbf{x}_i^{'} \\boldsymbol{\\beta}_j) = exp(0) = 1\\) với mọi \\(\\textbf{x}_i\\). Phương trình (12.17) được viết lại như sau:\\[\\begin{align}\n& \\mathbb{P}\\left(Y = j\\right|x_i) = \\cfrac{h(\\beta_{j,0} + \\beta_{j,1} \\cdot x_{,1} + \\cdots + \\beta_{j,p} \\cdot x_{,p} ) }{1 + \\sum\\limits_{j=1}^{J-1} h(\\beta_{j,0} + \\beta_{j,1} \\cdot x_{,1} + \\cdots + \\beta_{j,p} \\cdot x_{,p} )} \\textit{ với } j < J \\\\\n& \\mathbb{P}\\left(Y = J\\right|x_i) = \\cfrac{1}{1 + \\sum\\limits_{j=1}^{J-1} h(\\beta_{j,0} + \\beta_{j,1} \\cdot x_{,1} + \\cdots + \\beta_{j,p} \\cdot x_{,p} )} \\\\\n\\tag{12.18}\n\\end{align}\\]Để minh họa cho xây dựng mô hình tuyến tính tổng quát trong trường hợp \\(Y\\) là biến rời rạc danh nghĩa, chúng ta sẽ xây dựng mô hình phân loại trên dữ liệu Travel_Insurance. Đây là dữ liệu cung cấp thông tin về các sản phẩm bảo hiểm du lịch từ một đại lý du lịch, với biến mục tiêu là product.name cho biết khách hàng đã lựa chọn sản phẩm sản phẩm bảo hiểm nào. Biến mục tiêu có bốn giá trị danh nghĩa tương ứng với bốn loại sản phẩm là Basic Plan, Bronze Plan, Silver Plan, và Value Plan. Mặc dù các sản phẩm này có mức giá khác nhau nhưng việc sánh các giá trị là không có nhiều ý nghĩa. Dữ liệu có 10 biến độc lập, tuy nhiên, để đơn giản hóa, chúng ta chỉ lấy hai biến là giới tính và độ tuổi của người tham gia bảo hiểm. Mối liên hệ giữa độ tuổi của khách hàng và loại sản phẩm khách hàng lựa chọn được mô tả bằng đồ thị dạng hộp như Hình 12.3.\nHình 12.3: Mối liên hệ giữa giới tính và loại hình bảo hiểm du lịch mà khách hàng lựa chọn\nTừ Hình 12.3 có thể thấy rằng Nam giới có xu hướng lựa chọn các loại hình bảo hiểm du lịch Basic Plan và Value Plan khi tỷ lệ Nam lựa chọn mỗi loại Hình bảo hiểm này lần lượt là 58% và 66%. Nữ giới có xu hướng lựa chọn Bronze Plan, Silver Plan khi tỷ lệ Nữ giới lựa chọn là 58% và 56%. Có thể kết luận được rằng có khác biệt giữa Nam và Nữ khi lựa chọn loại hình bảo hiểm du lịchChúng ta tiếp tục đánh giá mối liên hệ giữa biến độ tuổi đến loại hình bảo hiểm du lịch. Hình 12.4 sử dụng đồ thị dạng hộp để mô tả mối liên hệ giữa hai biến.\nHình 12.4: Mối liên hệ giữa giới tính và loại hình bảo hiểm du lịch mà khách hàng lựa chọn\nCó thể thấy rằng có sự khác biệt về phân phối của độ tuổi của những người lựa chọn các sản phẩm bảo hiểm: những người trẻ tuổi hơn có xu hướng lựa chọn Bronze Plan, Silver Plan trong khi những người lớn tuổi hơn có xu hướng lựa chọn Basic Plan và Value Plan.Từ các phân tích ở trên có thể thấy rằng có sự tác động của biến giới tính và độ tuổi lên biến mục tiêu. Để xây dựng mô hình tuyến tính tổng quát cho biến phân loại danh nghĩa, chúng ta sử dụng hàm multinom() từ thư viện nnet:Dựa trên kết quả ước lượng, chúng ta có công thức tính xác suất lựa chọn các sản phẩm bảo hiểm du lịch của một người có giới tính là \\(g_i\\) và độ tuổi \\(a_i\\) như sau:Với sản phẩm Basic Plan\\[\\begin{align}\n& \\mathbb{P}(Y = \\text{Basic}|g_i, a_i) = \\cfrac{1}{S_0} \\\\\n& \\mathbb{P}(Y = \\text{Bronze}|g_i, a_i) = \\cfrac{1}{S_0}  \\cdot e^{1.563 - 0.559 \\cdot g_i - 0.040 \\cdot a_i} \\\\\n& \\mathbb{P}(Y = \\text{Silver}|g_i, a_i) = \\cfrac{1}{S_0}  \\cdot e^{0.651 - 0.487 \\cdot g_i - 0.032 \\cdot a_i} \\\\\n& \\mathbb{P}(Y = \\text{Value}|g_i, a_i) = \\cfrac{1}{S_0} \\cdot e^{-1.10 + 0.338 \\cdot g_i - 0.002 \\cdot a_i}\n\\end{align}\\]với\\[\\begin{align}\nS_0 = 1 + \\mathbb{P}(Y = \\text{Bronze}|g_i, a_i) + \\mathbb{P}(Y = \\text{Silver}|g_i, a_i) +  \\mathbb{P}(Y = \\text{Value}|g_i, a_i)\n\\end{align}\\]Bảng ?? tính toán xác suất chấp nhận các sản phẩm du lịch theo một số độ tuổi điển hình và giới tínhKết quả từ mô hình cho thấy với các độ tuổi như trong Bảng ?? chỉ có Bronze Plan và Basic Plan được lựa chọn nếu chúng ta chỉ sử dụng hai biến độc lập là giới tính và độ tuổi. Đây là kết quả chưa chính xác vì mô hình chỉ sử dụng hai biến giới thích là độ tuổi và giới tính. Lựa chọn của khách hàng còn phụ thuộc vào nhiều yếu tố khác. Chúng tôi muốn nhấn mạnh về cách xây dựng mô hình cho biến phân loại hơn là cố gắng xây dựng một mô hình có khả năng dự báo tốt. Chúng ta sẽ thảo luận về đánh giá hiệu quả của mô hình ở các phần sau của chương sách.","code":"## # weights:  16 (9 variable)\n## initial  value 19625.769270 \n## iter  10 value 18535.510505\n## final  value 18040.676017 \n## converged## Call:\n## multinom(formula = Product.Name ~ Gender + Age, data = dat1)\n## \n## Coefficients:\n##             (Intercept)    GenderM          Age\n## Bronze Plan   1.5631988 -0.5589519 -0.039513361\n## Silver Plan   0.6512458 -0.4871457 -0.031596816\n## Value Plan   -1.0970424  0.3380718  0.002018711\n## \n## Std. Errors:\n##             (Intercept)    GenderM         Age\n## Bronze Plan  0.07047991 0.04302279 0.001653049\n## Silver Plan  0.08280363 0.05117056 0.001938850\n## Value Plan   0.08264747 0.05092679 0.001676413\n## \n## Residual Deviance: 36081.35 \n## AIC: 36099.35"},{"path":"mô-hình-tuyến-tính-tổng-quát..html","id":"biến-phụ-thuộc-là-biến-rời-rạc-có-thứ-tự","chapter":"Chương 12 Mô hình tuyến tính tổng quát.","heading":"12.2.3 Biến phụ thuộc là biến rời rạc có thứ tự","text":"Biến rời rạc có thứ tự, còn gọi là ordinal categorical variable, là các biến nhận giá trị rời rạc mà các giá trị rời rạc có thể sánh được với nhau. Một ví dụ điển hình cho biến rời rạc có thứ tự là số lần mà một người đi khám chữa bệnh sử dụng bảo hiểm y tế hoặc một khách hàng gửi yêu cầu bồi thường đến công ty bảo hiểm. Đây là trường hợp mà chúng ta có thể sử dụng một phân phối xác suất rời rạc có tham số để mô tả biến phụ thuộc \\(Y\\) giống như mô hình (??).","code":""},{"path":"mô-hình-tuyến-tính-tổng-quát..html","id":"biến-phụ-thuộc-có-phân-phối-poisson","chapter":"Chương 12 Mô hình tuyến tính tổng quát.","heading":"12.2.3.1 Biến phụ thuộc có phân phối Poisson","text":"Phân phối rời rạc thường được lựa chọn cho biến phụ thuộc rời rạc có thứ tự là phân phối Poisson. Hàm phân phối xác suất của biến ngẫu nhiên \\(Y\\) có phân phối Poisson với tham số \\(\\lambda > 0\\), ký hiệu \\(Y \\sim \\mathcal{P}(\\lambda)\\) được cho bởi công thức sau\n\\[\\begin{align}\n\\mathbb{P}(Y = y) = e^{-\\lambda} \\cdot \\cfrac{\\lambda^y}{y!} \\text{ với } y = 0, 1, 2, \\cdots\n\\end{align}\\]Phân phối Poisson thường được sử dụng để mô tả số lần một hiện tượng xảy ra trong một khoảng thời gian nhất định. Nguyên nhân là phân phối Poisson có mối liên hệ trực tiếp đến các mô hình có thời gian chờ có phân phối kiểu mũ. Thật vậy, nếu thời gian chờ giữa hai sự kiện liên tiếp xảy ra của một hiện tượng nào đó là một biến ngẫu nhiên liên tục có hàm phân phối xác suất kiểu mũ với tham số \\(\\gamma\\) thì số lần hiện tượng đó xảy ra trong một khoảng thời gian từ \\(t_1\\) đến \\(t_2\\) sẽ là một biến ngẫu nhiên phân phối Poisson với tham số \\(\\lambda = \\cfrac{(t_1 - t_2)}{\\gamma}\\).Gọi \\(T\\) là khoảng thời gian giữa 2 sự kiện liên tiếp xảy ra và \\(T\\) có phân phối mũ, hàm phân phối xác suất của \\(T\\) được viết như sau\n\\[\\begin{align}\n\\mathbb{P}(T \\leq x) = 1 - exp(-\\gamma x)\n\\end{align}\\]Gọi \\(N\\) là số lần xảy ra sự kiện, chẳng hạn như số lần đi khám, số lần lái xe gây ra tai nạn, giữa hai mốc thời gian \\(t_1 < t_2\\) thì \\(N\\) sẽ có phân phối Poisson với tham số \\(\\lambda\\):\n\\[\\begin{align}\n& \\mathbb{P}(N = k) = e^{-\\lambda} \\cdot \\cfrac{\\lambda^k}{k!}\\\\\n& \\lambda = \\cfrac{(t_1 - t_2)}{\\gamma}\n\\end{align}\\]Biến ngẫu nhiên có phân phối Poisson với tham số \\(\\lambda\\), ký hiệu \\(\\mathcal{P}(\\lambda)\\), có tính chất là giá trị trung bình và phương sai đều bằng tham số của biến đó là \\(\\lambda\\). Phân phối Poisson nằm trong họ các phân phối mũ nên sẽ rất thuận tiện trong xây dựng và ước lượng mô hình bằng phương pháp hợp lý tối đa. Ngoài ra, bằng cách cho tham số của phân phối Poisson một phân phối xác suất, chúng ta có thể thu được các phân phối rời rạc linh hoạt hơn trong mô tả các biến ngẫu nhiên dạng đếm được.Khi xây dựng mô hình tuyến tính với biến mục tiêu \\(Y\\) có phân phối Poisson, giá trị trung bình \\(Y\\) nhận giá trị dương nên chúng ta cần chọn các hàm liên kết \\(g\\) có miền xác định là tập các số thực dương \\(\\mathbb{R}^+\\) và miền giá trị là tập số thực \\(\\mathbb{R}\\). Hàm số thường được lựa chọn là hàm \\(\\text{log}\\).Chúng ta có thể viết mô hình tuyến tính tổng quát khi \\(Y\\) có phân phối Poisson, thường được gọi tắt là hồi quy Poisson, như sau\\[\\begin{align}\n& Y_i \\sim \\mathcal{P}(\\lambda_i) \\\\\n& \\mathbb{E}(Y_i) = \\lambda_i \\\\\n& log(\\lambda_i) = \\left(\\beta_0 + \\beta_1 \\cdot x_{,1} + \\beta_2 \\cdot x_{,2} + \\cdots + \\beta_p \\cdot x_{,p}\\right) \\\\\n& \\lambda_i = e^{\\beta_0 + \\beta_1 \\cdot x_{,1} + \\beta_2 \\cdot x_{,2} + \\cdots + \\beta_p \\cdot x_{,p}}\n\\tag{12.19}\n\\end{align}\\]Dữ liệu được sử dụng để mô tả mô hình tuyến tính tổng quát với biến phụ thuộc phân phối Poisson là dữ liệu \\(SingaporeAuto.csv\\). Dữ liệu được tổng hợp bởi Hiệp hội bảo hiểm Singapore trong năm 1993 mô tả số vụ tai nạn ô tô xảy ra cùng với các đặc điểm của người lái và đặc điểm của xe gây tai nạn. Cũng giống như các phần trước, chúng ta sẽ xây dựng mô hình ở mức độ đơn giản nhất để bạn đọc dễ dàng hình dung. Biến phụ thuộc trong mô hình là biến \\(Clm\\_Count\\) cho biết số vụ tai nạn mà một lái xe gây ra trong vòng một năm, hai biến phụ thuộc bao gồm có:Biến \\(PC\\) là biến nhận hai giá trị là 0 tương ứng với xe được đăng ký theo công ty và nhận giá trị 1 tương ứng với xe được đăng ký theo cá nhân.Biến \\(PC\\) là biến nhận hai giá trị là 0 tương ứng với xe được đăng ký theo công ty và nhận giá trị 1 tương ứng với xe được đăng ký theo cá nhân.Biến \\(NCD\\), viết tắt của Claims Discount, cho biết lịch sử gây ra tai nạn của lái xe. Giá trị \\(NCD\\) càng cao nghĩa là lịch sử người lái xe càng gây ra ít tai nạn.Biến \\(NCD\\), viết tắt của Claims Discount, cho biết lịch sử gây ra tai nạn của lái xe. Giá trị \\(NCD\\) càng cao nghĩa là lịch sử người lái xe càng gây ra ít tai nạn.Chúng ta sử dụng hàm glm() để xây dựng và ước lượng mô hình tuyến tính tổng quát:Bạn đọc có thể thấy rằng cả hai biến \\(PC\\) và \\(NCD\\) đều có tác động đến giá trị trung bình của biến mục tiêu \\(Clm\\_Count\\) các giá trị p-value đều nhỏ. Mối liên hệ giữa số lượng tai nạn xảy ra và các biến \\(PC\\) và \\(NCD\\) được mô tả như sau:\\[\\begin{align}\n& Clm\\_Count_i \\sim \\mathcal{P}(\\lambda_i) \\\\\n& \\log(\\lambda_i) = -2.641 + 0.432 \\cdot PC_{} - 0.013 \\cdot NCD_{} \\\\\n& \\lambda_i = \\exp\\left(-2.641 + 0.432 \\cdot PC_{} - 0.013 \\cdot NCD_{} \\right)\n\\tag{12.20}\n\\end{align}\\]Hệ số của biến \\(PC\\) là số dương, điều này cho biết các xe đăng ký dưới dạng cá nhân có khả năng gây tai nạn cao hơn với xe đăng ký dưới hình thức doanh nghiệp. Đồng thời, hệ số của biến \\(NCD\\) âm cho biết lái xe có lịch sử lái xe tốt, tương ứng với \\(NCD\\) cao, ít có khả năng gây ra tai nạn hơn lái xe có lịch sử lái xe không tốt, tương ứng với \\(NCD\\) thấp.","code":"\ndat<-read.csv(\"../KHDL_KTKD Final/Dataset/SingaporeAuto.csv\")\nglm2<-glm(Clm_Count~PC+NCD, data=dat, family = poisson(link = \"log\"))\nsummary(glm2)## \n## Call:\n## glm(formula = Clm_Count ~ PC + NCD, family = poisson(link = \"log\"), \n##     data = dat)\n## \n## Deviance Residuals: \n##     Min       1Q   Median       3Q      Max  \n## -0.4685  -0.3801  -0.3520  -0.3283   4.1716  \n## \n## Coefficients:\n##              Estimate Std. Error z value Pr(>|z|)    \n## (Intercept) -2.641681   0.073836 -35.778  < 2e-16 ***\n## PC           0.431980   0.091840   4.704 2.56e-06 ***\n## NCD         -0.013943   0.002618  -5.327 9.99e-08 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## (Dispersion parameter for poisson family taken to be 1)\n## \n##     Null deviance: 2887.2  on 7482  degrees of freedom\n## Residual deviance: 2848.3  on 7480  degrees of freedom\n## AIC: 3849.4\n## \n## Number of Fisher Scoring iterations: 6"},{"path":"mô-hình-tuyến-tính-tổng-quát..html","id":"phân-phối-rời-rạc-có-lạm-phát-tại-giá-trị-0.","chapter":"Chương 12 Mô hình tuyến tính tổng quát.","heading":"12.2.3.2 Phân phối rời rạc có lạm phát tại giá trị 0.","text":"Biến phụ thuộc dạng đếm chứa một tỷ lệ lớn giá trị bằng 0 là các kiểu biến phụ thuộc thường gặp phải khi làm việc trên dữ liệu trong lĩnh vực bảo hiểm. Tỷ lệ lớn ở đây thường được hiểu là trên 90% giá trị quan sát được. Đa số các phân phối xác suất rời rạc thông thường, bao gồm phân phối Poisson, không mô tả tốt khi biến phụ thuộc \\(Y\\) trong trường hợp này.Một giải pháp phổ biến khi làm việc với kiểu dữ liệu như vậy là thay đổi phân phối dạng đếm thông thường để làm tăng tỷ lệ giá trị nhận được tại 0, để thu được các phân phối thường được gọi là các phân phối lạm phát tại 0, hay Zero-inflated variable. Phân phối lạm phát tại 0 là hỗn hợp của hai phân phối xác suất rời rạc, bao gồm một phân phối nhị thức chỉ báo cho trường hợp 0, và một phân phối xác suất dành cho biến đếm thông thường. Hàm phân phối của biến ngẫu nhiên dạng đếm có lạm phát tại 0, ký hiệu \\(ZI_Y\\), có thể được mô tả như sau\n\\[\\begin{align}\n\\mathbb{P}(ZI_Y = y) = \\begin{cases}\n& \\omega + (1-\\omega) \\cdot \\mathbb{P}(Y=0) \\text{ khi y = 0} \\\\\n& (1-\\omega) \\cdot \\mathbb{P}(Y=y) \\text{ khi y > 0}\n\\end{cases}\n\\tag{12.21}\n\\end{align}\\]\ntrong đó biến ngẫu nhiên \\(Y\\) tuân theo phân phối số đếm tiêu chuẩn được. Trong trường hợp tham số \\(\\omega\\) bằng 0, phân phối của biến ngẫu nhiên \\(ZI_Y\\) sẽ tương ứng tương ứng với phân phối của biến Y.Tất cả các phân phối kiểu đếm đều có thể được sử dụng để tạo ra các biến mới có lạm phát tại 0. Trong các mô hình tuyến tính tổng quát, biến dạng đếm thường được mô tả bằng phân phối cổ điển Poisson. Với việc sử dụng phương trình (12.21), hàm phân phối của biến ngẫu nhiên Poisson có lạm phát tại 0, ký hiệu là \\(ZI_\\mathcal{P}\\) được cho bởi công thức sau:\\[\\begin{align}\n\\mathbb{P}(ZI_\\mathcal{P} = y) = \\begin{cases}\n& \\omega + (1-\\omega) \\cdot e^{-\\lambda} \\text{ khi y = 0} \\\\\n& (1-\\omega) \\cdot e^{-\\omega} \\cdot \\cfrac{\\lambda^y}{y!} \\text{ khi y > 0}\n\\end{cases}\n\\tag{12.22}\n\\end{align}\\]Chúng ta có thể xác định giá trị trung bình và phương sai của phân phối \\(ZI_Y\\) dựa trên tham số \\(\\omega\\) và giá trị trung bình cũng như phương sai của biến \\(Y\\) như sau\n\\[\\begin{align}\n& \\mathbb{E}(ZI_Y) = (1-\\omega) \\cdot \\mathbb{E}(Y) \\\\\n& \\mathbb{V}(ZI_Y) = (1-\\omega) \\cdot \\mathbb{V}(Y) + \\omega(1-\\omega) \\cdot \\mathbb{E}(Y)^2\n\\tag{12.23}\n\\end{align}\\]Trong trường hợp \\(Y\\) có phân phối Poission, chúng ta có giá trị trung bình và phương sai của biến \\(ZI_\\mathcal{P}\\):\n\\[\\begin{align}\n& \\mathbb{E}(ZI_\\mathcal{P}) = (1-\\omega) \\cdot \\lambda \\\\\n& \\mathbb{V}(ZI_\\mathcal{P}) = \\mathbb{E}(ZI_\\mathcal{P}) \\cdot (1 + \\lambda - \\mathbb{E}(ZI_\\mathcal{P}))\n\\tag{12.24}\n\\end{align}\\]Với giá trị trung bình và phương sai của biến \\(ZI_\\mathcal{P}\\) như phương trình (12.21), chúng ta có thể xây dựng mô hình tuyến tính tổng quát với biến phụ thuộc là \\(ZI_\\mathcal{P}\\) như sau: giá trị trung bình \\(\\left((1-\\omega) \\cdot \\lambda\\right)\\) sẽ được giải thích thông qua các biến phụ thuộc trong khi tham số \\(\\lambda\\) sẽ được ước lượng bằng phương pháp hợp lý tối đa.Biến phụ thuộc phân phối \\(ZI_\\mathcal{P}\\) sẽ hữu ích cho mục đích lập mô hình trong trường hợp dữ liệu quan sát có sự tập trung quá mức tại giá trị 0. Ngoài phân phối Poisson, các phân phối mở rộng từ phân phối Poisson cũng có thể được sử dụng với để tạo ra các phân phối có lạm phát tại 0, chẳng hạn như biến ngẫu nhiên phân phối Poisson - Gamma, Poisson - Inverse gaussian.Trong các nghiên cứu thực nghiệm, nhiều tác giả đã chứng minh rằng việc áp dụng phân phối có lạm phát tại 0 để mô hình hóa số lượng yêu cầu bồi thường bảo hiểm là phù hợp để mô tả hành vi của người được bảo hiểm. Thật vậy, trong ngành bảo hiểm không phải tất cả các vụ tai nạn đều được báo cáo, công ty bảo hiểm chỉ có thông tin về các yêu cầu bồi thường được báo cáo. Có hai cách để giải thích cách phân phối có lạm phát tại 0 như sau: (1) Một số người được bảo hiểm không gửi yêu cầu bồi thường dù có xảy ra sự kiện bảo hiểm, họ không có nhận thức được về việc được bảo hiểm, hoặc không có nhu cầu gửi yêu cầu bảo hiểm. (2) Một cách giải thích khác của mô hình lạm phát tại 0 là xem xét xác suất của mỗi vụ tai nạn được báo cáo. Một hành vi thực tế của người được bảo hiểm là nếu họ đã báo cáo vụ tai nạn đầu tiên thì những vụ tai nạn tiếp theo cũng sẽ được báo cáo, còn nếu vụ tai nạn đầu tiên không được báo cáo thì các vụ tai nạn sau sẽ không được báo cáo. Cả hai cách giải thích dựa trên hành vi này đều dẫn đến việc số lượng biến mục tiêu nhận giá trị bằng 0 cao hơn với số lượng tai nạn thực tế xảy ra.\nBảng 12.1: Khác nhau giữa GLM - Poisson và GLM - ZIP\nĐể xây dựng mô hình tuyến tính tổng quát trong đó biến phụ thuộc có phân phối \\(ZI_\\mathcal{P}\\) chúng ta sử dụng hàm số zeroinfl() của thư viện \\(pscl\\).Chúng ta có kết quả ước lượng các mô hình\nBảng 12.2: Kết quả ước lượng GLM - Poisson và GLM - ZIP\nCó thể thấy rằng mô hình tuyến tính tổng quát với biến phụ thuộc phân phối \\(ZI_\\mathcal{P}\\) có giá trị hàm hợp lý tối đa lớn hơn, điều này cũng có nghĩa là phân phối \\(ZI_\\mathcal{P}\\) phù hợp hơn phân phối Poisson thông thường khi mô tả biến phụ thuộc trong dữ liệu. Dựa vào kết quả ước lượng từ hai mô hình, chúng ta có thể tính toán xác suất không có tai nạn và xác suất để xảy ra một tai nạn theo độ tuổi và giới tính của người tham gia bảo hiểm như sauCó thể nhận thấy rằng xác suất mà một người được bảo hiểm không để xảy ra tai nạn trong mô hình tuyến tính tổng quát với biến phụ thuộc có phân phối \\(ZI_\\mathcal{P}\\) là luôn cao hơn với mô hình tuyến tính tổng quát với biến phụ thuộc có phân phối Poisson thông thường. Đồng thời, mô hình tuyến tính tổng quát với biến phụ thuộc có phân phối \\(ZI_\\mathcal{P}\\) cho kết quả xác suất xảy ra đúng một tai nạn thấp hơn với mô hình có biến phụ thuộc có phân phối Poisson thông thường.","code":"\ndat<-read.csv(\"../KHDL_KTKD Final/Dataset/exposure.csv\")\n# Biến phụ thuộc có phân phối Poisson\nglm1<-glm(Claim_Count~Age+Gender,\n          family = poisson(link = \"log\"),\n          data=dat)\n\n# Biến phụ thuộc có phân phối Zero inflated poisson\nzip.glm<-zeroinfl(Claim_Count~Age+Gender|1,\n                  dist = 'poisson',\n                  link = \"log\",\n                  data = dat)\nsummary(glm1)\nsummary(zip.glm)"},{"path":"mô-hình-tuyến-tính-tổng-quát..html","id":"biến-phụ-thuộc-có-phân-phối-liên-tục.","chapter":"Chương 12 Mô hình tuyến tính tổng quát.","heading":"12.2.4 Biến phụ thuộc có phân phối liên tục.","text":"Một giả thiết quan trọng của mô hình tuyến tính tổng quát là biến phụ thuộc nằm trong họ các biến ngẫu nhiên có phân phối kiểu mũ, hay exponential family. Chúng ta sẽ thảo luận kỹ hơn về họ các biến ngẫu nhiên có phân phối kiểu mũ trong phần sau của cuốn sách. Lưu ý rằng họ các biến ngẫu nhiên có phân phối kiểu mũ không tương đồng với khái niệm biến ngẫu nhiên có phân phối mũ. Phân phối mũ chỉ là mộ trường hợp đặc biệt của phân phối kiển mũ. Có nhiều biến ngẫu nhiên liên tục khác có phân phối nằm trong họ các phân phối kiểu mũ. Có thể kể đến như phân phối gamma, phân phối chuẩn, phân phối chuẩn ngược…Các bước để xây dựng mô hình tuyến tính tổng quát trong trường hợp biến phụ thuộc \\(Y\\) là biến ngẫu nhiên liên tục hoàn toàn tương tự như cách xây dựng mô hình tuyến tính tổng quát ở trên, bao gồm bước chọn phân phối xác suất cho biến mục tiêu và lựa chọn hàm liên kết phù hợp. Chúng ta sẽ tiếp tục xây dựng mô hình với dữ liệu “exposure.csv” đã đề cập ở các phần trên. Biến mục tiêu không còn là số lần khách hàng gửi yêu cầu bồi thường, mà là số tiền trung bình mỗi lần khách hàng gửi yêu cầu (biến \\(Ave\\_Amount\\)). Các biến giải thích vẫn tiếp tục là giới tính (\\(Gender\\)) và độ tuổi (age) của người được bảo hiểm.Mối liên hệ giữa \\(Ave\\_Amount\\) được thể hiện qua đồ thị dưới đâyĐồ thị bên trái cho thấy số tiền yêu cầu bồi thường trung bình của nữ là cao hơn nam giới. Phân phối xác suất của số tiền yêu cầu bồi thương trung bình là phân phối liên tục lệch phải, có đuôi bên phải lớn. Đồ thị bên phải cho thấy số tiền yêu cầu bồi thường trung bình có xu hướng tăng theo độ tuổi.Số tiền bảo hiểm trung bình là số dương nên chúng ta sẽ sử dụng phân phối \\(gamma\\) với hàm liên kết là hàm log(). Lưu ý rằng phân phối gamma() là phân phối có hai tham số, thường được ký hiệu là \\(\\alpha\\) và \\(\\gamma\\), với hàm mật độ xác suất, giá trị trung bình, và phương sai như sau\n\\[\\begin{align}\n& f_Y(y) = \\cfrac{\\beta^\\alpha}{\\Gamma(\\alpha)} \\ y^{\\alpha-1} \\ e^{-\\gamma y} \\\\\n& \\mathbb{E}(X) = \\cfrac{\\alpha}{\\gamma} \\\\\n& \\mathbb{V}(X) = \\cfrac{\\alpha}{\\gamma^2}\n\\end{align}\\]Khi phân phối \\(gamma\\) được sử dụng cho biến phụ thuộc, giá trị trung bình được tính bằng \\(\\alpha/\\gamma\\), sẽ được mô tả thông qua các biến độc lập trong khi tham số \\(\\alpha\\) sẽ được ước lượng dựa trên hàm hợp lý tối đa:Ngoài các hệ số của các biến độc lập trong mô hình tuyến tính tổng quát, hàm glm() còn cung cấp giá trị ước lượng được cho tham số \\(\\phi\\) (disperson paramter hay tham số phân tán) là \\(0.924\\). Tham số phân tán được định nghĩa trong họ các phân phối kiểu mũ sẽ được thảo luận trong phần tiếp theo. Đối với phân phối \\(gamma\\), tham số phân tán được tính bằng \\(\\phi = 1/\\alpha\\).Giá trị số tiền bồi thường trung bình được được mô tả bằng mô hình tuyến tính tổng quát như sau:\n\\[\\begin{align}\n& Y_i \\sim Gamma(\\mu_i = \\alpha_i/\\gamma_i , \\phi_i = 1/\\alpha_i) \\\\\n& \\phi_i = 0.924 \\\\\n& log(\\mu_i) = -0.0475 + 0.079 \\cdot Age_i + 0.534 \\cdot Gender_i\n\\end{align}\\]Trước khi đi vào chi tiết các thành phần của mô hình, chúng tôi muốn kết luận rằng mô hình tuyến tính tổng quát có thể được sử dụng để mô hình hóa dữ liệu trong nhiều hoàn cảnh khác nhau. Việc xây dựng mô hình tuyến tính tổng quát luôn được bắt đầu bằng việc lựa chọn phân phối cho biến phụ thuộc và lựa chọn hàm số liên kết. Có các quy tắc chung trong việc lựa chọn phân phối và hàm liên kết giống như chúng tôi đã trình bày ở phần trên. Tuy nhiên để đưa ra được mô hình tuyến tính tổng quát phù hợp cho từng dữ liệu cụ thể, hoặc để có thể mở rộng được mô hình tuyến tính tổng quát trong các kiểu dữ liệu phức tạp hơn, bạn đọc cần hiểu sâu hơn về các đặc điểm kỹ thuật của các mô hình này. Các kiến thức này sẽ được trình bày trong các phần tiếp theo của chương.","code":"\ndat<-read.csv(\"../KHDL_KTKD Final/Dataset/exposure.csv\")\ndat<-mutate(dat,Ave_Amount = ifelse(Claim_Count>0,Total_Claim/Claim_Count,0))\ndat1<-filter(dat,Claim_Count>0)\ndat1$Gender<-ifelse(dat1$Gender==0,\"F\",\"M\")\np1<-dat1%>%ggplot()+geom_boxplot(aes(Gender,y = Ave_Amount))+\n  ylim(0,200)+ggtitle(\"Số tiền yêu cầu bồi thường trung bình và giới tính\")\np2<-dat1%>%ggplot(aes(x=Age,y = Ave_Amount))+geom_point(alpha=0.2)+\n  geom_smooth(col=\"black\", size = 1, se = FALSE)+ylim(0,200)+\n  ggtitle(\"Số tiền yêu cầu bồi thường trung bình và độ tuổi\")\ngrid.arrange(p1,p2,ncol=2)\ndat<-read.csv(\"../KHDL_KTKD Final/Dataset/exposure.csv\")\ndat<-mutate(dat,Ave_Amount = ifelse(Claim_Count>0,Total_Claim/Claim_Count,0))\ndat1<-filter(dat,Claim_Count>0)\nglm3<-glm(Ave_Amount~Age+Gender, family = Gamma(link = \"log\"),data = dat1)\nsummary(glm3)## \n## Call:\n## glm(formula = Ave_Amount ~ Age + Gender, family = Gamma(link = \"log\"), \n##     data = dat1)\n## \n## Deviance Residuals: \n##     Min       1Q   Median       3Q      Max  \n## -4.3309  -0.9498  -0.3047   0.3360   2.9954  \n## \n## Coefficients:\n##              Estimate Std. Error t value Pr(>|t|)    \n## (Intercept) -0.047535   0.065517  -0.726    0.468    \n## Age          0.079089   0.001534  51.563   <2e-16 ***\n## Gender      -0.534010   0.041951 -12.730   <2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## (Dispersion parameter for Gamma family taken to be 0.9241118)\n## \n##     Null deviance: 4666.5  on 2110  degrees of freedom\n## Residual deviance: 2205.7  on 2108  degrees of freedom\n## AIC: 15710\n## \n## Number of Fisher Scoring iterations: 6"},{"path":"mô-hình-tuyến-tính-tổng-quát..html","id":"các-thành-phần-của-mô-hình-tuyến-tính-tổng-quát.","chapter":"Chương 12 Mô hình tuyến tính tổng quát.","heading":"12.3 Các thành phần của mô hình tuyến tính tổng quát.","text":"Như chúng ta đã thảo luận trong phần trước, mô hình tuyến tính tổng quát khắc phục hai giả thiết, cũng là hai nhược điểm của mô hình tuyến tính thông thường, đó là ) biến phụ thuộc có phân phối chuẩn, và ii) giá trị trung bình của biến phụ thuộc bằng một tổ hợp tuyến tính của các biến độc lập. Trong phần này của cuốn sách, chúng tôi sẽ thảo luận kỹ hơn vào cách tiếp cận để khắc phục các hạn chế kể trên.Thứ nhất, thay vì sử dụng phân phối chuẩn, mô hình tuyến tính tổng quát giả thiết rằng biến phụ thuộc có phân phối nằm trong họ các phân phối kiểu mũ (exponential family). Phân phối chuẩn chỉ là một trường hợp đặc biệt của họ các phân phối này.Thứ nhất, thay vì sử dụng phân phối chuẩn, mô hình tuyến tính tổng quát giả thiết rằng biến phụ thuộc có phân phối nằm trong họ các phân phối kiểu mũ (exponential family). Phân phối chuẩn chỉ là một trường hợp đặc biệt của họ các phân phối này.Thứ hai, mô hình tuyến tính tổng quát sử dụng một hàm \\(g\\), được gọi là hàm liên kết, để mô tả mối liên hệ giữa giá trị trung bình của biến phụ thuộc với tổ hợp tuyến tính của các biến độc lập. Lựa chọn hàm \\(g\\) có ý nghĩa quan trọng không chỉ trong cách luận giải kết quả của mô hình, mà còn ở việc dễ dàng ước lượng tham số của mô hình.Thứ hai, mô hình tuyến tính tổng quát sử dụng một hàm \\(g\\), được gọi là hàm liên kết, để mô tả mối liên hệ giữa giá trị trung bình của biến phụ thuộc với tổ hợp tuyến tính của các biến độc lập. Lựa chọn hàm \\(g\\) có ý nghĩa quan trọng không chỉ trong cách luận giải kết quả của mô hình, mà còn ở việc dễ dàng ước lượng tham số của mô hình.","code":""},{"path":"mô-hình-tuyến-tính-tổng-quát..html","id":"họ-các-biến-ngẫu-nhiên-có-phân-phối-kiểu-mũ.","chapter":"Chương 12 Mô hình tuyến tính tổng quát.","heading":"12.3.1 Họ các biến ngẫu nhiên có phân phối kiểu mũ.","text":"Thay vì giả thiết rằng \\(Y\\) có phân phối chuẩn trong mô hình tuyến tính thông thường, mô hình tuyến tính tổng quát giả thiết biến phụ thuộc \\(Y\\) nằm trong họ các biến ngẫu nhiên có phân phối kiểu mũ. Họ các biến ngẫu nhiên có phân phối kiểu mũ (exponential family) có hàm mật độ xác suất có thể viết dưới dạng như sau:\n\\[\\begin{align}\nf(y;\\theta,\\phi) = \\exp\\left[ \\cfrac{y \\theta - b(\\theta)}{(\\phi)} + c(y,\\phi) \\right]\n\\end{align}\\]\ntrong đóTham số \\(\\theta\\) được gọi là tham số \\(chính\\) \\(tắc\\) của phân phối kiểu mũ.Tham số \\(\\phi\\) được gọi là tham số \\(phân\\) \\(tán\\). Nguyên nhân là giá trị trung bình của biến ngẫu nhiên \\(Y\\) không phụ thuộc vào \\(\\phi\\). Tham số \\(\\phi\\), mà tổng quát hơn là hàm \\((\\phi)\\) sẽ xác định phương sai của biến phụ thuộc.Các hàm số \\(b(\\theta)\\), \\((\\phi)\\) và \\(c(y,\\phi)\\) sẽ quyết định kiểu phân phối của biến phụ thuộc.Giá trị trung bình và phương sai của biến phụ thuộc \\(Y\\) được cho bởi các công thức sau:\n\\[\\begin{align}\n& \\mathbb{E}(Y) = b^{'}(\\theta) \\\\\n& \\mathbb{V}(Y) = (\\phi) \\cdot b^{''}(\\theta)\n\\tag{12.25}\n\\end{align}\\]\nvới \\(b^{'}(\\theta)\\) và \\(b^{''}(\\theta)\\) lần lượt là đạo hàm bậc một và đạo hàm bậc hai của hàm số \\(b\\) theo biến \\(\\theta\\).Họ các biến ngẫu nhiên có phân phối kiểu mũ bao gồm đa số các biến ngẫu nhiên liên tục thông thường như phân phối chuẩn, phân phối mũ, phân phối Gamma, phân phối chuẩn ngược. Các biến ngẫu nhiên phân phối rời rạc như phân phối nhị thức, phân phối binomial, hoặc phân phối Poisson cũng nằm trong họ các biến ngẫu nhiên có phân phối kiểu mũ.Ví dụ 1: phân phối Poisson thường được sử dụng để mô tả phân phối của biến đếm trong mô hình tuyến tính tổng quát. Hàm phân phối của biến ngẫu nhiên Poisson với tham số \\(\\lambda\\), ký hiệu \\(\\mathcal{P}(\\lambda)\\), được cho bởi công thức\n\\[\\begin{align}\n\\mathbb{P}(Y = y; \\lambda) = exp(-\\lambda) \\ \\cfrac{\\lambda^y}{y!}\n\\end{align}\\]\nChúng ta có thể viết phân phối \\(\\mathcal{P}(\\lambda)\\) dưới dạng phân phối mũ như sau\n\\[\\begin{align}\n\\mathbb{P}(Y = y; \\theta) = exp\\left[ \\cfrac{\\theta y - exp(\\theta)}{1} - log(\\Gamma(y+1)) \\right] \\text{ với } \\lambda = exp(\\theta)\n\\end{align}\\]\nĐây là hàm phân phối của biến ngẫu nhiên nằm trong họ các phân phối kiểu mũ với \\((\\phi) = 1\\); \\(b(\\theta) = exp(\\theta)\\) và \\(c(y,\\phi) = log\\left(\\Gamma(y+1)\\right)\\). Bạn đọc có thể tính toán trung bình và phương sai của phân phối \\(\\lambda\\) dựa theo công thức (12.25)\n\\[\\begin{align}\n& \\mathbb{E}(Y) = b^{'}(\\theta) = exp(\\theta) = \\lambda \\\\\n& \\mathbb{V}(Y) = (\\phi) \\cdot b^{''}(\\theta) = 1 \\cdot exp(\\theta) = \\lambda\n\\end{align}\\]Ví dụ 1: phân phối Poisson thường được sử dụng để mô tả phân phối của biến đếm trong mô hình tuyến tính tổng quát. Hàm phân phối của biến ngẫu nhiên Poisson với tham số \\(\\lambda\\), ký hiệu \\(\\mathcal{P}(\\lambda)\\), được cho bởi công thức\n\\[\\begin{align}\n\\mathbb{P}(Y = y; \\lambda) = exp(-\\lambda) \\ \\cfrac{\\lambda^y}{y!}\n\\end{align}\\]\nChúng ta có thể viết phân phối \\(\\mathcal{P}(\\lambda)\\) dưới dạng phân phối mũ như sau\n\\[\\begin{align}\n\\mathbb{P}(Y = y; \\theta) = exp\\left[ \\cfrac{\\theta y - exp(\\theta)}{1} - log(\\Gamma(y+1)) \\right] \\text{ với } \\lambda = exp(\\theta)\n\\end{align}\\]\nĐây là hàm phân phối của biến ngẫu nhiên nằm trong họ các phân phối kiểu mũ với \\((\\phi) = 1\\); \\(b(\\theta) = exp(\\theta)\\) và \\(c(y,\\phi) = log\\left(\\Gamma(y+1)\\right)\\). Bạn đọc có thể tính toán trung bình và phương sai của phân phối \\(\\lambda\\) dựa theo công thức (12.25)\n\\[\\begin{align}\n& \\mathbb{E}(Y) = b^{'}(\\theta) = exp(\\theta) = \\lambda \\\\\n& \\mathbb{V}(Y) = (\\phi) \\cdot b^{''}(\\theta) = 1 \\cdot exp(\\theta) = \\lambda\n\\end{align}\\]Ví dụ 2: phân phối chuẩn là một biến ngẫu nhiên nằm trong họ các phân phối kiểu mũ. Thật vậy, chúng ta có hàm mật độ của biến ngẫu nhiên phân phối chuẩn với trung bình \\(\\mu\\) và độ lệch chuẩn \\(\\sigma\\), ký hiệu \\(\\mathcal{N}(\\mu,\\sigma)\\), như sau\n\\[\\begin{align}\nf(y,\\mu,\\sigma) = \\cfrac{1}{\\sqrt{2 \\pi \\sigma^2}} \\exp\\left[ \\cfrac{-(y - \\mu)^2}{2 \\ \\sigma^2} \\right]\n\\end{align}\\]\nHàm phân phối của \\(\\mathcal{N}(\\mu,\\sigma)\\) có thể được viết dưới dạng phân phối kiểu mũ\n\\[\\begin{align}\n\\cfrac{1}{\\sqrt{2 \\pi} \\sigma} \\exp\\left[ \\cfrac{-(y - \\mu)^2}{2 \\ \\sigma^2} \\right] &= \\exp\\left[ \\cfrac{-(y - \\mu)^2}{2 \\ \\sigma^2} - \\cfrac{1}{2} log(2 \\pi \\sigma^2) \\right] \\\\\n&= \\exp\\left[ \\cfrac{\\mu y - \\mu^2/2} {\\sigma^2} - \\cfrac{y^2}{2\\sigma^2} - \\cfrac{1}{2} log(2 \\pi \\sigma^2) \\right]\n\\end{align}\\]\nVới \\(\\theta = \\mu\\) và \\(\\phi = \\sigma^2\\) chúng ta có hàm mật độ của biến ngẫu nhiên \\(\\mathcal{N}(\\mu,\\sigma)\\) là hàm mật độ của biến ngẫu nhiên nằm trong họ các phân phối kiểu mũ, với \\((\\phi) = \\phi\\), \\(b(\\theta) = \\theta^2/2\\) và\n\\[\\begin{align}\nc(y,\\phi) = -\\cfrac{y^2}{2\\phi} - \\cfrac{1}{2} log(2 \\pi \\phi)\n\\end{align}\\]\nBạn đọc có thể kiểm tra giá trị trung bình và phương sai của phân phối \\(\\mathcal{N}(\\mu,\\sigma)\\) dựa theo công thức (12.25)\n\\[\\begin{align}\n& \\mathbb{E}(Y) = b^{'}(\\theta) = \\theta = \\mu \\\\\n& \\mathbb{V}(Y) = (\\phi) \\cdot b^{''}(\\theta) = \\phi \\cdot 1 = \\phi = \\sigma^2\n\\end{align}\\]Ví dụ 2: phân phối chuẩn là một biến ngẫu nhiên nằm trong họ các phân phối kiểu mũ. Thật vậy, chúng ta có hàm mật độ của biến ngẫu nhiên phân phối chuẩn với trung bình \\(\\mu\\) và độ lệch chuẩn \\(\\sigma\\), ký hiệu \\(\\mathcal{N}(\\mu,\\sigma)\\), như sau\n\\[\\begin{align}\nf(y,\\mu,\\sigma) = \\cfrac{1}{\\sqrt{2 \\pi \\sigma^2}} \\exp\\left[ \\cfrac{-(y - \\mu)^2}{2 \\ \\sigma^2} \\right]\n\\end{align}\\]\nHàm phân phối của \\(\\mathcal{N}(\\mu,\\sigma)\\) có thể được viết dưới dạng phân phối kiểu mũ\n\\[\\begin{align}\n\\cfrac{1}{\\sqrt{2 \\pi} \\sigma} \\exp\\left[ \\cfrac{-(y - \\mu)^2}{2 \\ \\sigma^2} \\right] &= \\exp\\left[ \\cfrac{-(y - \\mu)^2}{2 \\ \\sigma^2} - \\cfrac{1}{2} log(2 \\pi \\sigma^2) \\right] \\\\\n&= \\exp\\left[ \\cfrac{\\mu y - \\mu^2/2} {\\sigma^2} - \\cfrac{y^2}{2\\sigma^2} - \\cfrac{1}{2} log(2 \\pi \\sigma^2) \\right]\n\\end{align}\\]\nVới \\(\\theta = \\mu\\) và \\(\\phi = \\sigma^2\\) chúng ta có hàm mật độ của biến ngẫu nhiên \\(\\mathcal{N}(\\mu,\\sigma)\\) là hàm mật độ của biến ngẫu nhiên nằm trong họ các phân phối kiểu mũ, với \\((\\phi) = \\phi\\), \\(b(\\theta) = \\theta^2/2\\) và\n\\[\\begin{align}\nc(y,\\phi) = -\\cfrac{y^2}{2\\phi} - \\cfrac{1}{2} log(2 \\pi \\phi)\n\\end{align}\\]\nBạn đọc có thể kiểm tra giá trị trung bình và phương sai của phân phối \\(\\mathcal{N}(\\mu,\\sigma)\\) dựa theo công thức (12.25)\n\\[\\begin{align}\n& \\mathbb{E}(Y) = b^{'}(\\theta) = \\theta = \\mu \\\\\n& \\mathbb{V}(Y) = (\\phi) \\cdot b^{''}(\\theta) = \\phi \\cdot 1 = \\phi = \\sigma^2\n\\end{align}\\]Ví dụ 3: phân phối \\(Gamma(\\alpha,\\gamma)\\) nằm trong họ các phân phối kiểu mũ. Thật vậy\n\\[\\begin{align}\nf(y) &= \\cfrac{\\gamma^\\alpha}{\\Gamma(\\alpha)} \\ y^{\\alpha-1} \\ e^{-\\gamma \\cdot y} \\\\\n&= \\exp\\left[ -\\gamma \\cdot y + \\alpha \\log(\\gamma) - \\log(\\Gamma(\\alpha)) - (\\alpha-1) \\cdot \\log(y)    \\right]\n\\end{align}\\]\nVới \\(\\theta = -\\gamma/\\alpha\\) và \\(\\phi = 1/\\alpha\\) ta có\n\\[\\begin{align}\nf(y) & = \\exp\\left[ \\cfrac{\\theta y + \\log(-\\theta)}{1/\\alpha} - \\cfrac{\\log(\\alpha)}{1/\\alpha} - \\log(\\Gamma(\\alpha)) - (\\alpha-1) \\cdot \\log(y)    \\right] \\\\\n& = \\exp\\left[ \\cfrac{\\theta y + \\log(-\\theta)}{\\phi} + \\cfrac{\\log(\\phi)}{\\phi} - \\log(\\Gamma(1/\\phi)) - (1/\\phi-1) \\cdot \\log(y)    \\right]\n\\end{align}\\]\nChúng ta có phân phối kiểu mũ với \\((\\phi) = \\phi\\), \\(b(\\theta) = \\log(-\\theta)\\), và\n\\[\\begin{align}\nc(y,\\phi) = \\cfrac{\\log(\\phi)}{\\phi} - \\log(\\Gamma(1/\\phi)) - (1/\\phi-1) \\cdot \\log(y)\n\\end{align}\\]\nChúng ta kiểm tra giá trị trung bình và phương sai của phân phối \\(Gamma(\\alpha,\\beta)\\) theo công thức (12.25)\n\\[\\begin{align}\n& \\mathbb{E}(Y) = b^{'}(\\theta) = - \\cfrac{1}{\\theta} = \\cfrac{\\alpha}{\\gamma} \\\\\n& \\mathbb{V}(Y) = (\\phi) \\cdot b^{''}(\\theta) = \\cfrac{1}{\\alpha} \\cdot \\cfrac{\\alpha^2}{\\gamma^2} = \\cfrac{\\alpha}{\\gamma^2}\n\\end{align}\\]Ví dụ 3: phân phối \\(Gamma(\\alpha,\\gamma)\\) nằm trong họ các phân phối kiểu mũ. Thật vậy\n\\[\\begin{align}\nf(y) &= \\cfrac{\\gamma^\\alpha}{\\Gamma(\\alpha)} \\ y^{\\alpha-1} \\ e^{-\\gamma \\cdot y} \\\\\n&= \\exp\\left[ -\\gamma \\cdot y + \\alpha \\log(\\gamma) - \\log(\\Gamma(\\alpha)) - (\\alpha-1) \\cdot \\log(y)    \\right]\n\\end{align}\\]\nVới \\(\\theta = -\\gamma/\\alpha\\) và \\(\\phi = 1/\\alpha\\) ta có\n\\[\\begin{align}\nf(y) & = \\exp\\left[ \\cfrac{\\theta y + \\log(-\\theta)}{1/\\alpha} - \\cfrac{\\log(\\alpha)}{1/\\alpha} - \\log(\\Gamma(\\alpha)) - (\\alpha-1) \\cdot \\log(y)    \\right] \\\\\n& = \\exp\\left[ \\cfrac{\\theta y + \\log(-\\theta)}{\\phi} + \\cfrac{\\log(\\phi)}{\\phi} - \\log(\\Gamma(1/\\phi)) - (1/\\phi-1) \\cdot \\log(y)    \\right]\n\\end{align}\\]\nChúng ta có phân phối kiểu mũ với \\((\\phi) = \\phi\\), \\(b(\\theta) = \\log(-\\theta)\\), và\n\\[\\begin{align}\nc(y,\\phi) = \\cfrac{\\log(\\phi)}{\\phi} - \\log(\\Gamma(1/\\phi)) - (1/\\phi-1) \\cdot \\log(y)\n\\end{align}\\]\nChúng ta kiểm tra giá trị trung bình và phương sai của phân phối \\(Gamma(\\alpha,\\beta)\\) theo công thức (12.25)\n\\[\\begin{align}\n& \\mathbb{E}(Y) = b^{'}(\\theta) = - \\cfrac{1}{\\theta} = \\cfrac{\\alpha}{\\gamma} \\\\\n& \\mathbb{V}(Y) = (\\phi) \\cdot b^{''}(\\theta) = \\cfrac{1}{\\alpha} \\cdot \\cfrac{\\alpha^2}{\\gamma^2} = \\cfrac{\\alpha}{\\gamma^2}\n\\end{align}\\]Trong trường hợp hàm số \\((\\phi)\\) là hàm số tuyến tính theo \\(\\phi\\), nghĩa là tồn tại số \\(\\omega\\) sao cho \\((\\phi) = \\cfrac{\\phi}{\\omega}\\) chúng ta có hàm mật độ của biến ngẫu nhiên có phân phối kiểu mũ như sau\n\\[\\begin{align}\nf(y;\\theta,\\phi) = \\exp\\left[ \\cfrac{y \\theta - b(\\theta)}{\\phi/\\omega} + c(y,\\phi) \\right]\n\\end{align}\\]\nGiá trị trung bình và phương sai của biến ngẫu nhiên \\(Y\\) trong trường hợp này được xác định như sau:\n\\[\\begin{align}\n& \\mathbb{E}(Y) = b^{'}(\\theta) \\\\\n& \\mathbb{V}(Y) = \\cfrac{\\phi}{\\omega} \\cdot b^{''}(\\theta)\n\\end{align}\\]Giả sử hàm \\(b^{'}(.)\\) là hàm số đơn điệu và tồn tại hàm số ngược \\({b^{'}}^{-1}(.)\\) thì \\(\\theta = {b^{'}}^{-1}\\left(\\mathbb{E}(Y)\\right)\\). đó mối liên hệ giữa phương sai của biến ngẫu nhiên \\(Y\\) và giá trị trung bình của biến \\(Y\\) được thể hiện qua công thức sau\n\\[\\begin{align}\n\\mathbb{V}(Y) = \\cfrac{\\phi}{\\omega} \\cdot b^{''}({b^{'}}^{-1}(\\mathbb{E}(Y)))\n\\end{align}\\]Hàm số \\(V(\\cdot) = b^{''}({b^{'}}^{-1}(\\cdot))\\) được gọi là hàm phương sai của phân phối kiểu mũ. Tại sao chúng ta cần định nghĩa một hàm số phức tạp như vậy? Bởi vì hàm \\(V(\\cdot)\\) là cơ sở để người xây dựng mô hình kiểm soát phương sai của biến ngẫu nhiên kiểu mũ.Giả sử biến phụ thuộc quan sát được là \\(Y_1, Y_2, \\cdots, Y_n\\) có cùng phân phối kiểu mũ với cùng tham số \\(\\phi\\), cùng hàm \\(b(\\cdot)\\) và \\(c(\\cdot)\\), nhưng có giá trị \\(\\omega_i\\) khác nhau. Giá trị trung bình của biến \\(Y_i\\), ký hiệu là \\(\\mu_i\\) được giải thích thông qua các biến độc lập và không phụ thuộc vào \\(\\omega_i\\), trong khi phương sai của biến \\(Y_i\\) phụ thuộc vào \\(\\omega_i\\) và giá trị trung bình \\(\\mu_i\\):\n\\[\\begin{align}\n\\mathbb{V}(Y_i) = \\cfrac{\\phi}{\\omega_i} \\cdot V(\\mu_i)\n\\end{align}\\]Khi ước lượng mô hình tuyến tính tổng quát trên một dữ liệu cụ thể, giá trị hàm phương sai \\(V\\) phụ thuộc vào cách chúng ta lựa chọn hàm \\(b\\) và giá trị trung bình của \\(Y_i\\). Tham số \\(\\phi\\) không phụ thuộc vào quan sát \\(Y_i\\), đó hệ số \\(w_i\\) có ý nghĩa quyết định trong xác định phương sai của \\(Y_i\\). Chúng ta có thể lựa chọn đơn giản là cho \\(w_i\\) bằng 1 với mọi \\(\\), nếu chúng ta tin rằng tỷ lệ \\(\\cfrac{V(\\mu_i)}{\\mathbb{V}(Y_i)}\\) là hằng số. Trong một số trường hợp, \\(\\cfrac{V(\\mu_i)}{\\mathbb{V}(Y_i)}\\) thay đổi theo \\(\\), đó là lúc chúng ta cần lựa chọn \\(w_i\\) để cho kết quả tốt nhất. Chúng ta sẽ tiếp tục thảo luận về hàm phương sai của biến phụ thuộc trong phần hàm hợp lý tối đa của biến ngẫu nhiên nằm trong họ phân phối các phân phối kiểu mũ.","code":""},{"path":"mô-hình-tuyến-tính-tổng-quát..html","id":"hàm-liên-kết.","chapter":"Chương 12 Mô hình tuyến tính tổng quát.","heading":"12.3.2 Hàm liên kết.","text":"Hàm số liên kết \\(g(\\cdot)\\) liên kết giá trị trung bình của biến phụ thuộc với tổ hợp tuyến tính của các biến độc lập luôn được lựa chọn trong nhóm các hàm số đơn điệu và có đạo hàm trên miền xác định của hàm số đó. Các giả thiết này đảm bảo để hàm liên kết có hàm số ngược \\(g^{-1}(\\cdot)\\) cũng là hàm đơn điệu và có đạo hàm trên miền xác định. Một yếu tố quan trọng khác khi lựa chọn hàm liên kết đó là \\(g(\\cdot)\\) có miền xác định trùng với miền xác định của giá trị trung bình của biến phụ thuộc và miền giá trị của \\(g(\\cdot)\\) là tập số thực \\(\\mathbb{R}\\).Xin được nhắc lại rằng mối liên hệ giữa trung bình của biến phụ thuộc và tổ hợp tuyến tính của biến độc lập được mô tả thông qua hàm liên kết như sau:\n\\[\\begin{align}\n& g(\\mu_i) = \\beta_0 + \\beta_1 \\cdot x_{,1} +  \\beta_2 \\cdot x_{,2} + \\cdots + \\beta_p \\cdot x_{,d} \\\\\n& \\mu_i = g^{-1}\\left( beta_0 + \\beta_1 \\cdot x_{,1} +  \\beta_2 \\cdot x_{,2} + \\cdots + \\beta_p \\cdot x_{,d} \\right)\n\\end{align}\\]Danh sách các hàm số thường được sử dụng làm hàm liên kết được cho trong bảng dưới đâyVí dụ 1: khi \\(Y\\) mô tả số lần khách hàng gửi yêu cầu bảo hiểm, giá trị trung bình của \\(Y\\) sẽ là số thực dương. Các hàm số có miền xác định là tập các số thực dương sẽ phù hợp trong trường hợp này. Có thể thấy trong bảng trên các hàm \\(g(\\mu) = \\log(\\mu)\\) và hàm \\(g(\\mu) = 1/\\mu^2\\) là các hàm số có thể lựa chọn là hàm liên kết.Ví dụ 1: khi \\(Y\\) mô tả số lần khách hàng gửi yêu cầu bảo hiểm, giá trị trung bình của \\(Y\\) sẽ là số thực dương. Các hàm số có miền xác định là tập các số thực dương sẽ phù hợp trong trường hợp này. Có thể thấy trong bảng trên các hàm \\(g(\\mu) = \\log(\\mu)\\) và hàm \\(g(\\mu) = 1/\\mu^2\\) là các hàm số có thể lựa chọn là hàm liên kết.Ví dụ 2: khi \\(Y\\) mô tả khách hàng có gửi yêu cầu bảo hiểm hay không, hoặc mô tả sự kiện tai nạn có xảy ra hay không, giá trị trung bình của \\(Y\\) nằm trong khoảng \\((0,1)\\). đó các hàm \\(Logit\\), hàm \\(Probit\\), hàm \\(Log-Log\\), hoặc hàm \\(Cauchit\\) sẽ là lựa chọn phù hợp cho hàm liên kết.Ví dụ 2: khi \\(Y\\) mô tả khách hàng có gửi yêu cầu bảo hiểm hay không, hoặc mô tả sự kiện tai nạn có xảy ra hay không, giá trị trung bình của \\(Y\\) nằm trong khoảng \\((0,1)\\). đó các hàm \\(Logit\\), hàm \\(Probit\\), hàm \\(Log-Log\\), hoặc hàm \\(Cauchit\\) sẽ là lựa chọn phù hợp cho hàm liên kết.Khi lựa chọn hàm liên kết bạn đọc cần cân nhắc đến khả năng giải thích của mô hình và sự khó khăn có thể gặp phải khi ước lượng của tham số trong mô hình. Ví dụ như khi biến mục tiêu chỉ nhận giá trị 0 hoặc 1, hàm logit thường xuyên được sử dụng bởi vì khả năng giải thích tốt hơn hàm \\(Probit\\) hay \\(Log-Log\\). Hoặc khi cân nhắc lựa chọn giữa hàm \\(log\\) và hàm \\(inverse\\) \\(squared\\), hàm \\(log\\) thường được ưu tiên lựa chọn. Có thể sánh mối liên hệ giữa \\(\\mu_i\\) với các biến độc lập như dưới đây thông qua hàm \\(log\\) hoặc hàm \\(inverse\\) \\(squared\\) như sau\n\\[\\begin{align}\n&\\text{ Hàm log: } \\mu_i = exp(\\beta_0) \\cdot exp(\\beta_1 \\cdot x_{,1}) \\cdots exp(\\beta_p \\cdot x_{,p}) \\\\\n&\\text{ Inverse squared: } \\mu_i = \\cfrac{1}{\\sqrt{\\beta_0 + \\beta_1 \\cdot x_{,1} + \\cdots + \\beta_p \\cdot x_{,p}}}\n\\end{align}\\]\nKhi sử dụng hàm \\(log\\), mỗi biến độc lập tác động lên giá trị trung bình của biến phụ thuộc một cách độc lập với nhau theo quy tắc nhân, trong khi đó không dễ để đánh giá tác động của một biến độc lập lên biến phụ thuộc trong công thức của hàm \\(inverse\\) \\(squared\\). Hay nói cách khác, sử dụng hàm \\(log\\) thường sẽ dễ dàng giải thích kết quả hơn với khi sử dụng hàm \\(inverse\\) \\(squared\\).Với \\(Y\\) là biến ngẫu nhiên nằm trong họ các phân phối kiểu mũ ta có \\(\\mathbb{E}(Y_i) = b^{'}(\\theta_i)\\). Khi chúng ta lựa chọn hàm liên kết là \\(g(\\cdot) = (b^{'})^{-1}(\\cdot)\\) thì mối liên hệ giữa tham số chính tắc \\(\\theta_i\\) với các biến độc lập sẽ là tuyến tính. Thật vậy,\n\\[\\begin{align}\n& \\mu_i = g^{-1}(\\theta_i) \\\\\n& g(\\mu_i) = \\beta_0 + \\beta_1 \\cdot x_{,1} + \\cdots + \\beta_p \\cdot x_{,p}\\\\\n& \\rightarrow \\theta_i = \\beta_0 + \\beta_1 \\cdot x_{,1} + \\beta_2 \\cdot x_{,2} \\cdots + \\beta_p \\cdot x_{,p}\n\\end{align}\\]Như vậy, với lựa chọn hàm liên kết \\(g(\\cdot) = (b^{'})^{-1}(\\cdot)\\), tham số chính tắc \\(\\theta_i\\) của biến ngẫu nhiên phân phối mũ \\(Y_i\\) bằng tổ hợp tuyến tính của các biến độc lập. Trong trường hợp này, hàm liên kết \\((b^{'})^{-1}(\\cdot)\\) còn được gọi là hàm liên kết chính tắc của biến phụ thuộc \\(Y\\).Ví dụ 1: khi \\(Y\\) là biến ngẫu nhiên có phân phối chuẩn \\(\\mathcal{N}(\\mu, \\sigma)\\), chúng ta có \\(b(\\mu) = \\mu^2/2\\) và \\(b^{'}(\\mu) = \\mu\\) đó hàm liên kết chính tắc của biến ngẫu nhiên phân phối chuẩn \\(\\mathcal{N}(\\mu, \\sigma)\\) là \\(g(\\mu) = \\mu\\).Ví dụ 1: khi \\(Y\\) là biến ngẫu nhiên có phân phối chuẩn \\(\\mathcal{N}(\\mu, \\sigma)\\), chúng ta có \\(b(\\mu) = \\mu^2/2\\) và \\(b^{'}(\\mu) = \\mu\\) đó hàm liên kết chính tắc của biến ngẫu nhiên phân phối chuẩn \\(\\mathcal{N}(\\mu, \\sigma)\\) là \\(g(\\mu) = \\mu\\).Ví dụ 2: khi \\(Y\\) là biến ngẫu nhiên có phân phối \\(gamma(\\alpha,\\gamma)\\), chúng ta có \\(b(\\mu) = \\log(-\\mu)\\), và \\(b^{'}(\\mu) = -1/\\mu\\), đó hàm liên kết chính tắc của biến ngẫu nhiên phân phối gamma là \\(g(\\mu) = -1/\\mu\\). Trong trường hợp này hàm liên kết không có miền xác định là tập các số thực dương. Có thể thấy rằng lựa chọn hàm liên kết chính tắc không phải là một lựa chọn phù hợp.Ví dụ 2: khi \\(Y\\) là biến ngẫu nhiên có phân phối \\(gamma(\\alpha,\\gamma)\\), chúng ta có \\(b(\\mu) = \\log(-\\mu)\\), và \\(b^{'}(\\mu) = -1/\\mu\\), đó hàm liên kết chính tắc của biến ngẫu nhiên phân phối gamma là \\(g(\\mu) = -1/\\mu\\). Trong trường hợp này hàm liên kết không có miền xác định là tập các số thực dương. Có thể thấy rằng lựa chọn hàm liên kết chính tắc không phải là một lựa chọn phù hợp.","code":""},{"path":"mô-hình-tuyến-tính-tổng-quát..html","id":"hàm-hợp-lý-tối-đa-và-ước-lượng-mô-hình.","chapter":"Chương 12 Mô hình tuyến tính tổng quát.","heading":"12.4 Hàm hợp lý tối đa và ước lượng mô hình.","text":"Hệ số tuyến tính của các biến độc lập \\(\\boldsymbol{\\beta} = (\\beta_0, \\beta_1, \\cdots, \\beta_p)\\) được ước lượng từ dữ liệu \\(\\textbf{y} = (y_1, y_2, \\cdots y_n)\\) và \\(\\textbf{x} = (\\textbf{x}_1, \\textbf{x}_2, \\cdots \\textbf{x}_n)\\) bằng phương pháp tối đa hóa hàm hợp lý, hay còn gọi là hàm likelihood. Khi biến mục tiêu \\(Y_i\\) có phân phối kiểu mũ, hàm likelihood được viết như sau\n\\[\\begin{align}\nL(\\textbf{y},\\boldsymbol{\\beta}) = \\prod\\limits_{=1}^n \\ \\exp\\left[ \\cfrac{y_i \\theta - b(\\theta)}{a_i(\\phi)} + c_i(y_i,\\phi) \\right]\n\\end{align}\\]\nTrong hầu hết các trường hợp, chúng ta sẽ thực hiện tính toán trên hàm Log-likelihood thay vì hàm likelihood. Hàm Log-likelihood được ký hiệu \\(l(\\textbf{y},\\beta)\\) được xác định như sau\\[\\begin{align}\nl(\\textbf{y},\\beta) &= \\log \\left( \\prod\\limits_{=1}^n \\ \\exp\\left[ \\cfrac{y_i \\theta_i - b(\\theta_i)}{a_i(\\phi)} + c_i(y_i,\\phi) \\right] \\right) \\\\\n&= \\sum\\limits_{=1}^n  \\left(\\cfrac{y_i \\theta_i - b(\\theta_i)}{a_i(\\phi)} + c_i(y_i,\\phi)\\right)\n\\end{align}\\]\nVéc-tơ hệ số \\(\\boldsymbol{\\beta}\\) được ước lượng sao cho giá trị của hàm Log-likelihood đạt giá trị lớn nhất. Như vậy \\(\\boldsymbol{\\beta}\\) là nghiệm của hệ phương trình\n\\[\\begin{align}\n\\cfrac{\\partial l(\\textbf{y},\\boldsymbol{\\beta})}{\\partial \\beta_j} & = 0 \\ \\ \\forall j = 1,2, \\cdots, p\n\\end{align}\\]\nLưu ý rằng \\(b^{'}(\\theta_i) = \\mu_i\\) nên trong các thành phần của hàm mật độ xác suất của biến \\(Y_i\\) chỉ có hàm số \\(b(\\cdot)\\) và tham số \\(\\theta_i\\) phụ thuộc vào hệ số \\(\\boldsymbol{\\beta}\\). Đạo hàm của hàm Log-likelihood theo \\(\\beta\\) có thể viết được như sau\n\\[\\begin{align}\n\\cfrac{\\partial l(\\textbf{y},\\beta)}{\\partial \\beta_j} & = \\sum\\limits_{=1}^n  \\cfrac{1}{a_i(\\phi)} \\left(y_i \\cfrac{\\partial \\theta_i}{\\partial \\beta_j} - \\cfrac{\\partial b(\\theta_i)}{\\partial \\beta_j} \\right) \\\\\n& = \\sum\\limits_{=1}^n  \\cfrac{y_i - b^{'}(\\theta_i)}{a_i(\\phi)} \\cfrac{\\partial \\theta_i}{\\partial \\beta_j}\n\\end{align}\\]Lưu ý rằng \\(\\mu_i = b^{'}(\\theta_i)\\), đo đó\n\\[\\begin{align}\n\\theta_i = (b^{'})^{-1}(\\mu_i) = (b^{'})^{-1}\\left(g^{-1}(\\beta_0 + \\beta_1 \\cdot x_{,1} + \\cdots \\beta_p \\cdot x_{,p})\\right)\n\\end{align}\\]\nđồng thời, đạo hàm của hàm ngược được của các hàm \\(b^{'}(\\cdot)\\) và \\(g(\\cdot)\\) được xác định như sau\n\\[\\begin{align}\n& \\left((b^{'})^{-1}\\right)^{'}(\\mu_i) = \\cfrac{1}{(b^{''}\\left((b^{'})^{-1}(\\mu_i)\\right)} = \\cfrac{1}{b^{''}(\\theta_i)} \\\\\n& (g^{-1}(\\psi_i))^{'} = \\cfrac{1}{(g^{'}(g^{-1}(\\psi_i))} =  \\cfrac{1}{g^{'}(\\mu_i)}\n\\end{align}\\]\nvới \\(\\psi_i = \\beta_0 + \\beta_1 \\cdot x_{,1} + \\cdots \\beta_p \\cdot x_{,p}\\).Như vậy, đạo hàm của \\(\\theta_i\\) theo \\(\\beta_j\\) được tính như sau\n\\[\\begin{align}\n\\cfrac{\\partial \\theta_i}{\\partial \\beta_j} = \\cfrac{\\partial (b^{'})^{-1}(g^{-1}(\\psi_i(\\beta_j)))}{\\partial \\beta_j} = \\cfrac{x_{,j}}{b^{''}(\\theta_i) \\cdot g^{'}(\\mu_i)}\n\\end{align}\\]Ta có đạo hàm của hàm Log-likelihood theo các tham số \\(\\beta_j\\)\n\\[\\begin{align}\n\\cfrac{\\partial l(\\textbf{y},\\beta)}{\\partial \\beta_j} = \\sum\\limits_{=1}^n  \\cfrac{(y_i - \\mu_i) \\cdot x_{,j}}{a_i(\\phi) \\cdot b^{''}(\\theta_i) \\cdot g^{'}(\\mu_i)}\n\\end{align}\\]Với lựa chọn \\(a_i(\\phi) = \\cfrac{\\phi}{\\omega_i}\\), phương sai của biến phụ thuộc là \\(a_i(\\phi) \\cdot b^{''}(\\theta_i)\\) nên ta có thể viết hàm Log-likelihood theo các tham số \\(\\beta_j\\) theo giá trị trung bình và phương sai của biến phụ thuộc\n\\[\\begin{align}\n\\cfrac{\\partial l(\\textbf{y},\\boldsymbol{\\beta})}{\\partial \\beta_j} &= \\sum\\limits_{=1}^n  \\cfrac{(y_i - \\mu_i) \\cdot x_{,j}}{\\mathbb{V}(y_i) \\cdot g^{'}(\\mu_i)} \\\\\n& = \\sum\\limits_{=1}^n w_i \\cfrac{(y_i - \\mu_i) \\cdot x_{,j}}{\\phi V(\\mu_i) \\cdot g^{'}(\\mu_i)} \\\\\n& =\\cfrac{1}{\\phi} \\sum\\limits_{=1}^n w_i \\cfrac{x_{,j}}{V(\\mu_i) \\cdot g^{'}(\\mu_i)} (y_i - \\mu_i)\n\\end{align}\\]\nCó thể thầy rằng:Thứ nhất, \\(\\beta_j\\) là nghiệm của phương trình \\(\\cfrac{\\partial l(\\textbf{y},\\beta)}{\\partial \\beta_j} = 0\\) sẽ không phụ thuộc vào giá trị của tham số phân tán \\(\\phi\\).Thứ nhất, \\(\\beta_j\\) là nghiệm của phương trình \\(\\cfrac{\\partial l(\\textbf{y},\\beta)}{\\partial \\beta_j} = 0\\) sẽ không phụ thuộc vào giá trị của tham số phân tán \\(\\phi\\).Thứ hai, giá trị của \\(\\mu_i\\) phụ thuộc vào giá trị của các biến độc lập \\(\\textbf{x}_i\\) và phụ thuộc vào các hệ số \\(\\boldsymbol{\\beta}\\), đó giá trị \\(\\cfrac{x_{,j}}{V(\\mu_i) \\cdot g^{'}(\\mu_i)}\\) không phụ thuộc hoàn toàn vào cách chúng ta xây dựng mô hình.Thứ hai, giá trị của \\(\\mu_i\\) phụ thuộc vào giá trị của các biến độc lập \\(\\textbf{x}_i\\) và phụ thuộc vào các hệ số \\(\\boldsymbol{\\beta}\\), đó giá trị \\(\\cfrac{x_{,j}}{V(\\mu_i) \\cdot g^{'}(\\mu_i)}\\) không phụ thuộc hoàn toàn vào cách chúng ta xây dựng mô hình.Thứ ba, giá trị \\(w_i\\) không phụ thuộc vào dữ liệu đó có thể được sử dụng linh hoạt để ước lượng mô hình có kết quả tốt nhất.Thứ ba, giá trị \\(w_i\\) không phụ thuộc vào dữ liệu đó có thể được sử dụng linh hoạt để ước lượng mô hình có kết quả tốt nhất.Giải hệ phương trình với ẩn là véc-tơ tham số \\(\\boldsymbol{\\beta}\\) như trên thường phải sử dụng các phương pháp giải số. Phương pháp thường được sử dụng là thuật toán Newton Raphson.Hàm glm() sử dụng xuyên suốt trong chương sách cho phép bạn đọc ước lượng mô hình tuyến tính tổng quát cho đa số các phân phối thường gặp của \\(Y_i\\). Tham số \\(weight\\) trong hàm glm() là véc-tơ tham số \\(w_i\\) như chúng ta đã trình bày ở trên. Lựa chọn giá trị cho \\(w_i\\) hoàn toàn cách tiếp cận của người xây dựng mô hình.Ví dụ 1: khi lựa chọn \\(Y_i\\) có phân phối Poisson với tham số \\(\\lambda_i = exp(\\theta_i)\\) và hàm liên kết là hàm \\(g(\\cdot) = log(\\cdot)\\). Chúng ta có \\((\\phi) = 1\\) đó \\(w_i = 1 \\forall \\).\n\\[\\begin{align}\n& g(\\mu_i) = log(\\mu_i) \\rightarrow g^{'}(\\mu_i) = 1/\\mu_i \\\\\n& b(\\theta) = exp(\\theta) \\rightarrow b^{'}(\\theta) = b^{''}(\\theta) = exp(\\theta) \\rightarrow (b^{'})^{-1}(\\theta) = \\log(\\theta) \\rightarrow V(\\theta) = b^{''}(b^{'})^{-1})(\\theta) = \\theta\n\\end{align}\\]\nĐạo hàm của hàm Log-likelihood theo \\(\\beta_j\\) trở thành\n\\[\\begin{align}\n\\sum\\limits_{=1}^n w_i \\cdot \\cfrac{x_{,j}}{\\phi V(\\mu_i) \\cdot g^{'}(\\mu_i)} (y_i - \\mu_i) &= \\sum\\limits_{=1}^n x_{,j} (y_i - \\mu_i) \\\\\n&= \\sum\\limits_{=1}^n x_{,j} \\left[y_i - \\exp(\\beta_0 + \\beta_1 \\cdot x_{,1} + \\cdots + \\beta_p \\cdot x_{,p})\\right]\n\\end{align}\\]\nhay nói một cách khác, véc-tơ tham số \\(\\beta\\) để tối đa hóa giá trị của hàm Log-likelihood là nghiệm của hệ phương trình\n\\[\\begin{align}\n\\sum\\limits_{=1}^n x_{,j} \\left[y_i - \\exp \\left(\\beta_0 + \\beta_1 \\cdot x_{,1} + \\cdots + \\beta_p \\cdot x_{,p} \\right) \\right] = 0 \\ \\forall j\n\\end{align}\\]Ví dụ 1: khi lựa chọn \\(Y_i\\) có phân phối Poisson với tham số \\(\\lambda_i = exp(\\theta_i)\\) và hàm liên kết là hàm \\(g(\\cdot) = log(\\cdot)\\). Chúng ta có \\((\\phi) = 1\\) đó \\(w_i = 1 \\forall \\).\n\\[\\begin{align}\n& g(\\mu_i) = log(\\mu_i) \\rightarrow g^{'}(\\mu_i) = 1/\\mu_i \\\\\n& b(\\theta) = exp(\\theta) \\rightarrow b^{'}(\\theta) = b^{''}(\\theta) = exp(\\theta) \\rightarrow (b^{'})^{-1}(\\theta) = \\log(\\theta) \\rightarrow V(\\theta) = b^{''}(b^{'})^{-1})(\\theta) = \\theta\n\\end{align}\\]\nĐạo hàm của hàm Log-likelihood theo \\(\\beta_j\\) trở thành\n\\[\\begin{align}\n\\sum\\limits_{=1}^n w_i \\cdot \\cfrac{x_{,j}}{\\phi V(\\mu_i) \\cdot g^{'}(\\mu_i)} (y_i - \\mu_i) &= \\sum\\limits_{=1}^n x_{,j} (y_i - \\mu_i) \\\\\n&= \\sum\\limits_{=1}^n x_{,j} \\left[y_i - \\exp(\\beta_0 + \\beta_1 \\cdot x_{,1} + \\cdots + \\beta_p \\cdot x_{,p})\\right]\n\\end{align}\\]\nhay nói một cách khác, véc-tơ tham số \\(\\beta\\) để tối đa hóa giá trị của hàm Log-likelihood là nghiệm của hệ phương trình\n\\[\\begin{align}\n\\sum\\limits_{=1}^n x_{,j} \\left[y_i - \\exp \\left(\\beta_0 + \\beta_1 \\cdot x_{,1} + \\cdots + \\beta_p \\cdot x_{,p} \\right) \\right] = 0 \\ \\forall j\n\\end{align}\\]Ví dụ 2: khi \\(Y_i\\) có phân phối Gamma với tham số \\(\\alpha\\), \\(\\gamma_i\\) chúng ta có \\(b(\\theta_i) = log(-\\theta_i)\\) với \\(\\theta_i = - \\cfrac{\\gamma_i}{\\alpha}\\) đồng thời \\((\\phi) = \\phi\\) đó \\(w_i = 1\\) \\(\\forall \\). Giả sử hàm liên kết được lựa chọn là hàm \\(log\\): \\(g(\\cdot) = log(\\cdot)\\).\n\\[\\begin{align}\n& g(\\mu_i) = log(\\mu_i) \\rightarrow g^{'}(\\mu_i) = 1/\\mu_i \\\\\n& b(\\theta_i) = log(-\\theta_i) \\rightarrow b^{'}(\\theta_i) = -1/\\theta_i \\rightarrow b^{''}(\\theta) = 1/\\theta^2 \\rightarrow (b^{'})^{-1}(\\theta) = -1/\\theta \\rightarrow V(\\mu) = b^{''}(b^{'})^{-1})(\\mu) = \\mu^2\n\\end{align}\\]\nĐạo hàm của hàm Log-likelihood theo \\(\\beta_j\\) trở thành\n\\[\\begin{align}\n\\sum\\limits_{=1}^n w_i \\cdot \\cfrac{x_{,j}}{\\phi V(\\mu_i) \\cdot g^{'}(\\mu_i)} (y_i - \\mu_i) &= \\sum\\limits_{=1}^n \\cfrac{x_{,j}}{\\mu_i} (y_i - \\mu_i)\n\\end{align}\\]\nvéc-tơ tham số \\(\\boldsymbol{\\beta}\\) để tối đa hóa giá trị của hàm Log-likelihood là nghiệm của hệ phương trình\n\\[\\begin{align}\n\\sum\\limits_{=1}^n x_{,j} \\left\\{y_i \\cdot \\exp\\left[-(\\beta_0 + \\beta_1 \\cdot x_{,1} + \\cdots + \\beta_p \\cdot x_{,p}) \\right] - 1 \\right\\} = 0 \\ \\forall j\n\\end{align}\\]Ví dụ 2: khi \\(Y_i\\) có phân phối Gamma với tham số \\(\\alpha\\), \\(\\gamma_i\\) chúng ta có \\(b(\\theta_i) = log(-\\theta_i)\\) với \\(\\theta_i = - \\cfrac{\\gamma_i}{\\alpha}\\) đồng thời \\((\\phi) = \\phi\\) đó \\(w_i = 1\\) \\(\\forall \\). Giả sử hàm liên kết được lựa chọn là hàm \\(log\\): \\(g(\\cdot) = log(\\cdot)\\).\n\\[\\begin{align}\n& g(\\mu_i) = log(\\mu_i) \\rightarrow g^{'}(\\mu_i) = 1/\\mu_i \\\\\n& b(\\theta_i) = log(-\\theta_i) \\rightarrow b^{'}(\\theta_i) = -1/\\theta_i \\rightarrow b^{''}(\\theta) = 1/\\theta^2 \\rightarrow (b^{'})^{-1}(\\theta) = -1/\\theta \\rightarrow V(\\mu) = b^{''}(b^{'})^{-1})(\\mu) = \\mu^2\n\\end{align}\\]\nĐạo hàm của hàm Log-likelihood theo \\(\\beta_j\\) trở thành\n\\[\\begin{align}\n\\sum\\limits_{=1}^n w_i \\cdot \\cfrac{x_{,j}}{\\phi V(\\mu_i) \\cdot g^{'}(\\mu_i)} (y_i - \\mu_i) &= \\sum\\limits_{=1}^n \\cfrac{x_{,j}}{\\mu_i} (y_i - \\mu_i)\n\\end{align}\\]\nvéc-tơ tham số \\(\\boldsymbol{\\beta}\\) để tối đa hóa giá trị của hàm Log-likelihood là nghiệm của hệ phương trình\n\\[\\begin{align}\n\\sum\\limits_{=1}^n x_{,j} \\left\\{y_i \\cdot \\exp\\left[-(\\beta_0 + \\beta_1 \\cdot x_{,1} + \\cdots + \\beta_p \\cdot x_{,p}) \\right] - 1 \\right\\} = 0 \\ \\forall j\n\\end{align}\\]Ví dụ 3: khi \\(Y_i\\) là số tiền bồi thường trung bình cho 1 tai nạn trong vòng 1 năm của một khách hàng, biết rằng khách hàng được bồi thường \\(n_i\\) lần trong năm và số tiền bồi thường của một vụ tai nạn là biến ngẫu nhiên \\(Y^*_i\\) phân phối Gamma với tham số \\(\\alpha\\), \\(\\beta_i\\). \\(Y_i\\) là giá trị trung bình của \\(n_i\\) biến phân phối Gamma độc lập với cùng tham số \\(\\alpha\\), \\(\\beta_i\\) nên \\(Y_i\\) sẽ có phân phối Gamma với tham số \\((n_i \\alpha)\\) và \\((n_i \\beta_i)\\). Khi viết \\(Y_i\\) dưới dạng phân phối kiểu mũ, chúng ta có \\(b(\\theta_i) = log(-\\theta_i)\\) với \\(\\theta_i = - \\cfrac{\\beta_i}{\\alpha}\\) và \\((\\phi) = \\phi/n_i\\). Véc-tơ tham số \\(\\boldsymbol{\\beta}\\) để tối đa hóa giá trị của hàm Log-likelihood là nghiệm của hệ phương trình\n\\[\\begin{align}\n\\sum\\limits_{=1}^n n_i \\cdot \\ x_{,j} \\cdot  \\left[y_i \\cdot \\exp\\left(-(\\beta_0 + \\beta_1 \\cdot x_{,1} + \\cdots + \\beta_p \\cdot x_{,p}) \\right) - 1 \\right] = 0 \\ \\forall j\n\\end{align}\\]Ví dụ 3: khi \\(Y_i\\) là số tiền bồi thường trung bình cho 1 tai nạn trong vòng 1 năm của một khách hàng, biết rằng khách hàng được bồi thường \\(n_i\\) lần trong năm và số tiền bồi thường của một vụ tai nạn là biến ngẫu nhiên \\(Y^*_i\\) phân phối Gamma với tham số \\(\\alpha\\), \\(\\beta_i\\). \\(Y_i\\) là giá trị trung bình của \\(n_i\\) biến phân phối Gamma độc lập với cùng tham số \\(\\alpha\\), \\(\\beta_i\\) nên \\(Y_i\\) sẽ có phân phối Gamma với tham số \\((n_i \\alpha)\\) và \\((n_i \\beta_i)\\). Khi viết \\(Y_i\\) dưới dạng phân phối kiểu mũ, chúng ta có \\(b(\\theta_i) = log(-\\theta_i)\\) với \\(\\theta_i = - \\cfrac{\\beta_i}{\\alpha}\\) và \\((\\phi) = \\phi/n_i\\). Véc-tơ tham số \\(\\boldsymbol{\\beta}\\) để tối đa hóa giá trị của hàm Log-likelihood là nghiệm của hệ phương trình\n\\[\\begin{align}\n\\sum\\limits_{=1}^n n_i \\cdot \\ x_{,j} \\cdot  \\left[y_i \\cdot \\exp\\left(-(\\beta_0 + \\beta_1 \\cdot x_{,1} + \\cdots + \\beta_p \\cdot x_{,p}) \\right) - 1 \\right] = 0 \\ \\forall j\n\\end{align}\\]","code":""},{"path":"mô-hình-tuyến-tính-tổng-quát..html","id":"so-sánh-và-lựa-chọn-mô-hình-tuyến-tính-tổng-quát.","chapter":"Chương 12 Mô hình tuyến tính tổng quát.","heading":"12.5 So sánh và lựa chọn mô hình tuyến tính tổng quát.","text":"Nắm được các nguyên tắc chung bạn đọc có thể tự xây dựng nhiều mô hình tuyến tính tổng quát khác nhau cho một dữ liệu cụ thể. Thách thức đặt ra là một mô hình liệu có thực sự có tốt hơn các mô hình khác, hay trong số các mô hình bạn lựa chọn mô hình nào là phù hợp nhất? Phần này của chương sẽ thảo luận về vấn đề sánh mô hình và lựa chọn mô hình. Các chỉ tiêu thống kê được sủ dụng để sánh mô hình sẽ xoay quanh giá trị của hàm hợp lý tối đa, bao gồm có thước đo deviance, chỉ tiêu AIC, AICC, hay BIC.","code":""},{"path":"mô-hình-tuyến-tính-tổng-quát..html","id":"thước-đo-deviance","chapter":"Chương 12 Mô hình tuyến tính tổng quát.","heading":"12.5.1 Thước đo deviance","text":"Thước đo được gọi là deviance thường được sử dụng để đánh giá và sánh các mô hình tuyến tính tổng quát có cùng phân phối của biến độc lập. Chỉ tiêu này được tính toán dựa trên giá trị hàm Log-likelihood. Khi \\(Y\\) là biến ngẫu nhiên trong nhóm các phân phối mũ, hàm log-likelihood được viết như sau\n\\[\\begin{align}\nl(\\textbf{y},\\theta) & = \\sum\\limits_{=1}^n  \\left(\\cfrac{y_i \\theta_i - b(\\theta_i)}{a_i(\\phi)} + c_i(y_i,\\phi)\\right)\n\\end{align}\\]\nvới \\(\\boldsymbol{\\theta}\\) là véc-tơ các tham số chính tắc, \\(\\boldsymbol{\\theta} = (\\theta_1, \\theta_2, \\cdots, \\theta_n)\\). Sau khi phân phối của \\(Y\\) và hàm liên kết được lựa chọn, chúng ta ước lượng được véc-tơ tham số \\(\\boldsymbol{\\beta}\\) là hệ số của các biến độc lập bằng cách tối đa hóa hàm log-likelihood. Sau khi đã xác định được véc-tơ tham số \\(\\boldsymbol{\\beta}\\), với mỗi lựa chọn cho phân phối của \\(Y\\) và hàm liên kết, chúng ta sẽ tính toán được các tham số chính tắc của mô hình tuyến tính tổng quát. Nếu hàm liên kết được lựa chọn là hàm liên kết chính tắc, \\(g(\\cdot) = (b^{'})^{-1}(\\cdot)\\), chúng ta có các tham số chính tắc chính là tổ hợp tuyến tính của các biến độc lập \\(\\boldsymbol{\\theta}^M = (\\theta^M_1, \\theta^M_2, \\cdots, \\theta^M_n)\\) với\n\\[\\begin{align}\n\\theta^M_i = \\beta_0 + \\beta_1 \\cdot x_{,1} + \\cdots + \\beta_p \\cdot x_{,p}\n\\tag{12.26}\n\\end{align}\\]Với mỗi lựa chọn cho mô hình tuyến tính tổng quát, bao gồm lựa chọn cho phân phối của \\(Y\\) và hàm liên kết \\(g\\), tạm gọi là mô hình \\(M\\), chúng ta gọi \\(l(\\textbf{y},\\boldsymbol{\\theta}^M)\\) là giá trị của hàm Log-likelihood tại tham số \\(\\boldsymbol{\\theta}^M\\) được xác định qua phương trình (12.26). Lưu ý rằng véc-tơ \\(\\boldsymbol{\\theta}^M\\) có độ dài là \\(n\\), bằng với kích thước của dữ liệu, tuy nhiên các tham số này được tính toán từ \\((p+1)\\) giá trị \\(\\beta\\) ước lượng được và giá trị của biến độc lập.Điều gì xảy ra nếu tham số chính tắc \\(\\boldsymbol{\\theta}\\) hoàn toàn tự và không phụ thuộc vào biến độc lập? Hàm Log-likelihood sẽ đạt giá trị cực đại tại \\(\\theta^S = (\\theta^S_1, \\theta^S_2, \\cdots, \\theta^S_n)\\) với \\(\\theta^S_i\\) là giá trị sao cho đạo hàm của hàm Log-likelihood tại \\(\\theta^S_i\\) bằng 0. \\(\\theta\\) hoàn toàn tự nên chỉ có thành phần thứ \\(\\) của hàm Log-likelihood phụ thuộc vào \\(\\theta^S_i\\)\n\\[\\begin{align}\n\\cfrac{\\partial l(\\textbf{y},\\theta)}{\\partial \\theta_i} & =  \\cfrac{y_i - b^{'}(\\theta_i)}{a_i(\\phi)}\n\\end{align}\\]\nCho đạo hàm của \\(l(\\textbf{y},\\theta)\\) theo \\(\\theta_i\\) bằng 0 chúng ta có \\(y_i - b^{'}(\\theta^S_i) = 0\\) hay \\(\\theta^S_i = (b^{'})^{-1}(y_i)\\).Bạn đọc lưu ý rằng giá trị hàm Log-likelihood đạt cực đại tại \\(\\boldsymbol{\\theta}^S\\) không có nghĩa là tham số \\(\\boldsymbol{\\theta}^S\\) là lựa chọn tốt nhất cho mô hình tuyến tính tổng quát bởi tham số có đến \\(n\\) bậc tự . \\(\\boldsymbol{\\theta}^S\\) cho chúng ta thông tin về giá trị cận trên của \\(l(\\textbf{y},\\theta)\\) khi \\(\\theta\\) thay đổi. Thước đo deviance của mô hình \\(M\\), ký hiệu \\(D*(y,\\theta^M)\\) được định nghĩa là hai lần khoảng cách từ \\(l(\\textbf{y},\\boldsymbol{\\theta}^M)\\) đến giá trị tối đa \\(l(\\textbf{y},\\boldsymbol{\\theta}^S)\\). Deviance của một mô hình cho biết mô hình được lựa chọn gần với phân phối quan sát được như thế nào, và mô hình có deviance càng nhỏ thì càng giải thích tốt hơn biến phụ thuộc\n\\[\\begin{align}\nD^{*}(y, \\theta^M) &= 2 \\left( l(\\textbf{y},\\theta^S) - l(\\textbf{y},\\theta^M) \\right) \\\\\n& = 2 \\sum\\limits_{=1}^n  \\cfrac{y_i (\\theta^S_i - \\theta^M_i) - (b(\\theta^S_i) - b(\\theta^M_i))}{a_i(\\phi)}\n\\end{align}\\]Trong trường hợp hàm \\(a_i(\\phi)\\) là tuyến tính theo \\(\\phi\\); \\(a_i(\\phi) = \\cfrac{\\phi}{w_i}\\), ta có\n\\[\\begin{align}\nD^{*}(y, \\theta^M) & = \\cfrac{1}{\\phi} \\sum\\limits_{=1}^n 2w_i \\cdot \\left[y_i (\\theta^S_i - \\theta^M_i) - (b(\\theta^S_i) - b(\\theta^M_i)) \\right]  = \\cfrac{D(y, \\theta^M)}{\\phi}\n\\end{align}\\]\nvới\n\\[\\begin{align}\nD(y, \\theta^M) & = \\sum\\limits_{=1}^n 2w_i \\cdot \\left[y_i (\\theta^S_i - \\theta^M_i) - (b(\\theta^S_i) - b(\\theta^M_i)) \\right]\n\\end{align}\\]\ntrong đó \\(D(y, \\theta^M)\\) là thước đo deviance bỏ qua ảnh hưởng của tham số dispersion \\(\\phi\\).Ví dụ 1: biến mục tiêu \\(Y_i\\) có phân phối chuẩn: \\(Y_i \\sim \\mathcal{N}(\\mu_i, \\sigma)\\) và hàm liên kết \\(g(x) = x\\). Có thể viết hàm mật độ của \\(Y_i\\) dưới dạng phân phối mũ như sau\n\\[\\begin{align}\nf(y, \\theta_i, \\phi) & = \\exp\\left[ \\cfrac{\\theta_i y - \\theta_i^2/2} {\\phi} - \\cfrac{y^2}{2\\phi} - \\cfrac{1}{2} log(2 \\pi \\phi) \\right]\n\\end{align}\\]\nvới \\(\\theta_i = \\mu_i\\) và \\(\\phi = \\sigma^2\\). Ta có \\(\\theta^S_i = (b^{'})^{-1}(y_i) = y_i\\), đó deviance tính trên quan sát \\(\\textbf{y}\\) được xác định như sau\n\\[\\begin{align}\nD(y, \\theta^M) & = \\sum\\limits_{=1}^n 2 \\cdot \\left[y_i (\\theta^S_i - \\theta^M_i) - (b(\\theta^S_i) - b(\\theta^M_i)) \\right] \\\\\n& = \\sum\\limits_{=1}^n (y_i - \\theta^M_i)^2 \\\\\n& = \\sum\\limits_{=1}^n \\left[y_i -  (\\beta_0 + \\beta_1 \\cdot x_{,1} + \\cdots + \\beta_p \\cdot x_{p,1}) \\right]^2\n\\end{align}\\]\nTrong trường hợp hồi quy tuyến tính thông thường, deviance chính là tổng bình phương sai số.Ví dụ 1: biến mục tiêu \\(Y_i\\) có phân phối chuẩn: \\(Y_i \\sim \\mathcal{N}(\\mu_i, \\sigma)\\) và hàm liên kết \\(g(x) = x\\). Có thể viết hàm mật độ của \\(Y_i\\) dưới dạng phân phối mũ như sau\n\\[\\begin{align}\nf(y, \\theta_i, \\phi) & = \\exp\\left[ \\cfrac{\\theta_i y - \\theta_i^2/2} {\\phi} - \\cfrac{y^2}{2\\phi} - \\cfrac{1}{2} log(2 \\pi \\phi) \\right]\n\\end{align}\\]\nvới \\(\\theta_i = \\mu_i\\) và \\(\\phi = \\sigma^2\\). Ta có \\(\\theta^S_i = (b^{'})^{-1}(y_i) = y_i\\), đó deviance tính trên quan sát \\(\\textbf{y}\\) được xác định như sau\n\\[\\begin{align}\nD(y, \\theta^M) & = \\sum\\limits_{=1}^n 2 \\cdot \\left[y_i (\\theta^S_i - \\theta^M_i) - (b(\\theta^S_i) - b(\\theta^M_i)) \\right] \\\\\n& = \\sum\\limits_{=1}^n (y_i - \\theta^M_i)^2 \\\\\n& = \\sum\\limits_{=1}^n \\left[y_i -  (\\beta_0 + \\beta_1 \\cdot x_{,1} + \\cdots + \\beta_p \\cdot x_{p,1}) \\right]^2\n\\end{align}\\]\nTrong trường hợp hồi quy tuyến tính thông thường, deviance chính là tổng bình phương sai số.Ví dụ 2: biến mục tiêu \\(Y_i\\) có phân phối Poisson \\(Y_i \\sim \\mathcal{P}(\\lambda_i)\\) và hàm liên kết \\(g(\\cdot) = log(\\cdot)\\).\n\\[\\begin{align}\nf(y; \\theta_i) = exp\\left[ \\cfrac{\\theta_i y - exp(\\theta_i)}{1} - log(\\Gamma(y+1)) \\right] \\text{ với } \\lambda_i = exp(\\theta_i)\n\\end{align}\\]\nTa có \\(\\theta^S_i = (b^{'})^{-1}(y_i) = log(y_i)\\), đó deviance tính trên quan sát \\(\\textbf{y}\\) được xác định như sau\n\\[\\begin{align}\nD(y, \\theta^M) & = \\sum\\limits_{=1}^n 2 \\cdot \\left[y_i (\\theta^S_i - \\theta^M_i) - (b(\\theta^S_i) - b(\\theta^M_i)) \\right] \\\\\n& = \\sum\\limits_{=1}^n 2 \\cdot \\left[y_i (log(y_i) - \\theta^M_i) - (y_i - exp(\\theta^M_i)) \\right] \\\\\n& = \\sum\\limits_{=1}^n 2 \\cdot \\left[y_i \\left(log(y_i) - log(\\mu^M_i)\\right) - (y_i - \\mu^M_i) \\right]\n\\end{align}\\]\nvới \\(\\mu^M_i = \\exp(\\beta_0 + \\beta_1 \\cdot x_{,1} + \\cdots + \\beta_p \\cdot x_{p,1})\\).Ví dụ 2: biến mục tiêu \\(Y_i\\) có phân phối Poisson \\(Y_i \\sim \\mathcal{P}(\\lambda_i)\\) và hàm liên kết \\(g(\\cdot) = log(\\cdot)\\).\n\\[\\begin{align}\nf(y; \\theta_i) = exp\\left[ \\cfrac{\\theta_i y - exp(\\theta_i)}{1} - log(\\Gamma(y+1)) \\right] \\text{ với } \\lambda_i = exp(\\theta_i)\n\\end{align}\\]\nTa có \\(\\theta^S_i = (b^{'})^{-1}(y_i) = log(y_i)\\), đó deviance tính trên quan sát \\(\\textbf{y}\\) được xác định như sau\n\\[\\begin{align}\nD(y, \\theta^M) & = \\sum\\limits_{=1}^n 2 \\cdot \\left[y_i (\\theta^S_i - \\theta^M_i) - (b(\\theta^S_i) - b(\\theta^M_i)) \\right] \\\\\n& = \\sum\\limits_{=1}^n 2 \\cdot \\left[y_i (log(y_i) - \\theta^M_i) - (y_i - exp(\\theta^M_i)) \\right] \\\\\n& = \\sum\\limits_{=1}^n 2 \\cdot \\left[y_i \\left(log(y_i) - log(\\mu^M_i)\\right) - (y_i - \\mu^M_i) \\right]\n\\end{align}\\]\nvới \\(\\mu^M_i = \\exp(\\beta_0 + \\beta_1 \\cdot x_{,1} + \\cdots + \\beta_p \\cdot x_{p,1})\\).Kết quả ước lượng khi sử dụng hàm glm() trong R hiển thị hai giá trị là Null deviance và Residual deviance. Null deviance được tính toán với giả thiết là chỉ có hệ số chặn \\(\\beta_0\\), còn Residual deviance được tính toán với véc-tơ \\(\\boldsymbol{\\beta}\\) đầy đủ.Giá trị Residual deviance ngoài sử dụng để sánh hai mô hình có cùng phân phối của biến phụ thuộc còn được sử dụng để lựa chọn biến trong mô hình. Giả sử hai mô hình \\(M1\\) và \\(M2\\) có cùng phân phối cho biến phụ thuộc và có cùng hàm liên kết \\(g(.)\\), mô hình \\(M1\\) có \\(m_1\\) biến độc lập trong khi mô hình \\(M_2\\) có \\(m_2\\) biến độc lập, bao gồm tất cả các biến độc lập của mô hình \\(M1\\). Nói một cách khác, \\(M_2\\) có nhiều hơn \\(M_1\\) là \\((m_2 - m_1)\\) biến độc lập. Mô hình \\(M_2\\) sẽ có deviance lớn hơn mô hình \\(M_1\\) có nhiều biến độc lập hơn, tuy nhiên việc thêm \\((m_2 - m_1)\\) biến độc lập vào mô hình có ý nghĩa thống kê nếu hiệu số giữa \\(D^*(y, \\theta^{M_2})\\) và \\(D^*(y, \\theta^{M_1})\\) là đủ lớn.Có thể chứng minh được rằng khi kích thước dữ liệu đủ lớn, hiệu số giữa \\(D^*(y, \\theta^{M_2})\\) và \\(D^*(y, \\theta^{M_1})\\) là một biến ngẫu nhiên phân phối xấp xỉ phân phối \\(\\chi^2\\) với bậc tự \\((m_2 - m_1)\\)\n\\[\\begin{align}\nD^{*}(y, \\theta^{M_1}) - D^{*}(y, \\theta^{M_2}) = 2 \\cdot \\log\\left( \\cfrac{L(y,\\theta^{M_2})}{L(y,\\theta^{M_1})} \\right) \\sim \\chi^2(m_2 - m_1)\n\\end{align}\\]Ví dụ 3: trong dữ liệu \\(exposure.csv\\), khi biến \\(Claim\\_Count\\) được giả thiết có phân phối Poisson và giá trị trung bình được giải thích bằng độ tuổi, hoặc giới tính của người được bảo hiểm, hoặc cả hai biến. Chúng ta gọi mô hình \\(M_1\\) là mô hình mà giá trị trung bình của biến phụ thuộc được giải thích bằng biến độ tuổi, mô hình \\(M_2\\) là mô hình mà giá trị trung bình của biến phụ thuộc được giải thích bằng cả biến giới tính, và mô hình \\(M_3\\) là mô hình mà biến phụ thuộc phụ thuộc vào cả độ tuổi và giới tính của khách hàng.Mô hình \\(M_1\\) và \\(M_2\\) đều chỉ có một biến độc lập và \\(M_1\\) có deviance nhỏ hơn \\(M_2\\) đó mô hình \\(M_1\\) tốt hơn \\(M_2\\). sánh mô hình \\(M_1\\) với 1 biến độc lập là age với mô hình \\(M_3\\) có 2 biến độc lập là age và \\(Gender\\) cần dựa trên kiểm định \\(\\chi^2\\). Ta có \\(D^{*}(y, \\theta^{M_1}) - D^{*}(y, \\theta^{M_2}) = 8324.2 - 8316.8 = 7.4\\). Giá trị 7.4 tương ứng với mức xác suất \\(0.994\\) của phân phối \\(\\chi^2(1)\\). Điều này có ý nghĩa là ở mức độ tin cậy 99%, có thể kết luận rằng thêm biến \\(Gender\\) vào mô hình \\(M_1\\) là có ý nghĩa thống kê, tuy nhiên việc thêm biến \\(Gender\\) lại không có ý nghĩa thống kê ở mức độ tin cậy \\(99.5\\%\\).","code":"\ndat<-read.csv(\"../KHDL_KTKD Final/Dataset/exposure.csv\")\n\nglm_M1<-glm(Claim_Count~Age, family = poisson(link=\"log\"), data = dat)\nglm_M2<-glm(Claim_Count~Gender, family = poisson(link=\"log\"), data = dat)\nglm_M3<-glm(Claim_Count~Age+Gender, family = poisson(link=\"log\"), data = dat)\n\nsummary(glm_M1)\nsummary(glm_M2)\nsummary(glm_M3)"},{"path":"mô-hình-tuyến-tính-tổng-quát..html","id":"giá-trị-hàm-log-likelihood-aic-và-bic","chapter":"Chương 12 Mô hình tuyến tính tổng quát.","heading":"12.5.2 Giá trị hàm log-likelihood, AIC và BIC","text":"Deviance là một thước đo hữu ích trong sánh các mô hình có cùng phân phối của biến phụ thuộc và hàm liên kết. Khi các mô hình cần sánh không có chung phân phối của biến phụ thuộc thì hiệu giữa hai hàm log-likelihood sẽ không có phân phối \\(\\chi^2\\) và khi đó việc sánh các mô hình là không thế thực hiện được.Trong phần trước, chúng ta đã định nghĩa \\(l(y,\\theta^M)\\) là giá trị tối đa của hàm log-likelihood cho mô hình M với tập hợp các biến độc lập đã lựa chọn. Nhìn chung, giá trị của \\(l(y,\\theta^M)\\) không cho biết nhiều thông tin về độ phù hợp của mô hình, nhưng người xây dựng mô hình thường mong muốn giá trị này càng lớn thì càng tốt. Tuy nhiên, có một vấn đề khi sánh trực tiếp giá trị \\(l(y,\\theta^M)\\) giữa các mô hình là: một mô hình bao gồm nhiều biến độc lập hơn thì rất có thể có giá trị \\(l(y,\\theta^M)\\) lớn hơn. Để giải quyết vấn đề này, có ba thước đo được điều chỉnh từ \\(l(y,\\theta^M)\\) thường được sử dụng cho mô hình tuyến tính tổng quát với mục đích đánh giá và sánh giữa các mô hình là AIC, AICC và BIC.Chỉ tiêu AIC, viết tắt của Akaike information criterion, là chỉ tiêu đơn giản nhất được tính toán từ công thức như sau\n\\[\\begin{align}\nAIC = 2 \\left(-l(y,\\theta^M)+r\\right)\n\\end{align}\\]\ntrong đó \\(r\\) là số lượng tham số cần ước lượng trong mô hình. Giá trị \\(AIC\\) càng nhỏ thì mô hình càng khớp hơn với dữ liệu.Chỉ tiêu AICC (hoặc AICs) được điều chỉnh từ AIC, được sử dụng khi kích thước dữ liệu không quá lớn để thay thế cho AIC, và được tính bởi công thức như sau\n\\[\\begin{align}\nAICC = AIC + \\cfrac{2r(r+1)}{n-r-1}\n\\end{align}\\]\nKhi kích thước dữ liệu \\(n\\) lớn, AIC và AICC sẽ tương đương nhau \\(\\cfrac{2r(r+1)}{n-r-1}\\) nhỏ.Chỉ tiêu thứ ba là BIC, là viết tắt của Bayesian information criterion. Trong một vài tài liệu BIC còn được gọi là SBC. Cũng giống như AIC và AICC, BIC là chỉ tiêu được tính toán từ giá trị cực đại của hàm log-likelihood điểu chỉnh để phản ánh số lượng tham số sử dụng trong mô hình là nhiều hay ít\n\\[\\begin{align}\nBIC = 2 \\left(-l(y,\\theta^M)+r log(n)\\right)\n\\end{align}\\]Cơ sở lý thuyết cho các chỉ tiêu AIC, AICC, và BIC bạn đọc có thể tìm thấy trong bất kỳ tài liệu thống kê toán nào, đó chúng tôi sẽ không trình bày trong cuốn sách này. Chúng tôi muốn nhấn mạnh vào góc độ ứng dụng của các chỉ tiêu này khi sử dụng để sánh các mô hình.Ví dụ 1: chúng ta quay trở lại dữ liệu “MotoInsurance.csv” khi biến mục tiêu \\(Y\\) chỉ nhận hai giá trị là '' hoặc 'Yes'. Biến \\(Y\\) có thể là biến dạng nhị phân hoặc vừa có thể là biến dạng đếm nếu chúng ta cho tương đương giá trị '' tương đương với 0 và giá trị 'Yes' tương ứng với 1. Khi xây dựng mô hình tuyến tính tổng quát, chúng ta có thể sử dụng phân phối Poisson cho biến mục tiêu với hàm liên kết \\(g(\\cdot) = log(\\cdot)\\).\\(Y\\) là biến nhị phân nên một cách tự nhiên, bạn đọc sẽ cân nhắc sử dụng phân phối nhị phân cho \\(Y\\). Chúng ta xây dựng mô hình tuyến tính tổng quát với phân phối nhị thức cho \\(Y\\) và hàm liên kết là hàm \\(logit\\). Thay vì sử dụng cả 4 biến độc lập, chúng ta chỉ sử dụng hai biến độc lập là age và sex,Tương tự như mô hình \\(glm.pois\\), mô hình \\(glm.binom.logit\\) cũng có tất cả các hệ số của biến độc lập khác 0. Làm thế nào để biết rằng mô hình \\(glm.pois\\) sử dụng phân phối Poisson với 4 biến độc lập hay mô hình \\(glm.binom.logit\\) với phân phối nhị thức cho \\(Y\\) và 2 biến độc lập là tốt hơn? Nếu chúng ta sử dụng thước đo deviance thì kết quả sẽ không chính xác vì các mô hình có phân phối của biến phụ thuộc khác nhau. Chỉ tiêu \\(AIC\\) được tính toán sẵn từ hàm \\(glm\\) có thể được sử dụng để sánh hai mô hình trong trường hợp này. Chỉ tiêu AIC của \\(glm.pois\\) là 5202.7 trong khi chỉ tiêu AIC của \\(glm.binom.logit\\) là 4941.3. Mô hình \\(glm.binom.logit\\) có AIC nhỏ hơn nên sẽ tốt hơn để mô hình hóa dữ liệu trong trường hợp này.Ví dụ 2: chúng ta sẽ tiếp tục với dữ liệu “MotoInsurance.csv”. Một cách tự nhiên, khi sử dụng phân phối nhị thức cho biến phụ thuộc, bạn đọc sử dụng hàm liên kết là hàm \\(logit\\). Liệu lựa chọn này có là tốt nhất để mô hình hóa dữ liệu mà chúng ta đang nghiên cứu? Bạn đọc có thể sử dụng chỉ tiêu AIC để lựa chọn hàm liên kết phù hợp. Chúng ta sẽ sử dụng 4 biến độc lập là age, sex, urban, và seniority để giải thích giá trị trung bình của biến phụ thuộc khi xây dựng mô hìnhMô hình \\(glm.binom.probit\\) có deviance và AIC nhỏ hơn \\(glm.binom.logit\\), điều này cho biết sử dụng hàm liên kết \\(probit\\) sẽ cho kết quả tốt hơn với hàm liên kết \\(logit\\).1. Annette J. Dobson Adrian G. Barnett (2018). Introduction Generalized Linear Models.2. Alan Agresti. (2015). Foundations Linear Generalized Linear Models.","code":"\ndat<-read.csv(\"../KHDL_KTKD Final/Dataset/MotoInsurance.csv\")\ndat$Y<-ifelse(dat$Y==\"Yes\",1,0)\ndat$sex<-as.factor(dat$sex)\ndat$urban<-as.factor(dat$urban)\nglm.pois<-glm(Y~age+sex+urban+seniority,data=dat, family = poisson(link = \"log\"))\nsummary(glm.pois)## \n## Call:\n## glm(formula = Y ~ age + sex + urban + seniority, family = poisson(link = \"log\"), \n##     data = dat)\n## \n## Deviance Residuals: \n##     Min       1Q   Median       3Q      Max  \n## -2.0824  -0.7362  -0.5465   0.5711   1.9556  \n## \n## Coefficients:\n##              Estimate Std. Error z value Pr(>|z|)    \n## (Intercept) -0.286511   0.105615  -2.713  0.00667 ** \n## age         -0.034131   0.002546 -13.406  < 2e-16 ***\n## sexM        -0.491418   0.057331  -8.572  < 2e-16 ***\n## urban1       0.625032   0.053999  11.575  < 2e-16 ***\n## seniority    0.070888   0.004257  16.652  < 2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## (Dispersion parameter for poisson family taken to be 1)\n## \n##     Null deviance: 2938.1  on 3999  degrees of freedom\n## Residual deviance: 2418.7  on 3995  degrees of freedom\n## AIC: 5202.7\n## \n## Number of Fisher Scoring iterations: 5\ndat$Y<-as.factor(dat$Y) # đổi biến Y thành factor\nglm.binom.logit<-glm(Y~age+sex,data=dat, family = binomial(link = \"logit\"))\nsummary(glm.binom.logit)## \n## Call:\n## glm(formula = Y ~ age + sex, family = binomial(link = \"logit\"), \n##     data = dat)\n## \n## Deviance Residuals: \n##     Min       1Q   Median       3Q      Max  \n## -1.4544  -0.9261  -0.7396   1.2607   2.0088  \n## \n## Coefficients:\n##              Estimate Std. Error z value Pr(>|z|)    \n## (Intercept)  1.109828   0.130876   8.480   <2e-16 ***\n## age         -0.026603   0.002709  -9.822   <2e-16 ***\n## sexM        -0.723479   0.076585  -9.447   <2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## (Dispersion parameter for binomial family taken to be 1)\n## \n##     Null deviance: 5163.3  on 3999  degrees of freedom\n## Residual deviance: 4935.3  on 3997  degrees of freedom\n## AIC: 4941.3\n## \n## Number of Fisher Scoring iterations: 4\nglm.binom.logit<-glm(Y~age+sex+seniority+urban,data=dat, family = binomial(link = \"logit\"))\nglm.binom.probit<-glm(Y~age+sex+seniority+urban,data=dat, family = binomial(link = \"probit\"))\nsummary(glm.binom.logit)## \n## Call:\n## glm(formula = Y ~ age + sex + seniority + urban, family = binomial(link = \"logit\"), \n##     data = dat)\n## \n## Deviance Residuals: \n##     Min       1Q   Median       3Q      Max  \n## -2.4424  -0.8115  -0.5046   1.0440   2.7439  \n## \n## Coefficients:\n##              Estimate Std. Error z value Pr(>|z|)    \n## (Intercept)  0.793757   0.144541   5.492 3.98e-08 ***\n## age         -0.058754   0.003525 -16.668  < 2e-16 ***\n## sexM        -0.983040   0.085377 -11.514  < 2e-16 ***\n## seniority    0.133569   0.006808  19.618  < 2e-16 ***\n## urban1       1.174515   0.078060  15.046  < 2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## (Dispersion parameter for binomial family taken to be 1)\n## \n##     Null deviance: 5163.3  on 3999  degrees of freedom\n## Residual deviance: 4295.8  on 3995  degrees of freedom\n## AIC: 4305.8\n## \n## Number of Fisher Scoring iterations: 4\nsummary(glm.binom.probit)## \n## Call:\n## glm(formula = Y ~ age + sex + seniority + urban, family = binomial(link = \"probit\"), \n##     data = dat)\n## \n## Deviance Residuals: \n##     Min       1Q   Median       3Q      Max  \n## -2.4931  -0.8168  -0.4987   1.0639   2.9359  \n## \n## Coefficients:\n##              Estimate Std. Error z value Pr(>|z|)    \n## (Intercept)  0.449579   0.085724   5.244 1.57e-07 ***\n## age         -0.034482   0.002009 -17.163  < 2e-16 ***\n## sexM        -0.577303   0.050821 -11.360  < 2e-16 ***\n## seniority    0.077888   0.003890  20.021  < 2e-16 ***\n## urban1       0.697759   0.046129  15.126  < 2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## (Dispersion parameter for binomial family taken to be 1)\n## \n##     Null deviance: 5163.3  on 3999  degrees of freedom\n## Residual deviance: 4292.4  on 3995  degrees of freedom\n## AIC: 4302.4\n## \n## Number of Fisher Scoring iterations: 5## \n## Attaching package: 'dplyr'## The following objects are masked from 'package:stats':\n## \n##     filter, lag## The following objects are masked from 'package:base':\n## \n##     intersect, setdiff, setequal, union## \n## Attaching package: 'kableExtra'## The following object is masked from 'package:dplyr':\n## \n##     group_rows## \n## Attaching package: 'gridExtra'## The following object is masked from 'package:dplyr':\n## \n##     combine## \n## Attaching package: 'pryr'## The following object is masked from 'package:dplyr':\n## \n##     where## Classes and Methods for R developed in the\n## Political Science Computational Laboratory\n## Department of Political Science\n## Stanford University\n## Simon Jackman\n## hurdle and zeroinfl functions by Achim Zeileis"},{"path":"mô-hình-tần-suất---mức-độ-nghiêm-trọng..html","id":"mô-hình-tần-suất---mức-độ-nghiêm-trọng.","chapter":"Chương 13 Mô hình tần suất - mức độ nghiêm trọng.","heading":"Chương 13 Mô hình tần suất - mức độ nghiêm trọng.","text":"Nhiều dữ liệu thu thập được trong lĩnh vực bảo hiểm có thông tin về tần suất, chẳng hạn như tần suất phát sinh yêu cầu bồi thường, cũng như mức độ nghiêm trọng của yêu cầu bồi thường. Phần này của cuốn sách giới thiệu các công cụ để xử lý phân phân phối xác suất đồng thời của tần suất và mức độ nghiêm trọng. Mô hình hóa tần suất rất quan trọng trong các ứng dụng bảo hiểm vì các đặc điểm của hợp đồng, hành vi của chủ hợp đồng, cơ sở dữ liệu mà các công ty bảo hiểm duy trì và cả các yêu cầu pháp lý. Việc lựa chọn mô hình tùy thuộc vào kiểu dữ liệu thu thập được.Đối với một số dữ liệu, chúng tôi quan sát số tiền yêu cầu bồi thường và coi yêu cầu bồi thường bằng 0 có nghĩa là không có yêu cầu bồi thường nào trong khoảng thời gian dữ liệu được thu thập. Đối với các dữ liệu khác, dữ liệu quan sát được lại là số tiền yêu cầu bồi thường của từng cá nhân. Cũng như các phần trước, chúng tôi sẽ không quá đi sâu vào các kỹ thuật xây dựng hay ước lượng mô hình mà sẽ nhấn mạnh vào các ứng dụng thực tế. Chương này của cuốn sách sử dụng công cụ thường được áp dụng trong thực tế để sánh các mô hình đó là xác nhận mô hình ngoài mẫu (--sample validation)","code":""},{"path":"mô-hình-tần-suất---mức-độ-nghiêm-trọng..html","id":"tại-sao-tần-suất-lại-có-ảnh-hưởng-đến-mức-độ-nghiêm-trọng","chapter":"Chương 13 Mô hình tần suất - mức độ nghiêm trọng.","heading":"13.1 Tại sao tần suất lại có ảnh hưởng đến mức độ nghiêm trọng?","text":"Hiểu một cách đơn giản nhất, hoạt động cơ bản nhất trong bảo hiểm là các công ty bảo hiểm nhận phí bảo hiểm để đổi lấy lời hứa bồi thường cho chủ hợp đồng về sự cố không chắc chắn của người được bảo hiểm. Mỗi lần khách hàng gặp sự cố sẽ gửi một thông báo đến công ty để yêu cầu công ty thanh toán tổn thất, thông báo này được gọi là yêu cầu bồi thường. Số tiền được gọi là mức độ nghiêm trọng của yêu cầu bồi thường, là khoản chi quan trọng đối với công ty bảo hiểm. Chúng ta cũng có thể coi yêu cầu bồi thường bằng 0 tương đương với việc sự kiện được bảo hiểm không xảy ra. Bỏ qua chi phí, một công ty bảo hiểm nếu chỉ quan tâm đến số tiền đã thanh toán sẽ không quan tâm đến hai yêu cầu bồi thường, mỗi yêu cầu bằng 100 triệu, sánh với một yêu cầu bồi thường 200 triệu, mặc dù số lượng yêu cầu bồi thường khác nhau.Tuy nhiên, nghiên cứu tần suất phát sinh các yêu cầu bồi thường, hay còn gọi tắt là tần suất, cũng là một chủ đề đáng quan tâm của những người nghiên cứu về bảo hiểm. Dưới đây là các nguyên nhân mà chúng ta cần quan tâm đến tần suất của sự kiện bảo hiểm:Thứ nhất: thông thường thì các hợp đồng bảo hiểm sẽ áp đặt các khoản khấu trừ và giới hạn trên cho mỗi lần xảy ra sự cố. Ví dụ: nếu hợp đồng có mức khấu trừ là 100 triệu cho mỗi lần xảy ra, thì hai khoản lỗ 100 triệu sẽ dẫn đến khoản thanh toán (hoặc yêu cầu bồi thường) bằng 0 từ công ty bảo hiểm, trong khi một sự kiện ở mức tổn thất 200 triệu sẽ dẫn đến khoản thanh toán là 100 triệu. Tổng số tiền tổn thất được bảo hiểm cần phải tính đến các khoản khấu trừ và giới hạn chi trả cho từng sự kiện được bảo hiểm.Thứ nhất: thông thường thì các hợp đồng bảo hiểm sẽ áp đặt các khoản khấu trừ và giới hạn trên cho mỗi lần xảy ra sự cố. Ví dụ: nếu hợp đồng có mức khấu trừ là 100 triệu cho mỗi lần xảy ra, thì hai khoản lỗ 100 triệu sẽ dẫn đến khoản thanh toán (hoặc yêu cầu bồi thường) bằng 0 từ công ty bảo hiểm, trong khi một sự kiện ở mức tổn thất 200 triệu sẽ dẫn đến khoản thanh toán là 100 triệu. Tổng số tiền tổn thất được bảo hiểm cần phải tính đến các khoản khấu trừ và giới hạn chi trả cho từng sự kiện được bảo hiểm.Thứ hai là hành vi của các bên liên quan đến hợp đồng bảo hiểm. Các mô hình tổn thất bảo hiểm giải thích rõ ràng hoặc ngầm định các hành vi dẫn đến quyết định của người dân và doanh nghiệp có thể ảnh hưởng đến cả tần suất và mức độ nghiêm trọng của tổn thất. Những người liên quan đến hợp đồng có thể không chỉ bao gồm chủ hợp đồng mà còn có cả người bảo hiểm, chuyên gia sửa chữa (oto, nhà), nhà cung cấp dịch vụ y tế, v.v. Các biến giải thích hành vi có thể có những tác động khác nhau lên tần suất xảy ra một sự kiện. Ví dụ, trong bảo hiểm nhà, khi xem xét một người mua bảo hiểm rất cẩn thận và sống trong một khu nhà đắt đỏ. Chúng ta có thể xem xét các đặc điểm của chủ nhà như một dấu hiệu cho thấy việc áp dụng các biện pháp ngăn ngừa tổn thất như những yếu tố quyết định cho thấy tần suất thấp. Đồng thời, chúng ta có thể xem xét mức thu nhập chung của khu vực địa lý nơi có ngôi nhà làm đại diện cho mức chi phí sửa chữa trong trường hợp xảy ra tai nạn, cho thấy mức độ nghiêm trọng cao. Trong bảo hiểm ô tô, chúng ta có thể nghĩ mật độ dân số có mối tương quan dương với tần suất tai nạn và có mối tương quan nghịch với mức độ nghiêm trọng. Ví dụ, ở một khu đô thị đông dân cư, tình trạng tắc nghẽn giao thông cao, có nghĩa là người lái xe có thể gặp tai nạn thường xuyên nhưng chi phí tương đối thấp. Điều này trái ngược với một khu vực nông thôn dân cư thưa thớt hơn, nơi có đường rộng, ít tắc nghẽn và lái xe có thể chạy với tốc độ cao, có nghĩa là tai nạn ít xảy ra hơn, nhưng tốc độ cao hơn có nghĩa là mức độ nghiêm trọng hơn. Lịch sử yêu cầu bồi thường trước đây là một biến số khác cung cấp thông tin về mức độ chấp nhận rủi ro của chủ hợp đồng. Đặc biệt trong bảo hiểm cá nhân, người ta thường sử dụng dấu hiệu cho biết liệu yêu cầu bồi thường có xảy ra hay không, chẳng hạn như trong ba năm qua, thay vì số tiền yêu cầu bồi thường. Ở nhiều quốc gia, phí bảo hiểm ô tô được điều chỉnh bằng hệ thống được gọi là hệ thống thưởng-phạt, trong đó tần suất yêu cầu trước được sử dụng để điều chỉnh linh hoạt phí bảo hiểm.Thứ hai là hành vi của các bên liên quan đến hợp đồng bảo hiểm. Các mô hình tổn thất bảo hiểm giải thích rõ ràng hoặc ngầm định các hành vi dẫn đến quyết định của người dân và doanh nghiệp có thể ảnh hưởng đến cả tần suất và mức độ nghiêm trọng của tổn thất. Những người liên quan đến hợp đồng có thể không chỉ bao gồm chủ hợp đồng mà còn có cả người bảo hiểm, chuyên gia sửa chữa (oto, nhà), nhà cung cấp dịch vụ y tế, v.v. Các biến giải thích hành vi có thể có những tác động khác nhau lên tần suất xảy ra một sự kiện. Ví dụ, trong bảo hiểm nhà, khi xem xét một người mua bảo hiểm rất cẩn thận và sống trong một khu nhà đắt đỏ. Chúng ta có thể xem xét các đặc điểm của chủ nhà như một dấu hiệu cho thấy việc áp dụng các biện pháp ngăn ngừa tổn thất như những yếu tố quyết định cho thấy tần suất thấp. Đồng thời, chúng ta có thể xem xét mức thu nhập chung của khu vực địa lý nơi có ngôi nhà làm đại diện cho mức chi phí sửa chữa trong trường hợp xảy ra tai nạn, cho thấy mức độ nghiêm trọng cao. Trong bảo hiểm ô tô, chúng ta có thể nghĩ mật độ dân số có mối tương quan dương với tần suất tai nạn và có mối tương quan nghịch với mức độ nghiêm trọng. Ví dụ, ở một khu đô thị đông dân cư, tình trạng tắc nghẽn giao thông cao, có nghĩa là người lái xe có thể gặp tai nạn thường xuyên nhưng chi phí tương đối thấp. Điều này trái ngược với một khu vực nông thôn dân cư thưa thớt hơn, nơi có đường rộng, ít tắc nghẽn và lái xe có thể chạy với tốc độ cao, có nghĩa là tai nạn ít xảy ra hơn, nhưng tốc độ cao hơn có nghĩa là mức độ nghiêm trọng hơn. Lịch sử yêu cầu bồi thường trước đây là một biến số khác cung cấp thông tin về mức độ chấp nhận rủi ro của chủ hợp đồng. Đặc biệt trong bảo hiểm cá nhân, người ta thường sử dụng dấu hiệu cho biết liệu yêu cầu bồi thường có xảy ra hay không, chẳng hạn như trong ba năm qua, thay vì số tiền yêu cầu bồi thường. Ở nhiều quốc gia, phí bảo hiểm ô tô được điều chỉnh bằng hệ thống được gọi là hệ thống thưởng-phạt, trong đó tần suất yêu cầu trước được sử dụng để điều chỉnh linh hoạt phí bảo hiểm.Thứ ba: là các cơ sở dữ liệu trong bảo hiểm. Đa số các công ty bảo hiểm lưu trữ dữ liệu theo hướng làm cơ sở để phát triển các mô hình tần suất và mức độ nghiêm trọng một cách riêng biệt. Ví dụ: các công ty bảo hiểm duy trì hồ sơ chủ hợp đồng được thiết lập khi soạn thảo hợp đồng. Hồ sơ này ghi lại thông tin phát hành và lịch sử hợp đồng, bao gồm có thông tin người được bảo hiểm, chẳng hạn như tuổi tác, giới tính và kinh nghiệm yêu cầu bồi thường trước đó; thông tin về phạm vi bảo hiểm, khoản khấu trừ và giới hạn; và thông tin về sự kiện yêu cầu bồi thường bảo hiểm. Một hồ sơ riêng biệt khác, thường được gọi là hồ sơ yêu cầu bồi thường, ghi lại chi tiết về yêu cầu bồi thường đối với công ty bảo hiểm. Cách lưu trữ dữ liệu như vậy giúp các công ty bảo hiểm mô hình hóa tần suất và mức độ nghiêm trọng thành các quy trình riêng biệt một cách hoàn toàn tự nhiên.Thứ ba: là các cơ sở dữ liệu trong bảo hiểm. Đa số các công ty bảo hiểm lưu trữ dữ liệu theo hướng làm cơ sở để phát triển các mô hình tần suất và mức độ nghiêm trọng một cách riêng biệt. Ví dụ: các công ty bảo hiểm duy trì hồ sơ chủ hợp đồng được thiết lập khi soạn thảo hợp đồng. Hồ sơ này ghi lại thông tin phát hành và lịch sử hợp đồng, bao gồm có thông tin người được bảo hiểm, chẳng hạn như tuổi tác, giới tính và kinh nghiệm yêu cầu bồi thường trước đó; thông tin về phạm vi bảo hiểm, khoản khấu trừ và giới hạn; và thông tin về sự kiện yêu cầu bồi thường bảo hiểm. Một hồ sơ riêng biệt khác, thường được gọi là hồ sơ yêu cầu bồi thường, ghi lại chi tiết về yêu cầu bồi thường đối với công ty bảo hiểm. Cách lưu trữ dữ liệu như vậy giúp các công ty bảo hiểm mô hình hóa tần suất và mức độ nghiêm trọng thành các quy trình riêng biệt một cách hoàn toàn tự nhiên.Thứ tư là quy định pháp luật. Bảo hiểm là một ngành dịch vụ được giám sát chặt chẽ. Các cơ quan quản lý thường xuyên yêu cầu công ty báo cáo cả số lượng và số tiền yêu cầu bồi thường. Các công ty bảo hiểm cần các hệ thống quản lý khác nhau để xử lý các tổn thất nhỏ, xảy ra thường xuyên như bảo hiểm sức khỏa, và các sự kiện hiếm xảy ra, có tác động lớn, chẳng hạn như bảo hiểm cháy nổ hay hàng hải. Mỗi yêu cầu bồi thường bảo hiểm có nghĩa là công ty phải chịu thêm chi phí, cho thấy tần suất yêu cầu bồi thường là yếu tố quan trọng quyết định chi phí của công ty.Thứ tư là quy định pháp luật. Bảo hiểm là một ngành dịch vụ được giám sát chặt chẽ. Các cơ quan quản lý thường xuyên yêu cầu công ty báo cáo cả số lượng và số tiền yêu cầu bồi thường. Các công ty bảo hiểm cần các hệ thống quản lý khác nhau để xử lý các tổn thất nhỏ, xảy ra thường xuyên như bảo hiểm sức khỏa, và các sự kiện hiếm xảy ra, có tác động lớn, chẳng hạn như bảo hiểm cháy nổ hay hàng hải. Mỗi yêu cầu bồi thường bảo hiểm có nghĩa là công ty phải chịu thêm chi phí, cho thấy tần suất yêu cầu bồi thường là yếu tố quan trọng quyết định chi phí của công ty.","code":""},{"path":"mô-hình-tần-suất---mức-độ-nghiêm-trọng..html","id":"các-nền-tảng-xây-dựng-mô-hình.","chapter":"Chương 13 Mô hình tần suất - mức độ nghiêm trọng.","heading":"13.2 Các nền tảng xây dựng mô hình.","text":"Hãy bắt đầu xây dựng mô hình bằng cách suy nghĩ về cơ sở dữ liệu của công ty bảo hiểm duy trì, bao gồm dữ liệu chủ hợp đồng và dữ liệu về yêu cầu bồi thường. Công ty bảo hiểm ký hợp đồng với người được bảo hiểm, quản lý dữ liệu hợp đồng và dữ liệu về yêu cầu bồi thường một cách liên tục theo thời gian. Đối với các mục đích, chẳng hạn như xác định số dự phòng tổn thất, sẽ rất hữu ích khi xem xét dữ liệu một cách liên tục theo thời gian. Phần này của cuốn sách chỉ tập trung vào xây dựng mô hình trong một khoảng thời gian cụ thể, chẳng hạn như 1 năm, và chỉ xem xét đến các yêu cầu bồi thường đã hoàn thành, nghĩa là đã được thanh toán đầy đủ hoặc không đủ điều kiện để bồi thường. Đây là cách xem xét dữ liệu gắn liền với các nghiệp vụ tính toán như xác định mức phí bảo hiểm (rate making) cho hợp đồng mới và tái bảo hiểm (reinsurance).Giả sử rằng có \\(n\\) hợp đồng trong danh mục nghiệp vụ bảo hiểm của công ty, các hợp đồng được ký hiệu là \\(1, 2, \\cdots, n\\) với\\(N_i\\) là số lượng yêu cầu bồi thường được chấp thuận của hợp đồng \\(\\) trong thời gian quan sát.\\(N_i\\) là số lượng yêu cầu bồi thường được chấp thuận của hợp đồng \\(\\) trong thời gian quan sát.Khi \\(N_i > 0\\), ký hiệu \\(y_{,j}\\), với \\(j = 1, 2, \\cdots, N_i\\) lần lượt là số tiền yêu cầu bồi thường của lần yêu cầu thứ \\(j\\).Khi \\(N_i > 0\\), ký hiệu \\(y_{,j}\\), với \\(j = 1, 2, \\cdots, N_i\\) lần lượt là số tiền yêu cầu bồi thường của lần yêu cầu thứ \\(j\\).Tổng số tiền bồi thường cho hợp đồng \\(\\) trong thời gian quan sát là\n\\[\\begin{align}\nS_i = \\begin{cases}\n0 & \\text{ nếu } N_i = 0 \\\\\n\\sum\\limits_{j=1}^{N_i} y_{,j} & \\text{ nếu } N_i > 0\n\\end{cases}\n\\end{align}\\]Tổng số tiền bồi thường cho hợp đồng \\(\\) trong thời gian quan sát là\n\\[\\begin{align}\nS_i = \\begin{cases}\n0 & \\text{ nếu } N_i = 0 \\\\\n\\sum\\limits_{j=1}^{N_i} y_{,j} & \\text{ nếu } N_i > 0\n\\end{cases}\n\\end{align}\\]Trong cơ sở dữ liệu của các công ty bảo hiểm, tổn thất có thể được lưu trữ dưới các dạng như sauChỉ có \\(S_i\\) được lưu lại, chẳng hạn như các hợp đồng bảo hiểm thương mại.\nChỉ có \\(S_i\\) được lưu lại, chẳng hạn như các hợp đồng bảo hiểm thương mại.Cả \\(N_i\\) và \\(S_i\\) đều được lưu lại.\nCả \\(N_i\\) và \\(S_i\\) đều được lưu lại.Tất cả các thông tin liên quan đến yêu cầu bồi thường đều được lưu lại, nghĩa là dữ liệu có đầy đủ thông tin về \\(y_{,j}\\), với \\(j = 1, 2, \\cdots, N_i\\) với mọi hợp đồng \\(\\).\nTất cả các thông tin liên quan đến yêu cầu bồi thường đều được lưu lại, nghĩa là dữ liệu có đầy đủ thông tin về \\(y_{,j}\\), với \\(j = 1, 2, \\cdots, N_i\\) với mọi hợp đồng \\(\\).Chúng ta có thể sử dụng quy tắc xác suất có điều kiện để xây dựng phân phối đồng thời cho các thành phần tần suất và mức độ nghiêm trọng. Để cụ thể, giả sử dữ liệu được quan sát một cách đầy đủ. Chúng ta có hàm phân phối đồng thời cho hai biến \\(N\\) - tần suất và \\(Y\\) mức độ nghiêm trọng như sau\n\\[\\begin{align}\nf(N,Y) = p(N) \\cdot f(Y|N)\n\\tag{13.1}\n\\end{align}\\]\ntrong đó \\(f(N,Y)\\) là hàm phân phối xác suất đồng thời của hai biến \\(N\\) và \\(Y\\), \\(p(N)\\) là phân phối xác suất của biến \\(N\\) và \\(f(Y|N)\\) là phân phối xác suất của mức độ nghiêm trọng \\(Y\\) với điều kiện \\(N\\).Khi dữ liệu thu thập được ở dạng thứ hai, chúng ta cũng có thể mô tả mối liên hệ giữa tần suất và mức độ nghiêm trọng một cách tương tự, thay thế véc-tơ tổn thất riêng \\(y\\) bằng tổng tổn thất \\(S\\). Đối với dữ liệu thu thập được ở dạng thứ nhất, chúng ta có thể phân tách dữ liệu bằng cách tách sự kiện \\(N = 0\\) thông qua ký hiệu chỉ báo \\(R_i = \\mathbb{}_{S_i > 0}\\) cho thành phần tần suất và chỉ sử dụng phần dữ liệu có \\(R_i = 1\\) cho thành phần mức độ nghiêm trọng.Thông qua việc phân tách \\(S_i\\) thành hai thành phần có phân phối như (13.1), chúng ta không yêu cầu giả thiết độc lập của các thành phần như truyền thống. Có nhiều cách để mô hình hóa sự phụ thuộc khi xét \\(f(N,Y)\\). Ví dụ: chúng ta có thể sử dụng một biến ngẫu nhiên ẩn ảnh hưởng đồng thời đến cả tần số \\(N\\) và lượng tổn thất \\(Y\\) để mô tả mối liên hệ cùng chiều, hoặc ngược chiều. Kỹ thuật copula là một công cụ hữu ích được sử dụng thường xuyên để mô hình hóa các mối liên hệ giữa \\(N\\) và \\(Y\\). Nhìn chung, xuất phát từ công thức xác suất có điều kiện là một tiếp cận hợp lý cho phép mô tả mối liên hệ giữa các thành phần và cung cấp nền tảng khởi đầu cho các tính toán thực nghiệm.Nền tảng thứ hai cho mô hình tần suất - mức độ nghiêm trọng là mô hình tuyến tính tổng quát. Khả năng tùy biến linh hoạt của GLM cho phép chúng ta xây dựng được mối liên hệ của tần suất và mức độ nghiêm trọng với các biến độc lập một cách đồng thời. Giả sử biến \\(Y_i\\) là biến mục tiêu, có thể là tần suất hoặc mức độ nghiêm trọng, chúng ta luôn có thể xây dựng mô hình tuyến tính tổng quát mà giá trị trung bình của biến phụ thuộc được giải thích thông qua các biến phụ thuộc. giá trị trung bình của tần suất và mức độ nghiêm trọng là số dương nên trong đa số các trường hợp chúng ta sử dụng hàm liên kết là hàm \\(log(\\cdot)\\). Ta có \\(\\mathbb{E}(Y_i) = exp(\\textbf{x}_i^{'} \\beta)\\).Trong một số trường hợp, giá trị trung bình của biến phụ thuộc được biết là thay đổi tỷ lệ thuận với một biến mà chúng ta gắn nhãn là \\(E_i\\), trong đó \\(E\\) là viết tắt của “Exposure” - một đại lượng mà chúng ta sẽ thảo luận ở phần dưới. Mối liên hệ giữa giá trị trung bình của biến phụ thuộc và exposure được mô tả như sau\\[\\begin{align}\n\\mathbb{E}(Y_i) = E_i \\cdot \\exp(\\textbf{x} \\beta)\n\\end{align}\\]Lấy \\(log\\) của hai vế, chúng ta có phương trình tuyến tính mà một trong các biến độc lập là \\(log(E_i)\\) với hệ số là 1.\n\\[\\begin{align}\n\\log(\\mu_i) = \\log(E_i) + \\textbf{x}_i^{'} \\beta\n\\end{align}\\]Khi xây dựng các mô hình trong bảo hiểm, các chuyên gia tính toán thường sử dụng khái niệm “exposure” để hiệu chỉnh quy mô của một rủi ro. Tùy theo loại hình bảo hiểm mà exposure được định nghĩa khác nhau. Ví dụ như trong bảo hiểm vật chất xe oto, exposure thường được sử dụng là 1 năm mà một xe ô tô được bảo hiểm. Trong bảo hiểm trách nhiệm lái xe ô tô, exposure lại có thể là tổng quãng đường lái xe tính theo km, chẳng hạn như mỗi 10,000 km. Khi xây dựng mô hình tần suất - mức độ nghiêm trọng, cần xác định được mối liên hệ giữa tần suất hoặc mức độ nghiêm trọng với exposure. Đối với bảo hiểm vật chất xe ô tô, tần suất có mối liên hệ trực tiếp đến exposure bởi vì thời gian bảo hiểm hoặc quãng đường lái xe càng dài thì tần suất cũng tăng theo tương ứng nhưng mức độ nghiêm trọng lại không có mối liên hệ. Trái lại, trong bảo hiểm thất nghiệp, exposure là tháng lương của người lao động nên sẽ được điều chỉnh theo lạm phát, nghĩa là exposure có tác động đến mức độ nghiêm trọng nhưng lại không có tác động lên tần suất.Để minh họa cách hiệu chỉnh mô hình khi sử dụng exposure, giả sử trong bảo hiểm xe ô tô, \\(E_i\\) là một phần của năm mà chủ hợp đồng được bảo hiểm. Logic đằng sau là số vụ tai nạn dự kiến tỷ lệ thuận với độ dài của thời gian bảo hiểm.Trong trường hợp tần suất xảy ra tai nạn có phân phối Poisson, xây dựng mô hình tuyến tính tổng quát là đơn giản vì nếu 1 đơn vị exposure có phân phối Poisson với tham số \\(\\mu_i\\) thì \\(E_i\\) đơn vị exposure sẽ có phân phối Poisson với tham số \\(\\lambda_i = E_i \\cdot \\mu_i\\). Mô hình tuyến tính tổng quát sẽ có dạng như sau\n\\[\\begin{align}\n& Y_i \\sim \\mathcal{P}(\\lambda_i) \\\\\n& \\log(\\lambda_i) = \\log(E_i) + \\textbf{x}_i^{'} \\beta\n\\tag{12.19}\n\\end{align}\\]Trong trường hợp tần suất xảy ra tai nạn có phân phối Poisson, xây dựng mô hình tuyến tính tổng quát là đơn giản vì nếu 1 đơn vị exposure có phân phối Poisson với tham số \\(\\mu_i\\) thì \\(E_i\\) đơn vị exposure sẽ có phân phối Poisson với tham số \\(\\lambda_i = E_i \\cdot \\mu_i\\). Mô hình tuyến tính tổng quát sẽ có dạng như sau\n\\[\\begin{align}\n& Y_i \\sim \\mathcal{P}(\\lambda_i) \\\\\n& \\log(\\lambda_i) = \\log(E_i) + \\textbf{x}_i^{'} \\beta\n\\tag{12.19}\n\\end{align}\\]Trong trường hợp biến phụ thuộc có giá trị nhị phân và hàm liên kết có dạng logit. Với một đơn vị exposure, xác suất xảy ra tai nạn là \\(p_i = \\cfrac{\\exp{\\left(\\textbf{x}_i^{'} \\beta\\right)}}{1 + \\exp{\\left(\\textbf{x}_i^{'} \\beta\\right)}}\\). Giả sử \\(Y_i\\) là biến phụ thuộc cho trường hợp có \\(E_i\\) đơn vị exposure. Có một số tiếp cận khác nhau để xây dựng mô hình tuyến tính tổng quát cho \\(Y_i\\) như sau\n\\[\\begin{align}\n\\text{Cách 1: } & P(Y_i = 1) = \\pi_i = E_i \\cdot p_i = E_i \\cfrac{\\exp{\\left(\\textbf{x}_i^{'} \\beta\\right)}}{1 + \\exp{\\left(\\textbf{x}_i^{'} \\beta\\right)}}\\\\\n\\text{Cách 2: } & P(Y_i = 1) = \\pi_i = 1 - (1 - p_i)^{E_i} = 1 - \\left(1 - \\cfrac{\\exp{\\left(\\textbf{x}_i^{'} \\beta\\right)}}{1 + \\exp{\\left(\\textbf{x}_i^{'} \\beta\\right)}} \\right)^{E_i}\\\\\n\\text{Cách 3: } & P(Y_i = 1) = \\pi_i = \\cfrac{E_i \\cdot p_i}{1 - (1 - E_i) \\cdot p_i} = \\cfrac{E_i \\cdot \\cfrac{\\exp{\\left(\\textbf{x}_i^{'} \\beta\\right)}}{1 + \\exp{\\left(\\textbf{x}_i^{'} \\beta\\right)}}} {1 - (1 - E_i) \\cdot \\cfrac{\\exp{\\left(\\textbf{x}_i^{'} \\beta\\right)}}{1 + \\exp{\\left(\\textbf{x}_i^{'} \\beta\\right)}}}\n\\end{align}\\]Trong trường hợp biến phụ thuộc có giá trị nhị phân và hàm liên kết có dạng logit. Với một đơn vị exposure, xác suất xảy ra tai nạn là \\(p_i = \\cfrac{\\exp{\\left(\\textbf{x}_i^{'} \\beta\\right)}}{1 + \\exp{\\left(\\textbf{x}_i^{'} \\beta\\right)}}\\). Giả sử \\(Y_i\\) là biến phụ thuộc cho trường hợp có \\(E_i\\) đơn vị exposure. Có một số tiếp cận khác nhau để xây dựng mô hình tuyến tính tổng quát cho \\(Y_i\\) như sau\n\\[\\begin{align}\n\\text{Cách 1: } & P(Y_i = 1) = \\pi_i = E_i \\cdot p_i = E_i \\cfrac{\\exp{\\left(\\textbf{x}_i^{'} \\beta\\right)}}{1 + \\exp{\\left(\\textbf{x}_i^{'} \\beta\\right)}}\\\\\n\\text{Cách 2: } & P(Y_i = 1) = \\pi_i = 1 - (1 - p_i)^{E_i} = 1 - \\left(1 - \\cfrac{\\exp{\\left(\\textbf{x}_i^{'} \\beta\\right)}}{1 + \\exp{\\left(\\textbf{x}_i^{'} \\beta\\right)}} \\right)^{E_i}\\\\\n\\text{Cách 3: } & P(Y_i = 1) = \\pi_i = \\cfrac{E_i \\cdot p_i}{1 - (1 - E_i) \\cdot p_i} = \\cfrac{E_i \\cdot \\cfrac{\\exp{\\left(\\textbf{x}_i^{'} \\beta\\right)}}{1 + \\exp{\\left(\\textbf{x}_i^{'} \\beta\\right)}}} {1 - (1 - E_i) \\cdot \\cfrac{\\exp{\\left(\\textbf{x}_i^{'} \\beta\\right)}}{1 + \\exp{\\left(\\textbf{x}_i^{'} \\beta\\right)}}}\n\\end{align}\\]Đối với phân bố Poisson, phương sai bằng giá trị trung bình, đó cả giá trị trung bình và phương sai đều tăng tỷ lệ với giá trị \\(E_i\\). đó, khi sử dụng phân phối Poisson với hàm liên kết \\(log(\\cdot)\\), chúng ta chỉ cần sử dụng \\(log(E_i)\\) để tự động tính đến phương sai ngày càng tăng. Tuy nhiên, điều này lại không đúng đối với các phân phối khác thuộc nhóm phân phối mũ.Ví dụ, hãy xem xét phân phối \\(gamma\\) với tham số \\(\\alpha\\) và \\(beta\\) khi viết dưới dạng phân phối kiểu mũ có tham số chính tắc \\(\\theta\\) và tham số phân tán \\(\\phi\\). Nếu có \\(E_i\\) biến ngẫu nhiên \\(gamma\\) độc lập trong cùng nhóm \\(\\), mỗi biến có trung bình \\(\\alpha/\\beta\\) và phương sai \\(\\alpha/\\beta^2\\), thì phân phối tổng của \\(E_i\\) expusure cũng sẽ là phân phối gamma với trung bình \\(E_i \\cdot \\alpha/\\beta\\) và phương sai \\(E_i \\cdot \\alpha/\\beta^2\\). Khi sử dụng phân phối \\(gamma\\) cho biến phụ thuộc và hàm liên kết \\(log(\\cdot)\\), giá trị trung bình của tổng sẽ tăng tỷ lệ thuận với \\(E_i\\), nhưng phương sai sẽ tăng tỷ lệ thuận với \\(E^2_i\\) chứ không phải \\(E_i\\).Giả sử dữ liệu quan sát được bao gồm \\(n\\) nhóm; với nhóm thứ \\(\\), \\(1 \\leq \\leq n\\), chúng ta có \\(E_i\\) quan sát độc lập với cùng phân phối thuộc họ các phân phối kiểu mũ hàm mũ. Phân phối của nhóm \\(\\) có tham số chính tắc \\(\\theta_i\\), trung bình \\(\\mu_i\\) và tham số phân tán \\(\\phi\\) (có thể phụ thuộc hoặc không phụ thuộc vào \\(\\)). Có hai hướng tiếp cận để xây dựng mô hình trong trường hợp này. Thứ nhất đó là tiến hành phân tích với tập dữ liệu dựa trên từng quan sát riêng lẻ \\(j = 1, 2, \\cdots, E_i\\) với mỗi \\(= 1, 2, \\cdots, n\\). Mặt khác, với những giả định này, thì trung bình của nhóm thứ \\(\\) cũng sẽ có phân phối kiểu mũ với cùng giá trị trung bình \\(\\mu_i\\), cùng tham số chính tắc \\(\\theta_i\\) nhưng có tham số phân tán \\(\\phi_i/E_i\\). đó hướng tiếp cận thứ hai là sử dụng mẫu dữ liệu theo nhóm bao gồm \\(n\\) quan sát và sử dụng nghịch đảo của \\(E_i\\) làm trọng số. Với quan sát riêng lẻ thứ \\(j\\) thuộc nhóm \\(\\), các biến phụ thuộc bao gồm \\((N_{,j}, Y_{i_j})\\). Nếu các biến giải thích có sẵn cho từng quan sát riêng lẻ thì việc tổng hợp thông tin cho nhóm \\(\\) sẽ gặp khó khăn vì chúng ta sẽ đi thông tin. Thay vào đó, nếu các biến giải thích chỉ có sẵn theo nhóm và chúng ta muốn xây dựng mô hình các biến tần suất và mức độ nghiêm trọng tổng: \\(N_i = \\sum\\limits_{=1}^{E_i} N_{,j}\\), và \\(S_i = \\sum\\limits_{=1}^{E_i} Y_{,j}\\) thìĐối với biến tần suất chúng ta sử dụng phân phối Poisson với biến phụ thuộc \\(N_i\\) được điều chỉnh bởi \\(log(E_i)\\), hoặc sử dụng biến ngẫu nhiên dạng đếm thuộc họ phân phối kiểu mũ cho biến phụ thuộc là \\(N_i/E_i\\) và tham số phân tán \\(\\phi/E_i\\).Đối với biến tần suất chúng ta sử dụng phân phối Poisson với biến phụ thuộc \\(N_i\\) được điều chỉnh bởi \\(log(E_i)\\), hoặc sử dụng biến ngẫu nhiên dạng đếm thuộc họ phân phối kiểu mũ cho biến phụ thuộc là \\(N_i/E_i\\) và tham số phân tán \\(\\phi/E_i\\).Đối với biến mức độ nghiêm trọng, chúng ta sử dụng biến phụ thuộc là \\(S_i/N_i\\) thuộc nhóm các phân phối kiểu mũ có tham số phân tán là \\(\\phi/N_i\\).Đối với biến mức độ nghiêm trọng, chúng ta sử dụng biến phụ thuộc là \\(S_i/N_i\\) thuộc nhóm các phân phối kiểu mũ có tham số phân tán là \\(\\phi/N_i\\).","code":""},{"path":"mô-hình-tần-suất---mức-độ-nghiêm-trọng..html","id":"mô-hình-tần-suất---mức-độ-nghiêm-trọng.-1","chapter":"Chương 13 Mô hình tần suất - mức độ nghiêm trọng.","heading":"13.3 Mô hình tần suất - mức độ nghiêm trọng.","text":"Như chúng ta đã nói ở trên, có ba kiểu dữ liệu được thu thập, đó là ) chỉ có tổng tổn thất theo hợp đồng \\(S_i\\); ii) có thông tin tần suất và tổng giá trị tổn thất \\((N_i, S_i)\\); và iii) có đầy đủ thông tin về \\(N_i\\) và các tổn thất riêng lẻ \\(y_{,j}\\) với \\(j = 1, 2, \\cdots, N_i\\). Tùy theo kiểu dữ liệu của công ty bảo hiểm mà mô hình tần suất mức độ nghiêm trọng được xây dựng theo các trình tự khác nhau.","code":""},{"path":"mô-hình-tần-suất---mức-độ-nghiêm-trọng..html","id":"mô-hình-hai-thành-phần","chapter":"Chương 13 Mô hình tần suất - mức độ nghiêm trọng.","heading":"13.3.1 Mô hình hai thành phần","text":"Mô hình hai thành phần thường được sử dụng để xây dựng mô hình trong trường hợp chúng ta chỉ có thông tin về tổng giá trị tổn thất của hợp đồng \\(S_i\\) với \\(= 1, 2, \\cdots, n\\). Đây là một trường hợp đặc biệt của mô hình tần suất - mức độ nghiêm trọng. Thành phần thứ nhất của mô hình là biến ngẫu nhiên \\(B_i\\) có phân phối nhị phân chỉ nhận hai giá trị là 0 hoặc 1, trong đó 0 tương ứng với hợp đồng không có yêu cầu bổi thường và 1 tương ứng với hợp đồng có yêu cầu bối thường. Thành phần thứ hai của mô hình là biến ngẫu nhiên \\(S_i\\) cho biết tổn thất đó có độ lớn như thế nào với điều kiện \\(B_i = 1\\).Quá trình ước lượng mô hình hai thành phần bao gồm hai mô hình tuyến tính tổng quát:Mô hình thứ nhất có biến phụ thuộc là \\(B_i\\) với véc-tơ các biến giải thích \\(\\textbf{x}_{,B}\\) và các hệ số tương ứng của các biến độc lập là \\(\\boldsymbol{\\beta}_B\\). Hàm liên kết được chọn thường là hàm logit.Mô hình thứ nhất có biến phụ thuộc là \\(B_i\\) với véc-tơ các biến giải thích \\(\\textbf{x}_{,B}\\) và các hệ số tương ứng của các biến độc lập là \\(\\boldsymbol{\\beta}_B\\). Hàm liên kết được chọn thường là hàm logit.Mô hình thứ hai có biến phụ thuộc là \\(S_i\\) với véc-tơ các biến giải thích \\(\\textbf{x}_{,S}\\) và các hệ số tương ứng của các biến độc lập là \\(\\boldsymbol{\\beta}_S\\). Dữ liệu để ước lượng mô hình là dữ liệu chỉ tính đến các hợp đồng có \\(S_i > 0\\). Lựa chọn phổ biến cho phân phối của biến độc lập là phân phối gamma và hàm liên kết là hàm log.Mô hình thứ hai có biến phụ thuộc là \\(S_i\\) với véc-tơ các biến giải thích \\(\\textbf{x}_{,S}\\) và các hệ số tương ứng của các biến độc lập là \\(\\boldsymbol{\\beta}_S\\). Dữ liệu để ước lượng mô hình là dữ liệu chỉ tính đến các hợp đồng có \\(S_i > 0\\). Lựa chọn phổ biến cho phân phối của biến độc lập là phân phối gamma và hàm liên kết là hàm log.Thông thường sẽ luôn luôn có các biến giải thích vừa nằm trong véc-tơ \\(\\textbf{x}_{,B}\\) và véc-tơ \\(\\textbf{x}_{,S}\\). Một giả thiết khác trong mô hình hai thành phần nói riêng và mô hình tổn thất - mức độ nghiêm trọng nói riêng là các véc-tơ hệ số \\(\\boldsymbol{\\beta}_B\\) và \\(\\boldsymbol{\\beta}_S\\) độc lập với nhau. Giả thiết này đảm bảo việc ước lượng các hệ số của mỗi thành phần hoàn toàn riêng biệt.Trong trường hợp dữ liệu xây dựng mô hình ở dạng thứ hai, nghĩa là chúng ta có quan sát đồng thời cho tần suất và tổng tổn thất của mỗi hợp đồng \\((N_i, S_i)\\), hai bước ước lượng mô hình tần suất - mức độ nghiêm trọng cụ thể như sauSử dụng một biến ngẫu nhiên dạng đếm để mô tả tần suất \\(N_i\\) với tập hợp các biến độc lập là \\(\\textbf{x}_{,N}\\). Hệ số của các biến độc lập được ước lượng là \\(\\boldsymbol{\\beta}_N\\). Phân phối thường được lựa chọn là phân phối Poisson hoặc phân phối nhị thức âm.Sử dụng một biến ngẫu nhiên dạng đếm để mô tả tần suất \\(N_i\\) với tập hợp các biến độc lập là \\(\\textbf{x}_{,N}\\). Hệ số của các biến độc lập được ước lượng là \\(\\boldsymbol{\\beta}_N\\). Phân phối thường được lựa chọn là phân phối Poisson hoặc phân phối nhị thức âm.Với điều kiện \\(N_i > 0\\), sử dụng mô hình tuyến tính tổng quát với \\(S_i/N_i\\) là biến phụ thuộc và \\(\\textbf{x}_{,S}\\) là các biến độc lập. Hệ số của các biến độc lập được ước lượng là \\(\\boldsymbol{\\beta}_S\\). Phân phối của biến phụ thuộc thường được lựa chọn là phân phối \\(gamma\\) sao cho tham số phân tán của quan sát thứ \\(\\) là \\(\\phi/N_i\\).Với điều kiện \\(N_i > 0\\), sử dụng mô hình tuyến tính tổng quát với \\(S_i/N_i\\) là biến phụ thuộc và \\(\\textbf{x}_{,S}\\) là các biến độc lập. Hệ số của các biến độc lập được ước lượng là \\(\\boldsymbol{\\beta}_S\\). Phân phối của biến phụ thuộc thường được lựa chọn là phân phối \\(gamma\\) sao cho tham số phân tán của quan sát thứ \\(\\) là \\(\\phi/N_i\\).Nếu dữ liệu xây dựng mô hình ở dạng thứ ba, nghĩa là chúng ta có thông tin chính xác về từng khoảng tổn thất riêng biệt, bước xây dựng mô hình cho tần suất hoàn toàn tương tự như khi dữ liệu ở dạng thứ hai. Trong bước xây dựng mô hình cho mức độ nghiêm trọng, chúng ta sử dụng \\(y_{,j}\\) là các biến phụ thuộc thay vì sử dụng giá trị trung bình của nhóm \\(\\). Phân phối được lựa chọn cho biến \\(y_{,j}\\) thường là phân phối gamma, hoặc phân phối chuẩn với hàm liên kết là hàm log.","code":""},{"path":"mô-hình-tần-suất---mức-độ-nghiêm-trọng..html","id":"phân-phối-tweedie","chapter":"Chương 13 Mô hình tần suất - mức độ nghiêm trọng.","heading":"13.3.2 Phân phối Tweedie","text":"Họ các phân phối kiểu mũ bao gồm các phân phối liên tục, chẳng hạn như phân phối chuẩn và phân phối gamma, cũng như các phân bố rời rạc, chẳng hạn như phân phối nhị thức và Poisson. Ngoài ra, các phân phối kiểu mũ cũng bao gồm cả các phân phối hỗn hợp, nghĩa là các phân phối vừa có thành phần rời rạc và vừa có thành phần liên tục. Trong mô hình hóa dữ liệu trong bảo hiểm, một phân phối hỗn hợp được sử dụng rộng rãi là phân phối Tweedie (1984). Phân phối này có mật độ xác suất dương tại điểm 0 (rời rạc) biểu thị rằng không có yêu cầu bồi thường nào, và phân phối này có thành phần liên tục tại mọi giá trị dương, biểu thị tổng số tiền cho một hoặc nhiều yêu cầu bồi thường.Phân phối Tweedie được định nghĩa là tổng Poisson của các biến ngẫu nhiên Gamma. Cụ thể, giả sử rằng \\(N\\) có phân phối Poisson với giá trị trung bình \\(\\lambda\\), mô tả tần suất yêu cầu bồi thường. Giả sử \\(Y_j\\), với \\(j = 1, 2, \\cdots, N_i\\), là các biến ngẫu nhiên độc lập có cùng phân phối và độc lập với \\(N\\). Hơn nữa, mỗi \\(Y_j\\) có phân phối gamma với các tham số \\(\\alpha\\) và \\(\\gamma\\), mô tả số tiền yêu cầu bồi thường thứ \\(j\\). Khi đó\\[\\begin{align}\nS_N = \\begin{cases}\n0 & \\text{ nếu } N = 0 \\\\\n\\sum\\limits_{j=1}^{N} y_{j} & \\text{ nếu } N > 0\n\\end{cases}\n\\end{align}\\]\nlà tổng Poisson của các phân phối gamma. Chúng ta nói rằng \\(S_N\\) có phân phối Tweedie với các tham số \\((\\lambda, \\alpha, \\gamma)\\).Hàm phân phối của biến ngẫu nhiên phân phối Tweedie được xác định như sau:\n\\[\\begin{align}\n\\mathbb{P}(S_N = 0) = \\mathbb{P}(N = 0) = exp(-\\lambda)\n\\end{align}\\]\nvới \\(s \\geq 0\\) thì\n\\[\\begin{align}\n\\mathbb{P}(S_N \\leq y) = exp(-\\lambda) + \\sum\\limits_{k = 1}^{\\infty} \\mathbb{P}(N = k) \\cdot \\mathbb{P}(S_k \\leq y)\n\\end{align}\\]\\(S_k\\) là tổng của \\(k\\) biến ngẫu nhiên phân phối gamma độc lập với tham số \\(\\alpha\\), \\(\\gamma\\) đó \\(S_k\\) cũng có phân phối gamma với tham số \\((k \\cdot \\alpha)\\), \\(\\gamma\\). Ta có hàm mật độ xác suất của biến \\(S_k\\)\n\\[\\begin{align}\nf_{S_k}(y) = \\cfrac{\\gamma^{k \\cdot \\alpha}}{\\Gamma(k \\cdot \\alpha)} \\cdot y^{(k \\cdot \\alpha - 1)} \\cdot e^{-\\gamma \\cdot y}\n\\end{align}\\]Vậy hàm mật độ của phân phối Tweedie tại \\(y > 0\\) được viết như sau\n\\[\\begin{align}\nf_{S_N}(y) = \\sum\\limits_{k = 1}^{\\infty} e^{-\\lambda} \\cdot \\cfrac{\\lambda^k}{k!} \\cdot \\cfrac{\\gamma^{k \\cdot \\alpha}}{\\Gamma(k \\cdot \\alpha)} \\cdot y^{(k \\cdot \\alpha - 1)} \\cdot e^{-\\gamma \\cdot y}\n\\end{align}\\]Phân phối Tweedie nằm trong họ các phân phối kiểu mũ. Tuy nhiên sẽ không dễ dàng để viết phân phối ở trên dưới dạng phân phối kiểu mũ nếu chúng ta không định nghĩa lại các tham số. Trước hết, ta có giá trị trung bình và phương sai của \\(S_N\\) được tính toán thông qua các tham số của phân phối Poisson và phân phối gamma như sau\n\\[\\begin{align}\n& \\mathbb{E}(S_N) = \\mathbb{E}(N) \\cdot \\mathbb{E}(S) = \\lambda \\cdot \\cfrac{\\alpha}{\\gamma} \\\\\n& \\mathbb{V}(S_N) = \\cfrac{\\lambda \\alpha}{\\gamma^2} \\cdot (1 + \\alpha)\n\\end{align}\\]Với các tham số \\(\\mu\\), \\(\\phi\\), \\(p\\) được định nghĩa từ các tham số ban đầu\n\\[\\begin{align}\n& \\mu = \\lambda \\cdot \\cfrac{\\alpha}{\\gamma} \\\\\n& p = \\cfrac{\\alpha + 2}{\\alpha + 1} \\\\\n& \\phi = \\cfrac{1}{\\mu^p} \\cdot \\cfrac{\\lambda \\alpha}{\\gamma^2} \\cdot (1 + \\alpha)\n\\end{align}\\]\nchúng ta có thể viết hàm phân phối của biến ngẫu nhiên phân phối Tweedie dưới dạng phân phối kiểu mũ như sau\\[\\begin{align}\nf_{S_N}(y) & = \\exp\\left( \\cfrac{1}{\\phi(1-p)} \\cdot \\left( y \\cdot \\mu^{1-p} - \\cfrac{1-p}{2-p} \\mu^{2-p} \\right) + H(y,p,\\phi) \\right)\n\\end{align}\\]\nvới hàm \\(H(y,p,\\phi)\\) chỉ phụ thuộc vào các tham số \\(\\phi\\) và \\(p\\)\n\\[\\begin{align}\nH(y,p,\\phi) = \\log\\left( \\cfrac{1}{y} \\sum\\limits_{k=1}^{\\infty} \\cfrac{1}{k! \\Gamma(k \\alpha)} \\cdot \\left( \\cfrac{y^\\alpha}{\\phi^{1/(1-p)}(2-p)(1-p)^\\alpha} \\right)^n  \\right)\n\\end{align}\\]Với tham số chính chắc \\(\\theta = \\mu^{1-p}\\), ta có \\(\\cfrac{1-p}{2-p} \\mu^{2-p} = \\cfrac{1-p}{2-p} \\theta^{(2-p)/(1-p)}\\) hay nói cách khác \\(b(\\theta) = - \\cfrac{\\theta^{-\\alpha}}{\\alpha}\\). Giá trị trung bình và phương sai của \\(S_N\\) tính toán theo hàm mật độ xác suất của phân phối kiểu mũ là\n\\[\\begin{align}\n\\mathbb{E}(S_N) = b^{'}(\\theta) = \\partial \\cfrac{-\\theta^{-\\alpha}}{\\alpha}/ \\partial \\theta = \\theta^{-(\\alpha+1)} = \\theta^{1/(1-p)} = \\mu \\\\\n\\mathbb{V}(S_N) = (\\phi) b^{''}(\\theta) = \\phi (1-p) \\cdot \\cfrac{1}{1-p} \\theta^{p/(1-p)} = \\phi \\cdot \\mu^p\n\\end{align}\\]Khi sử dụng phân phối Tweedie cho biến phụ thuộc trong mô hình tuyến tính tổng quát, chúng ta ước lượng một mô hình GLM thay vì ước lượng hai thành phần tần suất và mức độ nghiêm trọng. Giả sử các biến \\(\\textbf{x}_{,T}\\) là các biến độc lập trong mô hình Tweedie với hàm liên kết là hàm log và hệ số \\(\\boldsymbol{\\beta_T}\\) là các hệ số tương ứng của các biến độc lập, ta có mô hình tuyến tính tổng quát như sau\n\\[\\begin{align}\nY_i \\sim Tweedie(\\mu_i , p , \\phi) \\\\\n\\mu_i = \\exp(\\textbf{x}_{,T}^{'} \\cdot \\boldsymbol{\\beta_T})\n\\end{align}\\]Chúng ta sẽ sánh giữa giữa sử dụng mô hình tần suất - mức độ nghiêm trọng với sử dụng phân phối Tweedie trên dữ liệu “exposure.csv”. Chỉ tiêu để sánh hai mô hình là giá trị trung bình của tổng tổn thất theo mỗi hợp đồng trên dữ liệu ngoài mẫu. Giá trị trung bình của tổng tổn thất còn được gọi bằng thuật ngữ là phí bảo hiểm thuần. Dữ liệu sử dụng để xây dựng mô hình được lấy ngẫu nhiên 70% từ dữ liệu ban đầu và chúng ta sẽ sánh giá trị của phí bảo hiểm thuần được tính toán từ mô hình tần suất mức độ nghiêm trọng với mô hình Tweedie.Để sử dụng phân phối Tweedie trong hàm glm(), chúng ta cần cài đặt thêm thư viện \\(statmod\\).Hệ số của các biến độc lập của các mô hình ở trong bảng dưới đây\nBảng 2.2: Hệ số tuyến tính của biến độc lập\nCó thể thấy rằng hệ số của các biến độc lập trong mô hình Tweedie khá gần với tổng của hệ số của biến độc lập trong mô hình tần suất và hệ số của biến độc lập trong mô hình mức độ nghiêm trọng. Điều này có thể được giải thích bởi hàm liên kết được sử dụng trong cả ba mô hình đều là hàm \\(log(\\cdot)\\). Giá trị trung bình của tổng tổn thất được tính toán từ các mô hình như sau:\n\\[\\begin{align}\n\\mathbb{E}(S_N|\\text{x}_i) &= \\mathbb{E}(N|\\text{x}_i) \\cdot \\mathbb{E}(Y|\\text{x}_i) \\\\\n&= exp(\\beta^F_{0,} +  \\beta^F_{1,} \\cdot Age_i + \\beta^F_{2,} \\cdot Gender_i) \\cdot\n\n\\end{align}\\]","code":"\nlibrary(statmod)\ndat<-read.csv(\"../KHDL_KTKD Final/Dataset/exposure.csv\")\nn<-nrow(dat); p<-ncol(dat)\ndat$Gender<-as.factor(dat$Gender)\n\nset.seed(10)\ntrain.index<-sample(1:n,size = round(n*0.7),replace=FALSE)\ntrain<-dat[train.index,]\ntest<-dat[-train.index,]\n\nglm.ser<-glm(Ave_Claim~Age+Gender,family = Gamma(link = \"log\"),data=train, weight = Claim_Count)\nglm.fre<-glm(Claim_Count~Age+Gender,family = poisson(link = \"log\"), data = train)\nglm.twe<-glm(Total_Claim~Age+Gender,family=tweedie(var.power=1.5,link.power=0),data=train)\n\nglm.fre## \n## Call:  glm(formula = Claim_Count ~ Age + Gender, family = poisson(link = \"log\"), \n##     data = train)\n## \n## Coefficients:\n## (Intercept)          Age      Gender1  \n##    -0.54133     -0.02187      0.11533  \n## \n## Degrees of Freedom: 6999 Total (i.e. Null);  6997 Residual\n## Null Deviance:       5952 \n## Residual Deviance: 5785  AIC: 8934\nglm.ser## \n## Call:  glm(formula = Ave_Claim ~ Age + Gender, family = Gamma(link = \"log\"), \n##     data = train, weights = Claim_Count)\n## \n## Coefficients:\n## (Intercept)          Age      Gender1  \n##    -0.08272      0.07992     -0.51748  \n## \n## Degrees of Freedom: 1469 Total (i.e. Null);  1467 Residual\n##   (5530 observations deleted due to missingness)\n## Null Deviance:       3833 \n## Residual Deviance: 1713  AIC: 13250\nglm.twe## \n## Call:  glm(formula = Total_Claim ~ Age + Gender, family = tweedie(var.power = 1.5, \n##     link.power = 0), data = train)\n## \n## Coefficients:\n## (Intercept)          Age      Gender1  \n##    -0.50403      0.05485     -0.37003  \n## \n## Degrees of Freedom: 6999 Total (i.e. Null);  6997 Residual\n## Null Deviance:       91600 \n## Residual Deviance: 82290     AIC: NA\ndf<-data.frame(a = c(\"Tần suất\", \"Mức độ nghiêm trọng\", \"Tweedie\"),\n               b = c(\"glm.fre\", \"glm.ser\", \"glm.twe\"),\n               d = c(-0.541,-0.082,-0.504),\n               e = c(-0.021,0.079,0.055),\n               f = c(0.115,-0.517,-0.370))\nknitr::kable(df, booktabs = T,\n      caption = \"Hệ số tuyến tính của biến độc lập\",\n      col.names = c(\"Mô hình\", \"Tên mô hình trên R\",\"Hệ số chặn\", \"Hệ số của Age\", \"Hệ số của Gender\"),\n      escape=F, align = 'l') %>%\n  #column_spec(c(1,4,5,6,7),border_left = T) %>% column_spec(7,border_right = T) %>% \n  kable_styling(latex_options = \"scale_down\",full_width = F)## \n## Attaching package: 'dplyr'## The following objects are masked from 'package:stats':\n## \n##     filter, lag## The following objects are masked from 'package:base':\n## \n##     intersect, setdiff, setequal, union## \n## Attaching package: 'kableExtra'## The following object is masked from 'package:dplyr':\n## \n##     group_rows## \n## Attaching package: 'gridExtra'## The following object is masked from 'package:dplyr':\n## \n##     combine## \n## Attaching package: 'pryr'## The following object is masked from 'package:dplyr':\n## \n##     where## Classes and Methods for R developed in the\n## Political Science Computational Laboratory\n## Department of Political Science\n## Stanford University\n## Simon Jackman\n## hurdle and zeroinfl functions by Achim Zeileis"},{"path":"tính-toán-phí-bảo-hiểm-thuần-bằng-mô-hình-tần-suất---mức-độ-nghiêm-trọng..html","id":"tính-toán-phí-bảo-hiểm-thuần-bằng-mô-hình-tần-suất---mức-độ-nghiêm-trọng.","chapter":"Chương 14 Tính toán phí bảo hiểm thuần bằng mô hình tần suất - mức độ nghiêm trọng.","heading":"Chương 14 Tính toán phí bảo hiểm thuần bằng mô hình tần suất - mức độ nghiêm trọng.","text":"Tính toán phí của các sản phẩm bảo hiểm là một công việc phức tạp đòi hỏi phải kết hợp nhiều yếu tố khác nhau. Trước hết dữ liệu lịch sử phải được phân tích chính xác, các xu hướng kinh tế xã hội cũng phải được xác định và thậm chí hoạt động của các công ty cạnh tranh cũng phải được tính đến. Các chuyên gia tính toán (hay các actuary) được đào tạo bài bản có kiến thức trong tất cả các lĩnh vực này và có khả năng đề xuất những nội dung cần thiết để phát triển và thực hiện thành công chiến lược định giá sản phẩm bảo hiểm. Trong phần này, chúng tôi sẽ minh họa quá trình tính toán ra một trong những nền tảng cơ bản nhất của một quá trình định giá, đó là xác định phí bảo hiểm thuần (Net premium). Chúng tôi sẽ xác định các khoản phí bảo hiểm thuần này dựa trên các mô hình tuyến tính tổng quát về tần suất và mức độ nghiêm trọng. Quá trình xây dựng mô hình sẽ được minh họa thông qua tất cả các giai đoạn: xác định đặc điểm dữ liệu, khai phá dữ liệu, phân tích đơn biến và đa biến, kết hợp tần suất và mức độ nghiêm trọng thành phí bảo hiểm thuần, và cuối cùng là xác thực mô hình. Các kỹ thuật mà chúng tôi minh họa có thể áp dụng rộng rãi và chúng tôi khuyến khích bạn đọc thực hành thông qua các đoạn câu lệnh được viết bằng phần mềm R.Các chuyên gia tính toán phân tích dữ liệu lịch sử và giải thích các xu hướng kinh tế xã hội để xác định mức giá hợp lý của sản phẩm bảo hiểm. Những tính toán của Actuary là nền tảng cho mức giá cuối cùng mà công ty đưa ra cho khách hàng của mình. Đưa ra mức giá cuối cùng phải kết hợp nhiều yếu tố, chẳng hạn như hành động của đối thủ cạnh tranh, chiến lược tăng trưởng và sự hài lòng của người tiêu dùng, … đó, các chuyên gia tính toán, các nhà bảo lãnh phát hành, phòng tiếp thị phân phối sản phẩm, bộ phận tiếp nhận yêu cầu bồi thường, và ban quản lý công ty phải cùng nhau hợp tác trong việc định giá sản phẩm. Các đối tượng đa dạng này đều phải hiểu rõ các hướng tiếp cận về định giá sản phẩm để có thể đưa ra quyết định chung.Trong các phần này của cuốn sách, chúng ta sẽ tập trung vào các giai đoạn đầu tiên của quá trình hình thành giá của sản phẩm bào hiểm đó là xây dựng, cải tiến, và mô hình. Kết quả của các quá trình này là tính toán ra phí bảo hiểm thuần dựa trên dữ liệu lịch sử và mô hình tuyến tính tổng quát. Sẽ có hai mô hình được xây dựng, một mô hình dành cho tần suất xảy ra sự kiện bảo hiểm và mô hình còn lại sẽ có biến mục tiêu là mức độ nghiêm trọng. Vì phí bảo hiểm thuần chỉ phản ánh dữ liệu lịch sử nên sẽ không phù hợp để sử dụng làm dự báo giá cho tương lai không tính toán đến các xu hướng kinh tế xã hội trong khoảng thời gian tiếp theo. Phí bảo hiểm thuần cũng không tính toán đến các chi phí, chi phí vốn rủi ro, hay lợi nhuận biên cần thiết để giúp công ty tồn tại trên thị trường.Chương sách này được cấu trúc như sau: trước hết, chúng tôi mô tả các đặc điểm chính của tập dữ liệu mà chúng ta sử dụng làm cơ sở xây dựng mô hình. Điều này giúp bạn đọc có hiểu biết tổng quan về dữ liệu lịch sử. Tiếp theo, chúng tôi thực hiện các phân tích khai phá các biến trong tập dữ liệu. Đây là tập dữ liệu về bào hiểm đối với các xe ô tô chở khách tư nhân có nhiều đặc điểm điển hình mà bạn sẽ gặp phải mỗi khi làm việc với dữ liệu bảo hiểm. Những hiểu biết về dữ liệu bạn thu được khi khai phá dữ liệu về các biến riêng và mối quan hệ giữa các biến sẽ phục vụ tốt cho bạn trong giai đoạn xây dựng mô hình. Sau phân tích khai phá, chúng ta sẽ bắt đầu xây dựng các mô hình: mô hình cho tần suất và và mô hình cho mức độ nghiêm trọng tương ứng. Các mô hình sẽ được xây dựng từ đơn giản đến phức tạp, bắt đầu với các mô hình một biến và chuyển sang phân tích mô hình đa biến. Những mô hình chúng tôi xây dựng không nhất thiết phải là những mô hình tốt nhất và chúng tôi khuyến khích bạn đọc tự khám phá và cố gắng tạo ra những mô hình tốt hơn. Sau cùng, chúng ta sẽ kết hợp các mô hình tần suất và mức độ nghiêm trọng để tạo ra phí bảo hiểm thuần túy và xác thực mô hình trên tập dữ liệu ngoài mẫu.","code":""},{"path":"tính-toán-phí-bảo-hiểm-thuần-bằng-mô-hình-tần-suất---mức-độ-nghiêm-trọng..html","id":"dữ-liệu-để-xây-dựng-mô-hình","chapter":"Chương 14 Tính toán phí bảo hiểm thuần bằng mô hình tần suất - mức độ nghiêm trọng.","heading":"14.1 Dữ liệu để xây dựng mô hình","text":"Tập dữ liệu “pricing-modeling-dataset.csv” được sử dụng làm dữ liệu để chúng ta phân tích và xây dựng các mô hình tuyến tính tổng quát. Bộ dữ liệu bao gồm thông tin về các hợp đồng bảo hiểm ô tô chở khách thương mại và chứa cả thông tin về yêu cầu bồi thường. Dữ liệu được quan sát trong các năm từ 2010 đến 2013. Có tổng cộng 40,760 hàng và 24 biến.Cần lưu ý là một hàng dữ liệu có thể thể hiện nhiều yêu cầu bồi thường từ một đối tượng được bảo hiểm. Biến \\(clm.count\\) đo lường số lượng yêu cầu bồi thường và biến \\(clm.incurred\\) là tổng số tiền thanh toán yêu cầu bồi thường và bao gồm cả các khoản cần phải thanh toán trong tương lai. Các biến độc lập của dữ liệu có thể được nhóm thành bốn loại: biến điều khiển, biến đặc điểm người lái, biến đặc điểm địa lý, và biến đặc điểm phương tiện. Bảng dưới đây cho biết tên các biến trong tập dữ liệu\nBảng 2.2: Các biến trong dữ liệu lịch sử\nBiến \\(year\\) cho biết năm (năm dương lịch) mà xe ô tô chở khách được bảo hiểm và biến \\(exposure\\) đo lường khoảng thời gian (đo bằng năm) mà xe ô tô được bảo vệ trong năm dương lịch. Chúng ta chỉ có một biến địa lý là \\(region\\) cho chúng ta biết địa điểm (gara đăng ký) của xe. Điều đáng tiếc là biến \\(region\\) đã được mã hóa thành số và chúng ta không có bất kỳ thông tin nào về mối liên hệ về không gian của các vùng này. Đây là một nhược điểm đáng kể đối với tập dữ liệu mà chúng ta có. Xin được nhắc lại rằng việc chuẩn bị dữ liệu tốt trước khi xây dựng mô hình là rất quan trọng nên loại bỏ biến chỉ thực hiện khi chúng ta chắc chắn rằng biến không có chứa thông tin hữu ích.Các biến đặc tính của người lái xe bao gồm độ tuổi (\\(driver.age\\)), giới tính (\\(driver.gender\\)) của người điều khiển phương tiện, tình trạng hôn nhân (\\(marital.status\\)), số năm mà người điều khiển phương tiện đã được cấp phép (\\(yrs.licensed\\)), mức chiết khấu khi không yêu cầu bồi thường \\(ncd.level\\) (giá trị cao hơn phản ánh mức chiết khấu lớn hơn khi không có bất kỳ yêu cầu nào) và số lượng yêu cầu gửi trước đó \\(prior.claims\\). Biến \\(nb.rb\\) cho chúng ta biết hợp đồng này là hoạt động kinh doanh mới (nb) hay hoạt động kinh doanh tái tục (rb).Các biến đặc tính của xe đo lường các thuộc tính khác nhau như kiểu dáng thân xe (\\(body.code\\)); tuổi và giá trị của xe (\\(vehicle.age\\) và \\(vehicle.value\\)); số lượng chỗ ngồi (\\(seats\\)); và trọng lượng (\\(weight\\)), chiều dài (\\(length\\)), chiều rộng (\\(width\\)) và chiều cao (\\(height\\)) của xe. Các biến \\(ccm\\), \\(hp\\) và \\(fuel.type\\) lần lượt đo kích thước của động cơ theo xăng-ti-mét khối, mã lực và loại nhiên liệu (xăng, dầu diesel hoặc khí hóa lỏng).Như đã giới thiệu ở trên, hai biến mục tiêu là \\(clm.count\\) và \\(clm.incurred\\), cho biết số lần gửi yêu cầu bồi thường và tổng chi phí cuối cùng của những yêu cầu đó. Tất cả các biến trong tập dữ liệu đều có thể được phân loại là liên tục hoặc rời rạc. Các bảng dưới đây trình bày một số thông tin và chỉ số thống kê cho 14 biến liên tục và 12 biến rời rạc:\nBảng 14.1: Các biến kiểu số\n\nBảng 14.2: Các biến kiểu rời rạc\n\nBảng 14.3: Tần suất và mức độ nghiêm trọng trung bình theo năm\nLưu ý rằng khối lượng kinh doanh đã tăng khoảng 78% từ năm 2010 đến năm 2012 và sau đó giảm 17% vào năm 2013. Tần suất trong hai năm đầu tiên là khoảng 11% và sau đó tăng đáng kể lên khoảng 20%. Mức độ nghiêm trọng trung bình trong tất cả các năm dương lịch là 790, nhưng có mức tăng mạnh theo thời gian, ngoại trừ năm 2011.Thông thường quá trình xây dựng và xác thực mô hình sẽ cần chia dữ liệu của thành ba phần: tập dữ liệu huấn luyện (training dataset), tập dữ liệu kiểm tra mô hình (test dataset) và tập dữ liệu xác thực mô hình (validation dataset). Lưu ý rằng việc phân chia dữ liệu thành ba phần chỉ khả thi khi có một lượng lớn dữ liệu. Trong trường hợp của dữ liệu ở trên, chúng ta chỉ có khoảng 41.000 quan sát trong bốn năm dương lịch. Đây là một tập dữ liệu không đủ lớn, vì vậy chúng tôi sẽ sử dụng phương pháp tập xác thực. Thay vì chia dữ liệu thành ba tập, chúng ta sẽ chỉ chia dữ liệu thành hai tập: tập huấn luyện và tập xác thực. Tập dữ liệu huấn luyện để phát triển và thử nghiệm các mô hình chứa 70% dữ liệu. 30% dữ liệu được sử dụng để kiểm tra mô hình. Trong xác thực chéo, chúng tôi chia dữ liệu huấn luyện của mình thành năm tập hợp con (5-folds) và gắn nhãn cho các tập này từ 1 đến 5. Tham số cuối cùng của mô hình được sử dụng là giá trị trung bình của các tham số được ước lượng ra từ mỗi lần xác thực chéo.","code":"\ndat<-read_csv(\"../KHDL_KTKD Final/Dataset/pricing-modeling-dataset.csv\")\nhead(dat)## # A tibble: 6 × 24\n##   pol.id   year exposure nb.rb driver.age driver.gender marital.status\n##   <chr>   <dbl>    <dbl> <chr>      <dbl> <chr>         <chr>         \n## 1 POL0001  2010   1      RB            63 Male          Married       \n## 2 POL0002  2010   1      NB            33 Male          Married       \n## 3 POL0003  2010   1      RB            68 Male          Married       \n## 4 POL0004  2010   0.0833 RB            68 Male          Married       \n## 5 POL0005  2010   1      RB            68 Male          Married       \n## 6 POL0006  2010   0.0833 RB            68 Male          Married       \n## # ℹ 17 more variables: yrs.licensed <dbl>, ncd.level <dbl>, region <dbl>,\n## #   body.code <chr>, vehicle.age <dbl>, vehicle.value <dbl>, seats <dbl>,\n## #   ccm <dbl>, hp <dbl>, weight <dbl>, length <dbl>, width <dbl>, height <dbl>,\n## #   fuel.type <chr>, prior.claims <dbl>, clm.count <dbl>, clm.incurred <dbl>"},{"path":"tính-toán-phí-bảo-hiểm-thuần-bằng-mô-hình-tần-suất---mức-độ-nghiêm-trọng..html","id":"phân-tích-khai-phá-dữ-liệu","chapter":"Chương 14 Tính toán phí bảo hiểm thuần bằng mô hình tần suất - mức độ nghiêm trọng.","heading":"14.2 Phân tích khai phá dữ liệu","text":"Trong phần này, chúng ta bắt đầu bằng cách khai phá các biến riêng lẻ để hiểu rõ hơn về thông tin có trong tập dữ liệu của mình. Trong quá trình phân tích dữ liệu, chúng ta sẽ tập trung vào việc tìm hiểu mức độ phổ biến của từng biến, loại giá trị mà mỗi biến nhận, cách mã hóa các giá trị còn thiếu và mối quan hệ qua lại giữa các biến.","code":""},{"path":"tính-toán-phí-bảo-hiểm-thuần-bằng-mô-hình-tần-suất---mức-độ-nghiêm-trọng..html","id":"biến-tần-suất-và-mối-liên-hệ-đến-các-biến-khác","chapter":"Chương 14 Tính toán phí bảo hiểm thuần bằng mô hình tần suất - mức độ nghiêm trọng.","heading":"14.2.1 Biến tần suất và mối liên hệ đến các biến khác","text":"Từ phần trước, chúng ta biết rằng tần suất chung của toàn bộ tập dữ liệu là 16,5%. chúng ta xây dựng mô hình dựa trên cách tiếp cận xác thực chéo nên dữ liệu xây dựng mô hình chỉ bao gồm 70% dữ liệu hiện có. Chúng ta chia dữ liệu một cách ngẫu nhiên như sauTrên tập dữ liệu huấn luyện, tần suất trung bình là 16,35% rất gần với tần suất chung ở phần trước. Quá trình khai phá dữ liệu là quá trình tìm hiểu biến tần suất phụ thuộc như thế nào vào các biến số mà chúng ta có trong dữ liệu. Hãy bắt đầu bằng cách xem xét biến \\(nb.rb\\). Đây là một biến chỉ báo cho chúng ta biết hợp đồng bảo hiểm là hoạt động khai thác mới (NB) hay là hợp đồng tái tục (RB). Tần suất trong tập dữ liệu huấn luyện của chúng tôi theo chỉ số khai thác mới hoặc tái tục này nằm trong bảng dưới đây\nBảng 14.4: Tần suất trung bình theo năm của hợp đồng khai thác mới và tái tục\nCó thể thấy rằng trong dữ liệu đào tạo, tần suất trung bình của với hợp đồng khai thác mới là 18,7% và của các hợp đồng tái tục là 11,9%. Đây là một sự khác biệt đáng kể, đó biến \\(rb.nb\\) này là một lựa chọn tốt để đưa vào mô hình.Tiếp theo, chúng ta quan tâm đến biến vị trí địa lý \\(region\\). Mặc dù vị trí địa lý đã được mã hóa bằng các số tự nhiên từ 1 đến 38 và chúng ta không biết được vị trí thực sự của các vùng. Tuy nhiên, có sự phân bố không đồng đều về tần suất xảy ra tai nạn giữa các vùng địa lý theo thời gian. Thật vậy, hình vẽ dưới đây mô tả điều đóTiếp theo chúng ta xem xét \\(driver.age\\). Biến này cho chúng ta biết tuổi của người điều khiển chính phương tiện. Trong dữ liệu huấn luyện mô hình, chúng ta có độ tuổi từ 18 đến 89 và tuổi 93(!), tổng cộng có 73 độ tuổi duy nhất. Khi gặp dữ liệu như vậy, chúng ta nên nghi ngờ những độ tuổi rất cao này và cần kiểm tra xem dữ liệu có chính xác không. Ngoài ra, chúng ta cũng nên kiểm tra tổng số exposure theo từng độ tuổi.Có thể thấy rằng khi tổng số exposure nhỏ thì tần suất có biến động rất mạnh, điển hình là độ tuổi dưới 25 và trên 60. Độ tuổi có tần suất tai nạn cao nhất không được thể hiện trong đồ thị ở trên là tuổi 19 với tần suất là 182%, tuy nhiên tổng exposure cho độ tuổi này chỉ là 2,2 năm nên tần suất không có ý nghĩa thống kê. Nhìn chung có sự giảm dần của tần suất xảy ra tai nạn khi độ tuổi trung bình. Bạn đọc có thể quan sát thấy rằng đường thẳng mô tả mối quan hệ tuyến tính giữa tần suất và độ tuổi có độ dốc âm. Như vậy \\(driver.age\\) cũng là biến cần đưa vào mô hình.Các biến liên quan đến đặc điểm người lái xe còn có giới tính của người lái xe (\\(driver.gender\\)) và tình trạng hôn nhân (\\(marital.status\\). Mối liên hệ của các biến và tần suất được mô tả như đồ thị dưới đâyCó thể thấy rằng tần suất gây ra tai nạn của lái xe nữ là cao hơn với lái xe nam, sự khác biệt là tương đối rõ trong nhóm những người đã lập gia đình hoặc chưa từng lập gia đình. Cũng giống như kinh nghiệm nghiên cứu từ nhiều dữ liệu khác, nhóm những người góa vợ/chồng có tần suất gây ra tai nạn cao hơn đáng kể với nhóm khác. Có thể nhận xét rằng tình trạng hôn nhân và giới tính của người lái xe có ảnh hưởng đến tần suất mặc dù ảnh hưởng không rõ ràng như các biến khác.Chúng ta có thể tiếp tục quá trình trên cho các biến khác liên quan đến người lái xe. Tuy nhiên, mục tiêu cuối cùng sẽ là xây dựng mô hình cho biến tần suất, đó, bạn đọc có thể dựa trên các nguyên tắc ở trên để nghiên cứu mối liên hệ của các biến khác lên tần suất xảy ra tai nạn. Quá trình khai phá dữ liệu có ý nghĩa quan trọng trong việc lựa chọn đưa biến vào trong mô hình nhất là với các dữ liệu có nhiều biến. Bạn đọc có thể tiếp tục quá trình này với các biến liên quan đến đặc điểm của xe.","code":"\n# Đổi các biến rời rạc về kiểu factor\nfactor.variable<-c(\"year\",\"nb.rb\",\"driver.gender\",\"marital.status\",\"ncd.level\",\"region\",\n      \"body.code\",\"seats\",\"fuel.type\")\ndat<-as.data.frame(dat)\nfor (i in 1:ncol(dat)){\n  if(names(dat)[i] %in% factor.variable){\n    dat[,i]<-as.factor(dat[,i])\n  }\n}\n\n# Chia dữ liệu thành hai phần, dữ liệu huấn luyện mô hình và dữ liệu xác thực mô hình\nset.seed(10)\ntrain.index<-sample(1:nrow(dat),size = round(0.7*nrow(dat),0),replace = FALSE)\ntraining.dat<-dat[train.index,]\ntest.dat<-dat[-train.index,]\n# Tần suất trung bình trên tập dữ liệu huấn luyện\nsum(training.dat$clm.count)/sum(training.dat$exposure)## [1] 0.1635727"},{"path":"tính-toán-phí-bảo-hiểm-thuần-bằng-mô-hình-tần-suất---mức-độ-nghiêm-trọng..html","id":"biến-mức-độ-nghiêm-trọng-và-mối-liên-hệ-với-các-biến-khác","chapter":"Chương 14 Tính toán phí bảo hiểm thuần bằng mô hình tần suất - mức độ nghiêm trọng.","heading":"14.2.2 Biến mức độ nghiêm trọng và mối liên hệ với các biến khác","text":"Dữ liệu sử dụng để xây dựng mô hình cho mức độ nghiêm trọng chỉ bao gồm các hợp đồng có xảy ra yêu cầu bồi thường. Trong dữ liệu dùng để huấn luyện mô hình, có 2209 hợp đồng nằm trong phạm vi. Giá trị tiền bồi thường trung bình trên mỗi yêu cầu bồi thường là 800,57\n\\[\\begin{align}\n\\cfrac{\\textit{Tổng chi phí}}{\\textit{Tổng số yêu cầu}} = \\cfrac{1.906.958}{2382} = 800,57\n\\end{align}\\]Trước hết, chúng ta xem xét biến \\(nb.rb\\) có tác động lên mức độ nghiêm trọng hay không. Bảng dưới đây cho biết số lượng yêu cầu bồi thường, tổng mức bồi thường và giá trị tiền bồi thường trung bình của các hợp đồng bán mới và các hợp đồng tái tục.\nBảng 14.5: Mức độ nghiêm trọng trung bình của hợp đồng khai thác mới và tái tục\nCó thể thấy rằng mức độ nghiêm trọng trung bình của các hợp đồng khai thác mới (828) cao hơn đáng kể với các hợp đồng tái tục (705). Điều này cho thấy biến \\(nb.rb\\) là biến có ý nghĩa giải thích trong mô hình hóa mức độ nghiêm trọng của yêu cầu bảo hiểm.Việc phân tích các biến liên quan đến đặc điểm của người lái xe hoàn toàn được thực hiện tương tự như đối với biến tần suất nên chúng tôi sẽ không lặp lại. Việc lựa chọn các biến đưa vào mô hình tần suất sẽ được tiếp tục thảo luận trong phần xây dựng mô hình.","code":""},{"path":"tính-toán-phí-bảo-hiểm-thuần-bằng-mô-hình-tần-suất---mức-độ-nghiêm-trọng..html","id":"mô-hình-tuyến-tính-tổng-quát-cho-tần-suất.","chapter":"Chương 14 Tính toán phí bảo hiểm thuần bằng mô hình tần suất - mức độ nghiêm trọng.","heading":"14.3 Mô hình tuyến tính tổng quát cho tần suất.","text":"Khi phân tích khai phá dữ liêu, chúng ta biết rằng biến tần suất tập dữ liệu huấn luyện có giá trị trung bình là 16,4% và có một số biến có thể là phù hợp để đưa vào mô hình bao gồm tuổi của người lái xe hoặc chỉ số cho biết hợp đồng là bán mới hay tái tục. Chúng ta sẽ bắt đầu xây dựng mô hình tần số bằng cách thực hiện phân tích mô hình có một biến độc lập duy nhất, sau đó sẽ chuyển sang xây dựng mô hình có nhiều biến độc lập bằng cách bổ sung thêm các biến có ý nghĩa khác. Xây dựng mô hình có một biến độc lập là bước quan trọng để chúng ta có nhận định chính xác về khả năng giải thích của các biến độc lập và cách biến đổi các biến độc lập để tăng khả năng giải thích trước khi xây dựng mô hình có nhiều biến độc lậpTrước khi nói đến biến độc lập, chúng ta cần đưa ra lựa chọn cho hàm liên kết và phân phối cho biến tần suất. Đối với hầu hết các mô hình tính toán phí bảo hiểm thuần, chúng ta muốn có biến phụ thuộc được giải thích bởi biến phụ thuộc thông qua quy tắc nhân, đó chúng ta sẽ sử dụng hàm liên kết \\(log()\\). Đối với phân phối biến phụ thuộc, chúng ta có thể chọn giữa phân phối Poisson, phân phối nhị thức âm hoặc phân phối nhị thức biến tần suất là biến dạng đếm. Hãy nhớ rằng mục tiêu của chúng ta là ước tính tần suất trung bình và mặc dù các giả thiết về phân phối là cần thiết nhưng không quan trọng bằng việc tìm ra các biến độc lập chính xác để đưa vào mô hình.Nếu phân phối xác suất của số lượng yêu cầu bồi thường là Poisson thì phương sai của biến phụ thuộc phải bằng giá trị trung bình. Tuy nhiên, trong trường hợp dữ liệu chúng ta đang có, và trong hầu hết các tập dữ liệu bảo hiểm, phương sai của tần suất thường lớn hơn giá trị trung bình. Hiện tượng này được gọi là phân tán quá mức. Bạn đọc nên lựa chọn phân phối nhị thức âm trong các trường hợp như vậy. Tuy nhiên, phân phối nhị thức âm không nằm trong nhóm các phân phối kiểu mũ nên để tránh sự phức tạp không cần thiết, chúng tôi vẫn chọn phân phối Poisson cho biến phụ thuộc. Hàm liên kết được sử dụng là hàm log.","code":""},{"path":"tính-toán-phí-bảo-hiểm-thuần-bằng-mô-hình-tần-suất---mức-độ-nghiêm-trọng..html","id":"mô-hình-tuyến-tính-tổng-quát-với-một-biến-giải-thích.","chapter":"Chương 14 Tính toán phí bảo hiểm thuần bằng mô hình tần suất - mức độ nghiêm trọng.","heading":"14.3.1 Mô hình tuyến tính tổng quát với một biến giải thích.","text":"Mô hình tuyến tính tổng quát đơn giản nhất mà chúng ta có thể áp dụng cho dữ liệu tần suất là mô hình chỉ có duy nhất hệ số chặn. Mô hình này sẽ cho biết tần suất trung bình của biến phụ thuộc. Chúng ta xây dựng mô hình với biến phụ thuộc có phân phối Poisson, với hàm liên kết log, và chỉ có hệ số chặn như sauGiá trị ước lượng được của hệ số chặn là \\(-1.8105\\) và giá trị này khác 0 một cách có ý nghĩa thống kê. Mô hình tuyến tính tổng quát trong trường hợp này có thể được viết như sau:\n\\[\\begin{align}\n& Y_i \\sim \\mathcal{P}(\\lambda_i) \\\\\n& \\log(\\lambda_i) = \\log(e_i) + \\beta_0\n\\end{align}\\]\ntrong đó \\(e_i\\) là giá trị của biến \\(exposure\\) của hợp đồng thứ \\(\\). Nói cách khác, nếu hợp đồng được quan sát đủ 1 năm (\\(e_i = 1\\)) thì giá trị trung bình của tần suất xảy ra tai nạn là\n\\[\\begin{align}\n\\lambda_i = \\exp\\left(\\log(1) + \\beta_0\\right) = \\exp(-1.8105) = 0.164\n\\end{align}\\]\nGiá trị này rất gần với giá trị trung bình của biến tần suất mà chúng ta đã tính trong phần phân tích khai phá dữ liệu.Để xây dựng mô hình có một biến phụ thuộc, chúng ta sẽ sử dụng các tiêu chí để sánh mô hình được để cập trong chương mô hình tuyến tính tổng quát, bao gồm có thước đo \\(deviance\\) và chỉ tiêu AIC. Nhắc lại rằng thước đo \\(deviance\\) đo khoảng cách từ giá trị log-likelihood của một mô hình tuyến tính tổng quát đến giá trị log-likelihood lớn nhất có thể có được khi sử dụng cùng một phân phối của biến phụ thuộc trong khi AIC là được tính từ giá trị của hàm log-likelihood có điều chỉnh theo số lượng tham số. Mô hình tuyến tính tổng quát có thước đo \\(deviance\\) và chỉ tiêu AIC nhỏ hơn là các mô hình phù hợp hơn với dữ liệu.Trước khi xây dựng mô hình, bạn đọc cần lưu ý về biến độc lập kiểu số và biến độc lập kiểu factor. Biến \\(year\\) trong mô hình đang được coi là biến kiểu factor với 4 giá trị khác nhau là 2010, 2011, 2012, và 2014, nghĩa là khi chúng ta sử dụng \\(year\\) như một biến độc lập trong mô hình, sẽ có 4 tham số độc lập được ước lượng dành cho biến \\(year\\), mỗi tham số đại diện cho 1 giá trị. Nếu chúng ta coi biến \\(year\\) như một biến kiểu số, khi xây dựng mô hình sẽ chỉ có một tham số ứng với biến \\(year\\) và thêm 1 hệ số chặn. Với một số biến trong mô hình, chúng ta vừa có thể coi biến đó là kiểu số và vừa có thể coi biến đó là biến kiểu rời rạc. Danh sách các biến nằm trong bảng dưới đây\nBảng 14.6: Các biến vừa có thể là biến kiểu số vừa là rời rạc\nĐối với các biến này, chúng ta sẽ lưu trong dữ liệu huấn luyện mô hình đồng thời dưới dạng số (tên biến có \\(.num\\) và dưới dạng factor (tên biến có \\(.f\\)).Chúng ta ước lượng các mô hình GLM với một biến độc lập, tính toán \\(AIC\\) và \\(deviance\\) và mô tả giá trị \\(deviance\\) và \\(AIC\\) trên một đồ thị rải điểm như sauĐồ thị rải điểm mô tả chỉ tiêu AIC trên trục x và giá trị deviance trên trục y. Các biến giải thích tốt hơn sẽ xuất hiện ở góc dưới bên trái của đồ thị, nơi cả deviance và AIC đều có giá trị nhỏ. Chúng ta thấy các biến \\(ncd.level\\), \\(year\\) và \\(yrs.licensed\\) là nằm ở góc trái và phía dưới hơn các biến khác. Biến \\(driver.age\\) và \\(region\\) có tác động lên tần suất xảy ra tai nạn nhưng mô hình tuyến tính tổng quát xây dựng trên các biến này lại không có kết quả giống như ta mong muốn. Nguyên nhân là \n- Biến \\(region\\) là biến kiểu rời rạc và có 38 giá trị riêng biệt dẫn đến việc xây dựng mô hình sẽ có quá nhiều tham số.\n- Biến \\(driver.age\\) có các giá trị từ 75 tuổi trở lên là có quá ít quan sát và không có tai nạn nào xảy ra.Đối với các biến kiểu rời rạc trong mô hình tuyến tính tổng quát, kinh nghiệm xây dựng mô hình là kết hợp các nhóm có cùng hệ số ước lượng gần nhau để tạo thành nhóm mới. Điều này giúp mô hình ước lượng có phương sai nhỏ hơn nhưng không làm giảm sai số của mô hình đi quá nhiều. Hình vẽ dưới đây cho biết hệ số ước lượng cho mỗi vùng khác nhau của biến \\(region\\).\nBảng 14.7: Nhóm các vùng theo hệ số ước lượng\nBiến mới được tạo từ biến \\(region\\) được gọi tên là \\(region.f\\) được thêm vào dữ liệu như sauCó thể thấy rằng mô hình với biến \\(region.f\\) có chỉ tiêu AIC nhỏ hơn với \\(region\\) vì số lượng tham số sử dụng chỉ là 8 tham số thay vì 38 tham số như ban đầu. Bạn đọc có thể tiếp tục quá trình phân tích đơn biến như phân tích đối với biến \\(region\\) để có các biến đầu vào tốt hơn trong mô hình nhiều biến.Với các phân tích đơn biến ở trên, chúng ta có thể kết luận rằng các biến \\(ncd.levels\\), \\(year\\), \\(nb.rb\\), \\(region.f\\), \\(yrs.licenced.f\\), \\(driver.age\\) và \\(prior.claims\\) là các biến có ý nghĩa nhất va có ảnh hưởng đến tần suất gây ra tai nạn. Lưu ý rằng biến \\(year\\) không phải là biến liên quan đến đặc điểm của người lái xe hay phương tiện được bảo hiểm. Trong các mô hình tần suất và mức độ nghiêm trọng, biến thời gian thường được gọi là biến điều khiển và luôn có mặt trong mô hình đa biến.","code":"## \n## Call:\n## glm(formula = clm.count ~ 1, family = poisson(link = \"log\"), \n##     data = training.dat, offset = log(exposure))\n## \n## Deviance Residuals: \n##     Min       1Q   Median       3Q      Max  \n## -0.5720  -0.4953  -0.3692  -0.2335   4.5865  \n## \n## Coefficients:\n##             Estimate Std. Error z value Pr(>|z|)    \n## (Intercept) -1.81050    0.02049  -88.36   <2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## (Dispersion parameter for poisson family taken to be 1)\n## \n##     Null deviance: 11573  on 28531  degrees of freedom\n## Residual deviance: 11573  on 28531  degrees of freedom\n## AIC: 16097\n## \n## Number of Fisher Scoring iterations: 6\ntraining.dat<-training.dat%>%mutate(yrs.licensed.num = yrs.licensed,\n                                    vehicle.age.num = vehicle.age,\n                                    prior.claims.num = prior.claims,\n                                    yrs.licensed.f = as.factor(yrs.licensed),\n                                    vehicle.age.f = as.factor(vehicle.age),\n                                    prior.claims.f = as.factor(prior.claims)) %>%\n  select(-yrs.licensed,-vehicle.age,-prior.claims)\n\n# thực hiện biến đổi tương tự trên tập xác thực\ntest.dat<-test.dat%>%mutate(yrs.licensed.num = yrs.licensed,\n                                    vehicle.age.num = vehicle.age,\n                                    prior.claims.num = prior.claims,\n                                    yrs.licensed.f = as.factor(yrs.licensed),\n                                    vehicle.age.f = as.factor(vehicle.age),\n                                    prior.claims.f = as.factor(prior.claims)) %>%\n  select(-yrs.licensed,-vehicle.age,-prior.claims)\nexposure<-training.dat$exposure\nvariable.list<-names(training.dat)\nremove.list<-c(\"pol.id\",\"exposure\",\"clm.count\",\"clm.incurred\")\nvariable.list<-variable.list[!(variable.list %in% remove.list)]\ndeviance <- rep(0,length(variable.list))\nAIC <- rep(0,length(variable.list))\nfor (i in 1:length(variable.list)){\n  x <- variable.list[i]\n  simple.dat<-select(training.dat,x,clm.count)\n  simple.glm<-glm(clm.count~., family = poisson(link = \"log\"), data = simple.dat,\n                  offset = log(exposure))\n  AIC[i]<-simple.glm$aic\n  deviance[i]<-simple.glm$deviance\n}\nresult<-data.frame(variable_names = variable.list, aic = AIC, deviance = deviance)\nresult%>%ggplot()+geom_point(aes(x = AIC, y = deviance), shape = 21)+ \n  geom_label_repel(aes(x = AIC, y = deviance, label = variable.list))+\n  ggtitle(\"So sánh các mô hình GLM có một biến giải thích (1)\")+\n  theme_minimal()\nsimple.dat<-select(training.dat,region,clm.count)\nsimple.glm<-glm(clm.count~., family = poisson(link = \"log\"), data = simple.dat,\n                  offset = log(exposure))\ndat1<-data.frame(d.name = names(simple.glm$coefficients), coef = simple.glm$coefficients)\ndat1$d.name[1]<-\"region1\"\ndat1$coef[2:38]<-dat1$coef[2:38]+dat1$coef[1]\ndat1<-arrange(dat1,-coef)\ndat1%>%ggplot(aes(x = coef, y = d.name))+geom_point(shape=21)+\n  scale_y_discrete(limits = dat1$d.name)+theme_minimal()+\n  xlab(\"\")+ylab(\"\")+ggtitle(\"Hệ số của biến region trong mô hình GLM\")\ntraining.dat<-mutate(training.dat, region.f = ifelse(region %in% c(7,2), \"R1\", \n                                                     ifelse(region %in% c(20,22), \"R2\", \n                                                            ifelse(region %in% c(27,29,14,35), \"R3\",\n                                                                   ifelse(region %in% c(11,37,12,30,28,15,19), \"R4\",\n                                                                          ifelse(region %in% c(4,13,24,3,10,16), \"R5\",\n                                                                                 ifelse(region %in% c(1,38,34,31,21,32,25,23), \"R6\",\n                                                                                        ifelse(region %in% c(6,33,9,17), \"R7\",\"R8\"))))))))\n\n# Thực hiện biến đổi tương tự trên tập xác thực\n\ntest.dat<-mutate(test.dat, region.f = ifelse(region %in% c(7,2), \"R1\", \n                                                     ifelse(region %in% c(20,22), \"R2\", \n                                                            ifelse(region %in% c(27,29,14,35), \"R3\",\n                                                                   ifelse(region %in% c(11,37,12,30,28,15,19), \"R4\",\n                                                                          ifelse(region %in% c(4,13,24,3,10,16), \"R5\",\n                                                                                 ifelse(region %in% c(1,38,34,31,21,32,25,23), \"R6\",\n                                                                                        ifelse(region %in% c(6,33,9,17), \"R7\",\"R8\"))))))))"},{"path":"tính-toán-phí-bảo-hiểm-thuần-bằng-mô-hình-tần-suất---mức-độ-nghiêm-trọng..html","id":"xây-dựng-mô-hình-tuyến-tính-tổng-quát-nhiều-biến","chapter":"Chương 14 Tính toán phí bảo hiểm thuần bằng mô hình tần suất - mức độ nghiêm trọng.","heading":"14.4 Xây dựng mô hình tuyến tính tổng quát nhiều biến","text":"Sau khi đã có ý tưởng tốt hơn về những biến giải thích phù hợp với dữ liệu, chúng ta có thể chuyển sang xây dựng các mô hình kết hợp nhiều hơn một biến giải thích. Về mặt lý thuyết, để tìm ra mô hình đa biến tốt, chúng ta có thể xem xét cả hai mô hình biến và chọn mô hình tốt nhất; sau đó có thể xem xét cả ba mô hình biến và lại chọn mô hình tốt nhất, v.v. Chiến lược này gọi là \\(forward-stepwise-selection\\). Tuy nhiên với số lượng biến chúng ta hiện có thì chiến lược này không khả thi Một chiến lược khác là xây dựng một mô hình có đầy đủ các biến sau đó lần lượt loại đi các biến không cần thiết. Mặc dù chiến lược này không yêu cầu về mặt tính toán nhưng nhược điểm chính là sự phụ thuộc vào thứ tự chúng ta loại bỏ đi các biến giải thích. Phương pháp khả thi nhất trong trường hợp hày là xây dựng mô hình đa biến được lựa chọn từ một tập hợp con các biến có ý nghĩa trong phân tích một biến sau đó cố gắng thêm các biến khác vào mô hình.Chúng ta cho mô hình với 7 biến giải thích \\(ncd.levels\\), \\(year\\), \\(nb.rb\\), \\(region.f\\), \\(yrs.licenced.f\\), \\(driver.age\\) và \\(prior.claims.f\\) là mô hình cơ sở và sẽ tìm kiếm các biến khác thêm vào mô hình nhằm cải thiện các chỉ tiêu như \\(deviance\\) và \\(AIC\\).Đồ thị trên cho thấy các biến có làm tăng chất lượng của mô hình là \\(body.code\\), \\(height\\), và \\(marital.status\\). Cách tốt nhất để đưa thêm biến độc lập vào mô hình cơ sở là thêm lần lượt từng biến vào mô hình sau đó và lặp lại các bước ở trên. Tuy nhiên, để tránh sự phức tạp không cần thiết, chúng ta sẽ đưa đồng thời cả ba biến vào mô hình sau đó thực hiện kiểm định \\(\\chi^2\\): hiệu số giữa \\(residual\\) \\(deviance\\) của mô hình sau khi thêm biến và mô hình cơ sở có phân phối xấp xỉ phân phối \\(\\chi^2\\) với bậc tự \\(m_2 - m_1\\), với \\(m_2 - m_1\\) là số lượng tham số thêm vào trong mô hình mới. Biến \\(body.code\\) là biến kiểu factor có 8 giá trị riêng biệt, biết \\(height\\) là biến kiểu số, trong khi biến \\(marital.status\\) là biến kiểu factor có 4 giá trị riêng biệt, đó thêm 3 biến vào mô hình sẽ làm cho số lượng tham số của mô hình tăng lên là \\((8-1) + 1 + (4-1) = 11\\) tham số.\nBảng 14.8: Thước đo deviance của mô hình cơ sở và mô hình thêm biến\nGiá trị deviance giảm 54.99 sau khi thêm 11 tham số vào mô hình. Lưu ý rằng giá trị phân vị ngưỡng 99.99% của phân phối \\(\\chi^2(11)\\) là 37.37, nhỏ hơn 54.99, nên chúng ta có thể kết luận rằng việc thêm 3 biến \\(body.code\\), \\(height\\), và \\(marital.status\\) là có ý nghĩa thống kê. Bạn đọc có thể lặp lại các bước ở trên để tìm được thêm các biến có ý nghĩa đưa vào mô hình giải thích tần suất.","code":""},{"path":"tính-toán-phí-bảo-hiểm-thuần-bằng-mô-hình-tần-suất---mức-độ-nghiêm-trọng..html","id":"mô-hình-tuyến-tính-tổng-quát-cho-biến-mức-độ-nghiêm-trọng.","chapter":"Chương 14 Tính toán phí bảo hiểm thuần bằng mô hình tần suất - mức độ nghiêm trọng.","heading":"14.5 Mô hình tuyến tính tổng quát cho biến mức độ nghiêm trọng.","text":"Xây dựng mô hình cho mức độ nghiêm trọng khó khăn hơn xây dựng mô hình cho tần suất vì thứ nhất, có ít dữ liệu hơn và thứ hai, các giá trị quan sát được thường biến động lớn. Thông thường, phần lớn các hợp đồng có yêu cầu bồi thường có mức độ nghiêm trọng nhỏ và trong khi một số ít hợp đồng có mức độ nghiêm trọng rất lớn. Điều này thường dẫn đến phân phối của mức độ nghiêm trọng bị lệch phải, như được thể hiện trong biểu đồ ở dưới. Bạn đọc có thể thấy rằng gần như tất cả các quan sát đều tập trung ở phía bên trái của biểu đồ. Có rất ít giá trị lớn hơn 6.000; trên thực tế, chỉ có 11 hợp đồng có yêu cầu bồi thường có mức độ nghiêm trọng lớn hơn 6000 trong tổng số 2209 quan sát có yêu cầu bồi thường trong tập dữ liệu huấn luyện.Tương tự như khi xây dựng mô hình cho biến tần suất, chúng ta bắt đầu xây dựng mô hình cho biến mức độ nghiêm trọng bằng mô hình không có biến phụ thuộc.Mô hình không có biến giải thích có hệ số chặn là 6.685, nghĩa là giá trị trung bình của biến mức độ nghiêm trọng là \\(e^{6.685} = 800.31\\) tương tự như giá trị trung bình đã tính toán ở phần khai phá dữ liệu.Biến \\(year\\) kiểm soát tác động của thời gian lên mức độ nghiêm trọng, đó chúng ta sẽ đưa biến này vào tất cả các mô hình cho mức độ nghiêm trọng. Các bước để xây dựng mô hình giống như khi chúng ta xây dựng mô hình cho tần suất: chúng ta cần tìm kiếm các biến giải thích (ngoài biến \\(year\\)) có khả năng dự đoán mức độ nghiêm trọng của các yêu cầu bồi thường. Quá trình này tương tự như phần trước nên chúng tôi sẽ cố gắng trình bày ngắn gọn nhất có thể.","code":"\nglm(clm.serverity ~1, family = Gamma(link = \"log\"), data = training.dat, weights = clm.count , subset = clm.count>0)## \n## Call:  glm(formula = clm.serverity ~ 1, family = Gamma(link = \"log\"), \n##     data = training.dat, weights = clm.count, subset = clm.count > \n##         0)\n## \n## Coefficients:\n## (Intercept)  \n##       6.685  \n## \n## Degrees of Freedom: 2208 Total (i.e. Null);  2208 Residual\n## Null Deviance:       4626 \n## Residual Deviance: 4626  AIC: 36280"},{"path":"tính-toán-phí-bảo-hiểm-thuần-bằng-mô-hình-tần-suất---mức-độ-nghiêm-trọng..html","id":"mô-hình-đơn-biến-cho-mức-độ-nghiêm-trọng","chapter":"Chương 14 Tính toán phí bảo hiểm thuần bằng mô hình tần suất - mức độ nghiêm trọng.","heading":"14.5.1 Mô hình đơn biến cho mức độ nghiêm trọng","text":"Mô hình đơn biến được hiểu là mô hình bao gồm hệ số chặn, biến điều khiển \\(year\\) và một biến giải thích. Tương tự như khi xây dựng mô hình cho biến tần suất, chúng ta tính toán \\(deviance\\) và chỉ tiêu \\(AIC\\) cho các mô hình và biểu diễn kết quả trên một đồ thị rải điểmMô hình “Null” là mô hình chỉ bao gồm biến \\(year\\) và hệ số chặn. Có thể thấy rằng việc thêm biến giải thích vào mô hình sẽ làm cho \\(deviance\\) giảm, nhưng chưa chắc chỉ số AIC sẽ giảm vì AIC tính đến số lượng tham số trong mô hình. Một số biến có làm giảm AIC của mô hình bao gồm có \\(ncd.level\\), \\(nb.rb\\), \\(marital.status\\), \\(yrs.licensed.num\\), \\(driver.gender\\).","code":"\nvariable.list<-names(training.dat)\nremove.list<-c(\"pol.id\",\"exposure\",\"clm.count\",\"clm.incurred\",\"clm.serverity\", \"year\", \"region\")\nvariable.list<-variable.list[!(variable.list %in% remove.list)]\np<-length(variable.list)\nv.clm.count<-training.dat$clm.count\ndeviance <- rep(0,p+1)\nAIC <- rep(0,p+1)\nfor (i in 1:p){\n  x <- variable.list[i]\n  simple.dat<-select(training.dat,year,x,clm.serverity)\n  simple.glm<-glm(clm.serverity~., family = Gamma(link = \"log\"), data = simple.dat, weights = v.clm.count, subset = v.clm.count>0)\n  AIC[i]<-simple.glm$aic\n  deviance[i]<-simple.glm$deviance\n}\n\nsimple.dat<-select(training.dat,year,clm.serverity)\nsimple.glm<-glm(clm.serverity~., family = Gamma(link = \"log\"), data = simple.dat, weights = v.clm.count, subset = v.clm.count>0)\nAIC[p+1]<-simple.glm$aic\ndeviance[p+1]<-simple.glm$deviance\n\nresult<-data.frame(variable_names = c(variable.list,\"Null\"), aic = AIC, deviance = deviance, last.col = as.factor(c(rep(0,p),1)))\nresult%>%ggplot()+geom_point(aes(x = AIC, y = deviance,, size = last.col, fill = last.col), shape = 21)+ \n  geom_label_repel(aes(x = AIC, y = deviance, label = variable_names))+\n  ggtitle(\"Mô hình GLM đơn biến cho mức độ nghiêm trọng\")+\n  scale_fill_manual(values = c(\"white\",\"black\"))+\n  scale_size_manual(values = c(2,3))+\n  theme_minimal()+\n  theme(legend.position = \"none\")"},{"path":"tính-toán-phí-bảo-hiểm-thuần-bằng-mô-hình-tần-suất---mức-độ-nghiêm-trọng..html","id":"mô-hình-nhiều-biến-cho-mức-độ-nghiêm-trọng","chapter":"Chương 14 Tính toán phí bảo hiểm thuần bằng mô hình tần suất - mức độ nghiêm trọng.","heading":"14.5.2 Mô hình nhiều biến cho mức độ nghiêm trọng","text":"Chúng ta xây dựng mô hình đa biến bao gồm 5 biến kể trên sau đó tìm kiếm thêm các biến khác.Có thể thấy rằng biến \\(yrs.licensed.num\\) có thể làm giảm AIC trong mô hình đơn biến nhưng trong mô hình đa biến biến này lại không có ý nghĩa. Chúng ta sẽ loại bỏ biến \\(yrs.licensed.num\\) ra khỏi mô hình đa biến. Các biến khác đều là các biến dạng factor và không phải tất cả các giá trị của biến factor đều có ý nghĩa thống kê. Chẳng hạn như \\(ncd.level3\\) gần như không có khác biệt với \\(ncd.level1\\). Hoặc các giá trị \\(Divorced\\) và \\(Married\\) của biến \\(marital.status\\) cũng không có sự khác biệt. Điều này gợi ý cho bạn đọc về việc tạo các biến \\(ncd.level\\) và \\(marital.status\\) mới có ý nghĩa thống kê hơn. Trong phần này của cuốn sách, để tránh sự phức tạp không cần thiết, chúng tôi sẽ giữ nguyên các biến \\(ncd.level\\) và \\(marital.status\\) ở trạng thái hiện tại.Chúng ta coi mô hình bao gồm 5 biến giải thích \\(year\\), \\(ncd.level\\), \\(nb.rb\\), \\(marital.status\\), và \\(driver.gender\\) là mô hình cơ sở để tìm kiếm thêm các biến giải thích khác.Đồ thị trên gợi ý cho chúng ta biết biến \\(vehicle.age.num\\) là một biến giải thích tốt có làm giảm chỉ tiêu AIC. Chúng ta thực hiện kiểm định \\(\\chi^2\\) để kiểm tra xem việc thêm biến vào có ý nghĩa thống kê hay không\nBảng 14.9: Thước đo deviance của mô hình cơ sở và mô hình thêm biến\nGiá trị deviance giảm 7.6 sau khi thêm 1 tham số vào mô hình. Đây là giá trị phân vị ngưỡng 99.4% của phân phối \\(\\chi^2(1)\\) nên chúng ta có thể kết luận rằng việc thêm biến \\(vehicle.age.num\\) là có ý nghĩa thống kê. Bạn đọc có thể lặp lại các bước ở trên để tìm được thêm các biến có ý nghĩa đưa vào mô hình giải thích mức độ nghiêm trọng.","code":"\nglm1<-glm(clm.serverity ~ year + ncd.level+ nb.rb + marital.status + yrs.licensed.num + driver.gender, \n         family = Gamma(link = \"log\"), data = training.dat, weights = clm.count, subset = clm.count>0)\nsummary(glm1)## \n## Call:\n## glm(formula = clm.serverity ~ year + ncd.level + nb.rb + marital.status + \n##     yrs.licensed.num + driver.gender, family = Gamma(link = \"log\"), \n##     data = training.dat, weights = clm.count, subset = clm.count > \n##         0)\n## \n## Deviance Residuals: \n##     Min       1Q   Median       3Q      Max  \n## -3.9662  -1.5961  -0.6651   0.3050   3.7805  \n## \n## Coefficients:\n##                        Estimate Std. Error t value Pr(>|t|)    \n## (Intercept)            6.628362   0.227028  29.196   <2e-16 ***\n## year2011              -0.232547   0.106311  -2.187   0.0288 *  \n## year2012               0.128161   0.092530   1.385   0.1662    \n## year2013               0.271938   0.094109   2.890   0.0039 ** \n## ncd.level2            -0.237103   0.268439  -0.883   0.3772    \n## ncd.level3             0.007723   0.073769   0.105   0.9166    \n## ncd.level4            -0.123855   0.102547  -1.208   0.2273    \n## ncd.level5            -0.215120   0.125589  -1.713   0.0869 .  \n## ncd.level6            -0.196879   0.114226  -1.724   0.0849 .  \n## nb.rbRB               -0.086679   0.075003  -1.156   0.2479    \n## marital.statusMarried  0.102740   0.204406   0.503   0.6153    \n## marital.statusSingle   0.204259   0.241585   0.845   0.3979    \n## marital.statusWidow    0.428399   0.283793   1.510   0.1313    \n## yrs.licensed.num       0.002306   0.021181   0.109   0.9133    \n## driver.genderMale     -0.128635   0.086560  -1.486   0.1374    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## (Dispersion parameter for Gamma family taken to be 1.854572)\n## \n##     Null deviance: 4626.1  on 2208  degrees of freedom\n## Residual deviance: 4526.6  on 2194  degrees of freedom\n## AIC: 36243\n## \n## Number of Fisher Scoring iterations: 8"},{"path":"tính-toán-phí-bảo-hiểm-thuần-bằng-mô-hình-tần-suất---mức-độ-nghiêm-trọng..html","id":"xác-định-phí-bảo-hiểm-thuần-và-xác-thực-mô-hình","chapter":"Chương 14 Tính toán phí bảo hiểm thuần bằng mô hình tần suất - mức độ nghiêm trọng.","heading":"14.6 Xác định phí bảo hiểm thuần và xác thực mô hình","text":"Trong phần này, chúng ta kết hợp các mô hình tần suất và mức độ nghiêm trọng để xác định mức phí bảo hiểm thuần. Công thức xác định phí bảo hiểm thuần chỉ đơn giản là\n\\[\\begin{align}\n\\textit{Phí bảo hiểm thuần} = \\textit{Tần suất} \\times \\textit{Mức độ nghiêm trọng}\n\\end{align}\\]Chúng ta sử dụng mô hình tần suất và mô hình mức độ nghiêm trọng với các biến được xác định trong phần trước. Danh sách các biến được liệt kê trong bảng dưới đây.\nBảng 14.10: Các biến sử dụng trong xác định phí bảo hiểm thuần\nChúng ta thêm biến tần suất trung bình (\\(E.fre\\)), mức độ nghiêm trọng trung bình (\\(E.ser\\)) và phí thuần (\\(Net.prem\\)) vào dữ liệu huấn luyện mô hình.Chúng ta thực hiện sánh giữa tổng phí thuần của tất cả các hợp đồng với tổng số tiền thực trả cho các hợp đồng trong tập dữ liệu huấn luyện. Không có gì ngạc nhiên khi tổng tần suất trung bình bằng tổng số yêu cầu bổi thường và tổng phí bảo hiểm thuần xấp xỉ tổng số tiền yêu cầu bồi thường thực tế mô hình được xây dựng trên tập huấn luyện.\nBảng 14.11: Giá trị thực tế và dự đoán trên dữ liệu huấn luyên mô hình\nThước đo quan trọng nhất về hiệu quả của mô hình là khả năng dự đoán của mô hình trên tập dữ liệu được sử dụng để kiểm tra (\\(test.dat\\)). Tổng số lượng yêu cầu bồi thường và tổng chi phí bồi thường được dự đoán bằng mô hình tần suất - mức độ nghiêm trọng được sánh với giá trị thực tế như ở bảng dưới đây\nBảng 14.12: Giá trị thực tế và dự đoán trên dữ liệu huấn luyên mô hình\nBạn đọc có thể thấy rằng trên dữ liệu ngoài mẫu, tổng số lượng yêu cầu bồi thường dự đoán bằng 95.78% tổng số lượng yêu cầu bồi thường thực tế, trong khi tổng số tiền yêu cầu bồi thường dự đoán bằng 100.46% tổng số tiền yêu cầu bồi thường thực tế. Có thể thấy rằng khả năng dự đoán ngoài mẫu của mô hình về cơ bản không thể giống như dự đoán trong mẫu, tuy nhiên các tỷ lệ 95.78% và 100.46% là vượt quá mức độ kỳ vọng vì tổng số lượng hợp đồng ngoài mẫu là 12228 hợp đồng.Ngoài phương pháp xác thực bằng dữ liệu ngoài mẫu, bạn đọc cũng có thể sử dụng xác thực chéo để đánh giá khả năng dự đoán của mô hình. Chúng ta sẽ chia dữ liệu ban đầu thành 5 phần (một cách ngẫu nhiên), tạm gọi là các phần 1, 2, 3, 4 và 5. Mỗi lần xây dựng mô hình, chúng ta sẽ lấy 1 phần làm dữ liệu ngoài mẫu và sử dụng 4 phần còn lại để xây dựng mô hình. Một mô hình ổn định là mô hình có khả năng dự đoán trên các tập ngoài mẫu không bị biến động quá nhiều.","code":"\nglm.fre<-glm(clm.count~ year + ncd.level+ nb.rb+ yrs.licensed.f + region.f + driver.age + \n               prior.claims.f + marital.status +  body.code+ height, \n             family = poisson(link = \"log\"), data = training.dat,\n                offset = log(exposure))\n\nglm.ser<-glm(clm.serverity ~ year + ncd.level+ nb.rb + marital.status + driver.gender + vehicle.age.num, \n         family = Gamma(link = \"log\"), data = training.dat, weights = clm.count, subset = clm.count>0)\n\n## Tính toán expected frequency và expected severity\ntraining.dat<-mutate(training.dat, E.fre = exp(predict(glm.fre, training.dat)),\n                     E.ser = exp(predict(glm.ser, training.dat)),\n                     Net.prem = E.fre*E.ser)\n\n# Dự đoán expected frequency và expected severity trên test\ntest.dat<-mutate(test.dat, E.fre = exp(predict(glm.fre, test.dat)),\n                     E.ser = exp(predict(glm.ser, test.dat)),\n                 Net.prem = E.fre * E.ser)"},{"path":"tính-toán-phí-bảo-hiểm-thuần-bằng-mô-hình-tần-suất---mức-độ-nghiêm-trọng..html","id":"kết-luận","chapter":"Chương 14 Tính toán phí bảo hiểm thuần bằng mô hình tần suất - mức độ nghiêm trọng.","heading":"14.7 Kết luận","text":"Chúng ta đã thảo luận về một số bước cần thiết để phát triển một mô hình xác định phí bảo hiểm thuần, yếu tố quan trọng nhất trong quá trình hình thành phí bảo hiểm. Đê thực các bước này, chúng ta đã thực hiện một số phân tích dữ liệu khám phá để làm quen với dữ liệu và nhằm đạt được một số hiểu biết quan trọng trong giai đoạn lập mô hình. Tiếp theo chúng tôi bắt đầu vào việc tạo ra một số mô hình tuyến tính tổng quát cho biến tần suất yêu cầu bồi thường và biến mức độ nghiêm trọng của yêu cầu bồi thường. Các biến được sử dụng trong mô hình bao gồm cả các biến liên tục và biến rời rạc. Một vài biến được chuyển đổi để tăng khả năng giải thích các biến mục tiêu. Sau cùng, chúng tôi kết hợp hai mô hình lại để xác định phí bảo hiểm thuần. Mô hình được xác thực trên một tập dữ liệu ngoài mẫu và cho kết quả rất khả quan.Nhìn chung, chúng ta đã hoàn thành khá nhiều bước để tiến đến kết quả. Tuy nhiên trong thực tế, để đưa phí thuần vào hoạt động kinh doanh vẫn còn nhiều việc phải làm. Các bước tiếp theo vượt quá phạm vi của mô hình toán thông thường nên sẽ không nằm trong phạm vi của cuốn sách này.","code":"## \n## Attaching package: 'dplyr'## The following objects are masked from 'package:stats':\n## \n##     filter, lag## The following objects are masked from 'package:base':\n## \n##     intersect, setdiff, setequal, union## \n## Attaching package: 'kableExtra'## The following object is masked from 'package:dplyr':\n## \n##     group_rows## \n## Attaching package: 'gridExtra'## The following object is masked from 'package:dplyr':\n## \n##     combine## \n## Attaching package: 'pryr'## The following object is masked from 'package:dplyr':\n## \n##     where## Loading required package: lattice## Loading required package: splines## Loading required package: foreach## Loaded gam 1.22-3## randomForest 4.7-1.1## Type rfNews() to see new features/changes/bug fixes.## \n## Attaching package: 'randomForest'## The following object is masked from 'package:gridExtra':\n## \n##     combine## The following object is masked from 'package:ggplot2':\n## \n##     margin## The following object is masked from 'package:dplyr':\n## \n##     combine"},{"path":"mô-hình-cây-quyết-định.html","id":"mô-hình-cây-quyết-định","chapter":"Chương 15 Mô hình cây quyết định","heading":"Chương 15 Mô hình cây quyết định","text":"Trong chương này, chúng tôi trình bày các phương pháp hồi quy và phân loại dựa trên cây quyết định. Mô hình dạng cây quyết định liên quan đến kỹ thuật phân chia miền giá trị của các biến giải thích hành các miền nhỏ có tính chất tương tự nhau. Để đưa ra dự đoán cho một quan sát, mô hình cây quyết định hồi quy sử dụng giá trị trung bình của các quan sát nằm trong miền tương ứng. Đối với bài toán phân loại, cây quyết định thường sử dụng giá trị mode của các quan sát nằm trong miền này. Các quy tắc dùng để phân tách miền xác định của các biến giải thích được sử dụng có thể được mô tả theo kiểu một cây quyết định nên các kiểu xây dựng mô hình như vậy thường được gọi là mô hình dạng cây quyết định.Các phương pháp hồi quy và phân loại dựa trên cây quyết định khá đơn giản và dễ dàng trong việc diễn giải. Tuy nhiên, các phương pháp này thường không sánh được về khả năng dự đoán chính xác với các phương pháp học máy có giám sát đã được trình bày trong các chương trước, chẳng hạn như hồi quy Splines, Smoothing Splines, hoặc mô hình cộng tính tổng quát khi có nhiều biến giải thích. Tuy nhiên, mô hình dạng cây quyết định lại thích hợp khi sử dụng kết hợp vơi các kỹ thuật thống kê hiện đại như rừng ngẫu nhiên hoặc học tăng cường để cho kết quả dự đoán chính xác vượt trội. Một lợi thế khác của mô hình dạng cây quyết định đó là mô hình này có thể sử dụng trực tiếp với cả bài toán hồi quy và phân loại mà không cần một sự biến đổi đáng kể nào về cách tiếp cận.Trong chương này chúng tôi sẽ trình bày về mô hình dạng cây quyết định sử dụng trong bài toán hồi quy và phân loại và sau đó giới thiệu đến bạn đọc thuật toán kỹ thuật kết hợp nhiều cây quyết định để cải thiện khả năng dự đoán của mô hình còn được biến đến với tên gọi là thuật toán \\(rừng\\) \\(ngẫu\\) \\(nhiên\\). Kỹ thuật học tăng cường (boosting) sẽ được giới thiệu đến bạn đọc trong chương tiếp theo.","code":""},{"path":"mô-hình-cây-quyết-định.html","id":"cơ-bản-về-mô-hình-cây-quyết-định","chapter":"Chương 15 Mô hình cây quyết định","heading":"15.1 Cơ bản về mô hình cây quyết định","text":"Như chúng tôi đã đề cập, cây quyết định có thể được áp dụng cho cả bài toán hồi quy và bài toán phân loại. Chúng ta xem xét bài toán hồi quy trước sau đó chuyển sang cây quyết định phân loại. Bạn đọc sẽ thấy rằng cách xây dựng mô hình cây quyết định và cây quyết định hồi quy là hoàn toàn tương tự nhau, chỉ khác nhau về mục tiêu tối thiểu hóa.","code":""},{"path":"mô-hình-cây-quyết-định.html","id":"mô-hình-cây-quyết-định-hồi-quy","chapter":"Chương 15 Mô hình cây quyết định","heading":"15.1.1 Mô hình cây quyết định hồi quy","text":"","code":""},{"path":"mô-hình-cây-quyết-định.html","id":"xây-dựng-cây-quyết-định-hồi-quy-trên-dữ-liệu-boston","chapter":"Chương 15 Mô hình cây quyết định","heading":"15.1.1.1 Xây dựng cây quyết định hồi quy trên dữ liệu Boston","text":"Để bắt đầu với cây quyết định hồi quy, chúng ta bắt đầu bằng một ví dụ đơn giản trên một cây hồi quy đơn biến. Chúng tôi sử dụng dữ liệu Boston để dự đoán biến mục tiêu là giá nhà tại các vùng (biến \\(medv\\), đơn vị là nghìn USD) dựa trên tỷ lệ số người có mức sống thấp trong vùng đó (biến \\(lstat\\), đơn vị là %). Hình 15.1 mô tả một cây hồi quy phù hợp với dữ liệu này.\nHình 15.1: Cây quyết định hồi quy kích thước bằng 3 mô tả giá nhà phụ thuộc vào tỷ lệ người sống dưới mức trung bình trên dữ liệu Boston. Tại mỗi điểm phân nhánh, ký hiệu lstat < c chỉ định phân nhánh sang bên trái, nghĩa là các điểm dữ liệu thuộc có tính chất lstat < c nằm trong nhánh bên trái trong khi các điểm dữ liệu có lstat lớn hơn hoặc bằng c sẽ nằm sang nhánh bên phải. Cây quyết định ở trên có hai nút và ba lá tương ứng với ba tập hợp con của dữ liệu.\nBạn đọc có thể thấy rằng một cây quyết định bao gồm một chuỗi các quy tắc phân chia, bắt đầu từ ngọn cây xuống dưới. Phần phân chia trên cùng chỉ định các quan sát có \\(lstat < 9.725 (\\%)\\) cho nhánh cây bên trái và các quan sát có \\(lstat \\geq 9.725(\\%)\\) cho nhánh bên phải. Giá nhà cho các vùng có tỷ lệ người có thu nhập thấp lớn hơn hoặc bằng 9.725% được dự đoán bằng giá trị trung bình của các quan sát trong dữ liệu huấn luyện và bằng 17.34 nghìn USD.\nNhững vùng có \\(lstat \\geq 9.725(\\%)\\) được chỉ định vào nhánh bên trái và sau đó lại được chia nhỏ hơn thành hai nhánh, nhánh bên phải là các quan sát có \\(lstat < 4.65(\\%)\\) và nhánh bên phải là các quan sát có \\(lstat \\geq 4.65(\\%)\\). Giá nhà được dự đoán tại các vùng có \\(lstat < 4.65(\\%)\\) là giá trị trung bình của các ngôi nhà trong dữ liệu huấn luyện mô hình có tính chất tương ứng và bằng 39.72 nghìn USD. Tương tự, tại các vùng có \\(lstat\\) nhận giá trị từ 4.65% đến 9.725% giá nhà được dự đoán là 26.65 nghìn USD. Ba tập con riêng biệt của dữ liệu về giá nhà ở Boston được phân tách theo cây quyết định dựa trên biến \\(lstat\\) và giá trị ước lượng tương ứng cho giá nhà được tổng kết lại như sau:\n\\[\\begin{align}\nR_1 &= \\{X \\ | lstat < 4.65\\} \\rightarrow \\hat{y} = 39.72 \\\\\nR_2 &= \\{X \\ | lstat \\geq 4,65 \\ \\ \\& \\ \\ lstat < 9.725 \\} \\rightarrow \\hat{y} = 26.65  \\\\\nR_3 &= \\{X \\ | lstat \\geq 9.725 \\} \\rightarrow \\hat{y} = 17.34\n\\tag{15.1}\n\\end{align}\\]\nHình 15.2: Hàm f được ước lượng từ mô hình cây quyết định có dạng hàm bậc thang nhận giá trị bằng hằng số trên các miền giá trị được phân tách của biến giải thích.\nHình 15.2 mô tả hàm số được ước lượng theo cây quyết định khi có một biến giải thích duy nhất. Bạn đọc có thể thấy rằng trong trường hợp chỉ có một biến, cây quyết định cũng giống như các hồi quy theo đa thức theo từng đoạn, với bậc của các đa thức là bằng 0. Nói cách khác, hàm \\(f\\) là hằng số trên các khoảng giá trị khác nhau của biến giải thích. Các nút chia dữ liệu ra thành các miền con là 9.725 và 4.65 được tính toán sao cho sai số (RSS) trên dữ liệu huấn luyện mô hình là nhỏ nhất. Chúng ta sẽ thảo luận chi tiết về ước lượng mô hình cây quyết định ở phần tiếp theo của chương.\nĐiều gì xảy ra nếu chúng ta sử dụng đồng thời hai biến là biến \\(lstat\\) và biến \\(rm\\). Biến \\(rm\\) cho biết trung bình trong một ngôi nhà ở vùng quan sát có bao nhiêu phòng. Hình 15.3 mô tả một cây quyết định hồi quy khi sử dụng đồng thời hai biến \\(lstat\\) biến \\(rm\\)\nHình 15.3: Cây quyết định hồi quy kích thước bằng ba mô tả giá nhà phụ thuộc vào tỷ lệ người sống dưới mức trung bình và số lượng phòng trung bình của mỗi ngôi nhà trên dữ liệu Boston. Điểm phân nhánh (nút) đầu tiên phụ thuộc vào biến rm. Nút thứ hai phụ thuộc vào biến lstat.\nBạn đọc có thể nhận thấy sự khác biệt giữa mô hình cây quyết định có một biến giải thích trong Hình 15.1 và mô hình cây quyết định có hai biến giải thích 15.3. Khi có hai biến giải thích, nút đầu tiên được sử dụng để phân tách dữ liệu là \\(rm < 6.941\\) và nút thứ hai sử dụng để phân tách dữ liệu là \\(lstat < 14.4\\). Tuy nhiên, cần lưu ý rằng nút phân tách thứ hai không được thực hiện trên toàn bộ dữ liệu, mà chỉ được thực hiện trên miền bên trái của nút \\(rm < 6.941\\). Như vậy, ba miền dữ liệu được định nghĩa theo cây quyết định này và giá trị ước lượng tương ứng cho giá nhà có thể được tóm tắt như sau\n\\[\\begin{align}\nR_1 &= \\{X \\ | rm < 6.941 \\ \\ \\& \\ \\  lstat < 14.4 \\} \\rightarrow \\hat{y} = 23.35 \\\\\nR_2 &= \\{X \\ | rm < 6.941 \\ \\ \\& \\ \\ lstat \\geq 14.4  \\} \\rightarrow \\hat{y} = 14.96  \\\\\nR_3 &= \\{X \\ | rm \\geq 6.941 \\} \\rightarrow \\hat{y} = 37.24\n\\tag{15.2}\n\\end{align}\\]\nHình 15.4: Mô hình cây quyết định hồi quy chia hình chữ nhật thành ba phần R1, R2, và R3 và giá nhà trong mỗi miền là hằng số\nMô hình cây quyết định hồi quy được mô tả trong các hình 15.3, 15.4, hoặc phương trình (15.1) có thể được giải thích rất dễ dàng như sau: ở những vùng có số phòng trung bình lớn hơn 6.941 (miền R3) giá nhà được dự đoán là 37.24 nghìn USD, ở những vùng số phòng trung bình nhỏ hơn 6.941 và tỷ lệ số người thu nhập thấp dưới 14.4(%) (miền R1) thì giá nhà được dự đoán là 23.35 nghìn USD, và cuối cung ở những vùng số phòng trung bình nhỏ hơn 6.941 và tỷ lệ số người thu nhập thấp lớn hơn 14.4(%) (miền R2) thì giá nhà được dự đoán là 14.96 nghìn USD. Bạn đọc có thể thấy rằng các mô hình dạng cây quyết định là rất dễ dàng để giải thích, thậm chí còn dễ dàng hơn với các mô hình hồi quy tuyến tính.Một cách tổng quát, có thể tóm tắt lại quá trình xây dựng một cây quyết định hồi quy để mô tả mối liên hệ giữa biến mục tiêu \\(Y\\) và các \\(p\\) biến giải thích \\(X_1\\), \\(X_2\\), \\(\\cdots\\), \\(X_p\\) như sau:Thứ nhất: Chúng ta chia không gian các giá trị có thể có của các biến giải thích thành \\(J\\) các vùng riêng biệt và không chồng lấn lên nhau, tạm gọi là \\(R_1\\), \\(R_2\\), \\(\\cdots\\) , \\(R_K\\).Thứ hai: Đối với tất cả quan sát rơi vào vùng \\(R_k\\), với \\(1 \\leq k \\leq K\\), chúng ta đưa ra dự đoán giống nhau là giá trị trung bình của các giá trị của biến mục tiêu \\(Y\\) của các quan sát của dữ liệu huấn luyện nằm trong vùng \\(R_k\\). Ví dụ: giả sử ở bước thứ nhất, chúng ta có hai vùng, \\(R_1\\) và \\(R_2\\), và trung bình của biến mục tiêu của các quan sát trong dữ liệu huấn luyện ở vùng \\(R_1\\) là 5, trong khi trung bình của biến mục tiêu của các quan sát trong dữ liệu huấn luyện ở vùng \\(R_2\\) là 10. Khi đó, đối với một quan sát bất kỳ \\(X = x\\), nếu \\(x \\R_1\\) chúng ta sẽ dự đoán biến mục tiêu là 5 và nếu \\(x \\R_2\\) chúng ta sẽ dự đoán biến mục tiêu là 10.","code":""},{"path":"mô-hình-cây-quyết-định.html","id":"ước-lượng-tham-số-mô-hình-cây-quyết-định","chapter":"Chương 15 Mô hình cây quyết định","heading":"15.1.1.2 Ước lượng tham số mô hình cây quyết định","text":"Về lý thuyết, ước lượng tham số cho mô hình cây quyết định là quá trình tìm cách xây dựng các vùng \\(R_1\\), \\(R_2\\), \\(\\cdots\\), \\(R_K\\)? Các vùng này có thể có hình dạng bất kỳ miễn là các vùng không có chồng lấn và hợp của các vùng là miền giá trị của các biến giải thích. Tuy nhiên, nếu lựa chọn chia không gian các biến giải thích thành vùng có dạng hình chữ nhật nhiều chiều thì mô hình sẽ dễ diễn giải và ước lượng hơn rất nhiều. Một vùng hình chữ nhật nhiều chiều có thể được hiểu là một vùng mà mỗi biến giải thích \\(X_j\\) chỉ bị giới hạn bởi hai giá trị đầu mút là \\(a_j\\) và \\(b_j\\) và không chịu tác động từ các biến giải thích khác:\n\\[\\begin{align}\n\\cup_{j=1}^p \\{X_j \\[a_j, b_j] \\}\n\\end{align}\\]Với \\(R_k\\) là các hình chữ nhật nhiều chiều, chúng ta cần tìm cách phân chia miền giá trị của các biến giải thích ra thành \\(K\\) hình chữ nhật như vậy với mục tiêu là tối thiểu hóa sai số RSS. Lưu ý rằng RSS được tính dựa trên dự báo cho mỗi miền \\(R_k\\) bằng \\(\\hat{y}_{R_k}\\) là giá trị trung bình của biến mục tiêu trong miền này. Tuy nhiên khó khăn thực tế là không thể tính toán được hết mọi phân vùng có thể có của không gian các biến giải thích. Các tiếp cận khả thi để tìm kiếm phân vùng theo cách phân tách nhị phân từ trên xuống:Tại bước thứ nhất: chúng ta tìm cách chia toàn bộ không gian biến giải thích thành hai hình chữ nhật sao cho sai số RSS trên dữ liệu huấn luyện mô hình là nhỏ nhất.Tại bước thứ nhất: chúng ta tìm cách chia toàn bộ không gian biến giải thích thành hai hình chữ nhật sao cho sai số RSS trên dữ liệu huấn luyện mô hình là nhỏ nhất.Tại bước thứ hai: chúng ta tìm cách chia một trong hai hình chữ nhật thu được trong bước thứ nhất sao cho sai số RSS trên dữ liệu huấn luyện mô hình là nhỏ nhất.Tại bước thứ hai: chúng ta tìm cách chia một trong hai hình chữ nhật thu được trong bước thứ nhất sao cho sai số RSS trên dữ liệu huấn luyện mô hình là nhỏ nhất.……Tại bước thứ \\(K\\): chúng ta tìm cách chia một trong \\(K-1\\) hình chữ nhật thu được trong bước thứ \\(K-1\\) thành hai hình chữ nhật con sao cho sai số RSS tính từ \\(K\\) hình chữ nhật là nhỏ nhất.Tại bước thứ \\(K\\): chúng ta tìm cách chia một trong \\(K-1\\) hình chữ nhật thu được trong bước thứ \\(K-1\\) thành hai hình chữ nhật con sao cho sai số RSS tính từ \\(K\\) hình chữ nhật là nhỏ nhất.Đây là quá trình tìm kiếm \\(tham\\) \\(lam\\) bởi vì tại mỗi bước của quá trình xây dựng cây quyết định, chúng ta luôn tìm kiếm sự phân tách tốt nhất có thể dựa trên kết quả của bước tốt nhất trước đó. Cách tìm kiếm này là khả thi nhưng không chắc chắn rằng chúng ta có thể tìm được phân vùng tối ưu. Trong một hình hộp bất kỳ, để thực hiện phân tách thành hai phần, chúng ta cần lựa chọn biến giải thích \\(X_j\\) và điểm cắt \\(s\\) nằm trong miền giá trị của \\(X_j\\) sao cho khi chia hình chữ nhật thành hai vùng: vùng các biến giải thích có \\(X_j < s\\) và vùng các biến giải thích \\(X_j \\leq s\\)\n\\[\\begin{align}\nR_1(X_j, s) = \\{\\textbf{X} | X_j < s \\} \\\\\nR_2(X_j, s) = \\{\\textbf{X} | X_j \\leq s \\}\n\\tag{15.3}\n\\end{align}\\]\nsau đó chúng ta cần tìm \\(j\\) và \\(s\\) sao cho tổng bình phương sai số là nhỏ nhất:\n\\[\\begin{align}\n\\sum\\limits_{x_i \\R_1} (y_i - \\hat{y}_{R_1})^2 + \\sum\\limits_{x_i \\R_2} (y_i - \\hat{y}_{R_2})^2\n\\tag{15.4}\n\\end{align}\\]\ntrong đó \\(\\hat{y}_{R_1}\\) là giá trị trung bình của biến mục tiêu trong miền \\(R_1\\) và \\(\\hat{y}_{R_2}\\) là giá trị trung bình của biến mục tiêu trong miền \\(R_2\\). Quá trình tìm các giá trị của \\(j\\) và \\(s\\) để tối thiểu hóa (15.4) có thể được thực hiện khá nhanh, đặc biệt khi số lượng biến giải thích \\(p\\) không quá lớn. Chúng ta sẽ lặp lại quy trình tìm kiếm biến giải thích tốt nhất và điểm cắt tốt nhất trên biến giải thích đó để tiếp tục phân tách dữ liệu và để giảm RSS. Quá trình tiếp tục cho đến khi cây quyết định đạt đến một tiêu chí dừng nào đó. Chúng ta cần đặt ra các tiêu chí dừng bởi vì nếu tiếp tục quá trình phân tách dữ liệu cho đến \\(n\\) bước với \\(n\\) là số quan sát trong dữ liệu huấn luyện mô hình thì mô hình cây quyết định sẽ chia dữ liệu ra thành \\(n\\) vùng và mỗi vùng tương ứng với một quan sát. Một cây quyết định như vậy có sai số trên huấn luyện mô hình bằng 0 nhưng không có ý nghĩa trong diễn giải hoặc dự đoán biến mục tiêu. Các tiêu chí dừng thường được sử dụng thường là số lượng dữ liệu nằm trong một vùng không được phép ít hơn một số nào đó. Nếu mọi sự phân tách đều dẫn đến việc số lượng điểm dữ liệu trong một lá nhỏ hơn một ngưỡng, thường là 5 hoặc 10 điểm dữ liệu, thì chúng ta nên dừng quá trình phân tách.\nHình 15.5: Cây quyết định hồi quy kích thước bằng 4, mô tả giá nhà phụ thuộc vào tỷ lệ người sống dưới mức trung bình và số lượng phòng trung bình của mỗi ngôi nhà trên dữ liệu Boston. với cây quyết định có 3 lá ở trên, vùng R3 đã được chia thành hai phần là R3-1 và R3-2. Các vùng R1 và R2 không thay đổi\n\nHình 15.6: Cây quyết định hồi quy kích thước bằng 5, mô tả giá nhà phụ thuộc vào tỷ lệ người sống dưới mức trung bình và số lượng phòng trung bình của mỗi ngôi nhà trên dữ liệu Boston. với cây quyết định có 4 lá ở trên, vùng R1 đã được chia thành hai phần là R1-1 và R1-2. Các vùng R2, R3-1, và R3-2 không thay đổi\nCác Hình 15.5 và 15.6 mô tả các bước thứ tư và bước thứ năm trong xây dựng mô hình cây quyết định mô tả biến giá nhà (\\(medv\\)) phụ thuộc vào hai biến \\(lstat\\) và \\(rm\\). Để xây dựng mô hình cây quyết định có 4 lá, từ ba vùng \\(R_1\\), \\(R_2\\), và \\(R_3\\) thu được trong Hình 15.3, có sáu lựa chọn để chia không gian các biến giải thích thành hai vùng (có 3 vùng và mỗi vùng có hai lựa chọn để phân chia theo một trong hai biến là \\(rm\\) hoặc \\(lstat\\)). Trong số các lựa chọn đó, phân chia vùng \\(R3\\) theo biến \\(rm\\) tại điểm cắt \\(rm = 7.437\\) là cách phân chia làm cho giá trị RSS là nhỏ nhất. Kết quả thu được là một cây quyết định có bốn lá tương ứng với bốn vùng là \\(R_1\\), \\(R_2\\), \\(R_3-1\\), và \\(R_3-2\\).Quá trình ước lượng cây quyết định có năm lá cũng diễn ra tương tự. Từ bốn vùng thu được từ bước trước, chúng ta có 8 lựa chọn để tiếp tục phân chia dữ liệu thành năm vùng (4 vùng và 2 biến giải thích). Trong số các lựa chọn đó, phân chia vùng \\(R_1\\) thành hai vùng theo biến \\(lstat\\) tại điểm 4.91 là phân chia làm cho RSS giảm đi nhiều nhất. Kết quả thu được là một cây quyết định có năm lá tương ứng với năm vùng \\(R_1-1\\), \\(R_1-2\\), \\(R_2\\), \\(R_3-1\\), và \\(R_3-2\\).Như chúng tôi đã đề cập ở trên, nếu chúng ta tiếp tục quá trình phân chia dữ liệu sẽ thu được một cây quyết định đủ lớn để có thể nội suy lại chính xác biến mục tiêu. Kể cả khi chúng ta đặt ra các tiêu chí dừng, các cây quyết định có kích thước lớn thường gặp phải hiện thượng overfitting. đó, tham số kích thước của cây quyết định \\(K\\) thường được lựa chọn dựa trên xác thực chéo. Các cây quyết định có ưu điểm là đơn giản và không tốn nguồn lực tính toán nhiều, đó xác thực chéo hoàn toàn có thể thực hiện được trong đa số các trường hợp.\nHình 15.7: Mô hình cây quyết định biến medv phụ thuộc vào lstat và rm trên dữ liệu Boston. Kích thước cây quyết định được lựa chọn dựa trên xác thực chéo. Số lượng lá cho sai số xác thực chéo nhỏ nhất là 9\nTrước khi chuyển sang phần cây quyết định phân loại, chúng tôi sẽ thảo luận về lựa chọn điểm cắt phù hợp để tách dữ liệu thành hai phần. Chúng ta quay trở lại với mô hình cây quyết định với một biến giải thích duy nhất là \\(lstat\\) trong Hình 15.1, khi mà điểm cắt đầu tiên được ước lượng là 9.725%. Nguyên tắc ước lượng ra điểm cắt này như sau: với biến giải thích \\(X_j\\) là biến liên tục, chúng ta loại bỏ các quan sát của biến \\(X_j\\) bị trùng lặp và sắp xếp lại các quan sát này theo thứ tự tăng dần, chẳng hạn như \\(x_{1j}\\) < \\(x_{2j}\\) < \\(\\cdots\\) < \\(x_{nj}\\). Khi đó, có \\(n-1\\) điểm cắt cần được tính toán tổng sai số bình phương (RSS) là các điểm\n\\[\\begin{align}\n\\cfrac{x_{1j} + x_{2j}}{2}, \\cfrac{x_{2j} + x_{3j}}{2}, \\cdots, \\cfrac{x_{(n-1)j} + x_{nj}}{2}\n\\end{align}\\]\nvà điểm cắt tối ưu là điểm cắt có RSS nhỏ nhất. Bạn đọc có thể thấy rằng điểm cắt 9.725 là giá trị trung bình của hai giá trị quan sát được của biến \\(medv\\) trong dữ liệu là điểm 9.71 và điểm 9.74.Nếu biến giải thích \\(X_j\\) là biến dạng rời rạc hoặc biến định tính với có thể nhận \\(k\\) giá trị khác nhau. Khi đó chúng ta xắp xếp \\(k\\) giá trị đó theo thứ tự mà giá trị trung bình của biến mục tiêu tính trên nhóm đó tăng dần. Chẳng hạn như \\(k\\) giá trị của \\(X_j\\) được xắp xếp theo thứ tự là \\(c_{1} \\rightarrow c_{2} \\rightarrow \\cdots \\rightarrow c_k\\), nghĩa là giá trị trung bình của biến mục tiêu trong miền \\(X_j = c_i\\) nhỏ hơn giá trị trung bình của biến mục tiêu trong miền \\(X_j = c_{+1}\\). Khi đó, có \\(k-1\\) cách phân chia dữ liệu cần được cân nhắc\n\\[\\begin{align}\n&\\text{Cách 1. Vùng 1: } X_j = c_1 \\text{ và vùng 2: } X_j \\\\{c_2, c_3, \\cdots, c_k\\} \\\\\n&\\text{Cách 2. Vùng 1: } X_j \\\\{c_1, c_2\\} \\text{ và vùng 2: } X_j \\\\{c_3, c_4, \\cdots, c_k\\} \\\\\n&\\cdots \\\\\n&\\text{Cách (k-1). Vùng 1: } X_j \\\\{c_1, c_2, \\cdots, c_{k-1}\\} \\text{ và vùng 2: } X_j = c_k\n\\end{align}\\]\nvà lựa chọn phân vùng tối ưu là lựa chọn phân vùng có RSS nhỏ nhất. Ví dụ, chúng ta xây dựng mô hình cây quyết định trong đó biến mục tiêu \\(medv\\) phụ thuộc vào biến \\(rad\\) là biến rời rạc mô tả khả năng kết nối với đường cao tốc của vùng. Biến \\(rad\\) trong dữ liệu Boston có 9 giá trị riêng biệt là các số tự nhiên từ 1 đến 8 và số 24. Lưu ý rằng đây là biến rời rạc danh nghĩa không có ý nghĩa sánh giữa các giá trị với nhau. Giá trị trung bình của biến mục tiêu theo các giá trị riêng biệt của \\(rad\\) được cho trong bảng 15.1\nBảng 15.1: Giá nhà trung bình tính theo các giá trị riêng biệt của biến rời rạc rad\nDựa theo tính toán từ bảng 15.1, có 8 cách phân vùng có thể khi sử dụng biến \\(rad\\) để phân vùng được liệt kê như sau\n\\[\\begin{align}\n&\\text{Cách 1. Vùng 1: } rad = 24 \\text{ và vùng 2: } rad \\\\{6, 4, 1, 5, 2, 7, 3, 8 \\} \\\\\n&\\text{Cách 2. Vùng 1: } rad \\\\{24, 6 \\} \\text{ và vùng 2: } rad \\\\{4, 1, 5, 2, 7, 3, 8\\} \\\\\n&\\text{Cách 3. Vùng 1: } rad \\\\{24, 6, 4 \\} \\text{ và vùng 2: } rad \\\\{1, 5, 2, 7, 3, 8\\} \\\\\n&\\text{Cách 4. Vùng 1: } rad \\\\{24, 6, 4, 1 \\} \\text{ và vùng 2: } rad \\\\{5, 2, 7, 3, 8\\} \\\\\n&\\text{Cách 5. Vùng 1: } rad \\\\{24, 6, 4, 1, 5 \\} \\text{ và vùng 2: } rad \\\\{2, 7, 3, 8\\} \\\\\n&\\text{Cách 6. Vùng 1: } rad \\\\{24, 6, 4, 1, 5, 2 \\} \\text{ và vùng 2: } rad \\\\{7, 3, 8\\} \\\\\n&\\text{Cách 7. Vùng 1: } rad \\\\{24, 6, 4, 1, 5, 2, 7 \\} \\text{ và vùng 2: } rad \\\\{3, 8\\} \\\\\n&\\text{Cách 8. Vùng 1: } rad \\\\{24, 6, 4, 6, 4, 1, 5, 2, 7 \\} \\text{ và vùng 2: } rad = 8\n\\end{align}\\]\nvà cách phân vùng cho RSS nhỏ nhất sẽ là phân vùng được lựa chọn. Hình 15.8 mô tả cây quyết định có hai lá trong đó biến mục tiêu là \\(medv\\) và biến giải thích là biến \\(rad\\).\nHình 15.8: Cây quyết định hồi quy kích thước bằng hai mô tả giá nhà phụ thuộc vào biến rời rạc là khả năng kết nối của ngôi nhà đến đường cao tốc trên dữ liệu Boston.\nNhư vậy phân vùng có RSS nhỏ nhất là cách phân vùng thứ 3. Các giá trị rời rạc 24, 4, 6 của biến \\(rad\\) được cho vào nhánh bên trái cây quyết định với dự đoán cho giá nhà là 18.89 nghìn USD trong khi các giá trị rời rạc còn lại được cho vào nhánh bên phải của cây quyết định với dự đoán cho giá nhà là 26.63 nghìn USD","code":""},{"path":"mô-hình-cây-quyết-định.html","id":"cây-quyết-định-phân-loại","chapter":"Chương 15 Mô hình cây quyết định","heading":"15.1.2 Cây quyết định phân loại","text":"Cây quyết định có lợi thế là có thể sử dụng cho cả bài toán hồi quy và bài toán phân loại mà không cần có sự biến đổi đáng kể nào về mặt mô hình. Thay đổi duy nhất để cây quyết định phù hợp với bài toán phân loại đó là thay đổi hàm tổn thất từ RSS sang các hàm tổn thất phù hợp với bài toán phân loại. RSS không phù hợp trong bài toán phân loại vì giá trị dự đoán chúng ta đưa ra tại mỗi lá của cây quyết định là giá trị xuất hiện với tần suất lớn nhất chứ không phải giá trị trung bình của các biến nằm trong lá đó.Quá trình xây dựng cây quyết định phân loại hoàn toàn tương tự như quá trình xây dựng cây quyết định hồi quy. Chúng ta sử dụng chuỗi các phân tách nhị phân để phân vùng không gian giá trị các biến giải thích thành \\(k\\) vùng \\(R_1, R_2, \\cdots, R_K\\) không chồng lấn lên nhau và ở mỗi vùng \\(R_k\\) chúng ta đưa ra một dự đoán \\(\\hat{y}_{R_k}\\) cho biến mục tiêu. Trong mô hình hồi quy giá trị dự đoán là giá trị trung bình của biến mục tiêu tương ứng với các quan sát nằm trong vùng \\(R_k\\) còn trong bài toán phân loại giá trị dự đoán \\(\\hat{y}_{R_k}\\) là giá trị của biến mục tiêu xuất hiện với tần suất lớn nhất tương ứng với các quan sát trong vùng \\(R_k\\).Như đã trình bày trong cây phân loại hồi quy, để thực hiện phân tách một vùng thành hai phần, chúng ta cần lựa chọn biến giải thích \\(X_j\\) và điểm cắt \\(s\\) nằm trong miền giá trị của \\(X_j\\) sao cho khi chia hình chữ nhật thành hai vùng: vùng các biến giải thích có \\(X_j < s\\) và vùng các biến giải thích \\(X_j \\leq s\\)\n\\[\\begin{align}\nR_1(X_j, s) = \\{\\textbf{X} | X_j < s \\} \\\\\nR_2(X_j, s) = \\{\\textbf{X} | X_j \\leq s \\}\n\\end{align}\\]\nvới các dự đoán cho biến mục tiêu tương ứng với hai vùng là \\(\\hat{y}_{R_1}\\) và \\(\\hat{y}_{R_2}\\), chúng ta cần tìm \\(j\\) và \\(s\\) để tối thiểu hóa sai số của bài toán phân loại. Để lượng hóa sai số của bài toán phân loại, có nhiều cách tiếp cận. Cách tiếp cận tự nhiên và đơn giản nhất là sử dụng \\(tỷ\\) \\(lệ\\) \\(dự\\) \\(đoán\\) \\(sai\\) (còn được gọi là classification error rate hay viết tắt là CER). Cho một véc-tơ \\(\\textbf{y}\\) có độ dài \\(n\\) chỉ nhận các giá trị rời rạc là \\(1, 2, \\cdots, m\\) và tần suất xuất hiện của các giá trị rời rạc này lần lượt là \\(n_1\\), \\(n_2\\), \\(\\cdots\\), \\(n_m\\). Nếu \\(\\textbf{y}\\) là các giá trị của biến mục tiêu quan sát được trong một vùng của biến giải thích thì giá trị dự đoán cho biến mục tiêu sẽ là \\(h \\\\{1, 2, \\cdots, m \\}\\) sao cho \\(n_h = max(n_1, n_2, \\cdots, n_m)\\). Tỷ lệ dự đoán sai trên véc-tơ \\(\\textbf{y}\\) được tính như sau\n\\[\\begin{align}\nCER(\\textbf{y}) = 1 - \\cfrac{n_h}{n}\n\\tag{15.5}\n\\end{align}\\]\nCó thể thấy rằng CER là tỷ lệ số dự đoán sai trên véc-tơ \\(\\textbf{y}\\) khi sử dụng đoán là \\(h = mode(\\textbf{y})\\). CER có ưu điểm là dễ hiểu và đơn giản, tuy nhiên nhược điểm lớn nhất của CER là chỉ tính đến giá trị xuất hiện với tần suất nhiều nhất mà không tính đến các giá trị khác, đó chỉ tiêu này không phù hợp khi sử dụng để phân vùng các cây phân loại, đặc biệt là trong trường hợp biến giải thích nhận nhiều hơn 2 giá trị.Hai chỉ số khác thường xuyên được sử dụng thay thế cho nhau để tìm ra phân vùng tối ưu trong cây quyết định phân loại là chỉ số Gini và chỉ số Entropy. Hai chỉ số này có ưu điểm là tính toán đến sự xuất hiện của tất cả các giá trị có xuất hiện trong véc-tơ \\(\\textbf{y}\\)\n\\[\\begin{align}\nGini(\\textbf{y}) &= \\sum\\limits_{l = 1}^m \\ \\cfrac{n_l}{n} \\ \\left( 1 - \\cfrac{n_l}{n}\\right) \\\\\nEntropy(\\textbf{y}) &= - \\sum\\limits_{l = 1}^m \\ \\cfrac{n_l}{n} \\ \\log\\left(\\cfrac{n_l}{n}\\right)\n\\tag{15.6}\n\\end{align}\\]\nCác chỉ số Gini và Entropy còn được gọi là các thước đo độ thuần (purity) của véc-tơ \\(\\textbf{y}\\). \\(n_l/n\\) nằm trong đoạn \\([0,1]\\) và tổng các tần suất \\(n_l/n\\) bằng 1 nên các chỉ số Gini và Entropy sẽ có giá trị nhỏ khi tồn tại một giá trị \\(n_l/n\\) xấp xỉ 1 và các giá trị còn lại xấp xỉ 0. Nếu cố định \\(n_h/n\\) với \\(h\\) là giá trị xuất hiện nhiều nhất trong \\(y\\) và thay đổi các \\(n_l\\) khác thì CER sẽ không thay đổi trong khi Gini và Entropy sẽ thay đổi. Đây là lý tại sao chỉ số Gini và Entropy sẽ phù hợp hơn khi đo lường độ thuần của một véc-tơ.Để thấy được sự phù hợp của chỉ số Gini và chỉ số Entropy trong lựa chọn phân vùng giá trị của biến giải thích, hãy quan sát ví dụ trong Hình 15.9\nHình 15.9: Ví dụ về hai lựa chọn phân vùng khác nhau cho véc-tơ biến mục tiêu bao gồm ba giá trị riêng biệt là 1, 2, và 3 với tần suất xuất hiện là 6 lần, 3 lần và 3 lần. Nếu sử dụng chỉ số tỷ lệ dự đoán sai thì hai cách phân vùng là không có sự khác biệt. Nếu sử dụng chỉ số Entropy hoặc Gini thì cách phân vùng thứ 2 tốt hơn\nGiả sử chúng ta phải đưa ra lựa chọn đâu là phân vùng tốt hơn giữa hai cách phân vùng được mô tả trong hình 15.9. Một cách trực giác, bạn đọc có thể nhận thấy rằng cách phân vùng thứ hai sẽ dẫn đến một cây phân loại tốt hơn vì đã tách biệt được hoàn toàn giá trị 2 và giá trị 3 vào hai vùng. Với cách phân vùng như vậy, nếu chúng ta tiếp tục thực hiện phân vùng thì nhiều khả năng trong bước tiếp theo chúng ta sẽ phân loại được hoàn toàn giá trị 1 và giá trị 2 trong vùng \\(R_1\\) và phân loại được hoàn toàn giá trị 1 và giá trị 3 trong vùng \\(R_2\\). Trái lại, trong cách phân vùng thứ nhất, không thể phân tách hai giá trị 2 và 3 ra thành các vùng riêng biệt. Nếu chúng ta tiếp tục phát triển thêm 1 lá cho cây quyết định như trong cách thứ nhất, không thể thu được phân loại chính xác cho cả ba giá trị của biến mục tiêu.Bạn đọc có thể sử dụng các chỉ số sai số phân loại, Gini và Entropy để sánh hai cách phân vùng như sau:Cho cách phân vùng thứ nhất:\n\\[\\begin{align}\nCER & = \\cfrac{6}{12} \\times \\cfrac{3}{6} + \\cfrac{6}{12} \\times \\cfrac{3}{6} = 0.5 \\\\\nGini & = \\cfrac{6}{12} \\times \\left[\\cfrac{3}{6}\\left(1 - \\cfrac{3}{6}\\right) + \\cfrac{2}{6}\\left(1 - \\cfrac{2}{6}\\right) + \\cfrac{1}{6}\\left(1 - \\cfrac{1}{6}\\right) \\right] + \\\\\n& \\cfrac{6}{12} \\times \\left[\\cfrac{3}{6}\\left(1 - \\cfrac{3}{6}\\right) + \\cfrac{1}{6}\\left(1 - \\cfrac{1}{6}\\right) + \\cfrac{2}{6}\\left(1 - \\cfrac{2}{6}\\right) \\right] = 0.61\\\\\nEntropy & = - \\cfrac{6}{12} \\times \\left[\\cfrac{3}{6} \\log\\left(\\cfrac{3}{6}\\right) + \\cfrac{2}{6}\\log\\left(\\cfrac{2}{6}\\right) + \\cfrac{1}{6}\\log\\left(1 - \\cfrac{1}{6}\\right) \\right] + \\\\\n& \\cfrac{6}{12} \\times \\left[\\cfrac{3}{6}\\log\\left(\\cfrac{3}{6}\\right) + \\cfrac{1}{6}\\log\\left(\\cfrac{1}{6}\\right) + \\cfrac{2}{6}\\log\\left( \\cfrac{2}{6}\\right) \\right] = 1.01\n\\end{align}\\]Cho cách phân vùng thứ nhất:\n\\[\\begin{align}\nCER & = \\cfrac{6}{12} \\times \\cfrac{3}{6} + \\cfrac{6}{12} \\times \\cfrac{3}{6} = 0.5 \\\\\nGini & = \\cfrac{6}{12} \\times \\left[\\cfrac{3}{6}\\left(1 - \\cfrac{3}{6}\\right) + \\cfrac{2}{6}\\left(1 - \\cfrac{2}{6}\\right) + \\cfrac{1}{6}\\left(1 - \\cfrac{1}{6}\\right) \\right] + \\\\\n& \\cfrac{6}{12} \\times \\left[\\cfrac{3}{6}\\left(1 - \\cfrac{3}{6}\\right) + \\cfrac{1}{6}\\left(1 - \\cfrac{1}{6}\\right) + \\cfrac{2}{6}\\left(1 - \\cfrac{2}{6}\\right) \\right] = 0.61\\\\\nEntropy & = - \\cfrac{6}{12} \\times \\left[\\cfrac{3}{6} \\log\\left(\\cfrac{3}{6}\\right) + \\cfrac{2}{6}\\log\\left(\\cfrac{2}{6}\\right) + \\cfrac{1}{6}\\log\\left(1 - \\cfrac{1}{6}\\right) \\right] + \\\\\n& \\cfrac{6}{12} \\times \\left[\\cfrac{3}{6}\\log\\left(\\cfrac{3}{6}\\right) + \\cfrac{1}{6}\\log\\left(\\cfrac{1}{6}\\right) + \\cfrac{2}{6}\\log\\left( \\cfrac{2}{6}\\right) \\right] = 1.01\n\\end{align}\\]Cho cách phân vùng thứ hai\n\\[\\begin{align}\nCER & = \\cfrac{6}{12} \\times \\cfrac{3}{6} + \\cfrac{6}{12} \\times \\cfrac{3}{6} = 0.5 \\\\\nGini & = \\cfrac{6}{12} \\times \\left[\\cfrac{3}{6}\\left(1 - \\cfrac{3}{6}\\right) + \\cfrac{3}{6}\\left(1 - \\cfrac{3}{6}\\right)\\right] + \\\\\n& \\cfrac{6}{12} \\times \\left[\\cfrac{3}{6}\\left(1 - \\cfrac{3}{6}\\right) + \\cfrac{3}{6}\\left(1 - \\cfrac{3}{6}\\right) \\right] = 0.5\\\\\nEntropy & = - \\cfrac{6}{12} \\times \\left[\\cfrac{3}{6} \\log\\left(\\cfrac{3}{6}\\right) + \\cfrac{3}{6} \\log\\left(\\cfrac{3}{6}\\right) \\right] + \\\\\n& \\cfrac{6}{12} \\times \\left[\\cfrac{3}{6} \\log\\left(\\cfrac{3}{6}\\right) + \\cfrac{3}{6} \\log\\left(\\cfrac{3}{6}\\right) \\right] = 0.69\n\\end{align}\\]Cho cách phân vùng thứ hai\n\\[\\begin{align}\nCER & = \\cfrac{6}{12} \\times \\cfrac{3}{6} + \\cfrac{6}{12} \\times \\cfrac{3}{6} = 0.5 \\\\\nGini & = \\cfrac{6}{12} \\times \\left[\\cfrac{3}{6}\\left(1 - \\cfrac{3}{6}\\right) + \\cfrac{3}{6}\\left(1 - \\cfrac{3}{6}\\right)\\right] + \\\\\n& \\cfrac{6}{12} \\times \\left[\\cfrac{3}{6}\\left(1 - \\cfrac{3}{6}\\right) + \\cfrac{3}{6}\\left(1 - \\cfrac{3}{6}\\right) \\right] = 0.5\\\\\nEntropy & = - \\cfrac{6}{12} \\times \\left[\\cfrac{3}{6} \\log\\left(\\cfrac{3}{6}\\right) + \\cfrac{3}{6} \\log\\left(\\cfrac{3}{6}\\right) \\right] + \\\\\n& \\cfrac{6}{12} \\times \\left[\\cfrac{3}{6} \\log\\left(\\cfrac{3}{6}\\right) + \\cfrac{3}{6} \\log\\left(\\cfrac{3}{6}\\right) \\right] = 0.69\n\\end{align}\\]Bạn đọc có thể thấy rằng chỉ số CER trong hai cách phân vùng bằng nhau và bằng 0.5, nghĩa là CER không có khả năng phân biệt giữa cách phân vùng thứ nhất và cách phân vùng thứ hai. Chỉ số Gini và Entropy của phân vùng thứ hai đều nhỏ hơn cho với cách phân vùng thứ nhất, điều này cho thấy hai chỉ số này có khả năng phân biệt cách phân vùng tốt hơn với CER.Hình 15.10 mô tả cây quyết định được xây dựng trên dữ liệu OJ, dữ liệu chứa thông tin về 1070 lần khách hàng mua một trong hai loại sản phẩm nước cam Citrus Hill hoặc Minute Maid, với biến mục tiêu là \\(Purchase\\) chứa một trong hai giá trị là \\(CH\\) hoặc \\(MM\\) cho biết sản phẩm được mua tương ứng là Citrus Hill hay Minute Maid. Dữ liệu có 17 biến giải thích bao gồm các biến có nhiều khả năng cho ý nghĩa quan trọng trong xây dựng mô hình như sự khác biệt về giá của hai loại sản phẩm (biến \\(PriceDiff\\)) cho biết giá của \\(MM\\) cao hơn giá của \\(CH\\) như thế nào, hoặc biến \\(LoyalCH\\) là biến đo sự trung thành của khách hàng với nhãn hiệu Citrus Hill. Sự trung thành của khách hàng là thước đo sự mua hàng lặp lại trên một nhãn hiệu dựa trên đánh giá về chất lượng hoặc dịch vụ tốt hơn các đối thủ cạnh tranh và không phụ thuộc vào giá cả của nhãn hiệu đó.\nHình 15.10: Cây quyết định phân loại cho biết quyết định mua sản phẩm nước cam của khách hàng dựa vào các biến giải thích khác sử dụng dữ liệu OJ.\nCó thể thấy rằng biến \\(LoyalCH\\) xuất hiện ở hầu hết các nút của cây quyết định, điều này cho thấy sự trung thành của khách hàng với nhãn hiệu sản phẩm quyết định rất lớn đến quyết định mua hàng của khách hàng. Chúng ta có thể giải thích cây quyết định phân loại trên như sau:Khi lòng trung thành của khách hàng với sản phẩm Citrus Hill thấp (nhỏ hơn 0.5), tương ứng với nhánh cây quyết định bên trái của nút trên cùng, thì đa số các khách hàng sẽ lựa chọn sản phẩm Minute Maid. Trong nhánh bên trái này, những khách hàng có chỉ số lòng trung thành với Citrus Hill dưới 0.27 sẽ chọn sản phẩm Minute Maid bất kể sự khác biệt về giá cả. Còn những khách hàng có chỉ số trung thành với sản phẩm Citrus Hill từ 0.27 trở lên sẽ có thay đổi quyết định để mua sản phẩm Citrus Hill nếu giá của Minute Maid không lớn hơn giá của Citrus Hill cộng thêm 0.05.Khi lòng trung thành của khách hàng với sản phẩm Citrus Hill thấp (nhỏ hơn 0.5), tương ứng với nhánh cây quyết định bên trái của nút trên cùng, thì đa số các khách hàng sẽ lựa chọn sản phẩm Minute Maid. Trong nhánh bên trái này, những khách hàng có chỉ số lòng trung thành với Citrus Hill dưới 0.27 sẽ chọn sản phẩm Minute Maid bất kể sự khác biệt về giá cả. Còn những khách hàng có chỉ số trung thành với sản phẩm Citrus Hill từ 0.27 trở lên sẽ có thay đổi quyết định để mua sản phẩm Citrus Hill nếu giá của Minute Maid không lớn hơn giá của Citrus Hill cộng thêm 0.05.Khi lòng trung thành của khách hàng với sản phẩm Citrus Hill lớn (lớn hơn 0.5), tương ứng với nhánh cây quyết định bên phải của nút trên cùng, thì đa số các khách hàng sẽ lựa chọn sản phẩm Citrus Hill. Trong nhánh này, những khách hàng có chỉ số lòng trung thành với Citrus Hill trên 0.76 sẽ chọn sản phẩm Citrus Hill bất kể sự khác biệt về giá cả. Còn những khách hàng có chỉ số trung thành với sản phẩm Citrus Hill dưới 0.76 sẽ có thay đổi quyết định và mua sản phẩm Minute Maid nếu giá của Minute Maid thấp hơn giá của Citrus Hill 0.165Khi lòng trung thành của khách hàng với sản phẩm Citrus Hill lớn (lớn hơn 0.5), tương ứng với nhánh cây quyết định bên phải của nút trên cùng, thì đa số các khách hàng sẽ lựa chọn sản phẩm Citrus Hill. Trong nhánh này, những khách hàng có chỉ số lòng trung thành với Citrus Hill trên 0.76 sẽ chọn sản phẩm Citrus Hill bất kể sự khác biệt về giá cả. Còn những khách hàng có chỉ số trung thành với sản phẩm Citrus Hill dưới 0.76 sẽ có thay đổi quyết định và mua sản phẩm Minute Maid nếu giá của Minute Maid thấp hơn giá của Citrus Hill 0.165Một sự khác biệt trong kết quả của cây quyết định phân loại và cây phân loại hồi quy đó là hai lá tách ra từ một nút vẫn có thể nhận kết quả giống nhau. Chẳng hạn như trong hình 15.10 bạn đọc có thể thấy rằng tại nút \\(LoyalCH < 0.0356\\), cả hai lá đều cho kết quả là \\(MM\\). Nguyên nhân là chúng ta sử dụng chỉ số Gini để phân vùng dữ liệu. Sai số phân loại của cây quyết định sẽ không giảm nếu hai lá từ một nút cho cùng một kết quả là \\(MM\\) nhưng độ thuần (purity) của các nút sẽ giảm.","code":""},{"path":"mô-hình-cây-quyết-định.html","id":"cây-quyết-định-và-mô-hình-tuyến-tính","chapter":"Chương 15 Mô hình cây quyết định","heading":"15.1.3 Cây quyết định và mô hình tuyến tính","text":"Cây quyết định hồi quy và cây quyết định phân loại có cách tiếp cận hoàn toàn khác với các mô hình tuyến tính mà chúng tôi thảo luận trong cá chương trước. Nếu như mô hình tuyến tính cho rằng hàm \\(f\\) mô tả mối liên hệ giữa biến mục tiêu \\(Y\\) và các biến giải thích \\(X_1\\), \\(X_2\\), \\(\\cdots\\), \\(X_p\\) có dạng tuyến tính\n\\[\\begin{align}\nf(\\textbf{X}) = \\beta_0 + \\sum\\limits_{j=1}^p \\beta_j \\cdot X_j\n\\end{align}\\]\nthì mô hình cây quyết định sử dụng dạng của hàm \\(f\\) như sau\n\\[\\begin{align}\nf(\\textbf{X}) = \\sum\\limits_{m=1}^M c_m \\cdot \\mathbb{}_{\\textbf{X} \\R_m}\n\\end{align}\\]\ntrong đó \\(R_1, R_2, \\cdots, R_M\\) là một phân vùng của miền xác định của véc-tơ biến giải thích \\(\\textbf{X}\\). Không có câu trả lời cho câu hỏi là mô hình nào tốt hơn. Câu trả lời hoàn toàn tùy thuộc vào dữ liệu mà chúng ta xây dựng mô hình. Nếu mối liên hệ của biến mục tiêu và biến giải thích là mối liên hệ tuyến tính, mô hình tuyến tính sẽ cho kết quả tốt hơn, còn nếu mối liên hệ đó là phi tuyến, mô hình cây quyết định sẽ cho kết quả tốt hơn. Tất nhiên, với một dữ liệu cụ thể, không thể biết được chính xác mối liên hệ giữa biến mục tiêu với các biến khác là tuyến tính hay phi tuyến, đó cách tốt nhất để biết mô hình nào tốt hơn là xây dựng cả hai dạng mô hình và sau đó sử dụng sai số xác thực chéo để đưa ra kết luận.Việc lựa chọn mô hình cũng phụ thuộc vào mục tiêu của người xây dựng mô hình. Nếu mục tiêu của xây dựng mô hình là để suy diễn, đánh giá tác động của các biến giải thích lên biến mục tiêu thì mô hình cây quyết định là mô hình dễ diễn giải và mô tả dữ liệu hơn cả. Mô hình tuyến tính thông thường cũng có khả năng diễn giải kết quả tốt trong khi các mở rộng của mô hình tuyến tính như hồi quy Splines, hay mô hình cộng tính tổng quát dẫn đến các kết quả ít có ý nghĩa diễn giải. Ngược lại, nếu mục tiêu của người xây dựng mô hình là dự đoán biến mục tiêu, nghĩa là để giảm thiểu sai số dự đoán trên dữ liệu kiểm thử mô hình, các mở rộng của mô hình tuyến tính sẽ cho sai số dự báo tốt hơn nhiều với mô hình cây quyết định.Trước khi chuyển sang phần tiếp theo, chúng ta có thể tổng kết lại những điểm mạnh và điểm yếu của mô hình cây quyết định như sau:Lợi thế thứ nhất của mô hình cây quyết định trước tiên là khả năng diễn giải mô hình. Bạn đọc có thể thấy rằng mô hình cây quyết định thậm chí còn dễ giải thích hơn cả mô hình hồi quy tuyến tính thông thường. Một trong những nguyên nhân khiến cho cây quyết định dễ diễn giải là cách cây quyết định được xây dựng có mối liên hệ chặt chẽ với việc ra quyết định của não bộ của con người.Lợi thế thứ nhất của mô hình cây quyết định trước tiên là khả năng diễn giải mô hình. Bạn đọc có thể thấy rằng mô hình cây quyết định thậm chí còn dễ giải thích hơn cả mô hình hồi quy tuyến tính thông thường. Một trong những nguyên nhân khiến cho cây quyết định dễ diễn giải là cách cây quyết định được xây dựng có mối liên hệ chặt chẽ với việc ra quyết định của não bộ của con người.Thứ hai, mô hình cây quyết định có thể được trực quan hóa bằng đồ họa và có thể dễ dàng để hiểu được ngay cả với người không có nền tảng về toán học.Thứ hai, mô hình cây quyết định có thể được trực quan hóa bằng đồ họa và có thể dễ dàng để hiểu được ngay cả với người không có nền tảng về toán học.Thứ ba, mô hình cây quyết định có thể sử dụng cho dữ liệu có biến giải thích và biến mục tiêu định tính mà không cần có thay đổi đáng kể nào. Bạn đọc có thể thấy rằng gần như không có sự khác biệt trong cách xây dựng cây quyết định hồi quy và cây quyết định phân loại. Đồng thời cách xây dựng cây quyết định dựa trên biến giải thích định lượng và định tính gần như không có sự khác biệt.Thứ ba, mô hình cây quyết định có thể sử dụng cho dữ liệu có biến giải thích và biến mục tiêu định tính mà không cần có thay đổi đáng kể nào. Bạn đọc có thể thấy rằng gần như không có sự khác biệt trong cách xây dựng cây quyết định hồi quy và cây quyết định phân loại. Đồng thời cách xây dựng cây quyết định dựa trên biến giải thích định lượng và định tính gần như không có sự khác biệt.Để nói về những hạn chế khi sử dụng mô hình dạng cây, trước hết phải khẳng định rằng mô hình cây quyết định trình bày trong các phần trước không có khả năng dự báo chính xác như các mô hình khác. Đó cũng là sự đánh đổi mà bạn đọc thường xuyên gặp phải khi xây dựng mô hình trên dữ liệu, luôn có sự đánh đổi giữa khả năng dự báo và khả năng diễn giải của mô hình.Để nói về những hạn chế khi sử dụng mô hình dạng cây, trước hết phải khẳng định rằng mô hình cây quyết định trình bày trong các phần trước không có khả năng dự báo chính xác như các mô hình khác. Đó cũng là sự đánh đổi mà bạn đọc thường xuyên gặp phải khi xây dựng mô hình trên dữ liệu, luôn có sự đánh đổi giữa khả năng dự báo và khả năng diễn giải của mô hình.Nhược điểm thứ hai, cũng là nguyên nhân giải thích nhược điểm thứ nhất, đó là mô hình cây quyết định là các mô hình có phương sai lớn, nghĩa là chỉ cần có một thay đổi nhỏ trong dữ liệu xây dựng mô hình, kết quả của mô hình sẽ có sự thay đổi đáng kể.Nhược điểm thứ hai, cũng là nguyên nhân giải thích nhược điểm thứ nhất, đó là mô hình cây quyết định là các mô hình có phương sai lớn, nghĩa là chỉ cần có một thay đổi nhỏ trong dữ liệu xây dựng mô hình, kết quả của mô hình sẽ có sự thay đổi đáng kể.Trong phần cuối của chương sách này và trong chương sau, chúng tôi sẽ giới thiệu đến bạn đọc các kỹ thuật thống kê hiện đại có thể sử dụng kết hợp với mô hình cây quyết định để khắc phục được nhược điểm của mô hình này. Các kỹ thuật này bao gồm có mô hình rừng ngẫu nhiên (còn gọi là random forest) và kỹ thuật học tăng cường (còn được gọi là boosting).","code":""},{"path":"mô-hình-cây-quyết-định.html","id":"thực-hành-xây-dựng-mô-hình-cây-quyết-định-sử-dụng-r","chapter":"Chương 15 Mô hình cây quyết định","heading":"15.2 Thực hành: xây dựng mô hình cây quyết định sử dụng R","text":"","code":""},{"path":"mô-hình-cây-quyết-định.html","id":"cây-quyết-định-hồi-quy-trên-dữ-liệu-boston","chapter":"Chương 15 Mô hình cây quyết định","heading":"15.2.1 Cây quyết định hồi quy trên dữ liệu Boston","text":"","code":""},{"path":"mô-hình-cây-quyết-định.html","id":"cây-quyết-định-phân-loại-trên-dữ-liệu-titanic","chapter":"Chương 15 Mô hình cây quyết định","heading":"15.2.2 Cây quyết định phân loại trên dữ liệu Titanic","text":"","code":""},{"path":"mô-hình-cây-quyết-định.html","id":"thuật-toán-rừng-ngẫu-nhiên","chapter":"Chương 15 Mô hình cây quyết định","heading":"15.3 Thuật toán rừng ngẫu nhiên","text":"","code":""},{"path":"mô-hình-cây-quyết-định.html","id":"phương-pháp-lấy-mẫu-lặp-lại","chapter":"Chương 15 Mô hình cây quyết định","heading":"15.3.1 Phương pháp lấy mẫu lặp lại","text":"Phương pháp lấy mẫu lặp lại (thường gọi là boostrap) là một công cụ vô cùng quan trọng trong thống kê hiện đại. Phương pháp này liên quan đến việc lấy mẫu lặp lại nhiều lần từ dữ liệu huấn luyện ban đầu và ước lượng mô hình trên mỗi mẫu để có thông tin đầy đủ về mô hình hay tham số của mô hình mà chúng ta đang nghiên cứu. Ví dụ: để đánh giá mô hình hồi quy tuyến tính đơn biến trong đó biến mục tiêu \\(Y\\) phụ thuộc vào biến giải thích \\(X\\) dựa trên dữ liệu quan sát được: \\((x_i, y_i)\\) với \\(1 \\leq \\leq n\\), nếu chúng ta sử dụng toàn bộ \\(n\\) quan sát để ước tính hệ số chặn và hệ số góc, chúng ta chỉ có một ước lượng điểm duy nhất. Nếu không có giả thiết quan trọng của mô hình hồi quy tuyến tính là \\(Y\\) có phân phối chuẩn thì chúng ta sẽ không đưa ra được phân phối xác suất hay các khoảng tin cậy cho hệ số của mô hình tuyến tính. Kỹ thuật lấy mẫu lặp tiếp cận theo hướng hoàn toàn khác, thay vì xuất phát từ phân phối xác suất của biến mục tiêu, chúng ta có thể liên tục lấy các mẫu khác nhau từ dữ liệu huấn luyện ban đầu, với mỗi mẫu lấy được chúng ta ước lượng được một hệ số chặn và một hệ số góc, từ đó thu được một phân phối xác suất của các hệ số.Để mô tả phương pháp lấy mẫu lặp lại, chúng ta sử dụng ví dụ về dữ liệu Quảng cáo mà trong đó doanh thu bán sản phẩm phụ thuộc vào các biến giải thích là chi phí quảng cáo trên truyền hình (\\(TV\\)), chi phí quảng cáo trên mạng xã hội (\\(Social\\_Media\\)) và chi phí quảng cáo qua tờ rơi (\\(Flyer\\)). biến quảng cáo qua tờ rơi không có ý nghĩa trong mô hình hồi quy đa biến nên mô hình được lựa chọn chỉ bao gồm hai biến giải thích là \\(TV\\) và \\(Social\\_Media\\).\n\\[\\begin{align}\nSales = \\beta_0 + \\beta_1 \\cdot TV + \\beta_2 \\cdot Social\\_Media + \\epsilon\n\n\\end{align}\\]Hệ số ước lượng của mô hình tuyến tính đa biến như sau\nBảng 15.2: Các hệ số ước lượng trong mô hình hồi quy đa biến trên dữ liệu Quảng cáo.\nTừ kết quả ước lượng, chúng ta có các hệ số = \\(\\beta_0\\), \\(\\beta_1\\), \\(\\beta_2\\) là các biến ngẫu nhiên phân phối chuẩn với giá trị trung bình lần lượt là 4.0642, 0.0192, 0.0699 và độ lệch chuẩn lần lượt là 0.7349, 0.0049, 0.0128. Các tính toán này được dựa trên giả thiết quan trọng là phần dư \\(\\epsilon\\) có phân phối chuẩn \\(\\mathcal{N}(0,\\sigma^2)\\). Phương pháp lấy mẫu lặp không cần tính đến giả thiết phân phối của phần dư, mà sử dụng phép lấy mẫu lặp lại từ dữ liệu quảng cáo. Mỗi lần lấy mẫu lặp, chúng ta tạo ra một dữ liệu có số quan sát đúng bằng số quan sát của dữ liệu ban đầu, tuy nhiên một quan sát có thể bị lặp lại nhiều lần. Với mỗi mẫu lặp như vậy, chúng ta thực hiện một ước lượng bằng phương pháp bình phương nhỏ nhất để thu được các hệ số. Phân phối xác suất của các hệ số bằng phương pháp hồi quy truyền thống và phương pháp lấy mẫu lặp được trình bày trong hình 15.11\nHình 15.11: Phân phối xác suất của các hệ số của mô hình hồi quy tuyến tính đa biến. Cột bên trái: phân phối xác suất được tính toán từ phương pháp bình phương nhỏ nhất truyền thống. Cột bên phải: phân phối xác suất được tạo thành từ 1000 lần lấy mẫu lặp. Hàng thứ nhất: phân phối xác suất của hệ số chặn. Hàng thứ hai: phân phối xác suất của hệ số tuyến tính của biến TV. Hàng thứ ba: phân phối xác suất của hệ số tuyến tính của biến Social_Media. Các đường màu đỏ ở giữa cho biết giá trị trung bình của hệ số.\nBạn đọc có thể thấy rằng phân phối xác suất của các hệ số tính toán từ phương pháp lấy mẫu lặp không khác đáng kể với phân phối xác suất được ước lượng với giả thiết phân phối chuẩn. Như vậy với phương pháp lấy mẫu lặp, bạn đọc có thể tính toán được giá trị trung bình của các hệ số và sai số của ước lượng mà không cần thêm bất kỳ giả thiết nào về hình dạng của phân phối. Đánh đổi lại, để thực hiện ước lượng tham số bằng lấy mẫu lặp, nguồn lực tính toán tăng lên theo số lần chúng ta lấy mẫu.","code":""},{"path":"mô-hình-cây-quyết-định.html","id":"thuật-toán-rừng-ngẫu-nhiên-1","chapter":"Chương 15 Mô hình cây quyết định","heading":"15.3.2 Thuật toán rừng ngẫu nhiên","text":"Phương pháp lấy mẫu lặp lại, hay bootstrap, được giới thiệu trước bởi vì đó là ý tưởng chủ đạo trong mô hình rừng ngẫu nhiên. Các mô hình cây quyết định được biết đến là các mô hình có phương sai lớn, nghĩa là chỉ cần có một thay đổi nhỏ trong dữ liệu huấn luyện, mô hình có thể sẽ thay đổi đáng kể. Như bạn đọc đã biết, một phương pháp đơn giản để không làm thay đổi giá trị trung bình và giảm phương sai của dự đoán đó là dự đoán nhiều lần và sử dụng kết quả trung bình của các dự đoán. Nói một cách khác, giả sử chúng ta sử dụng một biến ngẫu nhiên \\(Z_1\\) có giá trị trung bình \\(\\mu\\) (không biết) để làm dự đoán cho \\(\\mu\\). Bằng một cách nào đó, thay vì sử dụng một dự đoán duy nhất, nếu chúng ta có thể tạo ra một dãy các biến ngẫu nhiên \\(Z_1\\), \\(Z_2\\), \\(\\cdots\\), \\(Z_n\\) và sử dụng \\(\\bar{Z}\\) là giá trị trung bình của các biến ngẫu nhiên kể trên, thì phương sai trong dự đoán sẽ nhỏ hơn phương sai khi sử dụng một dự đoán duy nhất. Trong trường hợp đặc biệt, nếu các \\(Z_i\\) độc lập với nhau, phương sai của \\(\\bar{Z}\\) bằng \\(\\frac{1}{n}\\) phương sai của một dự đoán duy nhất, với \\(n\\) là số lần dự đoán.Với ý tưởng tương tự như vậy, để giảm phương sai của các mô hình có phương sai lớn, chúng ta cần xây dựng nhiều mô hình dự đoán riêng bằng cách sử dụng nhiều dữ liệu huấn luyện và sử dụng giá trị trung bình để dự đoán kết quả. Nói cách khác, nếu chúng ta có \\(T\\) dữ liệu huấn luyện mô hình, chúng ta có thể xây dựng \\(T\\) mô hình cây quyết định (phân loại hoặc hồi quy) \\(\\hat{f}_1\\), \\(\\hat{f}_2\\), \\(\\cdots\\) ,\\(\\hat{f}_T\\) và sau đó lấy trung bình của các mô hình này để thu được một mô hình có phương sai thấp\n\\[\\begin{align}\n\\hat{f}_{avg} = \\cfrac{1}{T} \\ \\sum\\limits_{t = 1}^T \\ \\hat{f}_t\n\\tag{15.7}\n\\end{align}\\]Tất nhiên, chúng ta không thể có được \\(T\\) tập dữ liệu huấn luyện mô hình để xây dựng \\(T\\) mô hình dự đoán. Thay vào đó, chúng ta có thể thực hiện phương pháp lấy mẫu lặp lại \\(T\\) lần trên dữ liệu huấn luyện mô hình ban đầu. Với mỗi dữ liệu thu được, chúng ta xây dựng một cây quyết định và mô hình thu được sau cùng là giá trị trung bình của \\(T\\) mô hình thu được giống như phương trình (15.7). Với cây quyết định hồi quy giá trị trung bình được hiểu đúng với ý nghĩa, nghĩa là nếu một biến mục tiêu \\(y_i\\) được dự đoán từ \\(T\\) cây quyết định hồi quy lần lượt là \\(\\hat{y}_{,1}\\), \\(\\hat{y}_{,2}\\), \\(\\cdots\\), \\(\\hat{y}_{,T}\\) thì giá trị dự đoán của mô hình trung bình là\n\\[\\begin{align}\n\\hat{y}_i = \\cfrac{1}{T} \\sum\\limits_{t = 1}^T \\ \\hat{y}_{,t}\n\\end{align}\\]\nĐối với cây quyết định phân loại, giá trị dự đoán trong mô hình trung bình là mode của véc-tơ các giá trị dự đoán, nghĩa là nếu biến mục tiêu \\(y_i\\) được dự đoán từ \\(T\\) cây quyết định phân loại lần lượt là \\(\\hat{y}_{,1}\\), \\(\\hat{y}_{,2}\\), \\(\\cdots\\), \\(\\hat{y}_{,T}\\) thì giá trị dự đoán của mô hình trung bình là\n\\[\\begin{align}\n\\hat{y}_i =  mode\\left(\\hat{y}_{,1}, \\hat{y}_{,2}, \\cdots, \\hat{y}_{,T} \\right)\n\\end{align}\\]Sử dụng giá trị trung bình của nhiều mô hình để dự đoán cho phép cải thiện đáng kể khả năng dự đoán và có thể sử dụng trong nhiều kiểu mô hình khác nhau. Cách tiếp cận này đặc biệt hữu ích trên mô hình cây quyết định và thuật ngữ “forest” có thể hiểu đơn giản là sử dụng nhiều cây quyết định kết hợp với nhau thành một mô hình duy nhất. Mỗi cây quyết định riêng lẻ có phương sai cao nhưng độ lệch thấp, và trung bình của \\(T\\) cây như vậy có thể làm giảm phương sai. Thực tế chỉ ra rằng kỹ thuật này mang lại những cải tiến vượt bậc về độ chính xác trong dự đoán khi kết hợp hàng trăm hoặc thậm chí hàng nghìn cây vào một mô hình duy nhất.Chúng tôi đã giải thích thuật ngữ “forest”, tiếp theo sẽ là thuật ngữ \\(\"random\"\\). Quay trở lại ý tưởng khi sử dụng nhiều biến ngẫu nhiên \\(Z_i\\) để dự đoán giá trị trung bình \\(\\mathbb{E}(Z_i) = \\mu\\). Nếu các biến \\(Z_i\\) có tương quan dương rất cao với nhau, thì phương sai của \\(\\bar{Z}\\) sẽ không được giảm đi một cách đáng kể với phương sai của một biến duy nhất. Các cây quyết định được xây dựng để đưa ra dự đoán duy nhất trong phương trình (15.7) rất có khả năng là có tương quan cao với nhau, bởi vì các cây được xây dựng từ các dữ liệu được lấy mẫu lặp lại từ một dữ liệu huấn luyện mô hình, và các cây có cùng một tập hợp các biến giải thích cho cùng một biến mục tiêu. Hiện tượng tương quan cao với nhau giữa các cây thường xảy ra khi trong tập hợp các biến giải thích có một biến có ý nghĩa giải thích mạnh vượt trội với các biến giải thích khác. Khi chúng ta xây dựng các cây quyết định trên các dữ liệu được lấy mẫu từ dữ liệu ban đầu, biến giải thích này luôn luôn chiếm ưu thế và xuất hiện trong các nút phân vùng đầu tiên. Việc này làm cho kết quả dự đoán từ các cây quyết định có tương quan rất cao với nhau và dẫn đến phương sai của mô hình trung bình không được giảm đi đáng kể với một cây quyết định riêng lẻ.Để tránh gặp phải hiện tượng này, khi xây dựng cây quyết định từ một dữ liệu được boostrap từ dữ liệu huấn luyện mô hình, chúng ta chỉ sử dụng một tập hợp con bao gồm \\(m\\) biến giải thích, \\(m < p\\), được lựa chọn một cách ngẫu nhiên từ \\(p\\) biến giải thích từ mô hình ban đầu. Trong trường hợp dữ liệu có một hoặc một vài biến giải thích mạnh, khả năng mà một biến này được sử dụng trong mô hình là \\(\\cfrac{m}{p}\\). Nói một cách khác, bằng cách chỉ sử dụng ngẫu nhiên \\(m\\) biến trong số \\(p\\) biến giải thích tại mỗi lần xây dựng cây quyết định, chúng ta có thể xây dựng được các cây quyết định ít có tương quan cao với nhau hơn, và đó có thể thu được một mô hình trung bình có phương sai nhỏ hơn.Khi xây dựng mô hình rừng ngẫu nhiên bao gồm \\(T\\) cây quyết định, tại bước thứ \\(t\\) chúng ta có dữ liệu \\(Data_t\\) được boostrap từ dữ liệu huấn luyện mô hình, người xây dựng mô hình lựa chọn ngẫu nhiên \\(m_t\\) biến giải thích từ \\(p\\) biến giải thích ban đầu, sau đó xây dựng một cây quyết định \\(\\hat{f}_t\\) với kích thước \\(L_t\\) để mô tả mối quan hệ giữa biến giải thích và các biến mục tiêu. Các tham số \\(m_t\\) và \\(L_t\\) tại mỗi bước \\(t\\) và số lượng cây quyết định \\(T\\) là các tham số của mô hình rừng ngẫu nhiên. Thông thường thì tại mỗi bước \\(t\\), kích thước cây \\(L_t\\) sẽ được xác định bằng một tiêu chí dừng nào đó, giống như cách xây dựng một cây quyết định thông thường. Số lượng biến giải thích \\(m_t\\) nếu nhận giá trị khác nhau tại các bước thì rất khó để điều khiển mô hình, đó người xây dựng mô hình rừng ngẫu nhiên thường sử dụng \\(m_t\\) cố định . Nói một cách khác, mô hình rừng ngẫu nhiên có hai tham số cần phải ước lượng là: 1. Số lượng cây quyết định (tham số \\(T\\)) và 2. Số lượng biến giải thích để ước lượng cây quyết định \\(m\\). Các tham số này thường được tính toán sao cho sai số từ xác thực chéo là nhỏ nhất.\nHình 15.12: Sai số xác thực chéo theo số lượng cây khi sử dụng thuật toán rừng ngẫu nhiên. Mỗi đường thể hiện cho một lựa chọn khác nhau của tham số m là số lượng biến giải thích được lựa chọn trong mỗi cây.\nHình 15.12 mô tả quá trình sử dụng xác thực chéo để tìm tham số \\(T\\) và tham số \\(m\\) khi xây dựng mô hình rừng ngẫu nhiên trên dữ liệu Boston với biến mục tiêu là biến giá nhà (\\(medv\\)). Cặp tham số cho sai số xác thực chéo nhỏ nhất là \\(T = 120\\) cây và \\(m = 6\\) biến. giới hạn của khả năng tính toán nên chúng ta chỉ có thể thử trên các lựa chọn là \\(m = 2, 4\\) hoặc \\(6\\) và \\(T \\leq 500\\). Trên thực tế, \\(m\\) thương được lựa chọn xung quang giá trị của \\(p/2\\) nếu \\(p\\) nhỏ và \\(\\sqrt{p}\\) nếu \\(p\\) lớn trong khi \\(T\\) chỉ có thể được lựa chọn thông qua xác thực chéo. Đồng thời, khi \\(m\\) nhỏ thì tương quan giữa các cây quyết định sẽ thấp hơn nhưng khả năng dự đoán của các cây quyết định riêng lẻ sẽ kém đi nên thường cần nhiều cây quyết định hơn để đưa một kết quả có độ chính xác tương đương như khi \\(m\\) lớn. Bạn đọc có thể thấy rằng khi \\(m = 2\\) hoặc \\(m = 4\\) thì sai số xác thực chéo có xu hướng tiếp tục giảm kể cả khi chúng ta tăng \\(T\\) hơn 500, trong khi khi \\(m = 6\\) thì điểm tối ưu đạt được ngay khi \\(T = 120\\).Bạn đọc có thể nhận thấy ngay sự khác biệt trong khả năng dự đoán của mô hình rừng ngẫu nhiên trong Hình 15.12 và mô hình cây quyết định trong Hình 15.7. Sai số xác thực chéo của mô hình rừng ngẫu nhiên với \\(m = 6\\) và \\(T = 120\\) là khoảng 3100 nghìn USD trong khi sai số xác thực chéo của cây quyết định tốt nhất là 4600 USD. Tuy nhiên, giải thích hay suy diễn kết quả của một cây quyết định là khá đơn giản trong khi việc diễn giải kết quả cho mô hình rừng ngẫu nhiên là vô cùng khó khăn. Đây chính là sự đánh đổi thường phải chấp nhận đối với người xây dựng mô hình.Khi cố gắng giải thích một mô hình rừng ngẫu nhiên người xây dựng mô hình thường sử dụng thước đo sự quan trọng của từng biến giải thích. Có hai cách để định nghĩa sự quan trọng của biến giải thích: thứ nhất là tính toán giá trị trung bình của sự suy giảm độ chính xác của các mô hình cây quyết định khi biến đó không được tính vào trong mô hình, và thứ hai là tính toán tổng của sự suy giảm trong độ thuần (purity) của các nút có chứa biến đó trên tất cả các cây quyết định. Các thước đo này tính trên một biến giải thích lớn tương đối với các biến giải thích khác nghĩa là biến đó quan trọng hơn và có nhiều ý nghĩa hơn trong mô hình rừng ngẫu nhiên. Bạn đọc có thể tham khảo thêm về cách tính toán mức độ quan trọng của các biến của mô hình rừng ngẫu nhiên trong phần ??.\nHình 15.13: Mức độ quan trọng của các biến trong mô hình rừng ngẫu nhiên trên dữ liệu Boston. Hình bên trái: sự quan trọng tính bằng sự suy giảm trung bình trong khả năng dự báo của các cây quyết định khi bỏ biến giải thích ra khỏi mô hình. Hình bên phải: sự quan trọng tính bằng tổng sự suy giảm trong độ thuần của các nút khi sử dụng biến giải thích.\nHình 15.13 cho thấy hai biến quan trọng nhất khi sử dụng mô hình rừng ngẫu nhiên để dự đoán giá nhà là \\(rm\\) và \\(lstat\\), nhóm các biến quan trọng thứ hai bao gồm có \\(dis\\), \\(nox\\), \\(ptratio\\), và \\(crim\\). Nhóm các biến ít có ý nghĩa trong dự đoán giá nhà là \\(chas\\), \\(zn\\) và \\(rad\\).","code":""},{"path":"mô-hình-cây-quyết-định.html","id":"thực-hành-mô-hình-rừng-ngẫu-nhiên","chapter":"Chương 15 Mô hình cây quyết định","heading":"15.4 Thực hành: mô hình rừng ngẫu nhiên","text":"","code":""},{"path":"mô-hình-cây-quyết-định.html","id":"xây-dựng-mô-hình-rừng-ngẫu-nhiên-trên-dữ-liệu-boston","chapter":"Chương 15 Mô hình cây quyết định","heading":"15.4.1 Xây dựng mô hình rừng ngẫu nhiên trên dữ liệu Boston","text":"","code":""},{"path":"mô-hình-cây-quyết-định.html","id":"xây-dựng-mô-hình-rừng-ngẫu-nhiên-trên-dữ-liệu-oj","chapter":"Chương 15 Mô hình cây quyết định","heading":"15.4.2 Xây dựng mô hình rừng ngẫu nhiên trên dữ liệu OJ","text":"","code":""},{"path":"mô-hình-cây-quyết-định.html","id":"bài-tập-3","chapter":"Chương 15 Mô hình cây quyết định","heading":"15.5 Bài tập","text":"","code":""},{"path":"mô-hình-cây-quyết-định.html","id":"bài-tập-lý-thuyết-1","chapter":"Chương 15 Mô hình cây quyết định","heading":"15.5.1 Bài tập lý thuyết","text":"","code":""},{"path":"mô-hình-cây-quyết-định.html","id":"bài-tập-thực-hành-1","chapter":"Chương 15 Mô hình cây quyết định","heading":"15.5.2 Bài tập thực hành","text":"","code":""},{"path":"mô-hình-cây-quyết-định.html","id":"phụ-lục-5","chapter":"Chương 15 Mô hình cây quyết định","heading":"15.6 Phụ lục","text":"","code":""},{"path":"mô-hình-cây-quyết-định.html","id":"tính-toán-sự-quan-trọng-của-biến-giải-thích-trong-mô-hình-rừng-ngẫu-nhiên","chapter":"Chương 15 Mô hình cây quyết định","heading":"15.6.1 Tính toán sự quan trọng của biến giải thích trong mô hình rừng ngẫu nhiên","text":"","code":"## \n## Attaching package: 'dplyr'## The following objects are masked from 'package:stats':\n## \n##     filter, lag## The following objects are masked from 'package:base':\n## \n##     intersect, setdiff, setequal, union## \n## Attaching package: 'kableExtra'## The following object is masked from 'package:dplyr':\n## \n##     group_rows## \n## Attaching package: 'gridExtra'## The following object is masked from 'package:dplyr':\n## \n##     combine## \n## Attaching package: 'pryr'## The following object is masked from 'package:dplyr':\n## \n##     where"},{"path":"boosting-và-cây-quyết-định..html","id":"boosting-và-cây-quyết-định.","chapter":"Chương 16 Boosting và cây quyết định.","heading":"Chương 16 Boosting và cây quyết định.","text":"Trong một vài cuốn sách, boosting được dịch sang tiếng Việt là học tăng cường, tuy nhiên trong cuốn sách này, chúng tôi giữ nguyên khái niệm này bởi vì chúng tôi không nghĩ rằng học tăng cường giải nghĩa được chính xác ý tưởng của Boosting. Tính đến thời điểm chúng tôi đang viết cuốn sách này, có thể khẳng định rằng boosting là một trong những ý tưởng mạnh mẽ nhất trong lĩnh vực học máy. Ban đầu boosting được áp dụng cho các bài toán phân loại, nhưng bạn đọc sẽ thấy rằng có thể dễ dàng áp dụng boosting cho các bài toán hổi quy để cho kết quả hơn cả mong đợi. Ý tưởng chung của boosting là kết hợp kết quả đầu ra của nhiều hàm phân loại, hoặc hàm hồi quy “yếu”, để tạo ra một hàm phân loại hoặc hồi quy “mạnh”. Cùng là kết hợp nhiều hàm phân loại hay hổi quy, tuy nhiên boosting khác bagging hay random forest ở chỗ các hàm phân loại hoặc hồi quy được tạo ra theo thứ tự nhất định mà trong đó hàm phân loại hay hồi quy được tạo ra ở bước thứ \\(m\\) sẽ phụ thuộc vào kết quả của hàm đó tại các bước thứ \\(1, 2, \\cdots, (m-1)\\). Trong bagging hay random forest, cách xây dựng hàm phân loại hay hổi quy ở lần thứ \\(m\\) hoàn toàn không phụ thuộc vào kết quả của các bước trước đó.Khái niệm boosting lần đầu tiên được nhắc đến trong nghiên cứu của Freund và Schapire (1997). Chúng tôi gọi thuật toán được giới thiệu trong nghiên cứu của Freund và Schapire là “AdaBoost.M1” để phân biệt với thuật toán AdaBoost thông dụng được trình bày trong nghiên cứu của Friedman (2000). Trong bài toán phân loại, biến mục tiêu chỉ nhận hai giá trị \\(Y \\\\{-1,1\\}\\). Hàm phân loại được ký hiệu là \\(b^C\\) và với véc-tơ biến độc lập \\(\\textbf{x}_i\\) chúng ta có \\(b^C(\\textbf{x}_i) \\\\{-1,1\\}\\). Sai số của hàm phân loại \\(b^C\\) trên dữ liệu \\((\\textbf{x},y)\\) được tính như sau\n\\[\\begin{align}\nerr(\\textbf{x},y) = \\cfrac{1}{n} \\ \\sum\\limits_{=1}^n \\mathbb{}\\left(b^C(\\textbf{x}_i) \\neq y_i \\right)\n\\end{align}\\]Môt hàm phân loại “yếu” \\(b^C\\) là một hàm phân loại có khả năng dự báo chỉ tốt hơn một chút với việc phân loại một cách ngẫu nhiên. Ý tưởng của boosting là áp dụng tuần tự các hàm phân loại yếu trên các phiên bản dữ liệu được liên tục cập nhật dựa trên kết quả của các hàm phân loại trước đó. Hàm phân loại cuối cùng thu được bằng cách kết hợp có trọng số tất cả các hàm phân loại:\n\\[\\begin{align}\nf(x) =  sign\\left( \\sum\\limits_{m=1}^M \\alpha_m \\cdot b_m^C(x)  \\right)\n\\end{align}\\]\ntrong đó các hệ số \\(\\alpha_1, \\alpha_2, \\cdots, \\alpha_M\\) được tính toán dựa trên khả năng phân loại của các hàm \\(b^C_1, b^C_2, \\cdots, b^C_M\\).Quá trình cập nhật và thay đổi dữ liệu ở mỗi bước của boosting được thực hiện thông qua thay đổi véc-tơ trọng số \\(w^{(m)}_1, w^{(m)}_2, \\cdots ,w^{(m)}_n\\) cho từng quan sát trong dữ liệu xây dựng mô hình \\((\\textbf{x}_i,y_i)\\), với \\(= 1, 2, \\cdots ,n\\), và \\(m = 1, 2, \\cdots, M\\).Ban đầu tất cả các trọng số được cho bằng nhau tại bước thứ nhất \\(w^{(1)}_i = \\cfrac{1}{n}\\) với mọi \\(\\). Trong bước đầu tiên, hàm phân loại được xây dựng trên dữ liệu ban đầu theo cách thông thường. Đối với những lần xây dựng hàm phân loại tiếp theo \\(m = 2, 3, \\cdots ,M\\), trọng số \\(w^{(m)}_i\\) của quan sát thứ \\(\\) thay đổi và hàm phân loại được áp dụng lại cho dữ liệu với trọng số vừa cập nhật. Tại bước thứ \\(m\\), nếu quan sát thứ \\(\\) bị phân loại sai bởi hàm phân loại ở bước ngay liền trước đó, \\(b^C_{m-1}(x_i)\\), trọng số \\(w^{(m)}_i\\) sẽ được tăng lên. Ngược lại, nếu quan sát thứ \\(\\) được phân loại đúng ở bước \\((m-1)\\), trọng số \\(w^m_i\\) sẽ được giảm đi. Khi quá trình kể trên diễn ra lặp đi lặp lại, những quan sát khó phân loại chính xác sẽ nhận càng có tỷ trọng cao trong những bước phân loại tiếp theo. Những hàm phân loại xây dựng cho những bước sau sẽ tập trung vào phân loại những quan sát mà những hàm phân loại ở các bước trước đã bỏ sót.Thuật toán AdaBoost.M1 được mô tả trong nghiên cứu của Freund và Schapire (1997) được phát biểu như sau:Cho \\(w^{(1)}_i = \\cfrac{1}{n}\\) với mọi \\(= 1, 2, \\cdots, n\\) với \\(n\\) là số dòng của dữ liệu ban đầu.\nCho \\(w^{(1)}_i = \\cfrac{1}{n}\\) với mọi \\(= 1, 2, \\cdots, n\\) với \\(n\\) là số dòng của dữ liệu ban đầu.Tại bước thứ \\(m\\), với \\(m = 1, 2, \\cdots, M\\),\n2.() Xây dựng hàm phân loại \\(b^C_m\\) trên dữ liệu ban đầu với véc-tơ trọng số tương ứng với dòng \\(\\) là \\(w^{(m)}_i\\).\n2.(b) Tính toán sai số của hàm phân loại \\(b^C_m\\)\n\\[\\begin{align}\nerr_m = \\sum\\limits_{m=1}^M  w^{(m)}_i \\cdot \\mathbb{} \\left(b_m^C(\\textbf{x}_i) \\neq y_i \\right)\n\\end{align}\\]\n2.(c) Tính hệ số của hàm phân loại thứ \\(m\\) dựa trên sai số\n\\[\\begin{align}\n\\alpha_m = \\log\\left( \\cfrac{1 - err_m}{err_m} \\right)\n\\end{align}\\]\n2.(d). Cập nhật trọng số cho bước tiếp theo\n\\[\\begin{align}\nw^{(m+1)}_i = w^{(m)}_i \\cdot \\exp\\left[ \\alpha_m \\cdot \\mathbb{} \\left(b_m^C(\\textbf{x}_i) \\neq y_i \\right) \\right]\n\\end{align}\\]\n2.(e). Chuẩn hóa lại trọng số để tổng các trọng số bằng 1.\n\\[\\begin{align}\nw^{(m+1)}_i = \\cfrac{w^{(m+1)}_i}{\\sum\\limits_{=1}^n  w^{(m+1)}_i}\n\\end{align}\\]Tại bước thứ \\(m\\), với \\(m = 1, 2, \\cdots, M\\),2.() Xây dựng hàm phân loại \\(b^C_m\\) trên dữ liệu ban đầu với véc-tơ trọng số tương ứng với dòng \\(\\) là \\(w^{(m)}_i\\).2.() Xây dựng hàm phân loại \\(b^C_m\\) trên dữ liệu ban đầu với véc-tơ trọng số tương ứng với dòng \\(\\) là \\(w^{(m)}_i\\).2.(b) Tính toán sai số của hàm phân loại \\(b^C_m\\)2.(b) Tính toán sai số của hàm phân loại \\(b^C_m\\)\\[\\begin{align}\nerr_m = \\sum\\limits_{m=1}^M  w^{(m)}_i \\cdot \\mathbb{} \\left(b_m^C(\\textbf{x}_i) \\neq y_i \\right)\n\\end{align}\\]2.(c) Tính hệ số của hàm phân loại thứ \\(m\\) dựa trên sai số\\[\\begin{align}\n\\alpha_m = \\log\\left( \\cfrac{1 - err_m}{err_m} \\right)\n\\end{align}\\]2.(d). Cập nhật trọng số cho bước tiếp theo\\[\\begin{align}\nw^{(m+1)}_i = w^{(m)}_i \\cdot \\exp\\left[ \\alpha_m \\cdot \\mathbb{} \\left(b_m^C(\\textbf{x}_i) \\neq y_i \\right) \\right]\n\\end{align}\\]2.(e). Chuẩn hóa lại trọng số để tổng các trọng số bằng 1.\\[\\begin{align}\nw^{(m+1)}_i = \\cfrac{w^{(m+1)}_i}{\\sum\\limits_{=1}^n  w^{(m+1)}_i}\n\\end{align}\\]Kết thúc lần lặp thứ \\(M\\), trả lại kết quả hàm phân loại cuối cùng:\n\\[\\begin{align}\nf^C(x) =  sign\\left( \\sum\\limits_{m=1}^M \\alpha_m \\cdot b_m^C(x)  \\right)\n\\end{align}\\]\nKết thúc lần lặp thứ \\(M\\), trả lại kết quả hàm phân loại cuối cùng:\n\\[\\begin{align}\nf^C(x) =  sign\\left( \\sum\\limits_{m=1}^M \\alpha_m \\cdot b_m^C(x)  \\right)\n\\end{align}\\]Chúng tôi khuyên bạn đọc hãy hiểu ý tưởng của các bước kể trên thay vì cố gắng hiểu chính xác các công thức toán học. Các bước của thuật toán AdaBoost.M1 được trình bày ở trên khá rõ ràng, ngoại trừ bước 2.() là “Xây dựng hàm phân loại \\(b^C_m\\) trên dữ liệu ban đầu với véc-tơ trọng số tương ứng với dòng \\(\\) là \\(w^{(m)}_i\\)”. Quá trình xây dựng một hàm phân loại luôn luôn bao gồm hai bước: bước thứ nhất là lựa chọn kiểu mô hình và bước thứ hai là ước lượng tham số của mô hình với mục tiêu tối thiểu hóa một hàm tổn thất. Thuật toán AdaBoost.M1 ở trên có thể được áp dụng với mọi hàm phân loại (cây quyết định, hồi quy logistic, …) nhưng hàm tổn thất được lựa chọn phải là hàm tổn thất kiểu mũ. Trong các phần tiếp theo của cuốn sách bạn đọc sẽ được giải thích rằng công thức tính toán các trọng số \\(w^{(m)}_i\\) ở bước 2.(d) là kết quả của việc lựa chọn hàm tổn thất, trong khi hàm phân loại có thể là bất cứ dạng hàm nào.Thuật toán AdaBoost.M1 được Friedman (2000) gọi là thuật toán AdaBoost phân loại vì các hàm \\(b^C_m\\) được xây dựng ở bước thứ \\(m\\) luôn là các dạng hàm phân loại. Nghiên cứu của Friedman (2000) điều chỉnh AdaBoost.M1 để phù hợp hơn cho cả bài toán phân loại và bài toán hồi quy. Hàm phân loại trong nghiên cứu của Friedman luôn luôn có dạng là một cây quyết định có 1 node duy nhất, còn được gọi là một “stump”. Một stump chỉ là một hàm phân loại yếu, nhưng bằng cách kết hợp các stump như ý tưởng của AdaBoost.M1, khả năng dự đoán của hàm phân loại cuối cùng là đáng kinh ngạc. Thuật toán được giới thiệu trong nghiên cứu của Friedman (2000) chính là thuật toán AdaBoost được áp dụng rộng rãi hiện nay.","code":""},{"path":"boosting-và-cây-quyết-định..html","id":"những-cơ-sở-của-kỹ-thuật-boosting.","chapter":"Chương 16 Boosting và cây quyết định.","heading":"16.1 Những cơ sở của kỹ thuật boosting.","text":"","code":""},{"path":"boosting-và-cây-quyết-định..html","id":"nguyên-tắc-chung-của-boosting","chapter":"Chương 16 Boosting và cây quyết định.","heading":"16.1.1 Nguyên tắc chung của boosting","text":"Xây dựng mô hình dựa trên kỹ thuật boosting về cơ bản là kết hợp tuyến tính một tập hợp các hàm cơ bản nhằm cải thiện khả năng giải thích hoặc dự đoán. Một cách tổng quát, hàm \\(f\\) thu được từ kỹ thuật boosting có thể viết dưới dạng tổng của \\(M\\) hàm phân loại hoặc hồi quy như sau:\n\\[\\begin{align}\nf(\\textbf{x}) = \\sum\\limits_{m=1}^M \\ \\lambda_m \\cdot b(\\textbf{x},\\Theta_m)\n\\tag{16.1}\n\\end{align}\\]\ntrong đó \\(\\lambda_m\\) là hệ số tuyến tính, \\(b(\\textbf{x},\\Theta_m)\\) là một hàm phân loại hoặc hồi quy cơ bản có tham số là \\(\\Theta_m\\). Dạng tham số của hàm \\(b\\) thường được xác định trước khi xây dựng hàm \\(f\\) trong khi các tham số \\(\\lambda_m\\) và \\(\\Theta_m\\) được ước lượng tại mỗi bước nhằm tối thiểu hóa hàm tổn thất. Quá trình ước lượng tham số của mô hình (16.1) được thực hiện thông qua các bước như sau:Bước 1: Lựa chọn hàm tổn thất \\(\\sum\\limits_{=1}^n L(y_i, \\hat{y}_i)\\), dạng hàm cơ bản \\(b(\\textbf{x},\\Theta)\\), và cho \\(f_0(\\textbf{x}) = 0\\).Bước 1: Lựa chọn hàm tổn thất \\(\\sum\\limits_{=1}^n L(y_i, \\hat{y}_i)\\), dạng hàm cơ bản \\(b(\\textbf{x},\\Theta)\\), và cho \\(f_0(\\textbf{x}) = 0\\).Bước 2: với mỗi \\(m = 1, 2, \\cdots, M\\), tìm tham số (\\(\\lambda_m\\),\\(\\Theta_m\\)) như sau\n\\[\\begin{align}\n(\\lambda_m,\\Theta_m) = \\underset{\\lambda,\\Theta}{\\operatorname{argmax}} \\sum\\limits_{=1}^n L\\left( y_i, f_{m-1}(x_i) + \\lambda \\cdot b(\\textbf{x}_i,\\Theta) \\right)\n\\tag{16.2}\n\\end{align}\\]Bước 2: với mỗi \\(m = 1, 2, \\cdots, M\\), tìm tham số (\\(\\lambda_m\\),\\(\\Theta_m\\)) như sau\n\\[\\begin{align}\n(\\lambda_m,\\Theta_m) = \\underset{\\lambda,\\Theta}{\\operatorname{argmax}} \\sum\\limits_{=1}^n L\\left( y_i, f_{m-1}(x_i) + \\lambda \\cdot b(\\textbf{x}_i,\\Theta) \\right)\n\\tag{16.2}\n\\end{align}\\]Bước 3: cho \\(f_m(\\textbf{x}) = f_{m-1}(\\textbf{x}) + \\lambda_m \\cdot b(\\textbf{x}_i,\\Theta_m)\\).Bước 3: cho \\(f_m(\\textbf{x}) = f_{m-1}(\\textbf{x}) + \\lambda_m \\cdot b(\\textbf{x}_i,\\Theta_m)\\).Tại mỗi bước \\(m = 1, 2, \\cdots, M\\), chúng ta cần phải tìm các tham số (\\(\\lambda_m\\),\\(\\theta_m\\)) để tối thiểu hóa một hàm tổn thất. Khi giải bài toán tối ưu, lời giải chính xác luôn được ưu tiên trước, nếu không thể giải bằng lời giải chính xác mới cần sử dụng phương pháp số. Việc tồn tại hay không tồn tại lời giải chính xác cho mỗi bước \\(m\\) phụ thuộc vào lựa chọn hàm tổn thất \\(L\\) và hàm cơ bản \\(b\\). Hàm cơ bản \\(b\\) thường được lựa chọn ở mức độ đơn giản nhất, chẳng hạn như 1 cây quyết định với 1 node. Hàm tổn thất có thể là hàm tổn thất kiểu mũ, hàm tổn thất kiểu trung bình sai số, hàm hợp lý,…Ví dụ 1: trong bài toán hồi quy, khi hàm tổn thất là hàm tổng sai số bình phương,\n\\[\\begin{align}\nL(y_i, \\hat{y}_i) = \\cfrac{1}{2} \\sum\\limits_{=1}^n (y_i - \\hat{y}_i)^2\n\\end{align}\\]\ntham số \\((\\lambda_m,\\theta_m)\\) là lời giải của bài toán tối ưu sau\n\\[\\begin{align}\n(\\lambda_m,\\theta_m) & = \\underset{\\lambda,\\theta}{\\operatorname{argmin}} \\cfrac{1}{2} \\sum\\limits_{=1}^n \\left(y_i - f_{m-1}(x_i) - \\lambda \\cdot b(\\textbf{x}_i,\\theta) \\right)^2 \\\\\n& = \\underset{\\lambda,\\theta}{\\operatorname{argmin}} \\cfrac{1}{2} \\sum\\limits_{=1}^n \\left(\\epsilon_{,m-1}  - \\lambda \\cdot b(\\textbf{x}_i,\\theta) \\right)^2\n\\end{align}\\]\ntrong đó \\(\\epsilon_{,m-1}\\) là sai số của thuật toán Boosting sau bước thứ \\((m-1)\\). Bạn đọc có thể thấy rằng nếu chúng ta chọn hàm tổn thất là tổng sai số bình phương, tại bước thứ \\(m\\) của quá trình boosting, chúng ta sẽ cần tìm các hệ số \\((\\lambda_m,\\theta_m)\\) sao cho tổng sai số giữa \\(\\lambda_m \\cdot b(\\textbf{x}_i,\\theta_m)\\) và sai số tại bước thứ \\((m-1)\\), \\(\\epsilon_{,m-1}\\) là nhỏ nhất.Ví dụ 1: trong bài toán hồi quy, khi hàm tổn thất là hàm tổng sai số bình phương,\n\\[\\begin{align}\nL(y_i, \\hat{y}_i) = \\cfrac{1}{2} \\sum\\limits_{=1}^n (y_i - \\hat{y}_i)^2\n\\end{align}\\]\ntham số \\((\\lambda_m,\\theta_m)\\) là lời giải của bài toán tối ưu sau\n\\[\\begin{align}\n(\\lambda_m,\\theta_m) & = \\underset{\\lambda,\\theta}{\\operatorname{argmin}} \\cfrac{1}{2} \\sum\\limits_{=1}^n \\left(y_i - f_{m-1}(x_i) - \\lambda \\cdot b(\\textbf{x}_i,\\theta) \\right)^2 \\\\\n& = \\underset{\\lambda,\\theta}{\\operatorname{argmin}} \\cfrac{1}{2} \\sum\\limits_{=1}^n \\left(\\epsilon_{,m-1}  - \\lambda \\cdot b(\\textbf{x}_i,\\theta) \\right)^2\n\\end{align}\\]\ntrong đó \\(\\epsilon_{,m-1}\\) là sai số của thuật toán Boosting sau bước thứ \\((m-1)\\). Bạn đọc có thể thấy rằng nếu chúng ta chọn hàm tổn thất là tổng sai số bình phương, tại bước thứ \\(m\\) của quá trình boosting, chúng ta sẽ cần tìm các hệ số \\((\\lambda_m,\\theta_m)\\) sao cho tổng sai số giữa \\(\\lambda_m \\cdot b(\\textbf{x}_i,\\theta_m)\\) và sai số tại bước thứ \\((m-1)\\), \\(\\epsilon_{,m-1}\\) là nhỏ nhất.Ví dụ 2: trong bài toán phân loại mà biến mục tiêu \\(y\\) chỉ nhận hai giá trị là -1 hoặc 1, Freund và Schapire (1997) lựa chọn hàm tổn thất kiểu mũ\n\\[\\begin{align}\nL(y_i, \\hat{y}_i) = \\sum\\limits_{=1}^n exp(- y_i \\cdot \\hat{y_i})\n\\end{align}\\]\ntham số \\((\\lambda_m,\\theta_m)\\) là lời giải của bài toán tối ưu sau\n\\[\\begin{align}\n(\\lambda_m,\\theta_m) & = \\underset{\\lambda,\\theta}{\\operatorname{argmin}} \\sum\\limits_{=1}^n \\exp\\left[- y_i  \\cdot \\left(f_{m-1}(x_i) + \\lambda \\cdot b(\\textbf{x}_i,\\theta) \\right) \\right] \\\\\n& = \\underset{\\lambda,\\theta}{\\operatorname{argmin}} \\sum\\limits_{=1}^n \\exp\\left[ - y_i  \\cdot f_{m-1}(x_i)\\right]  \\cdot \\exp\\left[- y_i \\cdot \\lambda \\cdot b(\\textbf{x}_i,\\theta)  \\right] \\\\\n& = \\underset{\\lambda,\\theta}{\\operatorname{argmin}} \\sum\\limits_{=1}^n w_i^{(m)}  \\cdot \\exp\\left[- \\lambda \\cdot y_i \\cdot b(\\textbf{x}_i,\\theta)  \\right]\n\\end{align}\\]\nvới \\(w_i^{(m)} = \\exp\\left[ - y_i \\cdot f_{m-1}(x_i)\\right]\\). Bạn đọc có thể thấy rằng \\(w_i^{(m)}\\) không phụ thuộc vào \\(\\lambda\\) hay \\(\\theta\\) nên có thể coi như trọng số tương ứng với dữ liệu thứ \\(\\).Ví dụ 2: trong bài toán phân loại mà biến mục tiêu \\(y\\) chỉ nhận hai giá trị là -1 hoặc 1, Freund và Schapire (1997) lựa chọn hàm tổn thất kiểu mũ\n\\[\\begin{align}\nL(y_i, \\hat{y}_i) = \\sum\\limits_{=1}^n exp(- y_i \\cdot \\hat{y_i})\n\\end{align}\\]\ntham số \\((\\lambda_m,\\theta_m)\\) là lời giải của bài toán tối ưu sau\n\\[\\begin{align}\n(\\lambda_m,\\theta_m) & = \\underset{\\lambda,\\theta}{\\operatorname{argmin}} \\sum\\limits_{=1}^n \\exp\\left[- y_i  \\cdot \\left(f_{m-1}(x_i) + \\lambda \\cdot b(\\textbf{x}_i,\\theta) \\right) \\right] \\\\\n& = \\underset{\\lambda,\\theta}{\\operatorname{argmin}} \\sum\\limits_{=1}^n \\exp\\left[ - y_i  \\cdot f_{m-1}(x_i)\\right]  \\cdot \\exp\\left[- y_i \\cdot \\lambda \\cdot b(\\textbf{x}_i,\\theta)  \\right] \\\\\n& = \\underset{\\lambda,\\theta}{\\operatorname{argmin}} \\sum\\limits_{=1}^n w_i^{(m)}  \\cdot \\exp\\left[- \\lambda \\cdot y_i \\cdot b(\\textbf{x}_i,\\theta)  \\right]\n\\end{align}\\]\nvới \\(w_i^{(m)} = \\exp\\left[ - y_i \\cdot f_{m-1}(x_i)\\right]\\). Bạn đọc có thể thấy rằng \\(w_i^{(m)}\\) không phụ thuộc vào \\(\\lambda\\) hay \\(\\theta\\) nên có thể coi như trọng số tương ứng với dữ liệu thứ \\(\\).Từ kết quả của ví dụ 2, chúng ta đã có thể giải thích các bước trong thuật toán AdaBoost.M1. Với mọi \\(\\lambda > 0\\) và với một lựa chọn của hàm \\(b\\), tham số \\(\\theta_m\\) là giá trị tối thiểu hóa hàm tổn thất\n\\[\\begin{align}\n(\\theta_m) & = \\underset{\\theta}{\\operatorname{argmin}} \\sum\\limits_{=1}^n w_i^{(m)}  \\cdot \\exp\\left[- \\lambda \\cdot y_i \\cdot b(\\textbf{x}_i,\\theta)  \\right] \\\\\n\\tag{16.3}\n\\end{align}\\]\nBiến đổi công thức phía bên phải của phương trình (16.3) chúng ta có\n\\[\\begin{align}\n\\sum\\limits_{=1}^n w_i^{(m)}  \\cdot \\exp\\left[- \\lambda \\cdot y_i \\cdot b(\\textbf{x}_i,\\theta)  \\right] & = \\sum\\limits_{=1}^n w_i^{(m)}  \\cdot \\left[ e^{-\\lambda} \\cdot \\mathbb{}(y_i = b(\\textbf{x}_i,\\theta)) + e^{\\lambda} \\cdot \\mathbb{}(y_i \\neq b(\\textbf{x}_i,\\theta))  \\right]\\\\\n& = \\sum\\limits_{=1}^n w_i^{(m)}  \\cdot e^{-\\lambda} +  \\sum\\limits_{=1}^n w_i^{(m)}  \\cdot \\left[ e^{\\lambda} - e^{-\\lambda} \\right] \\cdot \\mathbb{}(y_i \\neq b(\\textbf{x}_i,\\theta)) \\\\\n& = e^{-\\lambda} \\cdot \\sum\\limits_{=1}^n w_i^{(m)}  + \\left[ e^{\\lambda} - e^{-\\lambda} \\right] \\cdot  \\sum\\limits_{=1}^n w_i^{(m)}  \\cdot  \\mathbb{}(y_i \\neq b(\\textbf{x}_i,\\theta))\n\\tag{16.4}\n\\end{align}\\]\\(e^{\\lambda} - e^{-\\lambda} > 0\\) và các \\(w_i^{(m)}\\) không phụ thuộc vào \\(\\theta\\) nên ta có giá trị \\(\\theta_m\\) tối thiểu hóa hàm tổn thất trong phương trình (16.3) cũng là giá trị \\(\\theta_m\\) tối thiểu hóa sai số dự đoán\n\\[\\begin{align}\n(\\theta_m) & = \\underset{\\theta}{\\operatorname{argmin}} \\sum\\limits_{=1}^n w_i^{(m)}  \\cdot  \\mathbb{}(y_i \\neq b(\\textbf{x}_i,\\theta))\n\\tag{16.5}\n\\end{align}\\]Với mỗi \\(\\theta_m\\) là lời giải của (16.5), chúng ta có giá trị \\(\\lambda_m\\) để tối thiểu hóa giá trị hàm tổn thất trong phương trình (16.3) là lời giải của phương trình\n\\[\\begin{align}\n\\sum\\limits_{=1}^n w_i^{(m)}  \\cdot \\cfrac{\\partial \\exp\\left[- \\lambda \\cdot y_i \\cdot b(\\textbf{x}_i,\\theta_m)  \\right]}{\\partial \\lambda} = 0 \\\\\n\\end{align}\\]Lấy đạo hàm của vế phải của phương trình (16.5) theo \\(\\lambda\\) chúng ta có:\n\\[\\begin{align}\n& - e^{-\\lambda_m} \\cdot \\sum\\limits_{=1}^n w_i^{(m)}  + \\left[ e^{\\lambda_m} + e^{-\\lambda_m} \\right] \\cdot  \\sum\\limits_{=1}^n w_i^{(m)}  \\cdot  \\mathbb{}(y_i \\neq b(\\textbf{x}_i,\\theta_m)) = 0 \\\\\n& \\rightarrow \\lambda_m = \\cfrac{1}{2} \\cdot \\log\\left(\\cfrac{1}{err_m} - 1 \\right)\n\\end{align}\\]\nvới \\(err_m\\) là sai số của hàm phân loại \\(b(\\textbf{x}_i,\\theta_m)\\)\n\\[\\begin{align}\nerr_m = \\cfrac{\\sum\\limits_{=1}^n w_i^{(m)}  \\cdot  \\mathbb{}(y_i \\neq b(\\textbf{x}_i,\\theta_m))}{\\sum\\limits_{=1}^n w_i^{(m)}}\n\\end{align}\\]Chúng ta cập nhật trọng số cho bước tiếp theo như sau\n\\[\\begin{align}\nw_i^{(m+1)}& = \\exp\\left[- y_i \\cdot f_m(x_i)\\right] \\\\\n& = exp\\left[- y_i \\cdot f_{m-1}(x_i) - y_i \\lambda_m b(x_i,\\theta_m) \\right] \\\\\n& = w_i^{(m)} \\cdot \\exp\\left[ \\lambda_m \\cdot(2 \\mathbb{}(b(x_i,\\theta_m) \\neq y_i) - 1)  \\right] \\\\\n& = w_i^{(m)} \\cdot \\exp\\left[ (2\\lambda_m) \\cdot \\mathbb{}(b(x_i,\\theta_m) \\neq y_i)  \\right] \\cdot \\exp(-\\lambda_m)\n\\end{align}\\]\nCông thức ở trên tương đương với bước 2.(d) trong thuật toán AdaBoost.M1 với \\((2\\lambda_m) = \\alpha_m\\). Giá trị \\(\\exp(-\\lambda_m)\\) không ảnh hưởng đến trọng số vì không phụ thuộc vào \\(\\).Ví dụ 1: chúng ta sẽ áp dụng thuật toán AdaBoost.M1 trên một dữ liệu có 10 quan sát như dưới đây. Dữ liệu có biến mục tiêu \\(Y\\) nhận giá trị 1 khi khách hàng đồng ý mua sản phẩm và nhận giá trị -1 khi khách hành không đồng ý. Có bốn biến giải thích là độ tuổi (\\(Age\\)), số năm kinh nghiệm lái xe (\\(seniority\\)), giới tính (\\(sex\\)) và thành thị (\\(urban\\)). Hàm phân loại chúng ta lựa chọn là cây quyết định có 1 node duy nhất.Tại bước \\(m=1\\) chúng ta có tỷ trọng của mỗi hàng dữ liệu là \\(w^{(1)}_i = 0.1\\); để xây dựng mô hình cây quyết định với 1 node và tôi thiểu hóa sai số trên dữ liệu, chúng ta thử trên từng cột dữ liệuCột \\(age\\), bạn đọc có thể kiểm tra rằng tại điểm cắt 55.5 (tuổi), cây quyết định cho sai số có trọng số \\(w^{(1)}_i\\) nhỏ nhất là 0.3. Lưu ý rằng điểm cắt 48 tuổi cũng có sai số là 0.3 tuy nhiên điểm cắt này chia dữ liệu thành một phần chỉ có 2 quan sát và một phần có 8 quan sát nên ít tối ưu hơn với điểm cắt 55.5.Cột \\(age\\), bạn đọc có thể kiểm tra rằng tại điểm cắt 55.5 (tuổi), cây quyết định cho sai số có trọng số \\(w^{(1)}_i\\) nhỏ nhất là 0.3. Lưu ý rằng điểm cắt 48 tuổi cũng có sai số là 0.3 tuy nhiên điểm cắt này chia dữ liệu thành một phần chỉ có 2 quan sát và một phần có 8 quan sát nên ít tối ưu hơn với điểm cắt 55.5.Cột \\(seniority\\), điểm cắt tối ưu là 24.5 (năm) và cũng cho sai số có trọng số là 0.3Cột \\(seniority\\), điểm cắt tối ưu là 24.5 (năm) và cũng cho sai số có trọng số là 0.3Cột \\(sex\\) chỉ có một lựa chọn là chia dữ liệu thành hai phần, Male và Female, cho sai số có trọng số là 0.3Cột \\(sex\\) chỉ có một lựa chọn là chia dữ liệu thành hai phần, Male và Female, cho sai số có trọng số là 0.3Cột \\(urban\\) chỉ có một lựa chọn là chia dữ liệu thành 0 và 1, cũng cho sai số có trọng số là 0.4Cột \\(urban\\) chỉ có một lựa chọn là chia dữ liệu thành 0 và 1, cũng cho sai số có trọng số là 0.4Chúng ta chọn cây quyết định dựa trên biến \\(age\\) với điểm cắt là 55.5 tuổi là hàm phân loại tại \\(m = 1\\). Cây quyết định cho giá trị là 1 khi biến \\(age\\) nhỏ hơn 55.5 và cho giá trị là -1 khi biến \\(age\\) cho giá trị lớn hơn 1. Hệ số của hàm phân loại thứ nhất trong hàm phân loại tổng là\n\\[\\begin{align}\n\\alpha_1 = log(\\cfrac{1 - err_1}{err_1}) = log(\\cfrac{1-0.3}{0.3}) = 0.8473\n\\end{align}\\]Trọng số cho bước thứ 2 được cập nhật, theo công thức 2.(d) như sau\n\\[\\begin{align}\nw^{(2)}_i = w^{(1)}_i \\cdot \\exp\\left[ 0.8473 \\cdot \\mathbb{} \\left(f_m^C(\\textbf{x}_i) \\neq y_i \\right) \\right] =\n  \\begin{cases}\n  0.1 \\cdot e^0 \\textit{ nếu }  f_m^C(\\textbf{x}_i) = y_i \\\\\n  0.1 \\cdot e^{0.8473} \\textit{ nếu }  f_m^C(\\textbf{x}_i) \\neq y_i\n  \\end{cases}\n\\end{align}\\]Bạn đọc có thể thấy rằng trọng số cho 3 hàng bị dự đoán sai đã được tăng lên thành \\(0.1 \\times e^{0.8473}\\) trong khi trọng số cho 7 hàng được dự đoán đúng vẫn là 0.1. Chuẩn hóa lại trọng số để có tổng bằng 1 chúng ta có bảng sauTại bước \\(m=2\\), chúng ta cần tìm cây quyết định để tối thiểu hóa sai số có trọng số \\(w^{(2)}\\) như bảng ở trên.Cột \\(age\\), bạn đọc có thể kiểm tra rằng tại điểm cắt 64.5 (tuổi), cây quyết định cho sai số có trọng số \\(w^{(2)}_i\\) nhỏ nhất là 0.342Cột \\(age\\), bạn đọc có thể kiểm tra rằng tại điểm cắt 64.5 (tuổi), cây quyết định cho sai số có trọng số \\(w^{(2)}_i\\) nhỏ nhất là 0.342Cột \\(seniority\\), điểm cắt tối ưu là 24.5 (năm) cho sai số có trọng số là 0.329Cột \\(seniority\\), điểm cắt tối ưu là 24.5 (năm) cho sai số có trọng số là 0.329Cột \\(sex\\) chỉ có một lựa chọn là chia dữ liệu thành hai phần, Male và Female, cho sai số có trọng số là 0.329Cột \\(sex\\) chỉ có một lựa chọn là chia dữ liệu thành hai phần, Male và Female, cho sai số có trọng số là 0.329Cột \\(urban\\) chỉ có một lựa chọn là chia dữ liệu thành 0 và 1, cũng cho sai số có trọng số là 0.342Cột \\(urban\\) chỉ có một lựa chọn là chia dữ liệu thành 0 và 1, cũng cho sai số có trọng số là 0.342Như vậy, cây quyết định tại bước thứ hai có thể dựa trên biến \\(seniority\\) hoặc \\(sex\\). Chúng ta sẽ lựa chọn biến \\(seniority\\) với điểm cắt là 24.5 (năm). Cây quyết định trả lại giá trị là \\(1\\) khi \\(seniority > 24.5\\) và trả lại giá trị \\(-1\\) khi \\(seniority < 24.5\\). Hệ số của hàm phân loại thứ hai trong hàm phân loại tổng là\n\\[\\begin{align}\n\\alpha_2 = log(\\cfrac{1-0.342}{0.342}) = 0.654\n\\end{align}\\]Với \\(M = 2\\) chúng ta có hàm phân loại từ thuật toán AdaBoost.M1 được xây dựng như sau\n\\[\\begin{align}\nf^C = sign(0.8473 \\cdot f^C_1 + 3.204 \\cdot f^C_2) = \\begin{cases}\n1 \\textit{ nếu } age < 55.5 \\textit{ và } seniority < 24.5 \\\\\n-1 \\textit{ nếu } age > 55.5 \\textit{ và } seniority > 24.5 \\\\\n1 \\textit{ nếu } age < 55.5 \\textit{ và } seniority < 24.5 \\\\\n-1 \\textit{ nếu } age > 55.5 \\textit{ và } seniority > 24.5\n\\end{cases}\n\\end{align}\\]","code":"\ndf<-read.csv(\"../KHDL_KTKD Final/Dataset/AdaBoostM1Example1.csv\")\nknitr::kable(df, booktabs = T,\n      col.names = c(\"Tuổi\", \"Kinh nghiệm\", \"Giới tính\", \"Thành thị\", \"Lựa chọn\"),\n      escape=F, align = 'r') %>%\n  #column_spec(c(1,4,5,6,7),border_left = T) %>% column_spec(7,border_right = T) %>% \n  kable_styling(latex_options = \"scale_down\",full_width = F)\ndf<-mutate(df, w1 = 0.1, pred1 = ifelse(age<48,1,-1))\ndf<-mutate(df, w2 = ifelse(Y == pred1, 0.1,exp(0.8573)))\ndf<-mutate(df, w2 = round(w2/sum(w2),3))\nknitr::kable(df, booktabs = T,\n      col.names = c(\"Tuổi\", \"Kinh nghiệm\", \"Giới tính\", \"Thành thị\", \"Y\", \"$w^{(1)}$\", \"$b(x_i,\\\\theta_1)$\", \"$w^{(2)}$\" ),\n      escape=F, align = 'r') %>%\n  kable_styling(latex_options = \"scale_down\",full_width = F)"},{"path":"boosting-và-cây-quyết-định..html","id":"hàm-cơ-bản-dạng-cây-quyết-định","chapter":"Chương 16 Boosting và cây quyết định.","heading":"16.1.2 Hàm cơ bản dạng cây quyết định","text":"Cây quyết định thảo luận trong phần trước là kỹ thuật chia không gian tất cả các biến giải thích thành \\(K\\) phần không giao nhau, \\(R_1, R_2, \\cdots, R_K\\), mỗi phần được mô tả bởi một lá của cây quyết định. Một hằng số \\(\\gamma_k\\) được gán cho giá trị của mỗi vùng \\(R_k\\) và nguyên tắc dự báo đơn giản là:\n\\[\\begin{align}\nx \\R_k \\rightarrow f(x) = \\gamma_k\n\\end{align}\\]\nNói cách khác, cây quyết định có thể được viết dưới dạng như sau\n\\[\\begin{align}\nT(x; \\Theta) =  \\sum\\limits_{k=1}^K \\gamma_k \\cdot \\mathbb{}(x \\R_k)\n\\end{align}\\]\nTham số của cây quyết định bao gồm có 1. cách phân vùng không gian các biến độc lập \\(R_1, R_2, \\cdots, R_K\\); và 2. các hằng số \\(\\gamma_k\\), \\(k = 1, 2, \\cdots K\\).Ước lượng tham số của cây quyết định bao gồm việc xác định phân vùng \\(\\{R_k\\}\\) và xác định các tham số \\(\\gamma_k\\). Thông thường thì ước lượng tham số sẽ bao gồm hai bướcThứ nhất: với mỗi phân vùng \\(\\{R_k\\}\\) cho trước, xác định \\(\\gamma_k\\) để tối thiểu hóa hàm tổn thất trên miền \\(R_k\\) tương ứng\n\\[\\begin{align}\n\\gamma_k = \\underset{\\gamma}{\\operatorname{argmin}} \\sum\\limits_{x_i \\R_k} L(y_i, \\gamma)\n\\end{align}\\]\nTùy vào hàm \\(L\\) mà giá trị của \\(\\gamma_k\\) sẽ có quy tắc xác định khác nhau: với hàm \\(L\\) là tổng bình phương sai số, \\(\\gamma_k\\) là giá trị trung bình của \\(y_i\\) trên phân vùng \\(R_k\\); với hàm \\(L\\) là tổng giá trị tuyệt đối sai số, \\(\\gamma_k\\) là giá trị trung vị của \\(y_i\\) trên phân vùng \\(R_k\\); với \\(L\\) là sai số phân loại, \\(\\gamma_k\\) là giá trị mode của \\(y_i\\),…Thứ nhất: với mỗi phân vùng \\(\\{R_k\\}\\) cho trước, xác định \\(\\gamma_k\\) để tối thiểu hóa hàm tổn thất trên miền \\(R_k\\) tương ứng\n\\[\\begin{align}\n\\gamma_k = \\underset{\\gamma}{\\operatorname{argmin}} \\sum\\limits_{x_i \\R_k} L(y_i, \\gamma)\n\\end{align}\\]\nTùy vào hàm \\(L\\) mà giá trị của \\(\\gamma_k\\) sẽ có quy tắc xác định khác nhau: với hàm \\(L\\) là tổng bình phương sai số, \\(\\gamma_k\\) là giá trị trung bình của \\(y_i\\) trên phân vùng \\(R_k\\); với hàm \\(L\\) là tổng giá trị tuyệt đối sai số, \\(\\gamma_k\\) là giá trị trung vị của \\(y_i\\) trên phân vùng \\(R_k\\); với \\(L\\) là sai số phân loại, \\(\\gamma_k\\) là giá trị mode của \\(y_i\\),…Thứ hai: là việc xác định phân vùng \\(\\{R_k\\}\\). Đây là một vấn đề không đơn giản và không có lời giải chính xác. Hướng tiếp cận thường là giống như cách xác định cây quyết định như trong phần trước. Chúng ta thử phân vùng trên từng biến giải thích riêng lẻ và lựa chọn phân vùng tốt nhất dựa trên sai số phân loại, hệ số gini, hoặc hệ số entropy và tiếp tục quá trình đó cho đến khi một số chỉ tiêu đạt được.Thứ hai: là việc xác định phân vùng \\(\\{R_k\\}\\). Đây là một vấn đề không đơn giản và không có lời giải chính xác. Hướng tiếp cận thường là giống như cách xác định cây quyết định như trong phần trước. Chúng ta thử phân vùng trên từng biến giải thích riêng lẻ và lựa chọn phân vùng tốt nhất dựa trên sai số phân loại, hệ số gini, hoặc hệ số entropy và tiếp tục quá trình đó cho đến khi một số chỉ tiêu đạt được.Chúng ta ký hiệu các hàm cơ bản là \\(T\\) (viết tắt của Tree) thay vì \\(b\\) như trong phần trước. Sau \\(M\\) bước boosting, chúng ta có hàm phân loại (hoặc hồi quy) có dạng\n\\[\\begin{align}\nf(\\textbf{x}) = \\sum\\limits_{m=1}^M \\lambda_m \\cdot T(\\textbf{x},\\Theta_m)\n\\end{align}\\]hàm \\(T(.)\\) nhận giá trị là hằng số trên mỗi phân vùng nên việc thêm tham số \\(\\lambda_m\\) là không cần thiết. đó các tham số cần ước lượng chỉ bao gồm tham số \\(\\Theta_m\\). Tại mỗi bước \\(m\\), chúng ta cần tìm tham số \\(\\Theta_m\\) của cây quyết định \\(T(\\textbf{x},.)\\) sao cho\n\\[\\begin{align}\n(\\Theta_m) = \\underset{\\Theta_m}{\\operatorname{argmin}} \\sum\\limits_{= 1}^n L\\left(y_i, f_{m-1}(\\textbf{x}_i) + T(\\textbf{x}_i, \\Theta) \\right)\n\\tag{16.6}\n\\end{align}\\]\nNhắc lại rằng tham số \\(\\Theta_m\\) bao gồm có phân vùng \\(\\{R^{(m)}_k\\}\\) tại bước thứ \\(m\\) và hằng số \\(\\gamma^{(m)}_k\\) của vùng \\(R^{(m)}_k\\). Tương tự như bài toán ước lượng tham số của cây quyết định thông thường, tại bước thứ \\(m\\) trong kỹ thuật boosting, nếu cho trước vùng \\(R^{(m)}_k\\), \\(\\gamma^{(m)}_k\\) sẽ được xác định như sau\n\\[\\begin{align}\n(\\gamma^{m}_k) = \\underset{\\gamma}{\\operatorname{argmin}} \\sum\\limits_{\\textbf{x}_i \\R^{(m)}_k} L\\left(y_i, f_{m-1}(\\textbf{x}_i) + \\gamma \\right)\n\\end{align}\\]Việc xác định phân vùng \\(\\{R^{(m)}_k\\}\\) là không đơn giản, thậm chí còn khó khăn hơn với việc tìm phân vùng cho một cây quyết định riêng lẻ. Trong một vài trường hợp, tìm kiếm phân vùng tối ưu sẽ có lời giải:Ví dụ 1: trong bài toán hồi quy và hàm tổn thất là hàm tổng bình phương sai số, tìm cây quyết định \\(T(\\textbf{x}_i, \\Theta_m)\\) tương đương với bài toán tìm cây quyết định với ma trận biến giải thích \\(\\textbf{x}\\) và biến mục tiêu là \\(y - f_{m-1}(\\textbf{x})\\). Với mỗi phân vùng \\(\\{R^{(m)}_k\\}\\), giá trị \\(\\gamma^{(m)}_k\\) là giá trị trung bình của \\(y - f_{m-1}(\\textbf{x})\\) trên miền \\(R^{(m)}_k\\).Ví dụ 1: trong bài toán hồi quy và hàm tổn thất là hàm tổng bình phương sai số, tìm cây quyết định \\(T(\\textbf{x}_i, \\Theta_m)\\) tương đương với bài toán tìm cây quyết định với ma trận biến giải thích \\(\\textbf{x}\\) và biến mục tiêu là \\(y - f_{m-1}(\\textbf{x})\\). Với mỗi phân vùng \\(\\{R^{(m)}_k\\}\\), giá trị \\(\\gamma^{(m)}_k\\) là giá trị trung bình của \\(y - f_{m-1}(\\textbf{x})\\) trên miền \\(R^{(m)}_k\\).Ví dụ 2: trong bài toán phân loại nhị phân và hàm tổn thất kiểu mũ, ước lượng tham số cho cây quyết định trong phương trình (16.6) được viết lại như sau\n\\[\\begin{align}\n(\\Theta_m) = \\underset{\\Theta_m}{\\operatorname{argmin}} \\sum\\limits_{= 1}^n w^{(m)}_i \\exp\\left[- y_i T(\\textbf{x}_i, \\Theta) \\right]\n\\tag{16.7}\n\\end{align}\\]\nvới \\(w^{(m)}_i = \\exp(-y_i f_{m-1}(\\textbf{x}_i))\\). Để giải bài toán tối ưu ở trên, phương pháp tiếp cận sẽ là thử qua các phân vùng có thể tìm kiếm được phân vùng tốt nhất (tìm kiếm tham lam). Trên mỗi vùng \\(R^{(m)}_k\\), giá trị \\(\\gamma^{(m)}_k\\) tối thiểu hóa giá trị hàm tổn thất trên vùng đó là\n\\[\\begin{align}\n\\gamma^{(m)}_k = \\log \\left(\\cfrac{\\sum\\limits_{x_i \\R^{(m)}_k} w^{(m)}_i \\mathbb{}(y_i = 1)}{\\sum\\limits_{x_i \\R^{(m)}_k} w^{(m)}_i \\mathbb{}(y_i = -1)} \\right)\n\\end{align}\\]Ví dụ 2: trong bài toán phân loại nhị phân và hàm tổn thất kiểu mũ, ước lượng tham số cho cây quyết định trong phương trình (16.6) được viết lại như sau\n\\[\\begin{align}\n(\\Theta_m) = \\underset{\\Theta_m}{\\operatorname{argmin}} \\sum\\limits_{= 1}^n w^{(m)}_i \\exp\\left[- y_i T(\\textbf{x}_i, \\Theta) \\right]\n\\tag{16.7}\n\\end{align}\\]\nvới \\(w^{(m)}_i = \\exp(-y_i f_{m-1}(\\textbf{x}_i))\\). Để giải bài toán tối ưu ở trên, phương pháp tiếp cận sẽ là thử qua các phân vùng có thể tìm kiếm được phân vùng tốt nhất (tìm kiếm tham lam). Trên mỗi vùng \\(R^{(m)}_k\\), giá trị \\(\\gamma^{(m)}_k\\) tối thiểu hóa giá trị hàm tổn thất trên vùng đó là\n\\[\\begin{align}\n\\gamma^{(m)}_k = \\log \\left(\\cfrac{\\sum\\limits_{x_i \\R^{(m)}_k} w^{(m)}_i \\mathbb{}(y_i = 1)}{\\sum\\limits_{x_i \\R^{(m)}_k} w^{(m)}_i \\mathbb{}(y_i = -1)} \\right)\n\\end{align}\\]","code":""},{"path":"boosting-và-cây-quyết-định..html","id":"hàm-tổn-thất","chapter":"Chương 16 Boosting và cây quyết định.","heading":"16.1.3 Hàm tổn thất","text":"Thuật toán AdaBoost.M1 mặc dù được giải thích dựa trên hàm tổn thất kiểu mũ, nhưng phương pháp tiếp cận ban đầu của thuật toán này lại không bắt đầu từ hàm tổn thất. Sau khi thuật toán được công bố thì mối liên hệ của thuật toán này và hàm phân loại kiểu mũ mới được tìm ra. Hàm tổn thất kiểu mũ trong ngữ cảnh của boosting có ưu điểm là cho phép tính toán nhanh và cho công thức chính xác của tỷ trọng trong các bước của quá trình boosting. Trong phần này của chương sách, chúng ta sẽ thảo luận kỹ hơn về hàm tổn thất kiểu mũ nói riêng các hàm tổn thất khác nói chung được sử dụng thường xuyên trong các bài toán hồi quy và phân loại.Với biến mục tiêu chỉ nhận hai giá trị là -1 và 1, có thể chứng minh được rằng giá trị \\(\\hat{y}^*\\) để tối thiểu giá trị trung bình của \\(\\exp(-Y \\hat{y} )\\) được xác định như sau\n\\[\\begin{align}\n\\hat{y}^* = \\underset{\\hat{y}}{\\operatorname{argmin}} \\mathbb{E}\\left( \\exp(-Y \\hat{y}) \\right) = \\cfrac{1}{2} \\cdot \\log \\left( \\cfrac{\\mathbb{P}(Y = 1)}{\\mathbb{P}(Y = -1)}\\right)\n\\end{align}\\]\nGiá trị tối ưu này giải thích tại sao trong bước (3). của AdaBoost.M1 lại lấy dấu của hàm \\(f^C(x_i) = \\left( \\sum\\limits_{m=1}^M \\alpha_m \\cdot f_m^C(x_i) \\right)\\) để dự đoán giá trị của \\(y_i\\). Quy tắc đơn giản là nếu \\(f^C(x_i) > 0\\), \\(f^C(x_i)\\) được xây dựng để xấp xỉ đến \\(\\hat{y_i}^*\\) nên ta có \\(\\log \\left( \\cfrac{\\mathbb{P}(Y = 1)}{\\mathbb{P}(Y = -1)}\\right) > 0\\) hay \\(\\mathbb{P}(Y_i = 1) > \\mathbb{P}(Y_i = -1)\\) đó \\(Y_i\\) có khả năng nhận giá trị bằng 1 cao hơn với khả năng nhận giá trị là -1.Hàm tổn thất kiểu mũ khác cũng cho giá trị tối ưu tương đương như hàm tổn thất kiểu mũ trong bài toán phân loại nhị phân là hàm cross-entropy. Hàm tổn thất cross-entropy hay còn gọi là \\(deviance\\) được sử dụng trong trường hợp biến \\(Y^{'}\\) có phân phối nhị thức với tham số \\(p\\) (chưa biết)\\[\\begin{align}\nl(Y^{'},\\hat{p}) = - \\left(Y^{'} log(\\hat{p}) + (1 - Y^{'}) log(1 - \\hat{p})\\right)\n\\tag{16.8}\n\\end{align}\\]\nvới \\(\\hat{p} \\(0,1)\\). Giá trị \\(\\hat{p}\\) để tối thiểu giá trị trung bình của hàm tổn thất là \\(\\hat{p}^* = \\mathbb{P}(Y^{'} = 1)\\).Với \\(\\hat{p} \\(0,1)\\), cho\n\\[\\begin{align}\n\\hat{y} = \\cfrac{1}{2} \\log\\left( \\cfrac{\\hat{p}}{1-\\hat{p}} \\right)\n\\end{align}\\]\nvà \\(Y = 2 \\times Y^{'} - 1\\), chúng ta sẽ thấy rằng bài toán tìm \\(\\hat{p}\\) để tối thiểu giá trị trung bình của hàm tổn thất trong (16.8) tương đương với việc tìm \\(\\hat{y}\\) để tối thiểu hóa hàm tổn thất kiểu mũ.Hàm tổn thất cross-entropy thường xuyên được sử dụng trong bài toán phân loại mà \\(Y\\) có thể nhận \\(J\\) giá trị. Giả sử các giá trị \\(Y\\) có thể nhận được mã hóa thành \\(J\\) số tự nhiên là \\(1, 2, \\cdots, J\\). Trong trường hợp này, hàm \\(f^C\\) là một véc-tơ có độ dài \\(J\\) mỗi phần tử trong véc-tơ là một hàm số thực. Giá sử \\(f^C_j\\) là phần tử thứ \\(j\\) của hàm \\(f^C\\), hàm tổn thất kiểu cross-entropy trong bài toán phân loại được viết như sau\n\\[\\begin{align}\nL(Y, \\hat{p}) & = - \\sum\\limits_{j = 1}^J \\mathbb{}(Y = j) \\log(\\hat{p}_j) \\\\\n& = - \\sum\\limits_{j = 1}^J \\mathbb{}(Y = j) \\cdot f^C_j + \\log \\left(\\sum\\limits_{j = 1}^J e^{f^C_j} \\right) \\text{ với } \\hat{p}_j & = \\cfrac{e^{f^C_l}}{\\sum\\limits_{l = 1}^J e^{f^C_l}}\n\\end{align}\\]Quá trình ước lượng hàm \\(f^C\\) trong kỹ thuật boosting bao gồm ước lượng các hàm \\(f^C_l\\) mà mỗi hàm đại diện cho 1 biến phân loại. Thông thường chúng ta sẽ cố định một thành phần bằng 0 để số lượng hàm số cần ước lượng tại mỗi bước là \\((J-1)\\).Những lý thuyết cơ bản về boosting mà chúng tôi trình bày từ đầu chương sách có thể gây khó hiểu cho bạn đọc không có nền tảng nâng cao về toán học. Tuy nhiên, các kiến thức này là cần thiết nếu bạn đọc muốn giải mã các thuật toán boosting cập nhật nhất hiện nay. Tại thời điểm chúng tôi viết cuốn sách này, các thuật toán như XGBoost hay LGBoost là các thuật toán học máy chiếm ưu thế hoàn toàn trong các cuộc thi về khoa học dữ liệu. Các thuật toán này dựa trên nền tảng lý thuyết mà chúng tôi đã trình bày ở trên kết hợp với một vài kỹ thuật tối ưu hóa bằng phương pháp số thay vì tối ưu hóa bằng lời giải chính xác như lý thuyết. Ý tưởng tối ưu hóa tại mỗi bước của kỹ thuật boosting dựa trên nguyên lý “gradient desent”, hay còn gọi là gradient boosting là chìa khóa để thuật toán boosting có thể được áp dụng với bất kỳ kiểu hàm tổn thất thất nào, và đó có thể được sử dụng trong cả bài toán hồi quy, bài toán phân loại nhị phân, hay bài toán phân loại nói chung.","code":""},{"path":"boosting-và-cây-quyết-định..html","id":"gradient-boosting","chapter":"Chương 16 Boosting và cây quyết định.","heading":"16.2 Gradient boosting","text":"Trong kỹ thuật boosting, tại mỗi bước \\(m\\) sẽ cần phải giải bài toán tối ưu. Bài toán tối ưu chỉ có lời giải với một số hàm tổn thất cụ thể như hàm tổn thất kiểu mũ trong bài toán phân loại nhị phân, hay hàm tổn thất kiểu tổng sai số bình phương trong bài toán hồi quy. Thêm vào đó, việc liên tục tìm kiếm giá trị tối ưu tại mỗi bước nhiều khả năng dẫn đến overfitting, nghĩa là hàm phân loại hay hồi quy tìm được sẽ cho sai số rất nhỏ trên tập dữ liệu huấn luyện mô hình, nhưng sai số trên tập kiểm thử sẽ lớn.Thay vì cố gắng giải bài toán tối ưu tại từng bước \\(m\\), kỹ thuật boosting theo gradient của hàm tổn thất, hay còn gọi là gradient boosting là một phương pháp tiếp cận có thể áp dụng với mọi hàm tổn thất và hạn chế được hiện tượng overfitting. Chúng ta sẽ thảo luận về kỹ thuật này trong các phần tiếp theo: trước tiên, chúng tôi sẽ giới thiệu về gradient boosting nói chung cùng với các ví dụ trên dữ liệu cụ thể, sau đó chúng tôi sẽ giới thiệu một kỹ thuật gradient boosting có ưu điểm vượt trội nhất hiện nay là eXtreme Gradient Boosting hay viết tắt là XGBoost.","code":""},{"path":"boosting-và-cây-quyết-định..html","id":"cơ-sở-toán-học-của-gradient-boosting.","chapter":"Chương 16 Boosting và cây quyết định.","heading":"16.2.1 Cơ sở toán học của gradient boosting.","text":"Nguyên lý chung của boosting là tại bước thứ \\(m\\), tìm một hàm cơ sở \\(b(\\textbf{x},\\Theta_m)\\) sao cho hàm tổn thất \\(L(y, \\hat{y})\\) tính tại \\(\\hat{y} = f_m(\\textbf{x}) = f_{m_1}(\\textbf{x}) + \\lambda_m b(\\textbf{x},\\Theta_m)\\) đạt giá trị nhỏ nhất. Tại bước thứ \\((m-1)\\) chúng ta có giá trị hàm tổn thất là \\(L(\\textbf{y}, \\hat{y})\\) đã được xác định trước. Chúng ta coi \\(f_{m_1}\\) là một véc-tơ có \\(n\\) thành phần \\(f_{m-1}(\\textbf{x}_i)\\) tương ứng với mỗi dữ liệu quan sát được và bỏ qua ràng buộc \\(b(\\textbf{x},\\Theta_m)\\) phải là một hàm có tham số \\(\\Theta_m\\) tính trên dữ liệu \\(\\textbf{x}\\). Khi đó cách tốt nhất để làm giảm giá trị của hàm tổn thất \\(L(y, f_{m-1}(\\textbf{x}))\\) khi thay đổi giá trị \\(f_{m-1}(\\textbf{x}))\\) là cho mỗi thành phần \\(f_{m-1}(\\textbf{x}_i)\\) thay đổi tỷ lệ nghịch với giá trị đạo hàm của hàm \\(L(y, f_{m-1}(\\textbf{x}))\\) tính tại \\(f_{m-1}(\\textbf{x}_i)\\). Gọi \\(g_{m-1,}\\) là đạo hàm của hàm \\(L(y_i, f_{m-1}(\\textbf{x}))\\) theo \\(f_{m-1}(\\textbf{x}_i)\\)\n\\[\\begin{align}\ng_{m-1,} = \\cfrac{ \\partial L(y_i, f_{m-1}(\\textbf{x}))}{\\partial f_{m-1}(\\textbf{x}_i)}\n\\end{align}\\]\nthì cách tốt nhất để giảm giá trị hàm tổn thất \\(L(\\textbf{y}, \\hat{y})\\) tính tại \\(\\hat{y} = f_{m-1}(\\textbf{x})\\) là thay đổi \\(\\hat{y}\\) như sau\n\\[\\begin{align}\n\\hat{y}_i = f_{m-1}(\\textbf{x}_i) - \\lambda_m \\cdot g_{m-1,}\n\\end{align}\\]\nvới \\(\\lambda_m\\) là hằng số không phụ thuộc vào \\(\\) làm thiểu hóa giá trị hàm tổn thất\n\\[\\begin{align}\n\\lambda_m = \\underset{\\lambda}{\\operatorname{argmin}} \\sum\\limits_{= 1}^n L\\left(y_i,  f_{m-1}(\\textbf{x}_i) - \\lambda \\cdot g_{m-1,}\\right)\n\\tag{16.9}\n\\end{align}\\]Như vậy, nếu có thể tìm được hàm \\(f_m(\\textbf{x})\\) mà mỗi thành phần của nó thỏa mãn \\(f_m(\\textbf{x}_i) = f_{m-1}(\\textbf{x}_i) - \\lambda_m \\cdot g_{m-1,}\\) chúng ta có thể chắc chắn rằng hàm tổn thất \\(L(\\textbf{y}, \\hat{y})\\) sẽ nhỏ đi khi cho \\(\\hat{y}\\) thay đổi từ \\(f_{m-1}(\\textbf{x})\\) đến \\(f_{m}(\\textbf{x})\\). Đây là nguyên tắc cơ bản trong các thuật toán tìm điểm tối ưu của một hàm số nhiều biến bằng phương pháp số hay còn gọi là phương pháp gradient descent.Hàm \\(f_m\\) được xây dựng như trên bao gồm \\(n\\) thành phần được xây dựng độc lập với nhau và hoàn toàn phụ thuộc vào dữ liệu huấn luyện mô hình. Mục tiêu của chúng ta là xây dựng hàm \\(f_m\\) vận hành tốt trên dữ liệu ngoài mô hình chứ không phải trên dữ liệu huấn luyện, đó sử dụng \\(n\\) thành phần độc lập là không khả thi. Thay vì tìm một hàm số \\(f_m\\) tối thiểu hóa giá trị hàm tổn thất tại bước thứ \\(m\\) như phương trình (16.1), ý tưởng của gradient boosting là hãy tìm một hàm số cơ bản, hay cụ thể hơn là một cây quyết định \\(T(\\textbf{x},\\Theta_m)\\) sao cho sai số giữa cây quyết định với các gradient (\\(-g_{m-1,}\\)) của hàm tổn thất là nhỏ nhất\n\\[\\begin{align}\n\\Theta_m = \\underset{\\Theta}{\\operatorname{argmin}} \\sum\\limits_{= 1}^n \\left(-g_{m-1,} -  T(\\textbf{x}_i,\\Theta_m) \\right)^2\n\\tag{16.10}\n\\end{align}\\]\nNhư vậy, miễn là hàm tổn thất là hàm số có đạo hàm theo \\(\\hat{y}\\), dù là bài toán hồi quy hay phân loại, tại bước thứ \\(m\\) của kỹ thuật gradient boosting, chúng ta luôn luôn phải tìm một cây quyết định hồi quy.Ví dụ 1: trong bài toán hồi quy với hàm tổn thất kiểu tổng sai số bình phương, gradient của hàm tổn thất tại \\(\\hat{y}_i\\) là\n\\[\\begin{align}\n& g_{} = \\cfrac{ \\partial (y_i - \\hat{y}_i)^2 }{\\partial \\hat{y}_i} = 2 \\cdot (\\hat{y}_i - y_i) \\\\\n& \\rightarrow - g_{,} = 2 \\cdot (y_i - \\hat{y}_i)\n\\end{align}\\]\nLưu ý rằng hàm sai số dạng tổng bình phương sai số thường được nhân với 0.5 để gradient tại \\(\\hat{y}_i\\) là \\((y_i - \\hat{y}_i)\\).Ví dụ 1: trong bài toán hồi quy với hàm tổn thất kiểu tổng sai số bình phương, gradient của hàm tổn thất tại \\(\\hat{y}_i\\) là\n\\[\\begin{align}\n& g_{} = \\cfrac{ \\partial (y_i - \\hat{y}_i)^2 }{\\partial \\hat{y}_i} = 2 \\cdot (\\hat{y}_i - y_i) \\\\\n& \\rightarrow - g_{,} = 2 \\cdot (y_i - \\hat{y}_i)\n\\end{align}\\]\nLưu ý rằng hàm sai số dạng tổng bình phương sai số thường được nhân với 0.5 để gradient tại \\(\\hat{y}_i\\) là \\((y_i - \\hat{y}_i)\\).Ví dụ 2: trong bài toán hồi quy với hàm tổn thất kiểu tổng giá trị tuyệt đối sai số, gradient của hàm tổn thất tại \\(\\hat{y}_i\\) là\n\\[\\begin{align}\n& g_{} = \\cfrac{ \\partial |y_i - \\hat{y}_i| }{\\partial \\hat{y}_i} = sign(\\hat{y}_i - y_i) \\\\\n& \\rightarrow - g_{} = sign(y_i - \\hat{y}_i)\n\\end{align}\\]Ví dụ 2: trong bài toán hồi quy với hàm tổn thất kiểu tổng giá trị tuyệt đối sai số, gradient của hàm tổn thất tại \\(\\hat{y}_i\\) là\n\\[\\begin{align}\n& g_{} = \\cfrac{ \\partial |y_i - \\hat{y}_i| }{\\partial \\hat{y}_i} = sign(\\hat{y}_i - y_i) \\\\\n& \\rightarrow - g_{} = sign(y_i - \\hat{y}_i)\n\\end{align}\\]Ví dụ 3: trong bài toán phân loại nhị phân với biến mục tiêu \\(y_i\\) nhận giá trị 0 hoặc 1 và hàm tổn thất là hàm cross-entropy, gradient của hàm tổn thất tại \\(\\hat{y}_i\\) làVí dụ 3: trong bài toán phân loại nhị phân với biến mục tiêu \\(y_i\\) nhận giá trị 0 hoặc 1 và hàm tổn thất là hàm cross-entropy, gradient của hàm tổn thất tại \\(\\hat{y}_i\\) là\\[\\begin{align}\n& g_{} = - \\cfrac{ \\partial y_i \\times \\hat{y}_i - \\log(1 + e^{\\hat{y}_i}) }{\\partial \\hat{y}_i} = \\cfrac{e^{\\hat{y}_i}}{1 + e^{\\hat{y}_i}} - y_i \\\\\n& \\rightarrow - g_{} = y_i - \\cfrac{e^{\\hat{y}_i}}{1 + e^{\\hat{y}_i}}\n\\end{align}\\]Ví dụ 4: trong bài toán phân loại mà biến mục tiêu \\(y_i\\) có thể nhận \\(J\\) giá trị được mã hóa là \\(1, 2, \\cdots, J\\) thì hàm tổn thất cross-entropy được viết như sau\n\\[\\begin{align}\nL(y_i, \\hat{y}_i) &= - \\sum\\limits_{j=1}^J \\mathbb{}(y_i = j) \\cdot \\log\\left(\\cfrac{e^{\\hat{y}_{,j}}}{ \\sum\\limits_{l=1}^J  e^{\\hat{y}_{,l}}} \\right) \\\\\n&=  \\log\\left( \\sum\\limits_{j=1}^J  e^{\\hat{y}_{,j}} \\right) - \\sum\\limits_{j=1}^J \\mathbb{}(y_i = j) \\cdot \\hat{y}_{,j}\n\\end{align}\\]\nThành phần thứ \\(J\\) của \\(\\hat{y}_i\\) được cố định bằng 0 đó gradient của hàm tổn thất theo thành phần thứ \\(J\\) của \\(\\hat{y}_i\\) cũng sẽ bằng 0. Với \\(1 \\leq j \\leq (J-1)\\) ta có\n\\[\\begin{align}\n& g_{,j} = \\cfrac{e^{\\hat{y}_{,j}}}{ \\sum\\limits_{l=1}^J  e^{\\hat{y}_{,l}}} - \\mathbb{}(y_i = j) \\\\\n& \\rightarrow - g_{,j} =  \\mathbb{}(y_i = j) - \\cfrac{e^{\\hat{y}_{,j}}}{ \\sum\\limits_{l=1}^J  e^{\\hat{y}_{,l}}}\n\\end{align}\\]\nTrong trường hợp này, tại bước thứ \\(m\\) của kỹ thuật gradient boosting có \\((J-1)\\) cây quyết định hồi quy được xây dựng độc lập nhau là \\(T(\\textbf{x}, \\Theta_{m,j})\\) với biến mục tiêu tương ứng là \\(-g_{m-1,,j}\\).Thuật toán gradient boosting được phát biểu như sau:Với \\(m = 0\\) cho\n\\[\\begin{align}\nf_0 = \\underset{\\gamma}{\\operatorname{argmin}} \\sum\\limits_{= 1}^n L\\left(y_i, \\gamma \\right)\n\\tag{16.9}\n\\end{align}\\]\nVới \\(m = 0\\) cho\n\\[\\begin{align}\nf_0 = \\underset{\\gamma}{\\operatorname{argmin}} \\sum\\limits_{= 1}^n L\\left(y_i, \\gamma \\right)\n\\tag{16.9}\n\\end{align}\\]Tại bước thứ \\(m\\), với \\(m = 1, 2, \\cdots, M\\),\n2.(). Với mỗi \\(= 1, 2, \\cdots, n\\) tính\n\\[\\begin{align}\n  g_{m,} = \\cfrac{ \\partial L(y_i, f_{m-1}(\\textbf{x}))}{\\partial f_{m-1}(\\textbf{x}_i)}\n\\end{align}\\]\n2.(b). Tìm cây quyết định hồi quy \\(T(\\textbf{x},\\Theta_m)\\) trên ma trận biến giải thích \\(\\textbf{x}\\) và biến mục tiêu là \\((-g_{m,})\\), tham số của cây quyết định \\(\\Theta_m\\) bao gồm phân vùng \\(\\{R_{m,k}\\}\\) với \\(k = 1, 2, \\cdots K_m\\) và hằng số \\(\\gamma_{m,k}\\)\n\\[\\begin{align}\n\\gamma_{m,k} = \\underset{\\gamma}{\\operatorname{argmin}} \\sum\\limits_{\\R_{m,k}}^n L\\left(y_i, f_{m-1}(\\textbf{x}_i) + \\gamma \\right)\n\\end{align}\\]\n2.(c). Cập nhật hàm \\(f_m(\\textbf{x})\\)\n\\[\\begin{align}\n  f_m(\\textbf{x}) =  f_{m-1}(\\textbf{x}) + \\lambda \\cdot T(\\textbf{x},\\Theta_m)\n\\end{align}\\]\nTại bước thứ \\(m\\), với \\(m = 1, 2, \\cdots, M\\),2.(). Với mỗi \\(= 1, 2, \\cdots, n\\) tính\\[\\begin{align}\n  g_{m,} = \\cfrac{ \\partial L(y_i, f_{m-1}(\\textbf{x}))}{\\partial f_{m-1}(\\textbf{x}_i)}\n\\end{align}\\]2.(b). Tìm cây quyết định hồi quy \\(T(\\textbf{x},\\Theta_m)\\) trên ma trận biến giải thích \\(\\textbf{x}\\) và biến mục tiêu là \\((-g_{m,})\\), tham số của cây quyết định \\(\\Theta_m\\) bao gồm phân vùng \\(\\{R_{m,k}\\}\\) với \\(k = 1, 2, \\cdots K_m\\) và hằng số \\(\\gamma_{m,k}\\)\\[\\begin{align}\n\\gamma_{m,k} = \\underset{\\gamma}{\\operatorname{argmin}} \\sum\\limits_{\\R_{m,k}}^n L\\left(y_i, f_{m-1}(\\textbf{x}_i) + \\gamma \\right)\n\\end{align}\\]2.(c). Cập nhật hàm \\(f_m(\\textbf{x})\\)\n\\[\\begin{align}\n  f_m(\\textbf{x}) =  f_{m-1}(\\textbf{x}) + \\lambda \\cdot T(\\textbf{x},\\Theta_m)\n\\end{align}\\]2.(c). Cập nhật hàm \\(f_m(\\textbf{x})\\)\\[\\begin{align}\n  f_m(\\textbf{x}) =  f_{m-1}(\\textbf{x}) + \\lambda \\cdot T(\\textbf{x},\\Theta_m)\n\\end{align}\\]Kết thúc vòng lặp, cho \\(f(\\textbf{x}) = f_M(\\textbf{x})\\)\nKết thúc vòng lặp, cho \\(f(\\textbf{x}) = f_M(\\textbf{x})\\)Một vài lưu ý đối với thuật toán gradient boosting ở trên:Chúng tôi không nói về \\(\\lambda_m\\) tại mỗi bước của gradient boosting bởi vì tham số này không có ý nghĩa trong quá trình tối ưu hóa cây quyết định mà cho \\(\\lambda_m\\) nhận giá trị bằng một hằng số \\(\\lambda\\) tại bước 2.(c). Giá trị của \\(\\lambda\\) luôn nhỏ hơn 1 với vai trò như một biến kiểm soát nhằm trách hiện tượng overfitting. Chúng ta sẽ thảo luận về \\(\\lambda\\) trong phần tiếp theo.Chúng tôi không nói về \\(\\lambda_m\\) tại mỗi bước của gradient boosting bởi vì tham số này không có ý nghĩa trong quá trình tối ưu hóa cây quyết định mà cho \\(\\lambda_m\\) nhận giá trị bằng một hằng số \\(\\lambda\\) tại bước 2.(c). Giá trị của \\(\\lambda\\) luôn nhỏ hơn 1 với vai trò như một biến kiểm soát nhằm trách hiện tượng overfitting. Chúng ta sẽ thảo luận về \\(\\lambda\\) trong phần tiếp theo.Trong bài toán phân loại mà biến mục tiêu nhận \\(J\\) giá trị, tại bước 2.() có \\((J-1)\\) véc-tơ gradient cần phải tính và tại bước 2.(b) có \\((J-1)\\) cây quyết định hồi quy cần được ước lượng.Trong bài toán phân loại mà biến mục tiêu nhận \\(J\\) giá trị, tại bước 2.() có \\((J-1)\\) véc-tơ gradient cần phải tính và tại bước 2.(b) có \\((J-1)\\) cây quyết định hồi quy cần được ước lượng.Trong quá trình thực hiện gradient boosting, có ba tham số điều khiển chất lượng của mô hình là số bước lặp \\(M\\), kích thước của cây quyết định \\(T(\\textbf{x},\\Theta_m)\\) tại mỗi bước, và tham số \\(\\lambda\\) - thường được gọi là “learning rate”. Chúng ta sẽ thảo luận về các tham số này trong phần tiếp theo của chương.","code":""},{"path":"boosting-và-cây-quyết-định..html","id":"những-cân-nhắc-khi-thực-hiện-gradient-boosting","chapter":"Chương 16 Boosting và cây quyết định.","heading":"16.2.2 Những cân nhắc khi thực hiện gradient boosting","text":"Những vấn đề được thảo luận dưới đây phần nhiều dựa trên kinh nghiệm của những người xây dựng mô hình hơn là dựa trên những cơ sở toán học vững chắc. Nói một cách khác, không có lời giải chính xác cho các tham số điều khiển kỹ thuật gradient boosting được thảo luận dưới đây. Với những tham số như vậy, cách duy nhất để đưa ra ước lượng phù hợp là sử dụng xác thực chéo.","code":""},{"path":"boosting-và-cây-quyết-định..html","id":"kích-thước-của-cây-quyết-định","chapter":"Chương 16 Boosting và cây quyết định.","heading":"16.2.2.1 Kích thước của cây quyết định","text":"Tại mỗi bước \\(m\\), bạn đọc có thể tùy chọn kích thước của cây quyết định để đảm bảo cây quyết định cho kết quả gần với các gradient của hàm tổn thất. Nếu tại mỗi bước, chúng ta luôn cố gắng tìm một kích thước cây tối ưu thì sẽ gặp phải các vấn đề là: thứ nhất thời gian thực hiện thuật toán chậm, và thứ hai dễ gặp hiện tượng overfitting. Cách tiếp cận như vậy luôn cho kết quả là các cây quyết định ban đầu sẽ cho kích thước lớn sai số lớn ở các bước ban đầu, sau đó các cây quyết định phía sau sẽ có kích thước nhỏ sai số đã giảm đáng kể sau khi xây dựng các cây lớn ở các bước ban đầu.Một chiến lược đơn giản để khắc phục hai vấn đề kể trên là cố định kích thước của cây quyết định tại tất cả các bước \\(K_m = K\\) \\(\\forall m\\). Tham số \\(K\\) khi đó trở thành siêu tham số của thuật toán và sẽ được ước lượng thông qua xác thực chéo. Để trả lời cho câu hỏi là miền giá trị nào của \\(K\\) nên là miền để tìm kiếm \\(K\\) trong xác thực chéo, chúng ta cần hiểu thêm một chút về kích thước các cây ảnh hưởng đến giá trị hàm \\(\\hat{f}\\) như thế nào.Mục tiêu của các thuật toán học máy là tìm hàm \\(\\hat{f}\\) sao cho\n\\[\\begin{align}\n  \\hat{f} = \\underset{f}{\\operatorname{argmin}} \\mathbb{E}\\left(Y,f(\\textbf{x})\\right)\n\\end{align}\\]\nVới ma trận \\(\\textbf{x}\\) bao gồm các biến \\((\\textbf{x}_1, \\textbf{x}_2, \\cdots, \\textbf{x}_p)\\), mọi hàm \\(f(\\textbf{x})\\) có thể được viết dưới dạng sau\n\\[\\begin{align}\n  f(\\textbf{x}) = \\sum\\limits_j f_j(\\textbf{x}_j) + \\sum\\limits_{j,k} f_{j,k}(\\textbf{x}_j, \\textbf{x}_k) + \\sum\\limits_{j,k,l} f_{j,k,l}(\\textbf{x}_j, \\textbf{x}_k, \\textbf{x}_l) + \\cdots\n  \\tag{16.11}\n\\end{align}\\]\ntrong đó \\(f_j(\\textbf{x}_j)\\) là thành phần của hàm mục tiêu chỉ bao gồm 1 biến duy nhất, \\(f_{j,k}(\\textbf{x}_j, \\textbf{x}_k)\\) là thành phần của hàm mục tiêu có tính toán đến tác động giữa hai biến, \\(f_{j,k,l}(\\textbf{x}_j, \\textbf{x}_k, \\textbf{x}_l)\\) là thành phần tính đến tác động giữa ba biến,… Đa số dữ liệu thực tế cho thấy rằng phân rã hàm \\(f\\) trong phương trình (16.11) các thành phần mô tả tương tác giữa một số nhỏ các biến luôn chiếm ưu thế.Phân tích kể trên có liên quan chặt chẽ đến kích thước của cây quyết định. Thật vậy, khi \\(K = 2\\) nghĩa là mỗi cây quyết định chỉ có duy nhất một biến giải thích. Khi \\(K = 3\\), cây quyết định có không quá 2 biến giải thích, … và nói chung, cây quyết định có kích thước bằng \\(K\\) nghĩa là cây quyết định có không quá \\((K-1)\\) biến giải thích. Như vậy, nếu hàm \\(f\\) có các thành phần ban đầu chiếm ưu thế, các cây quyết định với 2 hoặc 3 lá sẽ là phù hợp, còn với các hàm \\(f\\) có các thành phần phía sau chiếm ưu thế, cây quyết định cần có kích thước lớn hơn để mô tả sự tương tác giữa nhiều biến. Như chúng ta đã nói, đa số các dữ liệu thực tế có các thành phần ban đầu trong phương trình (16.11) chiếm ưu thế, đó kích thước của các cây quyết định trong Gradient boosting thường không lớn. Thực nghiệm cho thấy hiếm khi cần kích thước cây quyết định \\(K > 10\\). Nếu tính toán cho phép bạn đọc nên sử dụng xác thực chéo để lựa chọn \\(K\\) nhận giá trị từ 2 đến 10. Nếu thời gian và nguồn lực không cho phép, lựa chọn \\(K = 6\\) thường được áp dụng.","code":""},{"path":"boosting-và-cây-quyết-định..html","id":"số-lượng-cây-quyết-định-và-tốc-độ-học","chapter":"Chương 16 Boosting và cây quyết định.","heading":"16.2.2.2 Số lượng cây quyết định và tốc độ học","text":"Ngoài kích thước của các cây, số lượng cây quyết định \\(M\\) cũng là một siêu tham số. Sau mỗi bước \\(m\\), giá trị hàm tổn thất \\(L(\\textbf{y}, f_m)\\) sẽ giảm, nghĩa là hàm \\(f_m\\) sẽ ngày càng phù hợp hơn với dữ liệu huấn luyện mô hình. Tuy nhiên, mục tiêu của chúng ta là tìm kiếm hàm \\(f\\) vận hành tốt trên dữ liệu kiểm thử chứ không phải dữ liệu huấn luyện, đó tăng \\(M\\) không phải luôn là lựa chọn tốt. Không có\ncâu trả lời chính xác cho số lượng cây tối ưu, tham số này chỉ có thể được xác định thông qua xác thực chéo.Một tham số khác cũng cần được cân nhắc khi triển khai gradient boosting là tốc độ học \\(\\lambda\\). Tham số \\(lambda\\) xuất hiện trong bước 2.(c). của thuật toán gradient boosting cho biết đóng góp của mỗi cây quyết định vào kết quả cuối cùng. Cũng giống như khi triển khai thuật toán gradient decent thông thường, khi tốc độ học \\(\\lambda\\) nhỏ, quá trình tìm kiếm điểm tối ưu sẽ chậm hơn nhưng khả năng tìm được đúng điểm tối ưu sẽ lớn hơn. Nói cách khác, khi lựa chọn tham số \\(\\lambda\\) nhỏ, khoảng tìm kiếm của \\(M\\) cần phải ưu tiên các giá trị lớn hơn, thời gian thực hiện thuật toán sẽ lâu hơn và khả năng tìm được hàm \\(f\\) tối ưu sẽ lớn hơn.Nghiên cứu của Friedman (2001) đã chỉ ra rằng lựa chọn \\(\\lambda\\) nhỏ hơn sẽ làm cho sai số trên tập kiểm tra tốt hơn và yêu cầu các giá trị \\(M\\) lớn hơn tương ứng. Chiến lược tốt nhất để thực hiện boosting là cho \\(\\lambda\\) nhận giá trị rất nhỏ, thường là nhỏ hơn \\(0.1\\) và sau đó chọn M bằng cách dừng sớm. Cách tiếp cận này cải thiện đáng kể sai số trên tập kiểm tra cho cả bài toán hồi quy và phân loại với việc sử dụng \\(\\lambda = 1\\). Sự đánh đổi ở đây là thời gian và nguồn lực để thực hiện tính toán: \\(\\lambda\\) càng nhỏ thì càng cần phải làm tăng giá trị \\(M\\) làm cho thời gian và nguồn lực tính toán tăng lên tỷ lệ thuận. Tuy nhiên, với khả năng tính toán của đa số các máy tính hiện nay, cách tiếp cận này là khả thi ngay cả trên các tập dữ liệu rất lớn.","code":""},{"path":"boosting-và-cây-quyết-định..html","id":"stochastic-gradient-boosting","chapter":"Chương 16 Boosting và cây quyết định.","heading":"16.2.2.3 Stochastic gradient boosting","text":"Stochastic gradient boosting là một cải tiến của gradient boosting mà tại mỗi bước \\(m\\), thay vì sử dụng toàn bộ dữ liệu huấn luyện để xây dựng cây quyết định, chúng ta chỉ sử dụng một mẫu ngẫu nhiên của dữ liệu để xây dựng cây quyết định. Tỷ lệ mẫu ngẫu nhiên với dữ liệu huấn luyện được ký hiệu là \\(\\eta\\) và thường được lựa chọn bằng \\(0.5\\). Khi kích thước dữ liệu \\(n\\) lớn, có thể lựa chọn \\(\\eta\\) nhỏ hơn.Lợi thế trước tiên của stochastic gradient boosting đó là giảm thời gian tính toán dữ liệu để xây dựng cây quyết định nhỏ hơn đáng kể với dữ liệu huấn luyện. Một lợi thế đáng kể khác của cách tiếp cận này là trong nhiều trường hợp nó còn có thể tìm được điểm tối ưu tốt hơn với gradient boosting thông thường. Không có cách giải thích toán học đáng tin cậy nào về kết luận này, tuy nhiên có thể hiểu rằng khi xây dựng các cây quyết định trên một mẫu ngẫu nhiên của dữ liệu, nếu kích thước dữ liệu đủ lớn thì phân phối xác suất của mẫu và toàn bộ dữ liệu là tương đương, trong khi trong khi các quan sát nhiễu hay ngoại lai của dữ liệu huấn luyện ít có khả năng rơi vào mẫu ngẫu nhiên, việc này làm cho các cây quyết định ít bị ảnh hưởng bởi nhiễu hoặc các giá trị ngoại lai, đặc biệt là trong các bài toán hồi quy.","code":""},{"path":"boosting-và-cây-quyết-định..html","id":"thực-hiện-gradient-boosting-trên-r.","chapter":"Chương 16 Boosting và cây quyết định.","heading":"16.2.3 Thực hiện gradient boosting trên R.","text":"Thư viện để thực hiện gradient boosting trên R là thư viện \\(gbm\\). Tại thời điểm chúng tôi viết chương sách này, thư viện \\(gbm\\) được sử dụng đang ở phiên bản 2.1.8.1. Thư viện được xây dựng dựa trên nghiên cứu của Greg Ridgeway (1999) với ý tưởng hoàn toàn giống như chúng tôi đã trình bày trong các phần trước. Hàm số để thực hiện gradient boosting là hàm số gbm().Bạn đọc có thể đọc hướng dẫn ngắn gọn của R về các tham số và cách sử dụng trong hàm gbm() hoặc tìm hướng dẫn đầy đủ về thư viện gbm() theo đường dẫn . Chúng tôi chỉ tập trung giải thích vào các tham số quan trọng của thuật toán:\\(formula\\): cách sử dụng tương tự như hàm \\(lm\\) hay \\(glm\\); cần khai báo tên biến phụ thuộc và các biến giải thích.\\(formula\\): cách sử dụng tương tự như hàm \\(lm\\) hay \\(glm\\); cần khai báo tên biến phụ thuộc và các biến giải thích.\\(distribution\\): là phân phối xác suất của biến mục tiêu. Mặc dù không phải tham số bắt buộc nhưng lời khuyên của chúng tôi là bạn đọc hãy luôn luôn khai báo biến này để đảm bảo cách xây dựng mô hình đúng với ý định. Tham số này không được khai báo, hàm gbm() sẽ dựa trên kiểu biến mục tiêu để dự đoán phân phối xác suất: nếu \\(Y\\) có dạng factor và chỉ nhận hai giá trị phân phối nhị thức sẽ được sử dụng, nếu \\(Y\\) nhận nhiều hơn 2 giá trị, phân phối multinomial sẽ được sử dụng. Nếu \\(Y\\) là biến dạng số, phân phối Gaussian sẽ được sử dụng. Các giá trị có thể nhận được của tham số \\(distribution\\)\n\\(\"gaussian\"\\): cho bài toán hồi quy và hàm tổn thất hàm tổng sai số bình phương.\n\\(\"laplace\"\\): cho bài toán hồi quy và hàm tổn thất hàm tổng giá trị tuyệt đối sai số.\n\\(\"bernoulli\"\\): cho bài toán phân loại nhị phân và hàm tổn thất cross-entropy.\n\\(\"adaboost\"\\): cho bài toàn phân loại nhị phân và hàm tổn thất kiểu mũ.\n\\(\"multinomial\"\\): cho bài toán phân loại chung và hàm tổn thất cross-entropy.\n\\(distribution\\): là phân phối xác suất của biến mục tiêu. Mặc dù không phải tham số bắt buộc nhưng lời khuyên của chúng tôi là bạn đọc hãy luôn luôn khai báo biến này để đảm bảo cách xây dựng mô hình đúng với ý định. Tham số này không được khai báo, hàm gbm() sẽ dựa trên kiểu biến mục tiêu để dự đoán phân phối xác suất: nếu \\(Y\\) có dạng factor và chỉ nhận hai giá trị phân phối nhị thức sẽ được sử dụng, nếu \\(Y\\) nhận nhiều hơn 2 giá trị, phân phối multinomial sẽ được sử dụng. Nếu \\(Y\\) là biến dạng số, phân phối Gaussian sẽ được sử dụng. Các giá trị có thể nhận được của tham số \\(distribution\\)\\(\"gaussian\"\\): cho bài toán hồi quy và hàm tổn thất hàm tổng sai số bình phương.\\(\"laplace\"\\): cho bài toán hồi quy và hàm tổn thất hàm tổng giá trị tuyệt đối sai số.\\(\"bernoulli\"\\): cho bài toán phân loại nhị phân và hàm tổn thất cross-entropy.\\(\"adaboost\"\\): cho bài toàn phân loại nhị phân và hàm tổn thất kiểu mũ.\\(\"multinomial\"\\): cho bài toán phân loại chung và hàm tổn thất cross-entropy.\\(data\\) là dữ liệu huấn luyện mô hình.\\(data\\) là dữ liệu huấn luyện mô hình.\\(n.trees\\) tương đương với tham số \\(M\\) trong gradient boosting, là số lượng cây quyết định trong thuật toán.\\(n.trees\\) tương đương với tham số \\(M\\) trong gradient boosting, là số lượng cây quyết định trong thuật toán.\\(interaction.depth\\) cho biết kích thước (số node) của các cây phân loại. Giá trị mặc định là 1 cho biết các cây phân loại được mặc định là các stump.\\(interaction.depth\\) cho biết kích thước (số node) của các cây phân loại. Giá trị mặc định là 1 cho biết các cây phân loại được mặc định là các stump.\\(shrinkage\\) là tham số \\(\\lambda\\) trong gradient boosting. Giá trị mặc định là 0.1. Giá trị tham số \\(shrinkage\\) được gợi ý là khoảng từ 0.001 đến 0.1.\\(shrinkage\\) là tham số \\(\\lambda\\) trong gradient boosting. Giá trị mặc định là 0.1. Giá trị tham số \\(shrinkage\\) được gợi ý là khoảng từ 0.001 đến 0.1.\\(bag.fraction\\) là tham số \\(\\eta\\) trong stochastic gradient boosting. Giá trị mặc định của tham số này là 0.5 nghĩa là tại mỗi bước xây dựng cây quyết định chỉ có 50% dữ liệu huấn luyện được sử dụng để xây dựng mô hình. Lưu ý khi sử dụng \\(bag.fraction < 1\\) bạn đọc cần khởi tạo lại hàm sinh ngẫu nhiên (set.seed()) để đảm bảo kết quả các lần chạy giống nhau.\\(bag.fraction\\) là tham số \\(\\eta\\) trong stochastic gradient boosting. Giá trị mặc định của tham số này là 0.5 nghĩa là tại mỗi bước xây dựng cây quyết định chỉ có 50% dữ liệu huấn luyện được sử dụng để xây dựng mô hình. Lưu ý khi sử dụng \\(bag.fraction < 1\\) bạn đọc cần khởi tạo lại hàm sinh ngẫu nhiên (set.seed()) để đảm bảo kết quả các lần chạy giống nhau.\\(cv.folds\\) là tham số cho biết có sử dụng xác thực chéo hay không. Nếu \\(cv.folds > 1\\) thì xác thực chéo sẽ được thực hiện và sai số xác thực chéo sẽ được lưu lại trong output có tên là \\(cv.error\\).\\(cv.folds\\) là tham số cho biết có sử dụng xác thực chéo hay không. Nếu \\(cv.folds > 1\\) thì xác thực chéo sẽ được thực hiện và sai số xác thực chéo sẽ được lưu lại trong output có tên là \\(cv.error\\).1. Freund, Y. Schapire, R. (1997). decision-theoretic generalization online learning application boosting2. G. Ridgeway (1999). state boosting3. J.H. Friedman, T. Hastie, R. Tibshirani (2000). Additive Logistic Regression: Statistical View Boosting4. J.H. Friedman (2001). Greedy Function Approximation: Gradient Boosting Machine","code":"\nlibrary(gbm)\n? gbm## \n## Attaching package: 'dplyr'## The following objects are masked from 'package:stats':\n## \n##     filter, lag## The following objects are masked from 'package:base':\n## \n##     intersect, setdiff, setequal, union## \n## Attaching package: 'kableExtra'## The following object is masked from 'package:dplyr':\n## \n##     group_rows## \n## Attaching package: 'gridExtra'## The following object is masked from 'package:dplyr':\n## \n##     combine## \n## Attaching package: 'pryr'## The following object is masked from 'package:dplyr':\n## \n##     where## Loading required package: shape"},{"path":"neuralnetwork.html","id":"neuralnetwork","chapter":"Chương 17 Mô hình mạng nơ-ron","heading":"Chương 17 Mô hình mạng nơ-ron","text":"Chương sách này thảo luận một chủ đề quan trọng có ứng dụng rộng rãi nhất trong lĩnh vực trí tuệ nhân tạo là mô hình mạng học sâu (deep learning). Tại thời điểm nhóm tác giả viết cuốn sách (2023), học sâu là một lĩnh vực nghiên cứu tích cực nhất không chỉ trong khoa học máy tính, công nghệ thông tin mà còn cả trong các lĩnh vực khác như kinh tế, tài chính, y tế, xây dựng,… Nền tảng của mô hình mạng học sâu là mô hình mạng nơ-ron (hay neural network). Mô hình mạng nơ-ron đã được biết đến đến rộng rãi vào cuối những năm 1980 bởi cách vận hành của mô hình mô tả lại cách thức mà hệ thống thần kinh của con người xử lý thông tin. Mặc dù các đặc tính của mô hình mạng nơ-ron được phân tích bởi những nhà toán học và nhà thống kê nhiều thuật toán liên quan đến mô hình này đã được cải thiện với sự ra đời của các phương pháp học máy khác như SVM, rừng ngẫu nhiên, học tăng cường,…, mà mô hình mạng nơ-ron phần nào không được ưa chuộng.Từ những năm 2010, với nhu cầu xử lý các dữ liệu ngày càng phức tạp và sự ra đời của các kiến trúc máy tính lớn, mô hình mạng nơ-ron đã quay trở lại với tên mới là mạng học sâu (deep learning). Mạng học sâu vượt trội hoàn toàn các mô hình học máy thông thường trong phân loại hình ảnh/video và mô hình hóa ngôn ngữ tự nhiên bao gồm dữ liệu kiểu văn bản và giọng nói (natural langugue processing hay NLP). Các nhà khoa học trong lĩnh vực này tin rằng lý chính cho những thành công của mô hình mạng nơ-ron là càng ngày những người xây dựng mô hình càng chú trọng vào xây dựng các bộ dữ liệu khổng lồ để huấn luyện môn hình và cấu trúc của mô hình cho phép nó đáp ứng được với bất kỳ tập kích thước dữ liệu nào.","code":""},{"path":"neuralnetwork.html","id":"nnonelayer","chapter":"Chương 17 Mô hình mạng nơ-ron","heading":"17.1 Mạng nơ-rơn có một lớp ẩn","text":"Mô hình mạng nơ-ron lấy một véc-tơ đầu vào gồm \\(p\\) biến \\(\\textbf{X} = (X_1, X_2, \\cdots , X_p)\\) và xây dựng một hàm phi tuyến \\(\\hat{f}\\) để dự đoán biến mục tiêu \\(Y\\) . Chúng ta đã xây dựng các mô hình dự đoán phi tuyến trong các chương trước, ví dụ như mô hình cộng tính tổng quát, mô hình cây quyết định, mô hình rừng ngẫu nhiên, mô hình tăng cường. Điều làm nên sự khác biệt của mô hình mạng nơ-ron là cấu trúc xây dựng của mô hình. Hình 17.1 mô tả một mạng nơ-ron chuyển tiếp để mô hình biến mục tiêu \\(Y\\) định lượng từ \\(p = 3\\) biến giải thích là \\(X_1\\), \\(X_2\\), và \\(X_3\\).\nHình 17.1: Mô hình mạng nơ-ron có p = 3 đơn vị trong lớp đầu vào, một lớp ẩn có năm đơn vị, và một đơn vị đầu ra.\nTheo thuật ngữ chuyên môn, ba biến giải thích \\(X_1\\), \\(X_2\\), và \\(X_3\\) là các đơn vị (unit) của lớp đầu vào (input layer). Các mũi tên được dùng để mô tả rằng mỗi đơn vị đầu vào sẽ chuyển tiếp thông tin vào \\(k = 5\\) đơn vị của lớp ẩn (hidden layer) được ký hiệu là \\(H_i\\) với \\(= 1, 2, \\cdots, k\\). Dạng của hàm \\(f\\) trong mô hình mạng nơ-ron nhân tạo sẽ được viết như sau:\n\\[\\begin{align}\nf(\\textbf{X}) & = \\beta_0 + \\sum\\limits_{= 1}^k \\beta_i \\cdot h_i\\left(\\textbf{X}\\right) \\\\\n& = \\beta_0 + \\sum\\limits_{= 1}^k \\beta_i \\cdot g\\left(w_{,0} + \\sum\\limits_{j=1}^p w_{,j} \\cdot X_j\\right)\n\\tag{17.1}\n\\end{align}\\]\ntrong đó \\(g\\) là một hàm số phi tuyến được xác định trước, được gọi theo thuật ngữ chuyên môn là các hàm kích hoạt (activation function). Các \\(\\beta_i\\) và \\(w_{,j}\\) là các hằng số và cũng là các tham số cần được ước lượng của mô hình. Hàm \\(f\\) trong phương trình (??) được xây dựng theo hai bước:Bước thứ nhất, các đơn vị \\(H_i\\) trong lớp ẩn được tính bằng hàm kích hoạt tính trên tổ hợp tuyến tính của các biến đầu vào:\\[\\begin{align}\nH_i = g\\left(w_{,0} + \\sum\\limits_{j=1}^p w_{,j} \\cdot X_j\\right)\n\\tag{17.2}\n\\end{align}\\]Bước thứ hai, \\(k\\) đơn vị của lớp ẩn là yếu tố đầu vào để tính toán giá trị biến đầu ra định lượng:\n\\[\\begin{align}\nY = \\beta_0 + \\sum\\limits_{= 1}^k \\beta_i \\cdot H_i\n\\tag{17.3}\n\\end{align}\\]Quá trình tính toán đi từ lớp đầu vào qua các lớp ẩn và kết thúc ở mạng đầu ra được gọi là quá trình chuyển tiếp, và hàm \\(f\\) trong phương trình (17.1) được gọi là một mạng nơ-ron chuyển tiếp. Tất cả các tham số \\(\\beta_0\\), \\(\\beta_1\\), \\(\\cdots\\), \\(\\cdots\\) , \\(\\beta_k\\) và \\(w_{1,0}\\) , \\(\\cdots\\) , \\(w_{k,p}\\) được ước lượng từ dữ liệu. Các hàm kích hoạt thường được sử dụng là hàm Sigmoid và hàm ReLU. Hàm sigmoid là hàm được thường xuyên sử dụng trong hồi quy logistic để chuyển hàm tuyến tính thành xác suất giữa 0 và 1 trong khi hàm ReLU là hàm phi tuyến được xây dựng một cách đơn giản nhất nhằm mục đích dễ dàng tuyến tính trong các mạng phức tạp.\n\\[\\begin{align}\n\\text{Sigmoid: } & g(x) = \\cfrac{e^x}{1 + e^{x}} = \\cfrac{1}{1 + e^{-x}} \\\\\n\\text{ReLU: } & g(x) = max(x , 0) = (x)^+\n\\end{align}\\]\nHình 17.2: Hàm kích hoạt sigmoid và hàm kích hoạt ReLU. Hàm Sigmoid có đạo hàm tại mọi điểm trong khi hàm ReLU không có đạo hàm tại 0\nHình 17.2 mô tả giá trị của hàm Sigmoid và hàm ReLU trên đoạn từ -4 đến 4. Trong thời kỳ đầu của mô hình mạng nơ-ron, hàm Sigmoid được ưa chuộng vì hàm số này có đạo hàm liên tục tại mọi giát trị của \\(x\\). Sau đó, hàm ReLU lại là lựa chọn ưa thích trong các mô hình mạng nơ-ron hiện đại vì hàm số này đơn giản, dễ tính toán và cho hiệu quả tốt hơn với hàm Sigmoid.Có thể tóm tắt lại mô hình được mô tả trong Hình 17.1 như sau: từ 3 biến giải thích ban đầu là \\(X_1\\), \\(X_2\\), và \\(X_3\\) chúng ta tạo ra năm biến giải thích mới là \\(H_1\\), \\(H_2\\), \\(H_3\\), \\(H_4\\) và \\(H_5\\) được tính toán bằng giá trị của hàm kích hoạt \\(g(.)\\) trên các tổ hợp tuyến tính của các biến giải thích ban đầu. Sau đó chúng ta sử dụng năm biến giải thích \\(H_i\\), \\(= 1, 2, 3, 4, 5\\) để xây dựng một mô hình hồi quy tuyến tính mà trong đó biến phụ thuộc là \\(Y\\). Tham số của các mô hình bao gồm các hệ số \\(w_{,j}\\) để tính các biến \\(H_i\\), và các hệ số \\(\\beta_j\\) trong mô hình hồi quy tuyến tính \\(Y\\) theo các biến \\(H\\).Mô hình có tên là mạng nơ-ron bởi vì cấu trúc của mô hình bao gồm các đơn vị \\(H_i\\) hoạt động giống như các tế bào thần kinh trong não bộ của con người. Các đơn vị \\(H_i\\) xấp xỉ hoặc bằng 0 giống như các tế bào thần kinh im lặng (slient neuron), những tế bào ít bị kích hoạt trong quá trình lan truyền thông tin, trong khi các đơn vị \\(H_i\\) lớn (khi sử dụng hàm ReLU), hoặc xấp xỉ 1 (khi sử dụng hàm Sigmoid) giống như những tế bào bị kích hoạt mạng trong quá trình lan truyền thông tin.Sử dụng các hàm kích hoạt \\(g(.)\\) phi tuyến là đặc biệt quan trọng tong vì nếu không hàm \\(f\\) sẽ suy biến thành mô hình tuyến tính thông thường với \\(p = 3\\) biến giải thích trong \\(X_1\\), \\(X_2\\), và \\(X_3\\). Ngoài ra, hàm kích hoạt phi tuyến cho phép mô hình mạng nơ-ron mô tả được những mối liên hệ phi tuyến và phức tạp giữa các biến giải thích \\(\\textbf{X}\\) và biến mục tiêu \\(Y\\).Giả sử trong mô hình mạng nơ-ron được mô tả trong Hình 17.1, chúng ta có hàm kích hoạt \\(g(x) = x^2\\) và giá trị của các hệ số \\(\\boldsymbol{\\beta} = (\\beta_0, \\beta_1, \\beta_2, \\beta_3, \\beta_4, \\beta_5)\\) và \\(w_{,j}\\), với \\(1 \\leq \\leq 5\\) và \\(0 \\leq j \\leq 3\\), được cho như sau\n\\[\\begin{align}\n& \\text{hệ số chặn: } \\beta_0 = w_{1,0} = w_{2,0} = w_{3,0} = w_{4,0} = w_{5,0} 0 \\\\\n& \\\\\n& \\begin{pmatrix}\n\\beta_1 \\\\\n\\beta_2 \\\\\n\\beta_3 \\\\\n\\beta_4 \\\\\n\\beta_5\n\\end{pmatrix} = \\begin{pmatrix}\n0.5 \\\\\n0.5 \\\\\n0.5 \\\\\n1 \\\\\n2\n\\end{pmatrix} \\ \\text{ và } \\ \\begin{pmatrix}\nw_{1,1} & w_{1,2} & w_{1,3} \\\\\nw_{2,1} & w_{2,2} & w_{2,3} \\\\\nw_{3,1} & w_{3,2} & w_{3,3} \\\\\nw_{4,1} & w_{4,2} & w_{4,3} \\\\\nw_{5,1} & w_{5,2} & w_{5,3}\n\\end{pmatrix} = \\begin{pmatrix}\n1 & -1 & 1 \\\\\n1 & 2 & -1 \\\\\n0 & 0 & 0 \\\\\n0 & 0 & 0 \\\\\n0 & 0 & 0\n\\end{pmatrix}\n\\tag{17.4}\n\\end{align}\\]Trước hết, để tránh sự phức tạp chúng tôi cho giá trị các hàng 3, 4, và 5 của ma trận \\(\\boldsymbol{w}\\) đều bằng 0, điều này dẫn đến giá trị tại các đơn vị \\(H_3\\), \\(H_4\\) và \\(H_5\\) của lớp ẩn sẽ bằng 0. Các đơn vị này hoạt động như các tế bào im lặng trong mạng nơ-rơn và không có ảnh hưởng đến biến mục tiêu \\(Y\\). Chúng ta có giá trị tại \\(H_1\\) và \\(H_2\\) được tính theo các biến giải thích và hàm kích hoạt:\n- Giá trị tại \\(H_1\\)\n\\[\\begin{align}\nH_1\\left(X_1, X_2, X_3\\right) & = g\\left(w_{1,1} \\cdot X_1 + w_{1,2} \\cdot X_2 + w_{1,3} \\cdot X_3\\right) \\\\\n& = \\left(w_{1,1} \\cdot X_1 + w_{1,2} \\cdot X_2 + w_{1,3} \\cdot X_3\\right)^2 \\\\\n& = \\left(X_1 - X_2 + X_3\\right)^2\n\\tag{17.5}\n\\end{align}\\]Giá trị tại \\(H_2\\)\n\\[\\begin{align}\nH_2 \\left(X_1, X_2, X_3\\right) & = g\\left(w_{2,1} \\cdot X_1 + w_{2,2} \\cdot X_2 + w_{2,3} \\cdot X_3\\right) \\\\\n& = \\left(w_{2,1} \\cdot X_1 + w_{2,2} \\cdot X_2 + w_{2,3} \\cdot X_3\\right)^2 \\\\\n& = \\left(X_1 + 2 \\cdot X_2 - X_3\\right)^2\n\\tag{17.6}\n\\end{align}\\]Giá trị của hàm số \\(f(\\text{X})\\) là đầu ra của mạng nơ-ron được xác định như sau:\n\\[\\begin{align}\nf(X_1, X_2, X_3) & = 0.5 \\cdot H_1\\left(X_1, X_2, X_3\\right) + 0.5 \\cdot H_2 \\left(X_1, X_2, X_3\\right) \\\\\n& = X_1^2 + 2.5 \\cdot X_2^2 + X_3^2 + X_1 \\cdot X_2 - 3 \\cdot X_2 \\cdot X_3\n\\tag{17.7}\n\\end{align}\\]Bạn đọc có thể thấy rằng, việc sử dụng hàm kích hoạt phi tuyến cho phép chúng ta có hàm đầu ra bao gồm hàm phi tuyến trên các giá trị biến đầu vào, mà còn tính đến cả biến tương tác giữa các biến ban đầu. Trong ví dụ ở phương trình (17.7) là các giá trị \\(X_1 \\cdot X_2\\) và \\(3 \\cdot X_2 \\cdot X_3\\). Trong thực tế, chúng ta sẽ không sử dụng hàm bậc hai hay hàm đa thức cho hàm kích hoạt \\(g(.)\\) hàm kích hoạt đa thức sẽ dẫn tới kết quả cũng chỉ là dạng hàm đa thức. Các hàm kích hoạt Sigmoid hoặc ReLU không bị giới hạn như vậy.Trong ví dụ trên chúng ta đã cho trước các tham số bao gồm các hệ số \\(\\boldsymbol{\\beta}\\) và ma trận \\(\\boldsymbol{w}\\). Tuy nhiên trong thực tế, các tham số này được lựa chọn để giảm thiểu sai số giữa giá trị dự đoán và giá trị quan sát của biến mục tiêu:\n\\[\\begin{align}\n(\\hat{\\boldsymbol{\\beta}},\\hat{\\boldsymbol{w}}) = \\underset{\\boldsymbol{\\beta},\\boldsymbol{w}}{\\operatorname{argmin}} \\sum\\limits_{=1}^n \\left( y_i - f(\\textbf{x}_i) \\right)^2\n\\tag{17.8}\n\\end{align}\\]Chúng ta sẽ thảo luận về ước lượng tham số cho mô hình mạng nơ-ron trong phần 17.3.","code":""},{"path":"neuralnetwork.html","id":"nnmultilayer","chapter":"Chương 17 Mô hình mạng nơ-ron","heading":"17.2 Mạng nơ-ron có nhiều lớp ẩn","text":"Mô hình mạng nơ-ron được sử dụng hiện tại thường có nhiều hơn một lớp ẩn và có nhiều đơn vị trên mỗi lớp. Về lý thuyết, một lớp ẩn duy nhất với số lượng lớn các đơn vị có khả năng xấp xỉ hầu hết các dạng hàm \\(f\\). Tuy nhiên, thực tế chỉ ra rằng xây dựng những cấu trúc nhiều lớp và mỗi lớp có kích thước hợp lý là giải pháp tốt hơn với cấu trúc chỉ có một lớp ẩn và có rất nhiều đơn vị trên cùng một lớp.Cấu trúc có thể mở rộng của mô hình mạng nơ-ron nhân tạo cho phép nó có khả năng mô hình hóa tốt những bộ dữ liệu phức tạp, mà điển hình là dữ liệu dạng ảnh, dạng văn bản, hay dạng tín hiệu. Để minh họa cho khả năng phù hợp của mô hình với những dữ liệu phức tạp, chúng ta sẽ xây dựng một cấu trúc mạng nơ-ron có nhiều lớp ẩn để dự đoán dữ liệu là ảnh chứa các chữ số viết tay. Dữ liệu được sử dụng để huấn luyện mô hình là tập dữ liệu chữ số viết tay nổi tiếng \\(\\textbf{MNIST}\\). Hình 17.3 minh họa một số quan sát trong dữ liệu về các chữ số viết tay được lưu trữ trong dữ liệu \\(\\textbf{MNIST}\\). Mỗi quan sát của dữ liệu sử dụng để xây dựng mô hình là một hình ảnh có kích thước \\(p = 28 \\times 28 = 784\\) pixel và biến mục tiêu là giá trị số của hình ảnh xuất hiện trong biến giải thích. Có tất cả 10 giá trị khác nhau cho biến mục tiêu là các chữ số viết tay tương ứng từ 0 đến 9.\nHình 17.3: Năm mươi giá trị đầu tiên trong dữ liệu số viết tay MNIST. Mỗi số viết tay là một quan sát trong dữ liệu. Một bức ảnh được lưu dưới dạng một véc-tơ có độ dài p = 784. Mỗi giá trị trong véc-tơ là một số tự nhiên nhận giá trị từ 0 đến 255 cho biết độ tối của điểm ảnh.\nÝ tưởng là xây dựng một mô hình để phân loại các hình ảnh thành chữ số từ 0 đến 9. Chúng ta sẽ sử dụng cấu trúc mạng nơ-ron với hai lớp ẩn được minh họa trong Hình 17.4 để xây dựng mô hình phân loại hình ảnh số viết tay.\nHình 17.4: Mạng nơ-ron xây dựng trên dữ liệu MNIST có hai lớp ẩn, mỗi lớp ẩn có nhiều đơn vị và lớp đầu ra có 10 đơn vị tương ứng với m = 10 giá trị có thể của các số viết tay từ 0 đến 9. Lớp đầu vào có p = 784 đơn vị tương ứng với 784 điểm ảnh.\nGiá trị đầu ra trong cấu trúc mạng nơ-ron trong Hình 17.4 là biến kiểu factor, được biểu thị bằng véc-tơ \\(\\textbf{Y} = (Y_1, Y_2, \\cdots , Y_{m})\\) với \\(m = 10\\). Dữ liệu có 60 nghìn bức ảnh được sử dụng để huấn luyện mô hình và 10 nghìn bức ảnh được sử dụng để kiểm tra mô hình. Cấu trúc mạng trong Hình 17.4 có hai lớp ẩn thay vì một lớp ẩn giống như trước.Lớp ẩn thứ nhất có \\(k_1\\) đơn vị được tính toán từ \\(p\\) đầu vào ban đầu với hàm kích hoạt \\(g_1(.)\\). Giả sử các nút trong lớp ẩn đầu tiên lần lượt là \\(H^{(1)}_1\\), \\(H^{(1)}_2\\), \\(\\cdots\\), \\(H^{(1)}_{k_1}\\). Ta có \\(H^{(1)}_j\\) được tính toán từ các đầu vào với \\((p+1)\\) tham số \\(w^{(1)}_{j,}\\) với \\(0 \\leq \\leq p\\) như sau:\n\\[\\begin{align}\nH^{(1)}_j = g_1\\left( w^{(1)}_{j,0} + w^{(1)}_{j,1} X_1 + w^{(1)}_{j,2} X_2 + \\cdots + w^{(1)}_{j,p} X_p \\right)\n\\tag{17.9}\n\\end{align}\\]\nCó thể thấy rằng, để tính toán tất cả \\(k_1\\) đơn vị trong lớp ẩn thứ nhất, chúng ta cần sử dụng \\(k_1 \\times (p+1)\\) tham số.Lớp ẩn thứ nhất có \\(k_1\\) đơn vị được tính toán từ \\(p\\) đầu vào ban đầu với hàm kích hoạt \\(g_1(.)\\). Giả sử các nút trong lớp ẩn đầu tiên lần lượt là \\(H^{(1)}_1\\), \\(H^{(1)}_2\\), \\(\\cdots\\), \\(H^{(1)}_{k_1}\\). Ta có \\(H^{(1)}_j\\) được tính toán từ các đầu vào với \\((p+1)\\) tham số \\(w^{(1)}_{j,}\\) với \\(0 \\leq \\leq p\\) như sau:\n\\[\\begin{align}\nH^{(1)}_j = g_1\\left( w^{(1)}_{j,0} + w^{(1)}_{j,1} X_1 + w^{(1)}_{j,2} X_2 + \\cdots + w^{(1)}_{j,p} X_p \\right)\n\\tag{17.9}\n\\end{align}\\]\nCó thể thấy rằng, để tính toán tất cả \\(k_1\\) đơn vị trong lớp ẩn thứ nhất, chúng ta cần sử dụng \\(k_1 \\times (p+1)\\) tham số.Lớp ẩn thứ hai có \\(k_2\\) đơn vị được tính toán từ \\(k_1\\) đơn vị của lớp ẩn thứ nhất và hàm kích hoạt \\(g_2(.)\\). Gọi các nút trong lớp ẩn thứ hai lần lượt là \\(H^{(2)}_1\\), \\(H^{(2)}_2\\), \\(\\cdots\\), \\(H^{(2)}_{k_2}\\). Ta có \\(H^{(2)}_j\\) được tính toán từ lớp ẩn thứ nhất vào với \\((k_1+1)\\) tham số \\(w^{(2)}_{j,}\\) với \\(0 \\leq \\leq k_1\\) như sau:\n\\[\\begin{align}\nH^{(2)}_j = g_2\\left( w^{(2)}_{j,0} + w^{(2)}_{j,1} H^{(2)}_1 + w^{(2)}_{j,2} H^{(2)}_1 + \\cdots + w^{(2)}_{j,k_1} H^{(2)}_{k_1} \\right)\n\\tag{17.10}\n\\end{align}\\]\nĐể tính toán tất cả \\(k_2\\) đơn vị trong lớp ẩn thứ hai, chúng ta cần sử dụng \\(k_2 \\times (k_1+1)\\) tham số.Lớp ẩn thứ hai có \\(k_2\\) đơn vị được tính toán từ \\(k_1\\) đơn vị của lớp ẩn thứ nhất và hàm kích hoạt \\(g_2(.)\\). Gọi các nút trong lớp ẩn thứ hai lần lượt là \\(H^{(2)}_1\\), \\(H^{(2)}_2\\), \\(\\cdots\\), \\(H^{(2)}_{k_2}\\). Ta có \\(H^{(2)}_j\\) được tính toán từ lớp ẩn thứ nhất vào với \\((k_1+1)\\) tham số \\(w^{(2)}_{j,}\\) với \\(0 \\leq \\leq k_1\\) như sau:\n\\[\\begin{align}\nH^{(2)}_j = g_2\\left( w^{(2)}_{j,0} + w^{(2)}_{j,1} H^{(2)}_1 + w^{(2)}_{j,2} H^{(2)}_1 + \\cdots + w^{(2)}_{j,k_1} H^{(2)}_{k_1} \\right)\n\\tag{17.10}\n\\end{align}\\]\nĐể tính toán tất cả \\(k_2\\) đơn vị trong lớp ẩn thứ hai, chúng ta cần sử dụng \\(k_2 \\times (k_1+1)\\) tham số.Trong lớp đầu ra, đây là bài toán phân loại, nên chúng ta sử dụng \\(m = 10\\) đơn vị tương ứng với 10 chữ số viết tay từ 0 đến 9. Trong bài toán phân loại, hàm kích hoạt để tính toán các đơn vị trong lớp đầu ra thường là hàm softmax. Giá trị tại đơn vị \\(Y_j\\) với \\(1 \\leq m\\) trong lớp đầu ra được xác định như sau:\n\\[\\begin{align}\nZ_j &= \\beta_{j,0} + \\beta_{j,1} H^{(2)}_1 + \\beta_{j,2} H^{(2)}_2 + \\cdots +\\beta_{j,k_2} H^{(2)}_{k_2} \\\\\n\\textbf{Y} &= softmax(\\textbf{Z}) \\rightarrow Y_j = \\cfrac{exp(Z_j)}{exp(Z_1) + exp(Z_2) + \\cdots + exp(Z_m)}\n\\tag{17.11}\n\\end{align}\\]\nĐể tính toán \\(m\\) đơn vị trong lớp đầu ra, chúng ta cần \\(m \\times (k_2 + 1)\\) tham số. Lưu ý rằng khi sử dụng hàm softmax thì tổng giá trị của các đơn vị trong lớp đầu ra luôn bằng 1.Trong lớp đầu ra, đây là bài toán phân loại, nên chúng ta sử dụng \\(m = 10\\) đơn vị tương ứng với 10 chữ số viết tay từ 0 đến 9. Trong bài toán phân loại, hàm kích hoạt để tính toán các đơn vị trong lớp đầu ra thường là hàm softmax. Giá trị tại đơn vị \\(Y_j\\) với \\(1 \\leq m\\) trong lớp đầu ra được xác định như sau:\n\\[\\begin{align}\nZ_j &= \\beta_{j,0} + \\beta_{j,1} H^{(2)}_1 + \\beta_{j,2} H^{(2)}_2 + \\cdots +\\beta_{j,k_2} H^{(2)}_{k_2} \\\\\n\\textbf{Y} &= softmax(\\textbf{Z}) \\rightarrow Y_j = \\cfrac{exp(Z_j)}{exp(Z_1) + exp(Z_2) + \\cdots + exp(Z_m)}\n\\tag{17.11}\n\\end{align}\\]\nĐể tính toán \\(m\\) đơn vị trong lớp đầu ra, chúng ta cần \\(m \\times (k_2 + 1)\\) tham số. Lưu ý rằng khi sử dụng hàm softmax thì tổng giá trị của các đơn vị trong lớp đầu ra luôn bằng 1.Như vậy, để tính toán được \\(m = 10\\) giá trị đầu ra cho cấu trúc mạng nơ-ron được trình bày trong Hình 17.4, số lượng tham số cần sử dụng là\n\\[\\begin{align}\nk_1 \\cdot (p + 1) + k_2 \\cdot (k_1 + 1) + m \\cdot (k_2+1)\n\\end{align}\\]Ví dụ, nếu chúng ta sử dụng 512 đơn vị trong lớp ẩn thứ nhất và 256 đơn vị trong lớp ẩn thứ hai, số lượng tham số của mạng nơ-ron với 784 đơn vị đầu vào và 10 đơn vị đầu ra là\n\\[\\begin{align}\n512 \\cdot (784 + 1) + 256 \\cdot (512 + 1) + 10 \\cdot (256 + 1) = 535.818\n\\end{align}\\]\nNói cách khác, mô hình sử dụng hơn 500 nghìn tham số để tính toán 10 đơn vị đầu ra từ 784 đơn vị đầu vào. Các tham số này được tính toán sao cho sai số giữa véc-tơ đầu ra tính toán từ mô hình và giá trị đầu ra quan sát được trên dữ liệu là nhỏ nhất. Trong bài toán phân loại, sai số thường được sử dụng là hàm cross-entropy. Với quan sát thứ \\(\\) của biến giải thích \\(\\textbf{x}_i\\) thì quan sát \\(y_i\\) tương ứng của biến mục tiêu (nhận một trong các giá trị từ \\(1\\) đến \\(m\\)) sẽ được viết dưới dạng véc-tơ đầu ra \\(\\textbf{y}_i = (y^{}_{1},y^i_2,\\cdots,y^i_m)\\) sao cho\n\\[\\begin{align}\ny^i_j &= 1 \\text{ nếu } y_i = j \\\\\ny^i_j &= 0 \\text{ nếu } y_i \\neq j\n\\tag{17.12}\n\\end{align}\\]\nCách biến đổi biến này thường được gọi là one-hot encoding. Với véc-tơ đầu ra được tính toán từ véc-tơ đầu vào \\(\\textbf{x}_i\\) theo cấu trúc mạng nơ-ron bằng các phương trình (17.9), (17.10), và (17.11) là \\(\\hat{\\textbf{y}}_i = (\\hat{y}^{}_{1},\\hat{y}^i_2,\\cdots,\\hat{y}^i_m)\\) thì sai số tính bằng cross-entropy tại quan sát thứ \\(\\) là\n\\[\\begin{align}\n\\sum\\limits_{j=1}^m \\ y^{}_{j} \\cdot \\log(\\hat{y}^{}_{j}) = y^{}_{1} \\cdot \\log(\\hat{y}^{}_{1}) + y^{}_{2} \\cdot \\log(\\hat{y}^{}_{2}) + \\cdots + y^{}_{m} \\cdot \\log(\\hat{y}^{}_{m})\n\\tag{17.13}\n\\end{align}\\]\nvà sai số tính bằng cross-entropy trên \\(n\\) dữ liệu huấn luyện mô hình là\n\\[\\begin{align}\nCE\\_loss = \\sum\\limits_{=1}^n \\sum\\limits_{j=1}^m \\ y^{}_{j} \\cdot \\log(\\hat{y}^{}_{j})\n\\tag{17.14}\n\\end{align}\\]Trong mô hình mạng nơ-ron, để đơn giản hóa ký hiệu, chúng ta sẽ sử dụng ký hiệu dạng ma trận. Cấu trúc mạng nơ-ron có hai lớp ẩn được mô tả trong hình 17.4 có ba ma trận tham số \\(\\boldsymbol{w}_1\\), \\(\\boldsymbol{w}_2\\), và \\(\\boldsymbol{\\beta}\\) được định nghĩa như sau\n\\[\\begin{align}\n\\boldsymbol{\\beta} &= \\begin{pmatrix}\n\\beta_{1,0} & \\beta_{1,1} & \\cdots & \\beta_{1,k_2} \\\\\n\\beta_{2,0} & \\beta_{2,1} & \\cdots & \\beta_{2,k_2} \\\\\n\\cdots & \\cdots & \\cdots & \\cdots \\\\\n\\beta_{m,0} & \\beta_{m,1} & \\cdots & \\beta_{m,k_2}\n\\end{pmatrix} \\\\\n& \\\\\n\\boldsymbol{w}_1 &= \\begin{pmatrix}\nw^{(1)}_{1,0} & w^{(1)}_{1,1} & \\cdots & w^{(1)}_{1,p} \\\\\nw^{(1)}_{2,0} & w^{(1)}_{2,1} & \\cdots & w^{(1)}_{2,p} \\\\\n\\cdots & \\cdots & \\cdots & \\cdots \\\\\nw^{(1)}_{k_1,0} & w^{(1)}_{k_1,1} & \\cdots & w^{(1)}_{k_1,p}\n\\end{pmatrix};\n\\boldsymbol{w}_2 = \\begin{pmatrix}\nw^{(2)}_{1,0} & w^{(2)}_{1,1} & \\cdots & w^{(2)}_{1,k_1} \\\\\nw^{(2)}_{2,0} & w^{(2)}_{2,1} & \\cdots & w^{(2)}_{2,p} \\\\\n\\cdots & \\cdots & \\cdots & \\cdots \\\\\nw^{(2)}_{k_2,0} & w^{(2)}_{k_2,1} & \\cdots & w^{(2)}_{k_2,k_1}\n\\end{pmatrix}\n\\tag{17.15}\n\\end{align}\\]Quá trình ước lượng tham số cho mạng nơ-ron là quá trình tìm các ma trận tham số \\(\\boldsymbol{w}_1\\), \\(\\boldsymbol{w}_2\\), và \\(\\boldsymbol{\\beta}\\) để tối thiểu hóa tổn thất tính bằng cross-entropy\\[\\begin{align}\n(\\hat{\\boldsymbol{w}}_1, \\hat{\\boldsymbol{w}}_2, \\hat{\\boldsymbol{\\beta}}) &= \\underset{\\boldsymbol{w}_1, \\boldsymbol{w}_2, \\boldsymbol{\\beta}}{\\operatorname{argmin}} \\sum_{=1}^n \\sum_{j=1}^m y^{}_{j} \\cdot \\log(\\hat{y}^{}_{j})\n\\label{#eq:nn016}\n\\end{align}\\]Như chúng tôi đã đề cập ở phía trước, các mô hình mạng nơ-ron có nhiều lớp ẩn với số lượng đơn vị trong các lớp ẩn không quá nhiều thường cho kết quả tốt hơn với các mô hình có ít lớp ẩn và sử dụng nhiều đơn vị trong một lớp. Tuy nhiên khi tăng số lớp ẩn lên sẽ làm cho số lượng tham số cần được ước lượng tăng lên rất nhanh, khiến cho mô hình mạng nơ-ron rất dễ rơi vào tình trạng overfitting, nghĩa là sai số trên tập dữ liệu huấn luyện mô hình nhỏ nhưng sai số trên dữ liệu kiểm tra mô hình lại rất lớn. Chính vì thế, trong quá trình ước lượng tham số, người xây dựng mô hình thường sử dụng thêm các ràng buộc tham số, chẳng hạn như ràng buộc tham số kiểu hồi quy ridge. Nghĩa là tổn thất của mô hình tính bằng cross-entropy sẽ được điều chỉnh để cân bằng giữa phương sai và độ lệch của mô hình. Chúng ta sẽ thảo luận về vấn đề này trong phần ước lượng tham số cho mạng nơ-ron.","code":""},{"path":"neuralnetwork.html","id":"nnestimation","chapter":"Chương 17 Mô hình mạng nơ-ron","heading":"17.3 Ước lượng tham số của mạng nơ-ron","text":"Tham số của mạng nơ-ron được chia thành hai nhóm:Nhóm thứ nhất bao gồm các siêu tham số như số lượng lớp ẩn trong cấu trúc mạng và trong mỗi lớp ẩn có bao nhiêu đơn vị. Chẳng hạn như cấu trúc được mô tả trong hình 17.4 có hai lớp ẩn, lớp ẩn thứ nhất có 512 đơn vị, lớp ẩn thứ hai có 256 đơn vị. Trong trường hợp chúng ta có sử dụng ràng buộc tham số, chúng ta có thêm một tham số điều chỉnh sự đánh đổi giữa sai lệch và phương sai giống như tham số \\(\\lambda\\) trong hồi quy ridge.Nhóm thứ nhất bao gồm các siêu tham số như số lượng lớp ẩn trong cấu trúc mạng và trong mỗi lớp ẩn có bao nhiêu đơn vị. Chẳng hạn như cấu trúc được mô tả trong hình 17.4 có hai lớp ẩn, lớp ẩn thứ nhất có 512 đơn vị, lớp ẩn thứ hai có 256 đơn vị. Trong trường hợp chúng ta có sử dụng ràng buộc tham số, chúng ta có thêm một tham số điều chỉnh sự đánh đổi giữa sai lệch và phương sai giống như tham số \\(\\lambda\\) trong hồi quy ridge.Với mỗi lựa chọn cho các tham số trong nhóm thứ nhất, chúng ta có các tham số để tính toán cấu trúc mạng bao gồm các ma trận \\(\\boldsymbol{w}\\) và ma trận \\(\\boldsymbol{\\beta}\\) được định nghĩa trong phương trình (17.15). Quá trình ước lượng các tham số này là quá trình giải bài toán tối thiểu hóa hàm tổn thất dạng tổng sai số bình phương trong bài toán hồi quy hoặc hàm cross-entropy trong bài toán phân loại. Nhìn chung, không thể tính toán được lời giải chính xác cho bài toán tối ưu mà chúng ta sẽ phải ước lượng tham số bằng các phương pháp giải số, mà cụ thể là phương pháp stochastic gradient descent. đó, chúng ta thường phải xác định định thêm các tham số như tốc độ học, số lượng dữ liệu được sử dụng trong mỗi bước tính toán, hay số vòng lặp của thuật toán gradient descent.Với mỗi lựa chọn cho các tham số trong nhóm thứ nhất, chúng ta có các tham số để tính toán cấu trúc mạng bao gồm các ma trận \\(\\boldsymbol{w}\\) và ma trận \\(\\boldsymbol{\\beta}\\) được định nghĩa trong phương trình (17.15). Quá trình ước lượng các tham số này là quá trình giải bài toán tối thiểu hóa hàm tổn thất dạng tổng sai số bình phương trong bài toán hồi quy hoặc hàm cross-entropy trong bài toán phân loại. Nhìn chung, không thể tính toán được lời giải chính xác cho bài toán tối ưu mà chúng ta sẽ phải ước lượng tham số bằng các phương pháp giải số, mà cụ thể là phương pháp stochastic gradient descent. đó, chúng ta thường phải xác định định thêm các tham số như tốc độ học, số lượng dữ liệu được sử dụng trong mỗi bước tính toán, hay số vòng lặp của thuật toán gradient descent.Lựa chọn tham số trong nhóm thứ nhất có thể ảnh hưởng lớn đến kết quả của mô hình mạng nơ-ron, nhưng lại không có phương pháp chính xác nào để xác định các tham số này. Nếu nguồn lực tính toán cho phép, các tham số này sẽ được xác định bằng cách thử nghiệm và lựa chọn. Nếu nguồn lực tính toán không cho phép, người xây dựng mô hình thường lựa chọn các tham số này dựa trên kinh nghiệm và cấu trúc mạng đã có sẵn trên các bộ dữ liệu tương tự.Trong phần này, chúng tôi sẽ tập trung vào các tham số cần được ước lượng trong nhóm thứ hai, nghĩa là tập trung vào ước lượng các ma trận \\(\\boldsymbol{w}\\) và ma trận \\(\\boldsymbol{\\beta}\\) khi chúng ta đã có một cấu trúc mạng cụ thể.","code":""},{"path":"neuralnetwork.html","id":"ước-lượng-tham-số-cho-mạng-nơ-ron-hồi-quy-có-một-lớp-ẩn","chapter":"Chương 17 Mô hình mạng nơ-ron","heading":"17.3.1 Ước lượng tham số cho mạng nơ-ron hồi quy có một lớp ẩn","text":"Quá trình ước lượng mạng nơ-ron đòi hỏi kiến thức và các kỹ thuật toán học khá phức tạp và chúng tôi sẽ cố gắng chỉ trình bày tổng quan và ngắn gọn. Bạn đọc cảm thấy khó khăn về phần này này có thể yên tâm bỏ qua và chuyển sang phần tiếp theo bởi chúng ta có thể sử dụng các thư viện như hay để ước lượng mô hình mà không cần hiểu quá sâu về các chi tiết kỹ thuật trong quy trình xây dựng mô hình.Chúng ta sẽ bắt đầu bằng mô hình mạng nơ-ron hồi quy có một lớp ẩn duy nhất được trình bày trong hình 17.1 và phương trình (17.1). Để mô hình giữ nguyên tính tổng quát, chúng tôi sử dụng \\(p\\) là số tham số đầu vào, \\(k\\) là số lượng đơn vị trong lớp ẩn. Chúng ta cần tìm các tham số là véc-tơ \\(\\boldsymbol{\\beta}\\) và ma trận \\(\\boldsymbol{w}\\) để tối thiểu hóa tổng sai số bình phương:\\[\\begin{align}\nRSS\\left( \\boldsymbol{\\beta}, \\boldsymbol{w} \\right) & = \\cfrac{1}{2} \\ \\sum\\limits_{=1}^n \\ \\left(y_i - f_{\\boldsymbol{\\beta}, \\boldsymbol{w}}\\left(\\textbf{x}_i\\right) \\right)^2 \\\\\n& = \\cfrac{1}{2} \\sum\\limits_{=1}^n \\ \\left(y_i - \\beta_0 - \\sum\\limits_{t = 1}^k \\beta_t \\cdot g\\left(z_{t,}\\right) \\right)^2\n\\tag{17.16}\n\\end{align}\\]\nvới\n\\[\\begin{align}\nz_{,t} = w_{t,0} + \\sum\\limits_{j=1}^p w_{t,j} \\cdot x_{,j}\n\\tag{17.17}\n\\end{align}\\]\nMặc dù hàm \\(RSS\\left( \\boldsymbol{\\beta}, \\boldsymbol{w} \\right)\\) trong phương trình (17.16) không quá phức tạp, nhưng để giải bài toán tối thiểu hóa hàm số này trên dữ liệu \\((\\textbf{x}_i,y_i)\\) với \\(= 1, 2, \\cdots, n\\) và hàm số \\(g\\) cho trước không phải là một nhiệm vụ dễ dàng. Trước hết, mặc dù chúng ta có dạng hàm tường minh cho các tham số, nhưng đây không phải là hàm số lồi theo các tham số, đó quá trình giải bài toán tối ưu thường chỉ cho đáp số là một điểm cực tiểu địa phương chứ không chắc chắn là điểm cực tiểu toàn cục. Thứ hai, không thể có lời giải chính xác cho bài toán tối ưu nên chúng ta sẽ cần tìm lời giải số, trong trường hợp này là phương pháp gradient descent. Việc lựa chọn các tham số cho thuật toán này cũng sẽ là câu hỏi cần được giải đáp cho quá trình xây dựng mô hình. Thứ ba, mô hình mạng nơ-ron có rất nhiều tham số, đó rất dễ dẫn đến hiện tượng mô hình quá khớp với dữ liệu huấn luyện mô hình. Hàm mục tiêu thường sẽ bằng RSS cộng thêm một hàm phạt, khiến cho quá trình giải số trở nên khó khăn hơn.Trước tiên, giả sử bài toán tối ưu trong (17.16) không có ràng buộc tham số, chúng ta cần xác định gradient của RSS theo \\(\\boldsymbol{\\beta}\\) và \\(\\boldsymbol{w}\\). Để đơn giản hóa, chúng ta giả sử bình phương của sai số thứ \\(\\) là \\(RSS_i\\)\n\\[\\begin{align}\nRSS_i = \\cfrac{1}{2} \\ \\left[y_i - \\beta_0 - \\sum\\limits_{t = 1}^k \\beta_t \\cdot g\\left(z_{,t}\\right) \\right]^2\n\\end{align}\\]Đạo hàm của bình phương sai số thứ \\(\\) theo \\(\\beta_l\\), với \\(0 \\leq l \\leq k\\), được xác định như sau\n\\[\\begin{align}\n\\cfrac{\\partial RSS_i}{\\partial \\beta_l} & = \\begin{cases}\n-\\left(y_i - f\\left(\\textbf{x}_i\\right) \\right) & \\text{ nếu } l = 0 \\\\\n-g\\left(z_{,l}\\right)  \\left(y_i - f\\left(\\textbf{x}_i\\right) \\right) & \\text{ nếu } l > 0\n\\end{cases}\n\\tag{17.18}\n\\end{align}\\]Đạo hàm của bình phương sai số thứ \\(\\) theo \\(\\beta_l\\), với \\(0 \\leq l \\leq k\\), được xác định như sau\n\\[\\begin{align}\n\\cfrac{\\partial RSS_i}{\\partial \\beta_l} & = \\begin{cases}\n-\\left(y_i - f\\left(\\textbf{x}_i\\right) \\right) & \\text{ nếu } l = 0 \\\\\n-g\\left(z_{,l}\\right)  \\left(y_i - f\\left(\\textbf{x}_i\\right) \\right) & \\text{ nếu } l > 0\n\\end{cases}\n\\tag{17.18}\n\\end{align}\\]Đạo hàm của bình phương sai số thứ \\(\\) theo \\(w_{l,j}\\), với \\(1 \\leq l \\leq k\\) và \\(0 \\leq j \\leq p\\), được xác định như sauĐạo hàm của bình phương sai số thứ \\(\\) theo \\(w_{l,j}\\), với \\(1 \\leq l \\leq k\\) và \\(0 \\leq j \\leq p\\), được xác định như sau\\[\\begin{align}\n\\cfrac{\\partial RSS_i}{\\partial w_{l,j}} & = - \\cfrac{\\partial  \\beta_l \\cdot g\\left(z_{,l}\\right) }{\\partial w_{l,j}} \\left(y_i - f\\left(\\textbf{x}_i\\right) \\right) \\\\\n& =\n\\begin{cases}\n- \\beta_l \\cdot g^{'}\\left(z_{,l}\\right) \\cdot \\left(y_i - f\\left(\\textbf{x}_i\\right) \\right) & \\text{ nếu } j = 0 \\\\\n- \\beta_l \\cdot x_{,j} \\cdot g^{'}\\left(z_{,l}\\right) \\cdot \\left(y_i - f\\left(\\textbf{x}_i\\right) \\right) & \\text{ nếu } j > 0\n\\end{cases}\n\\tag{17.19}\n\\end{align}\\]Trước tiên, có thể thấy rằng cả hai biểu thức đạo hàm của sai số bình phương này đều chứa phần dư \\(\\left(y_i − f(\\textbf{x})\\right)\\). Trong công thức (17.18) chúng ta thấy rằng giá trị tuyệt đối của gradient theo các \\(\\beta_l\\) bằng phần dư nhân với giá trị \\(g\\left(z_{,l}\\right)\\), chính là giá trị tại nút \\(H_l\\) tính theo đầu vào \\(\\textbf{x}_i\\). Tiếp theo, trong công thức (17.19) chúng ta thấy sự thay đổi của RSS theo tham số \\(w_{l,j}\\), tương ứng với hệ số của đầu vào \\(X_j\\) khi tính toán đơn vị \\(H_l\\) của mạng nơ-ron một lớp ẩn, cũng phụ thuộc vào phần dư. Sự ảnh hưởng của phần dư lên đạo hàm theo từng tham số của mô hình được gọi là quá trình lan truyền ngược trong mô hình mạng nơ-ron.Các công thức đạo hàm ở trên luôn yêu cầu tính toán giá trị của hàm kích hoạt và đạo hàm tại các điểm \\(z_{,l}\\). Về lý thuyết, mọi hàm đơn điệu tăng, có đạo hàm, và không tuyến tính đều có thể được sử dụng làm hàm kích hoạt. Tuy nhiên, nếu dạng hàm quá phức tạp, việc tính toán sẽ trở nên phưc tạp, nhất là khi sử dụng nhiều lớp ẩn và trong mỗi lớp ẩn có nhiều đơn vị. Điều này giải thích tại sao hàm ReLU thường xuyên được sử dụng làm hàm kích hoạt để tính toán gradient là đơn giản nhất có thể. Giả sử hàm \\(g\\) trong các công thức (17.18) và (17.19) là hàm ReLU, chúng ta có thể đơn giản hóa các đạo hàm như sau:Đạo hàm theo hệ số \\(\\beta_l\\):\\[\\begin{align}\n\\cfrac{\\partial RSS_i}{\\partial \\beta_l} & = \\begin{cases}\n-\\left(y_i - f\\left(\\textbf{x}_i\\right) \\right) & \\text{ nếu } l = 0 \\\\\n-\\mathbb{}(z_{,l} > 0) \\cdot z_{,l} \\cdot \\left(y_i - f\\left(\\textbf{x}_i\\right) \\right) & \\text{ nếu } l > 0\n\\end{cases}\n\\tag{17.20}\n\\end{align}\\]Đạo hàm theo \\(w_{l,0}\\),\\[\\begin{align}\n  \\cfrac{\\partial RSS_i}{\\partial w_{l,0}} & = - \\beta_l \\cdot \\mathbb{}(z_{,l} > 0) \\cdot \\left(y_i - f\\left(\\textbf{x}_i\\right) \\right)\n  \\tag{17.21}\n\\end{align}\\]Đạo hàm theo \\(w_{l,j}\\), với \\(j > 0\\) thì\\[\\begin{align}\n\\cfrac{\\partial RSS_i}{\\partial w_{l,j}} & =\n- \\beta_l \\cdot \\mathbb{}(z_{,l} > 0) \\cdot x_{,j} \\cdot \\left(y_i - f\\left(\\textbf{x}_i\\right) \\right)\n\\tag{17.22}\n\\end{align}\\]Gradient của tổng các \\(RSS_i\\) được xác định như sau:Theo \\(\\beta_0\\)\\[\\begin{align}\n\\cfrac{\\partial RSS}{\\partial \\beta_0} &= \\sum\\limits_{=1}^n \\cfrac{\\partial RSS_i}{\\partial \\beta_0} \\\\\n& = - \\sum\\limits_{=1}^n \\left(y_i - f\\left(\\textbf{x}_i\\right) \\right)\n\\tag{17.23}\n\\end{align}\\]Theo \\(\\beta_l\\) với \\(l > 0\\)\\[\\begin{align}\n\\cfrac{\\partial RSS}{\\partial \\beta_l} &= \\sum\\limits_{=1}^n \\cfrac{\\partial RSS_i}{\\partial \\beta_l} \\\\\n& = - \\sum\\limits_{=1}^n \\mathbb{}(z_{,l} > 0) \\cdot z_{,l} \\cdot \\left(y_i - f\\left(\\textbf{x}_i\\right) \\right)\n\\tag{17.24}\n\\end{align}\\]Theo \\(w_{l,0}\\),\\[\\begin{align}\n\\cfrac{\\partial RSS_i}{\\partial w_{l,0}} & =  - \\beta_l \\sum\\limits_{=1}^n \\mathbb{}(z_{,l} > 0) \\cdot \\left(y_i - f\\left(\\textbf{x}_i\\right) \\right)\n\\tag{17.25}\n\\end{align}\\]Theo \\(w_{l,j}\\) với j > 0\n\\[\\begin{align}\n\\cfrac{\\partial RSS_i}{\\partial w_{l,j}} & = - \\beta_l \\sum\\limits_{=1}^n \\mathbb{}(z_{,l} > 0) \\cdot x_{,j} \\cdot \\left(y_i - f\\left(\\textbf{x}_i\\right) \\right)\n\\tag{17.26}\n\\end{align}\\]Sau khi tính toán gradient của RSS theo các tham số, quá trình ước lượng sẽ được thực hiện thông qua thuật toán Stochastic gradient descent. Bạn đọc tham khảo thuật toán này trong Phụ lục ?? của chương Kiến thức R nâng cao. Kết quả của thuật toán Stochastic gradient descent phụ thuộc rất lớn vào giá trị khởi tạo ban đầu của các tham số, và nhất là khi số lượng tham số là rất lớn.Để mô tả quá trình ước lượng tham số của mạng nơ-ron, chúng ta sẽ sử dụng dữ liệu mô phỏng. Ma trận biến giải thích \\(\\textbf{X}\\) có kích thước \\(10^4 \\times 3\\) là các số ngẫu nhiên độc lập có phân phối chuẩn \\(\\mathcal{N}(0,1)\\). Hàm \\(f\\) được tạo bởi mô hình mạng nơ-ron với 3 đơn vị của lớp đầu vào, một lớp ẩn với 5 đơn vị, và một đơn vị trong lớp đầu đầu ra. Hàm kích hoạt được sử dụng là hàm ReLU. Biến giải thích \\(Y\\) được tính toán từ hàm \\(f(\\textbf{X})\\) công thêm một sai số độc lập với \\(\\textbf{X}\\) là một biến ngẫu nhiên phân phối chuẩn với trung bình bằng 0 và độ lệch chuẩn là 0.5. Các tham số dùng để tính toán \\(f\\) được đơn giản hóa: \\(\\beta_l = 2 \\forall l = 0, 1, \\cdots, 5\\) và $w_{l,j} = 1 l = 1, 2, , $ và $j = , $. Quá trình tối thiểu hóa tổng sai số bình phương được mô tả thông qua hình 17.5\nHình 17.5: Sai số của mô hình mạng nơ-ron giảm dần trong quá trình ước lượng tham số sử dụng thuật toán stochastic gradient descent. Hình bên trái: điểm bắt đầu của các tham số là biến ngẫu nhiên phân phối chuẩn độc lập có trung bình bằng 0 và phương sai bằng 3. Hình bên phải: điểm bắt đầu của các tham số expression{beta} là biến ngẫu nhiên có trung bình bằng 2 và phương sai bằng 1. Điểm bắt đầu của các tham số w là biến ngẫu nhiên có trung bình bằng 1 và phương sai bằng 1\nHình 17.5 mô tả quá trình tối thiểu hóa sai số tính bằng RSS trên dữ liệu mô phỏng bằng phương pháp stochastic gradient descent. Tổng số tham số cần được ước lượng của mô hình là 26, bao gồm 6 giá trị của véc-tơ \\(\\boldsymbol{\\beta}\\) và 20 giá trị của ma trận \\(\\boldsymbol{w}\\). Chúng tôi sử dụng 5 nghìn lần lặp và trong mỗi lần lặp sử dụng 5% dữ liệu để tính các gradient.Hình bên trái mô tả 10 quá trình ước lượng tham số mà các điểm bắt đầu của \\(\\boldsymbol{\\beta}\\) và \\(\\boldsymbol{w}\\) là hoàn toàn ngẫu nhiên. Chúng tôi cho các giá trị ban đầu là các biến ngẫu nhiên phân phối chuẩn với trung bình bằng 0 và phương sai bằng 3. Hình bên phải mô tả 10 quá trình ước lượng tham số với các điểm bắt đầu của \\(\\boldsymbol{\\beta}\\) có giá trị trung bình là 2 bằng với giá trị dùng để mô phỏng dữ liệu và \\(\\boldsymbol{w}\\) có giá trị trung bình là 1 cũng bằng với giá trị dùng để mô phỏng dữ liệu. Phương sai của 26 tham số khởi đầu đều bằng 1. Có thể thấy rằng khi các tham số khởi đầu là hoàn toàn ngẫu nhiên thì về trung bình các quá trình hội tụ về giá ngưỡng nho nhất của RSS chậm hơn khi chúng ta có các giá trị khởi đầu tốt hơn. Có hai trên tám quá trình sau 5000 bước lặp vẫn chưa cho sai số tiệm cận đến giá trị RSS nhỏ nhất. Trong hình bên phải thì các quá trình đều cho kết quả gần với giá trị RSS nhỏ nhất.","code":""},{"path":"neuralnetwork.html","id":"ước-lượng-tham-số-cho-mạng-nơ-ron-phân-loại-có-hai-lớp-ẩn","chapter":"Chương 17 Mô hình mạng nơ-ron","heading":"17.3.2 Ước lượng tham số cho mạng nơ-ron phân loại có hai lớp ẩn","text":"Giả sử cấu trúc mạng nơ-ron có hai lớp ẩn như hình 17.4. Các tham số cần ước lượng của mô hình bao gồm các ma trận \\(\\boldsymbol{w}_1\\), \\(\\boldsymbol{w}_2\\), và \\(\\boldsymbol{\\beta}\\) được cho bởi phương trình (17.15). đây là bài toán phân loại nên hàm mục tiêu được sử dụng là hàm cross-entropy\n\\[\\begin{align}\n(\\hat{\\boldsymbol{w}}_1, \\hat{\\boldsymbol{w}}_2, \\hat{\\boldsymbol{\\beta}}) &= \\underset{\\boldsymbol{w}_1, \\boldsymbol{w}_2, \\boldsymbol{\\beta}}{\\operatorname{argmin}} \\sum\\limits_{=1}^n \\sum\\limits_{j=1}^m \\ y^{}_{j} \\cdot \\log(\\hat{y}^{}_{j}) \\\\\n\\tag{17.27}\n\\end{align}\\]Giá trị của hàm cross-entropy tính trên quan sát thứ \\(\\) có thể được rút gọn như sau\n\\[\\begin{align}\n\\sum\\limits_{j=1}^m \\ y^{}_{j} \\cdot \\log(\\hat{y}^{}_{j}) &= \\sum\\limits_{j=1}^m \\ y^{}_{j} \\cdot \\log\\left(\\cfrac{exp\\left(z_{,j}\\right)}{exp\\left(z_{,1}\\right)+exp\\left(z_{,2}\\right)+\\cdots+exp\\left(z_{,m}\\right)}\\right) \\\\\n& = \\sum\\limits_{j=1}^m \\ y^{}_{j} \\cdot \\left(z_{,j} -  \\log\\left(exp\\left(z_{,1}\\right)+exp\\left(z_{,2}\\right)+\\cdots+exp\\left(z_{,m}\\right)\\right) \\right)\\\\\n& =  \\sum\\limits_{j=1}^m \\ y^{}_{j} \\cdot z_{,j} - \\log\\left(\\sum\\limits_{j=1}^m exp\\left(z_{,j}\\right)\\right)\n\\tag{17.28}\n\\end{align}\\]\nvới \\(z_{,j}\\) là tổ hợp tuyến tính của giá trị các nút ẩn thứ hai tính theo đầu vào \\(\\textbf{x}_i\\)\n\\[\\begin{align}\nz_{,j} = \\beta_{j,0} + \\beta_{j,1} h^{(2)}_{,1} + \\beta_{j,2} h^{(2)}_{,2} + \\cdots + \\beta_{j,k_2} h^{(2)}_{,k_2}\n\\tag{17.29}\n\\end{align}\\]Với mọi tham số \\(\\theta\\) được sử dụng để tính toán giá trị tại các nút đầu ra, đạo hàm của hàm cross-entropy sẽ được tính toán thông qua các \\(z_{,j}\\)\n\\[\\begin{align}\n\\cfrac{\\partial CE\\_Loss_i}{\\partial \\theta} &= \\sum\\limits_{j=1}^m \\ y^{}_{j} \\ \\cfrac{\\partial z_{,j}}{\\partial \\theta} - \\sum\\limits_{j=1}^m \\cfrac{\\partial z_{,j}}{\\partial \\theta} \\cfrac{exp(z_{,j})}{\\sum exp\\left(z_{,j}\\right)} \\\\\n& = \\sum\\limits_{j=1}^m  \\cfrac{\\partial z_{,j}}{\\partial \\theta} \\left(y^{}_{j} - p_{,j}\\right)\n\\tag{17.30}\n\\end{align}\\]\ntrong đó\n\\[\\begin{align}\np_{,j} = \\cfrac{exp(z_{,j})}{\\sum\\limits_{j=1}^m exp\\left(z_{,j}\\right)}\n\\tag{17.31}\n\\end{align}\\]Từ công thức (17.29) có thể thấy rằng: véc-tơ dữ liệu đầu ra \\(\\textbf{y}_i = (y^{}_{1}, y^{}_{2}, \\cdots, y^{}_{m})\\) nhận giá trị bằng 1 tại một vị trí và nhận giá trị bằng 0 tại \\(m-1\\) vị trí còn lại, trong khi đó \\(p_{,j}\\) có thể được hiểu là xác suất mà đầu ra thứ \\(\\) nhận giá trị bằng \\(j\\) được xác định bởi mô hình mạng nơ-ron. Đạo hàm của hàm tổn thất tính bằng cross-entropy theo tham số \\(\\theta\\) bất kỳ, là một phần tử của ma trận \\(\\boldsymbol{\\beta}\\) hoặc các \\(\\boldsymbol{w}\\), phụ thuộc vào sai số giữa véc-tơ xác suất từ dữ liệu quan sát được \\(\\textbf{y}^\\) và véc-tơ xác suất được xác định bởi mô hình mạng nơ-ron \\(\\textbf{p}_{} = (p_{,1}, p_{,2}, \\cdot, p_{,m})\\). Như vậy, gradient của hàm tổn thất theo từng tham số phụ thuộc vào sai số của mô hình hiện tại. Nói một cách khác, sai số của mô hình trong bước hiện tại sẽ tác động đến việc cập nhật tham số trong bước tiếp theo khi chúng ta sử dụng phương pháp gradient descent. Đây là quá trình lan truyền ngược trong mô hình mạng nơ-ron mà chúng tôi đã đề cập đến trong phần mô hình mạng nơ-ron hồi quy có một lớp ẩn.Quá trình tính toán đạo hàm của \\(z_{,j}\\) theo các tham số của ma trận \\(\\boldsymbol{\\beta}\\) là khá hiển nhiên \\(z_{,j}\\) là hàm tuyến tính theo các tham số này. Để tính toán đạo hàm của \\(z_{,j}\\) theo các tham số của các ma trận \\(\\boldsymbol{w}\\), chúng ta sử dụng nguyên tắc chain-rule đã trình bày trong phụ lục của phần Kiến thức R nâng cao. chỉ số của các tham số trong ma trận là quá phức tạp nên chúng tôi sẽ chỉ trình bày cách tính đạo hàm mang tính tổng quát. Ta có mỗi \\(z\\) được tính từ phương trình (17.29) được xác định từ dữ liệu đầu vào \\(x\\) thông qua các hàm kích hoạt \\(g_1\\) và \\(g_2\\) như sau\n\\[\\begin{align}\nz & = \\beta h^{(2)} \\\\\nh^{(2)} &= g_2(w^{(2)}\\cdot h^{(1)}) \\\\\nh^{(1)} &= g_1(w^{(1)}\\cdot x)\n\\tag{17.32}\n\\end{align}\\]Trong đó \\(\\beta\\), \\(w^{(2)}\\), và \\(w^{(1)}\\) là các tham số của mô hình. Giá trị đạo hàm của \\(z\\) theo các tham số được xác định như sauTheo \\(\\beta\\)\n\\[\\begin{align}\n\\cfrac{\\partial z}{\\partial \\beta} = h^{(2)}\n\\tag{17.33}\n\\end{align}\\]Theo \\(\\beta\\)\n\\[\\begin{align}\n\\cfrac{\\partial z}{\\partial \\beta} = h^{(2)}\n\\tag{17.33}\n\\end{align}\\]Theo \\(w^{(2)}\\), chúng ta áp dụng nguyên tắc chain-rule\n\\[\\begin{align}\n\\cfrac{\\partial z}{\\partial w^{(2)}} & = \\cfrac{\\partial z}{\\partial h^{(2)}} \\times \\cfrac{\\partial h^{(2)}}{\\partial w^{(2)}} \\\\\n& =  \\beta \\cdot h^{(1)} \\cdot  g^{'}_2(w^{(2)}\\cdot h^{(1)})\n\\tag{17.34}\n\\end{align}\\]Theo \\(w^{(2)}\\), chúng ta áp dụng nguyên tắc chain-rule\n\\[\\begin{align}\n\\cfrac{\\partial z}{\\partial w^{(2)}} & = \\cfrac{\\partial z}{\\partial h^{(2)}} \\times \\cfrac{\\partial h^{(2)}}{\\partial w^{(2)}} \\\\\n& =  \\beta \\cdot h^{(1)} \\cdot  g^{'}_2(w^{(2)}\\cdot h^{(1)})\n\\tag{17.34}\n\\end{align}\\]Theo \\(w^{(1)}\\), áp dụng nguyên tắc chain-rule\n\\[\\begin{align}\n\\cfrac{\\partial z}{\\partial w^{(1)}} & = \\cfrac{\\partial z}{\\partial h^{(2)}} \\times \\cfrac{\\partial h^{(2)}}{\\partial h^{(1)}} \\times \\cfrac{\\partial h^{(1)}}{\\partial w^{(1)}} \\\\\n& =  \\beta \\cdot w^{(2)} \\cdot g^{'}_2(w^{(2)}\\cdot h^{(1)}) \\cdot x \\cdot g^{'}_1(w^{(1)}\\cdot x)\n\\tag{17.35}\n\\end{align}\\]Theo \\(w^{(1)}\\), áp dụng nguyên tắc chain-rule\n\\[\\begin{align}\n\\cfrac{\\partial z}{\\partial w^{(1)}} & = \\cfrac{\\partial z}{\\partial h^{(2)}} \\times \\cfrac{\\partial h^{(2)}}{\\partial h^{(1)}} \\times \\cfrac{\\partial h^{(1)}}{\\partial w^{(1)}} \\\\\n& =  \\beta \\cdot w^{(2)} \\cdot g^{'}_2(w^{(2)}\\cdot h^{(1)}) \\cdot x \\cdot g^{'}_1(w^{(1)}\\cdot x)\n\\tag{17.35}\n\\end{align}\\]Số lượng tham số cần được tính toán trong mô hình mạng nơ-ron phân loại với \\(p\\) đầu vào, \\(m\\) đầu ra, hai lớp ẩn với số lượng đơn vị lần lượt là \\(k_1\\) và \\(k_2\\) là\\(m \\cdot (k_2+1)\\) tham số trong ma trận \\(\\boldsymbol{\\beta}\\),\\(m \\cdot (k_2+1)\\) tham số trong ma trận \\(\\boldsymbol{\\beta}\\),\\(k_2 \\cdot (k_1 + 1)\\) tham số trong ma trận \\(\\boldsymbol{w}_2\\)\\(k_2 \\cdot (k_1 + 1)\\) tham số trong ma trận \\(\\boldsymbol{w}_2\\)\\(k_1 \\cdot (p + 1)\\) tham số trong ma trận \\(\\boldsymbol{w}_1\\)\\(k_1 \\cdot (p + 1)\\) tham số trong ma trận \\(\\boldsymbol{w}_1\\)Số lượng tham số quá lớn sẽ dẫn đến các vấn đề bao gồm sự phức tạp khi tiến hành tính toán, khối lượng tính toán lớn, và rất dễ dẫn đến overfitting. Để khắc phục vấn đề này, mô hình mạng nơ-ron thường phải sử dụng thêm các kỹ thuật để thêm ràng buộc tham số hoặc loại bỏ đơn vị trong các lớp. Chúng ta sẽ thảo luận các kỹ thuật này trong phần tiếp theo.","code":""},{"path":"neuralnetwork.html","id":"khắc-phục-hiện-tượng-khớp-quá-mức-của-mạng-nơ-ron","chapter":"Chương 17 Mô hình mạng nơ-ron","heading":"17.3.3 Khắc phục hiện tượng khớp quá mức của mạng nơ-ron","text":"Phương pháp trước hết để hạn chế mô hình khớp quá mức là sử dụng ràng buộc tham số. Nguyên tắc ràng buộc tham số trong mô hình mạng nơ-ron cũng tương tự trong hồi quy ridge. Ví dụ, với mô hình mạng nơ-ron hồi quy được trình bày trong phần 17.1 hàm mục tiêu cần được tối thiểu hóa là tổng bình phương sai số cộng thêm một hàm phạt có dạng tổng bình phương các tham số sử dụng trong mô hình.\\[\\begin{align}\n(\\hat{\\boldsymbol{\\beta}},\\hat{\\boldsymbol{w}}) &= \\underset{\\boldsymbol{\\beta},\\boldsymbol{w}}{\\operatorname{argmin}} &= \\sum\\limits_{=1}^n \\ \\left(y_i - f_{\\boldsymbol{\\beta}, \\boldsymbol{w}}\\left(\\textbf{x}_i\\right) \\right)^2 + \\lambda_1 \\cdot \\sum\\limits_{j} \\beta_j^2 + \\lambda_2 \\sum\\limits_{l,j} w_{l,j}^2\n\\tag{17.36}\n\\end{align}\\]Trong mô hình mạng nơ-ron phân loại có hai lớp ẩn trong phần 17.2, hàm mục tiêu tính bằng cross-entropy cũng được biến đổi bằng cách thêm vào một hàm phạt dạng tổng bình phương của các tham số:Người xây dựng mô hình có thể sử dụng các hàm phạt khác như hàm tổng giá trị tuyệt đối trong hồi quy Lasso, hoặc kết hợp giữa Lasso và ridge. Một lưu ý khác khi sử dụng ràng buộc tham số đó là người xây dựng mô hình thường không sử dụng ràng buộc tham số trực tiếp tính lớp đầu ra, nghĩa là thường cho \\(\\lambda_1\\) trong các phương trình (17.36) và (18.2) bằng 0. Với mỗi giá trị của các tham số \\(\\lambda\\), chúng ta tiến hành ước lượng tham số của mạng nơ-ron giống như đã trình bày trong phần 17.3.Một phương pháp khác để giảm bớt hiện tượng khớp quá mức của mạng nơ-ron là phương pháp loại bỏ đơn vị (unit dropout). Các đơn vị của lớp đầu vào và các lớp ẩn có thể tham gia vào mô hình mạng nơ-ron với xác suất là \\(p\\) và không tham gia vào mô hình với xác suất \\((1-p)\\) trong mỗi bước của quá trình huấn luyện mô hình, trong đó \\(p\\) là tham số của kỹ thuật dropout. Ý tưởng của phương pháp này hoàn toàn tương tự như ý tưởng của thuật toán rừng ngẫu nhiên áp dụng trên cây quyết định. Quá trình ước lượng tham số của mô hình mạng nơ-ron bao gồm hai quá trình: quá trình chuyển tiếp từ đầu vào, qua các lớp ẩn, và kết thúc tại lớp đầu ra, và quá trình lan truyền ngược, khi sai số tính tại lớp đầu ra được sử dụng để tính toán sự thay đổi cho tất cả các tham số của mô hình hiện tại. Nếu trong tất cả các lần chuyển tiếp và lan truyền ngược, chúng ta giữ nguyên cấu trúc của mạng, trong mỗi lớp đầu vào hoặc lớp ẩn có thể có một hoặc một số đơn vị chiếm ưu thế với các đơn vị khác, làm cho kết quả tại đầu ra phụ thuộc rất lớn vào các đơn vị này. Cũng giống như ý tưởng của thuật toán rừng ngẫu nhiên, trong mỗi lần chuyển tiếp và lan truyền ngược tương ứng, tại lớp đầu vào và các lớp ẩn, người xây dựng mô hình chỉ lựa chọn một số ngẫu nhiên các đơn vị vào trong quá trình tính toán tham số. Nói cách khác, mỗi đơn vị được lựa chọn vào quá trình tính toán với xác suất là \\(p\\). Chúng tôi sẽ mô tả cách thực hiện kỹ thuật dropout trong phần thực hành.Hàng thứ hai trong Bảng 10.1 được dán nhãn dropout. Đây là một dạng chính quy hóa tương đối mới bị loại bỏ và hiệu quả, tương tự ở một số khía cạnh với chính quy hóa đường gờ. Lấy cảm hứng từ các khu rừng ngẫu nhiên (Phần 8.2), ý tưởng là loại bỏ ngẫu nhiên một phần φ của các đơn vị trong một lớp khi tạo mô hình. Hình 10.19 minh họa điều này. Việc này được thực hiện riêng biệt mỗi khi xử lý quan sát đào tạo. Các đơn vị còn sống thay thế cho những đơn vị bị thiếu và trọng số của chúng được tăng lên theo hệ số 1/(1 − φ) để bù đắp. Điều này ngăn các nút trở nên chuyên biệt hóa quá mức và có thể được coi là một hình thức chính quy hóa. Trong thực tế, việc bỏ học đạt được bằng cách đặt ngẫu nhiên các kích hoạt cho các đơn vị “bị bỏ” về 0, trong khi vẫn giữ nguyên kiến trúc.","code":""},{"path":"neuralnetwork.html","id":"pickands-dependent-function","chapter":"Chương 17 Mô hình mạng nơ-ron","heading":"17.4 Pickands dependent function","text":"\\[\\begin{align}\nC(u,v) = exp\\left( log(uv) \\cdot \\left(\\cfrac{log(v)}{log(u)+log(v)} \\right) \\right)\n\\end{align}\\]Đạo hàm của \\(C(u,v)\\) tính theo \\(\\)\n\\[\\begin{align}\n\\cfrac{\\partial C(u,v)}{\\partial u} &= C(u,v) \\times \\cfrac{1}{u} \\cdot \\left((t) - ^{'}(t) \\cdot t \\right) \\\\\n\\cfrac{\\partial C(u,v)}{\\partial v} &= C(u,v) \\times \\cfrac{1}{v} \\cdot \\left((t) + ^{'}(t) \\cdot (1-t) \\right) \\\\\n\\end{align}\\]\n\\(t = \\cfrac{log(v)}{log(u)+log(v)}\\)Density của \\(C(u,v)\\)\n\\[\\begin{align}\n\\cfrac{\\partial^2 C(u,v)}{\\partial u \\partial v}  &= C(u,v) \\times \\cfrac{1}{uv} \\cdot \\left[ \\left((t) - ^{'}(t) \\cdot t \\right) \\cdot \\left((t) + ^{'}(t) \\cdot (1-t) \\right) - \\cfrac{^{''}(t) t(1-t)}{log(uv)} \\right]\\\\\n\\end{align}\\]","code":""},{"path":"neuralnetwork.html","id":"nhóm-các-hàm-at-để-cuv-là-một-copula","chapter":"Chương 17 Mô hình mạng nơ-ron","heading":"17.5 Nhóm các hàm A(t) để C(u,v) là một copula","text":"Các điều kiện biên: \\(C(0,v) = 0\\), \\(C(u,v) = 0\\), \\(C(1,v) = v\\), và \\(C(u,1) = u\\)\nKhi \\(u \\rightarrow 0\\) thì \\(t \\rightarrow 0\\) và khi \\(v \\rightarrow 0\\) thì \\(t \\rightarrow 1\\)\nCác điều kiện biên: \\(C(0,v) = 0\\), \\(C(u,v) = 0\\), \\(C(1,v) = v\\), và \\(C(u,1) = u\\)Khi \\(u \\rightarrow 0\\) thì \\(t \\rightarrow 0\\) và khi \\(v \\rightarrow 0\\) thì \\(t \\rightarrow 1\\)\\[\\begin{align}\nC(0,v) = exp\\left( log(0 \\cdot v) \\cdot \\left(0 \\right) \\right) = 0^1 = 0\n\\end{align}\\]\\[\\begin{align}\nC(u,0) = exp\\left( log(u \\cdot 0) \\cdot \\left(0 \\right) \\right) = 0^1 = 0\n\\end{align}\\]Khi \\(u \\rightarrow 1\\) thì \\(t \\rightarrow 1\\) và khi \\(v \\rightarrow 1\\) thì \\(t \\rightarrow 0\\)\\[\\begin{align}\nC(1,v) = exp\\left( log(1 \\cdot v) \\cdot \\left(1 \\right) \\right) = v^1 = v\n\\end{align}\\]\\[\\begin{align}\nC(u,1) = exp\\left( log(u \\cdot 1) \\cdot \\left(0 \\right) \\right) = u^1 = u\n\\end{align}\\]Các hàm \\(C_u(u,v)\\) và \\(C_v(u,v)\\) là hàm phân phối xác suất: \\(C_u(u,v)\\) là hàm phân phối xác suất của biến \\(V\\) với mọi \\(u\\): Khi \\(v \\rightarrow 0\\),\\[\\begin{align}\nC(u,v) \\times \\cfrac{1}{u} \\cdot \\left((t) - ^{'}(t) \\cdot t \\right) &\\rightarrow C(u,0) \\times \\cfrac{1}{u} \\cdot \\left((1) - ^{'}(1) \\cdot 1 \\right)\\\\\n& = 0 \\times \\cfrac{1}{u} \\cdot \\left((1) - ^{'}(1) \\right) \\\\\n& = 0\n\\end{align}\\]Khi \\(v \\rightarrow 1\\),\\[\\begin{align}\nC(u,v) \\times \\cfrac{1}{u} \\cdot \\left((t) - ^{'}(t) \\cdot t \\right) &\\rightarrow C(u,1) \\times \\cfrac{1}{u} \\cdot \\left((0) - ^{'}(0) \\cdot 0 \\right)\\\\\n& = 1 \\times 1 \\\\\n& = 1\n\\end{align}\\]Đạo hàm của hàm \\(C_u(u,v)\\) theo \\(v\\) là hàm tăng theo \\(v\\): Nếu \\(^{''}(t) \\geq 0\\) \\(\\forall t\\) thì\\[\\begin{align}\nC(u,v) \\times \\cfrac{1}{uv} \\cdot \\left[ \\left((t) - ^{'}(t) \\cdot t \\right) \\cdot \\left((t) + ^{'}(t) \\cdot (1-t) \\right) - \\cfrac{^{''}(t) t(1-t)}{log(uv)} \\right] \\geq 0\n\\end{align}\\]\nvới mọi \\(u,v\\)","code":""},{"path":"neuralnetwork.html","id":"cách-thứ-nhất-để-tham-số-hóa-đa-thức-từng-phần","chapter":"Chương 17 Mô hình mạng nơ-ron","heading":"17.5.1 Cách thứ nhất để tham số hóa đa thức từng phần","text":"Hàm \\(f\\) là đa thức từng phần thỏa mãn điều kiện thành pickand dependent function\n\\[\\begin{align}\n\\theta \\[0.5,1] \\\\\n\\lambda_1 \\geq \\theta \\\\\n\\lambda_2 = \\cfrac{ \\left(\\cfrac{\\theta^3}{6}+\\lambda_1 \\cdot \\cfrac{\\theta^2}{2}\\right) \\cdot \\theta - \\left(\\theta+\\lambda_1\\right) \\cdot \\left(\\cfrac{\\theta^3}{6}-\\cfrac{\\theta}{2}+\\cfrac{1}{3}\\right)  }{\\cfrac{(1-\\theta)^2\\cdot(\\theta+\\lambda_1)}{2} - \\left(\\cfrac{\\theta^3}{6}+\\lambda_1 \\cdot \\cfrac{\\theta^2}{2}\\right)}\n\\end{align}\\]","code":""},{"path":"neuralnetwork.html","id":"cách-thứ-hai","chapter":"Chương 17 Mô hình mạng nơ-ron","heading":"17.5.2 Cách thứ hai","text":"Chọn điểm cắt \\(\\theta\\), lựa chọn hàm \\((t)\\) như sau\n\\[\\begin{align}\n(t) = \\mathbb{}_{(t \\leq \\theta)} \\left(a_0 + a_1 \\cdot x + a_2 \\cdot x^2 + a_3 \\cdot x^3\\right) +\n\\mathbb{}_{(t > \\theta)} \\left(b_0 + b_1 \\cdot (1-x) + b_2 \\cdot (1-x)^2 + b_3 \\cdot (1-x)^3\\right)\n\\end{align}\\]\nvới các ràng buộc:Hàm \\((t)\\) có đạo hàm cấp 0,1,2 liên tục tại \\(\\theta\\)\\[\\begin{align}\n& \\begin{pmatrix}\n(1-\\theta) & (1-\\theta)^2 & (1-\\theta)^3 \\\\\n- 1 & - 2\\cdot(1 - \\theta) & -3(1-\\theta)^2 \\\\\n0 & 1 & 3(1-\\theta)\\\\\n\\end{pmatrix} \\times\n\\begin{pmatrix}\nb_1\\\\\nb_2\\\\\nb_3\\\\\n\\end{pmatrix} =\n\\begin{pmatrix}\n\\theta & \\theta^2 & \\theta^3 \\\\\n1 & 2\\theta & 3\\theta^2 \\\\\n0 & 1 & 3\\theta\\\\\n\\end{pmatrix} \\times\n\\begin{pmatrix}\na_1\\\\\na_2\\\\\na_3\\\\\n\\end{pmatrix} \\\\\n& \\\\\n\\rightarrow &\n\\begin{pmatrix}\n(1-\\theta) & (1-\\theta)^2 & (1-\\theta)^3 \\\\\n- 1 & - 2\\cdot(1 - \\theta) & -3(1-\\theta)^2 \\\\\n0 & 1 & 3(1-\\theta)\\\\\n\\end{pmatrix}^{-1}\n\\times \\begin{pmatrix}\n\\theta & \\theta^2 & \\theta^3 \\\\\n1 & 2\\theta & 3\\theta^2 \\\\\n0 & 1 & 3\\theta\\\\\n\\end{pmatrix} \\times\n\\begin{pmatrix}\na_1\\\\\na_2\\\\\na_3\\\\\n\\end{pmatrix} = \\begin{pmatrix}\nb_1\\\\\nb_2\\\\\nb_3\\\\\n\\end{pmatrix}\n& \\\\\n\\rightarrow &\n\\begin{pmatrix}\n\\cfrac{3}{1-\\theta} & 2 & 1 - \\theta \\\\\n- \\cfrac{3}{(1-\\theta)^2} & - \\cfrac{3}{1-\\theta} & -2 \\\\\n\\cfrac{1}{(1-\\theta)^3} & \\cfrac{1}{(1-\\theta)^2} & \\cfrac{1}{(1-\\theta)}\\\\\n\\end{pmatrix}\n\\times \\begin{pmatrix}\n\\theta & \\theta^2 & \\theta^3 \\\\\n1 & 2\\theta & 3\\theta^2 \\\\\n0 & 1 & 3\\theta\\\\\n\\end{pmatrix} \\times\n\\begin{pmatrix}\na_1\\\\\na_2\\\\\na_3\\\\\n\\end{pmatrix} = \\begin{pmatrix}\nb_1\\\\\nb_2\\\\\nb_3\\\\\n\\end{pmatrix}\n& \\\\\n\\rightarrow &\n\\begin{pmatrix}\n\\cfrac{2+\\theta}{1-\\theta} & \\cfrac{2\\theta+1}{1-\\theta} & \\cfrac{3\\theta}{1 - \\theta} \\\\\n- \\cfrac{3}{(1-\\theta)^2} & \\cfrac{\\theta^2-2\\theta-2}{(1-\\theta)^2} & \\cfrac{3\\theta^2-6\\theta}{(1-\\theta)^2} \\\\\n\\cfrac{1}{(1-\\theta)^3} & \\cfrac{1}{(1-\\theta)^3} & \\cfrac{1 - (1-\\theta)^3}{(1-\\theta)^3}\\\\\n\\end{pmatrix}\n\\times\n\\begin{pmatrix}\na_1\\\\\na_2\\\\\na_3\\\\\n\\end{pmatrix} = \\begin{pmatrix}\nb_1\\\\\nb_2\\\\\nb_3\\\\\n\\end{pmatrix}\n\n\n\\end{align}\\]","code":""},{"path":"neuralnetwork.html","id":"thực-hành-1","chapter":"Chương 17 Mô hình mạng nơ-ron","heading":"17.6 Thực hành:","text":"","code":""},{"path":"neuralnetwork.html","id":"mô-hình-mạng-nơ-ron-trên-dữ-liệu-boston","chapter":"Chương 17 Mô hình mạng nơ-ron","heading":"17.6.1 Mô hình mạng nơ-ron trên dữ liệu Boston","text":"","code":""},{"path":"neuralnetwork.html","id":"mô-hình-mạng-nơ-ron-để-phân-loại-khách-hàng","chapter":"Chương 17 Mô hình mạng nơ-ron","heading":"17.6.2 Mô hình mạng nơ-ron để phân loại khách hàng","text":"","code":""},{"path":"neuralnetwork.html","id":"phụ-lục-6","chapter":"Chương 17 Mô hình mạng nơ-ron","heading":"17.7 Phụ lục","text":"","code":""},{"path":"neuralnetwork.html","id":"bài-tập-4","chapter":"Chương 17 Mô hình mạng nơ-ron","heading":"17.8 Bài tập","text":"","code":"## \n## Attaching package: 'dplyr'## The following objects are masked from 'package:stats':\n## \n##     filter, lag## The following objects are masked from 'package:base':\n## \n##     intersect, setdiff, setequal, union## \n## Attaching package: 'kableExtra'## The following object is masked from 'package:dplyr':\n## \n##     group_rows## \n## Attaching package: 'gridExtra'## The following object is masked from 'package:dplyr':\n## \n##     combine## \n## Attaching package: 'pryr'## The following object is masked from 'package:dplyr':\n## \n##     where## Loading required package: shape"},{"path":"neuralnetwork1.html","id":"neuralnetwork1","chapter":"Chương 18 Các mạng học sâu điển hình","heading":"Chương 18 Các mạng học sâu điển hình","text":"Mô hình mạng nơ-ron có nhiều lớp ẩn và trong mỗi lớp ẩn có nhiều đơn vị thường được gọi với tên gọi khác là các mạng học sâu (deep neural network). Như chúng ta đã thấy trong phần thực hành của chương trước, mạng học sâu không có ưu thế về khả năng diễn giải hoặc dự đoán với các mô hình học máy khác khi dữ liệu dùng để xây dựng mô hình là dữ liệu nhỏ và ở dạng quan sát - thuộc tính chẳng hạn như dữ liệu Boston. Ngoài ra, đặc điểm của mạng học sâu là mô hình cho phép sử dụng số lượng tham số rất lớn để mô tả dữ liệu. Đây vừa là nhược điểm cũng vừa là ưu điểm của mô hình mạng học sâu. Sử dụng nhiều tham số rất dễ dẫn đến hiện tượng overfitting đặc biệt là khi dữ liệu xây dựng mô hình nhỏ. Ngược lại, khi gặp dữ liệu lớn và phức tạp, cần có các mô hình có lượng tham số đủ lớn để mô tả sự phức tạp đó, điều này giải thích tại sao các mạng học sâu lại vượt trội hơn các mô hình học máy khác khi mô tả dữ liệu phức tạp như ảnh, video, âm thanh hay dữ liệu kiểu văn bản.Khó khăn lớn nhất khi xây dựng mô hình mạng học sâu là ước lượng các tham số của mô hình. Như chúng ta đã thấy ở phần","code":""},{"path":"neuralnetwork1.html","id":"mạng-nơ-ron-tích-chập","chapter":"Chương 18 Các mạng học sâu điển hình","heading":"18.1 Mạng nơ-ron tích chập","text":"Mô hình mạng nơ-ron quay trở lại thành công vào những năm 2010 gắn liền với những thành công lớn trong các mô hình phân loại hình ảnh. Vào khoảng thời gian này, với sự bùng nổ của điện thoại thông minh và kết nối internet, các bộ dữ liệu khổng lồ về hình ảnh được thu thập và gắn nhãn nhằm phục vụ cho xây dựng các thuật toán phân loại hình ảnh. Mộ ví dụ về một cơ sở dữ liệu như vậy là dữ liệu CIFAR100 - dữ liệu bao gồm 60 nghìn hình ảnh về 100 đối tượng hình ảnh khác nhau được mã hóa bằng một số tự nhiên từ 0 đến 99.\nHình 18.1: Năm mươi bức ảnh đại diện cho 10 nhóm được mã hóa bằng các số từ 0 đến 9. Mỗi cột tương ứng với một đối tượng. Cột đầu tiên là các bức ảnh về quả táo (apple) được mã hóa thành số 0, cột thứ hai là các bức ảnh về cá cảnh (aquarium fish) tương ứng với số 1, …, và cột thứ mười tương ứng với các bức ảnh của chai lọ (bottle) được mã hóa bằng số 9.\nKhác với hình ảnh trong dữ liệu MNIST, ảnh trong dữ liệu CIFAR100 là ảnh màu. Đối tượng dùng để lưu trữ một bức ảnh màu là một mảng 3 chiều thay vì một ma trận khi lưu trữ ảnh đen trắng. Kích thước của mỗi bức ảnh trong dữ liệu CIFAR100 là \\(32 \\times 32 \\times 3\\), nghĩa là độ phân giải của mỗi bức ảnh là \\(32 \\times 32\\) pixel, và chiều thứ ba cho biết màu sắc của các điểm ảnh được kết hợp từ ba màu đỏ, xanh lục và xanh lam như thế nào.Một kỹ thuật xây dựng mạng nơ-ron được gọi là mạng nơ-ron tích chập (Convolution Neural Network hay CNN) đã được phát triển với mục đích phân loại các hình ảnh như trong dữ liệu CIFAR100 và đã cho kết quả thành công vượt trội với các phương pháp trước đây. Mạng CNN phỏng theo cách con người phân loại hình ảnh bằng cách nhận dạng các đặc điểm hoặc đặc trưng cụ thể nằm ở bất kỳ điểm nào trong hình ảnh để phân biệt các đối tượng cụ thể. Hình 10.6 minh họa ý tưởng xây dựng mạng nơ-ron tích chập để xác định hình ảnh một …………Nguyên tắc xây dựng mạng nơ-ron tích chập là sử dụng các lớp để xác định các đặc điểm nhỏ trong hình ảnh đầu vào, chẳng hạn như các mảng màu sắc, các hình dạng nhỏ, các cạnh nhỏ, và những thứ tương tự. Các đặc điểm nhỏ này sau đó được kết hợp để tạo thành các đặc điểm lớn hơn, chẳng hạn như các bộ phận của mắt, mũi, tai trong hình … Cuối cùng, mạng nơ-ron kết hợp các đặc điểm lớn lại để tính toán xác suất các đặc điểm này xuất hiện đồng thời trên một đối tượng cụ thể. Nguyên tắc xây dựng mạng nơ-ron tích chập cho phân loại ảnh là sử dụng các lớp tích chập để tìm kiếm các đặc điểm nhỏ trong hình ảnh sau đó sử dụng các lớp tổng hợp để tìm kiếm một tập hợp các đặc điểm nổi bật của ảnh. Để đạt được kết quả vượt trội, kiến trúc mạng nơ-ron cần sử dụng nhiều lớp tích chập và lớp gộp. Cách xây dựng các lớp này được trình bày trong phần tiếp theo của chương sách.","code":""},{"path":"neuralnetwork1.html","id":"lớp-tích-chập-trong-mạng-cnn","chapter":"Chương 18 Các mạng học sâu điển hình","heading":"18.1.1 Lớp tích chập trong mạng CNN","text":"Một lớp tích chập được tạo thành từ một số lượng lớn các bộ lọc mà mỗi bộ lọc có vai trò để xác định xem một đặc điểm cụ thể có xuất hiện trong một hình ảnh hay không. Bộ lọc tích chập dựa trên một thao tác rất đơn giản đó là nhân liên tục các phần tử ma trận rồi cộng các kết quả. Để hiểu cách hoạt động của bộ lọc, bạn đọc có thể xem xét một ví dụ đơn giản về hình ảnh có kích thước 4 × 3:\\[\\begin{align}\n\\text{Ảnh } =  \n\n\\begin{bmatrix}\na_{11} & a_{12} & a_{13} \\\\\na_{21} & a_{22} & a_{23} \\\\\na_{31} & a_{32} & a_{33} \\\\\na_{41} & a_{42} & a_{43} \\\\\n\\end{bmatrix}\n\n\\end{align}\\]Một bộ lọc tích chập kích thước 2 × 2 tương ứng với một ma trận có kích thước tương ứng:\n\\[\\begin{align}\n\\text{Bộ lọc tích chập } =  \n\n\\begin{bmatrix}\n\\alpha & \\beta \\\\\n\\gamma & \\delta \\\\\n\\end{bmatrix}\n\\end{align}\\]Bức ảnh sau khi được tích chập với bộ lọc sẽ là một ma trận kích thước \\(3 \\times 3\\) được tính toán như sau\n\\[\\begin{align}\n\\text{Ảnh được tích chập} =  \n\n\\begin{bmatrix}\n\\alpha \\cdot a_{11} + \\beta \\cdot a_{12} + \\gamma \\cdot a_{21} + \\delta \\cdot a_{22} & \\alpha \\cdot a_{12} + \\beta \\cdot a_{13} + \\gamma \\cdot a_{22} + \\delta \\cdot a_{23} \\\\\n\\alpha \\cdot a_{21} + \\beta \\cdot a_{22} + \\gamma \\cdot a_{31} + \\delta \\cdot a_{32} & \\alpha \\cdot a_{22} + \\beta \\cdot a_{23} + \\gamma \\cdot a_{32} + \\delta \\cdot a_{33} \\\\\n\\alpha \\cdot a_{31} + \\beta \\cdot a_{32} + \\gamma \\cdot a_{41} + \\delta \\cdot a_{42} & \\alpha \\cdot a_{32} + \\beta \\cdot a_{33} + \\gamma \\cdot a_{42} + \\delta \\cdot a_{43} \\\\\n\\end{bmatrix}\n\\end{align}\\]Bức ảnh được tính chập là một ma trận kích thước \\(3 \\times 2\\) với phần tử trên cùng bên trái thu được từ việc nhân từng phần tử trong ma trận bộ lọc với phần tử tương ứng ở ma trận con kích thước \\(2 \\times 2\\) trên cùng bên trái của hình ảnh ban đầu và sau đó cộng các kết quả lại. Các phần tử khác thu được theo cách tương tự: bộ lọc tích chập được áp dụng cho mọi ma trận con kích thước \\(2 \\times 2\\) của ảnh gốc để thu được ảnh tích chập. Nếu ma trận con 2 × 2 của ảnh ban đầu giống với bộ lọc thì phép tích chập sẽ dẫn đến giá trị lớn trong kết quả, ngược lại, nếu ma trận con không giống như bộ lọc, giá trị trong kết quả sẽ nhỏ. Chính vì lý này, phép tích chập ảnh sẽ làm nổi bật các vùng của hình ảnh gốc giống với bộ lọc tích chập. Các bộ lọc tích chập nói chung là các mảng nhỏ có kích thước \\(k_1 \\times k_2\\), với \\(k_1\\) và \\(k_2\\) là các số nguyên dương nhỏ không nhất thiết phải bằng nhau. Bức ảnh ban đầu có kích thước \\(n_1 \\times n_2\\) thì ảnh tích chập sẽ có kích thước tương ứng là \\((n_1 - k_1 + 1) \\times (n_2 - k_2 + 1)\\).\nHình 18.2: Bộ lọc tích chập làm nổi bật các cạnh có hình ảnh giống với bộ lọc. Hình phía trên bên trái: Hình ảnh ban đầu. Hình phía trên bên phải: Bộ lọc là ma trận kích thước 15 * 15 có tất cả các giá trị bằng không ngoại trừ hàng thứ bảy. Hình phía dưới bên trái: Bộ lọc là ma trận kích thước 15 * 15 có tất cả các giá trị bằng không ngoại trừ cột thứ bảy. Hình phía dưới bên phải: Bộ lọc là ma trận kích thước 15 * 15 có tất cả các giá trị bằng không ngoại trừ đường chéo chính.\nHình ?? minh họa ứng dụng của bộ lọc tích chập cho một hình ảnh bao gồm các đường kẻ ngang, kẻ dọc và các đường chéo được hiển thị ở góc phía trên bên trái. Mỗi bộ lọc tích chập là một ma trận \\(15 \\times 15\\) chứa tất cả giá trị bằng 0 (màu đen) và một dải các số 1 (màu trắng) được định hướng theo chiều dọc, chiều ngang, hoặc đường chéo trong ảnh.Hình phía trên bên phải có ma trận bộ lọc có dải số 1 nằm ở cột thứ bảy trong khi tất cả các giá trị còn lại là số 0. Bạn đọc có thể nhận thấy rằng hình ảnh sau khi lọc có các đường nằm ngang và đường chéo đã bị làm mờ trong khi các đường thẳng (giống với hình ảnh bộ lọc) vẫn giữ nguyên được độ nét.Hình phía trên bên phải có ma trận bộ lọc có dải số 1 nằm ở cột thứ bảy trong khi tất cả các giá trị còn lại là số 0. Bạn đọc có thể nhận thấy rằng hình ảnh sau khi lọc có các đường nằm ngang và đường chéo đã bị làm mờ trong khi các đường thẳng (giống với hình ảnh bộ lọc) vẫn giữ nguyên được độ nét.Hình phía dưới bên trái có ma trận bộ lọc có dải số 1 nằm ở hàng thứ bảy trong khi tất cả các giá trị còn lại là số 0. Bạn đọc có thể nhận thấy rằng hình ảnh sau khi lọc có các đường dọc và đường chéo đã bị làm mờ trong khi các đường ngang vẫn giữ nguyên được độ nét.Hình phía dưới bên trái có ma trận bộ lọc có dải số 1 nằm ở hàng thứ bảy trong khi tất cả các giá trị còn lại là số 0. Bạn đọc có thể nhận thấy rằng hình ảnh sau khi lọc có các đường dọc và đường chéo đã bị làm mờ trong khi các đường ngang vẫn giữ nguyên được độ nét.Sau cùng, hình phía dưới bên phải có ma trận bộ lọc có dải số 1 nằm ở đường chéo chính của ma trận trong khi tất cả các giá trị còn lại là số 0. Bạn đọc có thể nhận thấy rằng hình ảnh sau khi lọc có các đường chéo chính và các đường song song với đường chéo chính vẫn giữ nguyên được độ nét trong khi tất cả các đường khác đều đã bị làm mờ.Sau cùng, hình phía dưới bên phải có ma trận bộ lọc có dải số 1 nằm ở đường chéo chính của ma trận trong khi tất cả các giá trị còn lại là số 0. Bạn đọc có thể nhận thấy rằng hình ảnh sau khi lọc có các đường chéo chính và các đường song song với đường chéo chính vẫn giữ nguyên được độ nét trong khi tất cả các đường khác đều đã bị làm mờ.Trong mạng CNN, chúng ta có thể coi các hệ số của ma trận lọc là các tham số đi từ lớp đầu vào đến lớp ẩn, với một đơn vị ẩn tương ứng với một pixel trong ảnh tích chập. Tuy nhiên, không giống như một mô hình mạng neural network thông thường, không phải tất cả các đơn vị trong lớp đầu vào đều được kết nối đến tất cả các đơn vị trong lớp ẩn.Kích thước của bộ lọc thường được lựa chọn để phù hợp với kích thước của ảnh. Các bức ảnh trong hình ?? có kích thước \\(500 \\times 500\\) và bộ lọc có kích thước là \\(15 \\times 15\\). Đối với dữ liệu CIFAR100, các bức ảnh có kích thước \\(32 \\times 32\\) đó bộ lọc tích chập có kích thước \\(3 \\times 3\\) là phù hợp. Một lưu ý đó là các bức ảnh trong dữ liệu CIFAR100 là bức ảnh màu được biểu thị qua ba kênh (channel) thay vì một kênh duy nhất như các bức ảnh đen trắng. Mỗi kênh của bức ảnh màu là một ma trận kích thước \\(32 \\times 32\\) đại diện cho một màu đỏ, xanh lá cây, hoặc xanh lam.Để tương thích với ba kênh của một bức ảnh màu, bộ lọc tích chập đối với những bức ảnh này cũng sẽ có ba kênh, mỗi kênh có kích thước \\(3 \\times 3\\), với các hệ số của ma trận tích chập trong mỗi kênh có thể khác nhau. Kết quả của ba phép tích chập là ba ma trận có cùng kích thước sẽ được cộng lại lấy tổng để tạo thành một ma trận duy nhất. Nói cách khác, nếu chúng ta sử dụng \\(K\\) bộ lọc, mỗi bộ lọc bao gồm ba kênh mà mỗi kênh là một ma trận kích thước \\(3 \\times 3\\), chúng ta sẽ có lớp ẩn thứ nhất có \\(K\\) kênh mà mỗi kênh là một ma trận hai chiều.Hàm kích hoạt thường được sử dụng trong mạng CNN là hàm ReLU. Trong nhiều trường hợp việc áp dụng hàm ReLU còn được coi như một lớp riêng biệt trong mạng nơ-ron tích chập. Chúng ta sẽ thảo luận về vấn đề này trong phần thực hành.Hiệu quả của deep learning trong chương này khá ấn tượng. Nó đã giải quyết được vấn đề phân loại chữ số và các CNN sâu đã thực sự cách mạng hóa việc phân loại hình ảnh. Chúng tôi xem báo cáo hàng ngày về những câu chuyện thành công mới về học sâu. Nhiều trong số này liên quan đến các nhiệm vụ phân loại hình ảnh, chẳng hạn như chẩn đoán máy chụp quang tuyến vú hoặc hình ảnh X-quang kỹ thuật số, quét mắt nhãn khoa, chú thích quét MRI, v.v. Tương tự như vậy, RNN đã đạt được rất nhiều thành công trong việc dịch ngôn ngữ và giọng nói, dự báo và mô hình hóa tài liệu. Câu hỏi đặt ra sau đó là: chúng ta có nên loại bỏ tất cả các công cụ cũ hơn và sử dụng deep learning cho mọi vấn đề với dữ liệu không? Để giải quyết câu hỏi này, chúng tôi xem lại tập dữ liệu Hitters từ Chương 6. Đây là một bài toán hồi quy, trong đó mục tiêu là dự đoán Mức lương của một cầu thủ bóng chày năm 1987 bằng cách sử dụng số liệu thống kê hiệu suất của anh ta từ năm 1986. Sau khi loại bỏ những cầu thủ thiếu câu trả lời, chúng tôi còn lại 263 người chơi và 19 biến. Chúng tôi chia ngẫu nhiên dữ liệu thành tập huấn luyện gồm 176 người chơi (hai phần ba) và tập thử nghiệm gồm 87 người chơi (một phần ba). Chúng tôi đã sử dụng ba phương pháp để tạo mô hình hồi quy cho những dữ liệu này. • Một mô hình tuyến tính được sử dụng để tính toán dữ liệu huấn luyện và đưa ra dự đoán về dữ liệu thử nghiệm. Mô hình có 20 tham số. • Mô hình tuyến tính tương tự được thực hiện với phép điều chỉnh Lasso. Tham số điều chỉnh được chọn bằng cách xác thực chéo 10 lần trên dữ liệu huấn luyện. Nó đã chọn một mô hình với 12 biến có hệ số khác 0. • Một mạng lưới thần kinh với một lớp ẩn bao gồm 64 đơn vị ReLU được kết nối với dữ liệu. Model này có 1.345 thông số.Chương sách này thảo luận một chủ đề quan trọng có ứng dụng rộng rãi nhất trong lĩnh vực trí tuệ nhân tạo là mô hình mạng học sâu (deep learning). Tại thời điểm nhóm tác giả viết cuốn sách (2023), học sâu là một lĩnh vực nghiên cứu tích cực nhất không chỉ trong khoa học máy tính, công nghệ thông tin mà còn cả trong các lĩnh vực khác như kinh tế, tài chính, y tế, xây dựng,… Nền tảng của mô hình mạng học sâu là mô hình mạng nơ-ron (hay neural network). Mô hình mạng nơ-ron đã được biết đến đến rộng rãi vào cuối những năm 1980 bởi cách vận hành của mô hình mô tả lại cách thức mà hệ thống thần kinh của con người xử lý thông tin. Mặc dù các đặc tính của mô hình mạng nơ-ron được phân tích bởi những nhà toán học và nhà thống kê nhiều thuật toán liên quan đến mô hình này đã được cải thiện với sự ra đời của các phương pháp học máy khác như SVM, rừng ngẫu nhiên, học tăng cường,…, mà mô hình mạng nơ-ron phần nào không được ưa chuộng.Từ những năm 2010, với nhu cầu xử lý các dữ liệu ngày càng phức tạp và sự ra đời của các kiến trúc máy tính lớn, mô hình mạng nơ-ron đã quay trở lại với tên mới là mạng học sâu (deep learning). Mạng học sâu vượt trội hoàn toàn các mô hình học máy thông thường trong phân loại hình ảnh/video và mô hình hóa ngôn ngữ tự nhiên bao gồm dữ liệu kiểu văn bản và giọng nói (natural langugue processing hay NLP). Các nhà khoa học trong lĩnh vực này tin rằng lý chính cho những thành công của mô hình mạng nơ-ron là càng ngày những người xây dựng mô hình càng chú trọng vào xây dựng các bộ dữ liệu khổng lồ để huấn luyện môn hình và cấu trúc của mô hình cho phép nó đáp ứng được với bất kỳ tập kích thước dữ liệu nào.","code":""},{"path":"neuralnetwork1.html","id":"pickands-dependent-function-1","chapter":"Chương 18 Các mạng học sâu điển hình","heading":"18.2 Pickands dependent function","text":"\\[\\begin{align}\nC(u,v) = exp\\left( log(uv) \\cdot \\left(\\cfrac{log(v)}{log(u)+log(v)} \\right) \\right)\n\\end{align}\\]Đạo hàm của \\(C(u,v)\\) tính theo \\(\\)\n\\[\\begin{align}\n\\cfrac{\\partial C(u,v)}{\\partial u} &= C(u,v) \\times \\cfrac{1}{u} \\cdot \\left((t) - ^{'}(t) \\cdot t \\right) \\\\\n\\cfrac{\\partial C(u,v)}{\\partial v} &= C(u,v) \\times \\cfrac{1}{v} \\cdot \\left((t) + ^{'}(t) \\cdot (1-t) \\right) \\\\\n\\end{align}\\]\n\\(t = \\cfrac{log(v)}{log(u)+log(v)}\\)Density của \\(C(u,v)\\)\n\\[\\begin{align}\n\\cfrac{\\partial^2 C(u,v)}{\\partial u \\partial v}  &= C(u,v) \\times \\cfrac{1}{uv} \\cdot \\left[ \\left((t) - ^{'}(t) \\cdot t \\right) \\cdot \\left((t) + ^{'}(t) \\cdot (1-t) \\right) - \\cfrac{^{''}(t) t(1-t)}{log(uv)} \\right]\\\\\n\\end{align}\\]","code":""},{"path":"neuralnetwork1.html","id":"nhóm-các-hàm-at-để-cuv-là-một-copula-1","chapter":"Chương 18 Các mạng học sâu điển hình","heading":"18.3 Nhóm các hàm A(t) để C(u,v) là một copula","text":"Các điều kiện biên: \\(C(0,v) = 0\\), \\(C(u,v) = 0\\), \\(C(1,v) = v\\), và \\(C(u,1) = u\\)\nKhi \\(u \\rightarrow 0\\) thì \\(t \\rightarrow 0\\) và khi \\(v \\rightarrow 0\\) thì \\(t \\rightarrow 1\\)\nCác điều kiện biên: \\(C(0,v) = 0\\), \\(C(u,v) = 0\\), \\(C(1,v) = v\\), và \\(C(u,1) = u\\)Khi \\(u \\rightarrow 0\\) thì \\(t \\rightarrow 0\\) và khi \\(v \\rightarrow 0\\) thì \\(t \\rightarrow 1\\)\\[\\begin{align}\nC(0,v) = exp\\left( log(0 \\cdot v) \\cdot \\left(0 \\right) \\right) = 0^1 = 0\n\\end{align}\\]\\[\\begin{align}\nC(u,0) = exp\\left( log(u \\cdot 0) \\cdot \\left(0 \\right) \\right) = 0^1 = 0\n\\end{align}\\]Khi \\(u \\rightarrow 1\\) thì \\(t \\rightarrow 1\\) và khi \\(v \\rightarrow 1\\) thì \\(t \\rightarrow 0\\)\\[\\begin{align}\nC(1,v) = exp\\left( log(1 \\cdot v) \\cdot \\left(1 \\right) \\right) = v^1 = v\n\\end{align}\\]\\[\\begin{align}\nC(u,1) = exp\\left( log(u \\cdot 1) \\cdot \\left(0 \\right) \\right) = u^1 = u\n\\end{align}\\]Các hàm \\(C_u(u,v)\\) và \\(C_v(u,v)\\) là hàm phân phối xác suất: \\(C_u(u,v)\\) là hàm phân phối xác suất của biến \\(V\\) với mọi \\(u\\): Khi \\(v \\rightarrow 0\\),\\[\\begin{align}\nC(u,v) \\times \\cfrac{1}{u} \\cdot \\left((t) - ^{'}(t) \\cdot t \\right) &\\rightarrow C(u,0) \\times \\cfrac{1}{u} \\cdot \\left((1) - ^{'}(1) \\cdot 1 \\right)\\\\\n& = 0 \\times \\cfrac{1}{u} \\cdot \\left((1) - ^{'}(1) \\right) \\\\\n& = 0\n\\end{align}\\]Khi \\(v \\rightarrow 1\\),\\[\\begin{align}\nC(u,v) \\times \\cfrac{1}{u} \\cdot \\left((t) - ^{'}(t) \\cdot t \\right) &\\rightarrow C(u,1) \\times \\cfrac{1}{u} \\cdot \\left((0) - ^{'}(0) \\cdot 0 \\right)\\\\\n& = 1 \\times 1 \\\\\n& = 1\n\\end{align}\\]Đạo hàm của hàm \\(C_u(u,v)\\) theo \\(v\\) là hàm tăng theo \\(v\\): Nếu \\(^{''}(t) \\geq 0\\) \\(\\forall t\\) thì\n\\[\\begin{align}\nC(u,v) \\times \\cfrac{1}{uv} \\cdot \\left[ \\left((t) - ^{'}(t) \\cdot t \\right) \\cdot \\left((t) + ^{'}(t) \\cdot (1-t) \\right) - \\cfrac{^{''}(t) t(1-t)}{log(uv)} \\right] \\geq 0\n\\end{align}\\]\nvới mọi \\(u,v\\)","code":""},{"path":"neuralnetwork1.html","id":"cách-thứ-nhất-để-tham-số-hóa-đa-thức-từng-phần-1","chapter":"Chương 18 Các mạng học sâu điển hình","heading":"18.3.1 Cách thứ nhất để tham số hóa đa thức từng phần","text":"Hàm \\(f\\) là đa thức từng phần thỏa mãn điều kiện thành pickand dependent function\n\\[\\begin{align}\n\\theta \\[0.5,1] \\\\\n\\lambda_1 \\geq \\theta \\\\\n\\lambda_2 = \\cfrac{ \\left(\\cfrac{\\theta^3}{6}+\\lambda_1 \\cdot \\cfrac{\\theta^2}{2}\\right) \\cdot \\theta - \\left(\\theta+\\lambda_1\\right) \\cdot \\left(\\cfrac{\\theta^3}{6}-\\cfrac{\\theta}{2}+\\cfrac{1}{3}\\right)  }{\\cfrac{(1-\\theta)^2\\cdot(\\theta+\\lambda_1)}{2} - \\left(\\cfrac{\\theta^3}{6}+\\lambda_1 \\cdot \\cfrac{\\theta^2}{2}\\right)}\n\\end{align}\\]","code":""},{"path":"neuralnetwork1.html","id":"cách-thứ-hai-1","chapter":"Chương 18 Các mạng học sâu điển hình","heading":"18.3.2 Cách thứ hai","text":"Chọn điểm cắt \\(\\theta\\), lựa chọn hàm \\((t)\\) như sau\n\\[\\begin{align}\n(t) = \\mathbb{}_{(t \\leq \\theta)} \\left(a_0 + a_1 \\cdot x + a_2 \\cdot x^2 + a_3 \\cdot x^3\\right) +\n\\mathbb{}_{(t > \\theta)} \\left(b_0 + b_1 \\cdot (1-x) + b_2 \\cdot (1-x)^2 + b_3 \\cdot (1-x)^3\\right)\n\\end{align}\\]\nvới các ràng buộc:Hàm \\((t)\\) có đạo hàm cấp 0,1,2 liên tục tại \\(\\theta\\)\\[\\begin{align}\n& \\begin{pmatrix}\n(1-\\theta) & (1-\\theta)^2 & (1-\\theta)^3 \\\\\n- 1 & - 2\\cdot(1 - \\theta) & -3(1-\\theta)^2 \\\\\n0 & 1 & 3(1-\\theta)\\\\\n\\end{pmatrix} \\times\n\\begin{pmatrix}\nb_1\\\\\nb_2\\\\\nb_3\\\\\n\\end{pmatrix} =\n\\begin{pmatrix}\n\\theta & \\theta^2 & \\theta^3 \\\\\n1 & 2\\theta & 3\\theta^2 \\\\\n0 & 1 & 3\\theta\\\\\n\\end{pmatrix} \\times\n\\begin{pmatrix}\na_1\\\\\na_2\\\\\na_3\\\\\n\\end{pmatrix} \\\\\n& \\\\\n\\rightarrow &\n\\begin{pmatrix}\n(1-\\theta) & (1-\\theta)^2 & (1-\\theta)^3 \\\\\n- 1 & - 2\\cdot(1 - \\theta) & -3(1-\\theta)^2 \\\\\n0 & 1 & 3(1-\\theta)\\\\\n\\end{pmatrix}^{-1}\n\\times \\begin{pmatrix}\n\\theta & \\theta^2 & \\theta^3 \\\\\n1 & 2\\theta & 3\\theta^2 \\\\\n0 & 1 & 3\\theta\\\\\n\\end{pmatrix} \\times\n\\begin{pmatrix}\na_1\\\\\na_2\\\\\na_3\\\\\n\\end{pmatrix} = \\begin{pmatrix}\nb_1\\\\\nb_2\\\\\nb_3\\\\\n\\end{pmatrix}\n& \\\\\n\\rightarrow &\n\\begin{pmatrix}\n\\cfrac{3}{1-\\theta} & 2 & 1 - \\theta \\\\\n- \\cfrac{3}{(1-\\theta)^2} & - \\cfrac{3}{1-\\theta} & -2 \\\\\n\\cfrac{1}{(1-\\theta)^3} & \\cfrac{1}{(1-\\theta)^2} & \\cfrac{1}{(1-\\theta)}\\\\\n\\end{pmatrix}\n\\times \\begin{pmatrix}\n\\theta & \\theta^2 & \\theta^3 \\\\\n1 & 2\\theta & 3\\theta^2 \\\\\n0 & 1 & 3\\theta\\\\\n\\end{pmatrix} \\times\n\\begin{pmatrix}\na_1\\\\\na_2\\\\\na_3\\\\\n\\end{pmatrix} = \\begin{pmatrix}\nb_1\\\\\nb_2\\\\\nb_3\\\\\n\\end{pmatrix}\n& \\\\\n\\rightarrow &\n\\begin{pmatrix}\n\\cfrac{2+\\theta}{1-\\theta} & \\cfrac{2\\theta+1}{1-\\theta} & \\cfrac{3\\theta}{1 - \\theta} \\\\\n- \\cfrac{3}{(1-\\theta)^2} & \\cfrac{\\theta^2-2\\theta-2}{(1-\\theta)^2} & \\cfrac{3\\theta^2-6\\theta}{(1-\\theta)^2} \\\\\n\\cfrac{1}{(1-\\theta)^3} & \\cfrac{1}{(1-\\theta)^3} & \\cfrac{1 - (1-\\theta)^3}{(1-\\theta)^3}\\\\\n\\end{pmatrix}\n\\times\n\\begin{pmatrix}\na_1\\\\\na_2\\\\\na_3\\\\\n\\end{pmatrix} = \\begin{pmatrix}\nb_1\\\\\nb_2\\\\\nb_3\\\\\n\\end{pmatrix}\n\n\n\\end{align}\\]","code":""},{"path":"neuralnetwork1.html","id":"copula-có-2-kendall-tau","chapter":"Chương 18 Các mạng học sâu điển hình","heading":"18.4 Copula có 2 kendall tau","text":"Khi Kendall tau bằng 0 thì chưa chắc 2 biến đã độc lập, có thể xảy ra trường hợp copula là mix của 2 copula, 1 copula có kendall tau dương và một copula có kendall tau âm. Cách mix các copula thông thường có thể như sau\n\\[\\begin{align}\nc(u,v) = p \\times c_1(u,v) + (1 - p) c_2(u,v)\n\\end{align}\\]\ntrong đó copula \\(C_1\\) là Copula có kendall tau dương và \\(C_2\\) là copula có kendall tau âm. Để ước lượng copula như vậy có thể sử dụng thuật toán EM sau đó dùng ML (thường phải giải bằng phương pháp số chứ không có lời giải chính xác).Cách thứ hai để mix copula như sau\n\\[\\begin{align}\nc(u,v) = \\mathbb{}_{\\{u \\leq p\\}} \\cdot c_1\\left(\\cfrac{u}{p},v \\right) +\n\\mathbb{}_{\\{u > p\\}} \\cdot c_2\\left(\\cfrac{u - p}{1 - p},v \\right)\n\\end{align}\\]","code":""},{"path":"neuralnetwork1.html","id":"hàm-at-là-đa-thức-bậc-k","chapter":"Chương 18 Các mạng học sâu điển hình","heading":"18.5 Hàm A(t) là đa thức bậc k","text":"Nếu (t) chỉ đến bậc 3 thì (t) nhỏ nhất là 0.666 trong khi giá trị cần là 0.5 -> cần sử dụng đa thức bậc cao hơn.Chọn điểm cắt \\(\\theta\\), lựa chọn hàm \\((t)\\) như sau\n\\[\\begin{align}\n(x) = \\mathbb{}_{(x \\leq \\theta)} \\left(a_0 + a_1 \\cdot x + a_2 \\cdot x^2 + \\cdots + a_k \\cdot x^k\\right) +\n\\mathbb{}_{(t > \\theta)} \\left(b_0 + b_1 \\cdot (1-x) + b_2 \\cdot (1-x)^2 + \\cdots +  b_k \\cdot (1-x)^k\\right)\n\\end{align}\\]\\((0) = (1) = 0\\) nên \\(a_0 = b_0 = 1\\)Với các ràng buộc đạo hàm cấp 0, 1, 2 tại \\(\\theta\\) liên tục\\[\\begin{align}\n& \\begin{pmatrix}\n(1-\\theta) & (1-\\theta)^2 &  (1-\\theta)^3 & \\cdots & (1-\\theta)^k  \\\\\n- 1 & - 2\\cdot(1 - \\theta) & -3(1-\\theta)^2 & \\cdots & -k(1-\\theta)^{k-1} \\\\\n0 & 2 & 6(1-\\theta) & \\cdots & k(k-1) (1-\\theta)^{k-2} \\\\\n\\end{pmatrix} \\times\n\\begin{pmatrix}\nb_1\\\\\nb_2\\\\\nb_3\\\\\n\\cdots \\\\\nb_k \\\\\n\\end{pmatrix} =\n\\begin{pmatrix}\n\\theta & \\theta^2 & \\theta^3 & \\cdots & \\theta^k \\\\\n1 & 2\\theta & 3\\theta^2 & \\cdots & k \\theta^{k-1} \\\\\n0 & 2 & 6\\theta & \\cdots & k(k-1) \\theta^{k-2} \\\\\n\\end{pmatrix} \\times\n\\begin{pmatrix}\na_1\\\\\na_2\\\\\na_3\\\\\n\\cdots \\\\\na_k \\\\\n\\end{pmatrix}\n\\end{align}\\]\\(\\cfrac{\\partial C(u,v)}{\\partial u} > 0\\) nên \\((t) - '(t).t > 0\\) với mọi \\(t\\). \\(''(t) > 0\\) nên điều kiện đủ là \\(b_1 < 1\\)\\(\\cfrac{\\partial C(u,v)}{\\partial u} > 0\\) nên \\((t) - '(t).t > 0\\) với mọi \\(t\\). \\(''(t) > 0\\) nên điều kiện đủ là \\(b_1 < 1\\)\\(\\cfrac{\\partial C(u,v)}{\\partial v} > 0\\) nên \\((t) + '(t).(1-t) > 0\\) với mọi \\(t\\). \\(''(t) > 0\\) nên điều kiện đủ là \\(a_1 > -1\\)\\(\\cfrac{\\partial C(u,v)}{\\partial v} > 0\\) nên \\((t) + '(t).(1-t) > 0\\) với mọi \\(t\\). \\(''(t) > 0\\) nên điều kiện đủ là \\(a_1 > -1\\)","code":""},{"path":"neuralnetwork1.html","id":"thực-hành-2","chapter":"Chương 18 Các mạng học sâu điển hình","heading":"18.6 Thực hành:","text":"","code":""},{"path":"neuralnetwork1.html","id":"mô-hình-mạng-nơ-ron-trên-dữ-liệu-boston-1","chapter":"Chương 18 Các mạng học sâu điển hình","heading":"18.6.1 Mô hình mạng nơ-ron trên dữ liệu Boston","text":"","code":""},{"path":"neuralnetwork1.html","id":"mô-hình-mạng-nơ-ron-để-phân-loại-khách-hàng-1","chapter":"Chương 18 Các mạng học sâu điển hình","heading":"18.6.2 Mô hình mạng nơ-ron để phân loại khách hàng","text":"","code":""},{"path":"neuralnetwork1.html","id":"phụ-lục-7","chapter":"Chương 18 Các mạng học sâu điển hình","heading":"18.7 Phụ lục","text":"","code":""},{"path":"neuralnetwork1.html","id":"bài-tập-5","chapter":"Chương 18 Các mạng học sâu điển hình","heading":"18.8 Bài tập","text":"","code":"## \n## Attaching package: 'dplyr'## The following objects are masked from 'package:stats':\n## \n##     filter, lag## The following objects are masked from 'package:base':\n## \n##     intersect, setdiff, setequal, union## \n## Attaching package: 'kableExtra'## The following object is masked from 'package:dplyr':\n## \n##     group_rows## \n## Attaching package: 'gridExtra'## The following object is masked from 'package:dplyr':\n## \n##     combine## \n## Attaching package: 'pryr'## The following object is masked from 'package:dplyr':\n## \n##     where"},{"path":"học-máy-không-có-giám-sát.html","id":"học-máy-không-có-giám-sát","chapter":"Chương 19 Học máy không có giám sát","heading":"Chương 19 Học máy không có giám sát","text":"Chương sách này sẽ tập trung vào học máy không giám sát; một tập hợp các công cụ thống kê được sử dụng cho trường hợp chúng ta có một dữ liệu bao gồm \\(p\\) biến \\(X_1, X_2, \\cdots, X_p\\) và \\(n\\) quan sát. Mục đích của chúng ta là khám phá những giá trị bên trong dữ liệu mà không quan tâm đến có hay không có biến mục tiêu. Các câu hỏi đặt ra bao gồm cóCó cách nào thông tin để trực quan hóa dữ liệu? Chúng ta có thể khám phá các nhóm con giữa các biến hoặc giữa các quan sát không? Học tập không giám sát đề cập đến một tập hợp các kỹ thuật đa dạng để trả lời các câu hỏi như thế này. Trong chương này, chúng ta sẽ tập trung vào hai loại học tập không giám sát cụ thể: phân tích thành phần chính, một công cụ được sử dụng để trực quan hóa dữ liệu hoặc xử lý trước dữ liệu trước khi áp dụng các kỹ thuật có giám sát và phân cụm, một loại phương pháp rộng rãi để khám phá. các nhóm con chưa biết trong dữ liệu.","code":""},{"path":"học-máy-không-có-giám-sát.html","id":"phương-pháp-phân-tích-thành-phần-chính","chapter":"Chương 19 Học máy không có giám sát","heading":"19.1 Phương pháp phân tích thành phần chính","text":"","code":""},{"path":"học-máy-không-có-giám-sát.html","id":"các-phương-pháp-phân-cụm","chapter":"Chương 19 Học máy không có giám sát","heading":"19.2 Các phương pháp phân cụm","text":"","code":""},{"path":"học-máy-không-có-giám-sát.html","id":"k-mean","chapter":"Chương 19 Học máy không có giám sát","heading":"19.2.1 K-mean","text":"","code":""},{"path":"học-máy-không-có-giám-sát.html","id":"hierarchical-clustering","chapter":"Chương 19 Học máy không có giám sát","heading":"19.2.2 Hierarchical Clustering","text":"","code":""},{"path":"học-máy-không-có-giám-sát.html","id":"thực-hành-3","chapter":"Chương 19 Học máy không có giám sát","heading":"19.3 Thực hành:","text":"","code":""}]
