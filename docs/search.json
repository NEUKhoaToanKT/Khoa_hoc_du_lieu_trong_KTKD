[{"path":"index.html","id":"prerequisites","chapter":"Chương 1 Prerequisites","heading":"Chương 1 Prerequisites","text":"sample book written Markdown. can use anything Pandoc’s Markdown supports, e.g., math equation \\(^2 + b^2 = c^2\\).bookdown package can installed CRAN Github:Remember Rmd file contains one one chapter, chapter defined first-level heading #.compile example PDF, need XeLaTeX. recommended install TinyTeX (includes XeLaTeX): https://yihui.name/tinytex/.","code":"\n colorize <- function(x, color) {\n if (knitr::is_latex_output()) {\n sprintf(\"\\\\textcolor{%s}{%s}\", color, x)\n } else if (knitr::is_html_output()) {\n sprintf(\"<span style='color: %s;'>%s<\/span>\", \n color,\n x)\n } else x\n }\ninstall.packages(\"bookdown\")\n# or the development version\n# devtools::install_github(\"rstudio/bookdown\")"},{"path":"giới-thiệu-chung.html","id":"giới-thiệu-chung","chapter":"Chương 2 Giới thiệu chung","heading":"Chương 2 Giới thiệu chung","text":"","code":""},{"path":"giới-thiệu-chung.html","id":"giới-thiệu-về-khoa-học-dữ-liệu-và-các-ứng-dụng","chapter":"Chương 2 Giới thiệu chung","heading":"2.1 Giới thiệu về Khoa học dữ liệu và các ứng dụng","text":"Khoa học dữ liệu kết hợp giữa toán học với khoa học máy tính và một kiến thức chuyên ngành để khám phá những thông tin hữu ích và có giá trị trong dữ liệu để hỗ trợ việc ra quyết định và lập kế hoạch hành động. Một quá trình ứng dụng KHDL vào giải quyết một vấn đề cụ thể thường bao gồm các bước được mô tả như Hình @ref().\nFigure 2.1: Quy trình áp dụng Khoa học dữ liệu để giải quyết một vấn đề thực tế\n\\(\\textbf{Nhập liệu}\\) là quá trình tìm kiếm và thu thập dữ liệu từ các nguồn khác nhau để phục vụ cho mục đích ra quyết định. Có những dự án làm việc trực tiếp trên dữ liệu có sẵn và đã được thiết kế sẵn sàng cho mục tiêu phân tích nhưng cũng có những dự án mà quá trình nhập liệu lại chiếm phần lớn thời gian và quyết định sự thành công hay thất bại của dự án.\\(\\textbf{Nhập liệu}\\) là quá trình tìm kiếm và thu thập dữ liệu từ các nguồn khác nhau để phục vụ cho mục đích ra quyết định. Có những dự án làm việc trực tiếp trên dữ liệu có sẵn và đã được thiết kế sẵn sàng cho mục tiêu phân tích nhưng cũng có những dự án mà quá trình nhập liệu lại chiếm phần lớn thời gian và quyết định sự thành công hay thất bại của dự án.\\(\\textbf{Sắp xếp}\\) dữ liệu hay còn được gọi là tiền xử lý dữ liệu là các bước biến dữ liệu từ dạng thô thành dữ liệu theo đúng như định dạng mong muốn.\\(\\textbf{Sắp xếp}\\) dữ liệu hay còn được gọi là tiền xử lý dữ liệu là các bước biến dữ liệu từ dạng thô thành dữ liệu theo đúng như định dạng mong muốn.\\(\\textbf{Biến đổi}\\) dữ liệu là quá trình tính toán trên các biến hoặc các quan sát của dữ liệu để dữ liệu có thể đưa vào các công cụ trực quan hoặc đưa vào trong các mô hình phân tích.\\(\\textbf{Biến đổi}\\) dữ liệu là quá trình tính toán trên các biến hoặc các quan sát của dữ liệu để dữ liệu có thể đưa vào các công cụ trực quan hoặc đưa vào trong các mô hình phân tích.\\(\\textbf{Xây dựng mô hình}\\) là quá trình tính toán và đánh giá mối liên hệ giữa các biến trong dữ liệu đến các biến mục tiêu là đầu ra của toàn bộ quá trình. Mô hình thường được xây dựng với một trong hai mục đích là xem xét sự tác động của một biến đến các biến mục tiêu, hoặc nhằm mục đích dự đoán giá trị của các biến mục tiêu.\\(\\textbf{Xây dựng mô hình}\\) là quá trình tính toán và đánh giá mối liên hệ giữa các biến trong dữ liệu đến các biến mục tiêu là đầu ra của toàn bộ quá trình. Mô hình thường được xây dựng với một trong hai mục đích là xem xét sự tác động của một biến đến các biến mục tiêu, hoặc nhằm mục đích dự đoán giá trị của các biến mục tiêu.Mô hình trên dữ liệu được xây dựng dựa trên những nguyên lý của toán học và xác suất thống kê. Dữ liệu được sử dụng để xây dựng mô hình có thể là các dữ liệu nhỏ với một vài cột và vài chục quan sát, nhưng cũng có thể là các dữ liệu lớn với hàng nghìn cột dữ liệu và hàng triệu quan sát. Dữ liệu thậm chí không có dạng bảng biểu như chúng ta gặp hàng ngày mà có thể là các hình ảnh, các văn bản, giọng nói, dạng đồ thị, … Để xử lý các bộ dữ liệu khổng lồ, hay các dữ liệu không có cấu trúc bảng thông thường, người xử lý dữ liệu và cần có các kiến thức về khoa học máy tính và lập trình để thực hiện các tính toán trên máy tính điện tử. Những ứng dụng của KHDL có thể thuộc về bất kỳ lĩnh vực nào như kinh doanh, y học, vật lý, thiên văn, quản lý nhà nước, chính sách công, … nên đòi hỏi người xây dựng mô hình cũng cần có kiến thức chuyên môn trong lĩnh vực tương ứng để không bị sai định hướng trong quá trình làm việc với dữ liệu.Để minh họa ứng dụng của KHDL trong lĩnh vực Kinh tế và Kinh doanh, chúng tôi thảo luận ngắn gọn về ba dữ liệu được thu nhập trong thế giới thực được xem xét trong cuốn sách này.","code":""},{"path":"giới-thiệu-chung.html","id":"dữ-liệu-chi-phí-quảng-cáo","chapter":"Chương 2 Giới thiệu chung","heading":"2.1.1 Dữ liệu chi phí quảng cáo","text":"","code":""},{"path":"giới-thiệu-chung.html","id":"dữ-liệu-bảo-hiểm-xe-ô-tô","chapter":"Chương 2 Giới thiệu chung","heading":"2.1.2 Dữ liệu bảo hiểm xe ô tô","text":"","code":""},{"path":"giới-thiệu-chung.html","id":"dữ-liệu-về-giá-nhà","chapter":"Chương 2 Giới thiệu chung","heading":"2.1.3 Dữ liệu về giá nhà","text":"","code":""},{"path":"giới-thiệu-chung.html","id":"sơ-lược-quá-trình-phát-triển-của-xây-dựng-mô-hình-dữ-liệu","chapter":"Chương 2 Giới thiệu chung","heading":"2.2 Sơ lược quá trình phát triển của xây dựng mô hình dữ liệu","text":"Mặc dù thuật ngữ xây dựng mô hình trên dữ liệu, hay được gọi một cách kỹ thuật hơn là học máy, còn khá mới mẻ nhưng những khái niệm nền tảng cho lĩnh vực này đã được phát triển từ lâu. Vào đầu thế kỷ 19, phương pháp bình phương nhỏ nhất đã được phát triển và áp dụng để ước lượng các mô hình hồi quy tuyến tính. Mô hình này lần đầu tiên được áp dụng và cho kết quả thành công trong các vấn đề liên quan đến thiên văn học. Vào đầu thế kỷ thứ 20, mô hình hồi quy tuyến tính được sử dụng để dự đoán các giá trị định lượng, chẳng hạn như mức lương của một cá nhân hoặc để dự đoán các giá trị định tính, chẳng hạn như bệnh nhân sống hay chết, hay thị trường chứng khoán tăng hay giảm. Vào những năm 1940, nhiều tác giả đã đưa ra một cách tiếp cận khác, đó là hồi quy logistic. Vào đầu những năm 1970, thuật ngữ mô hình tuyến tính tổng quát đã được phát triển để mô tả toàn bộ lớp phương pháp học thống kê bao gồm cả hồi quy tuyến tính và hồi quy logistic như các trường hợp đặc biệt. Vào cuối những năm 1970, nhiều kỹ thuật xây dựng mô hình trên dữ liệu đã xuất hiện. Tuy nhiên, các mô hình này chỉ xoay quanh là các phương pháp tuyến tính vì việc tạo ra các mối quan hệ phi tuyến tính rất khó khăn về mặt tính toán.Đến những năm 1980, sự phát triển của máy tính điện tử đã hỗ trợ tích cực về mặt tính toán cho các các phương pháp phi tuyến tính. Các mô hình phi tuyến được giới thiệu vào đầu những năm của thập niên 80 bao gồm có mô hình cây quyết định và mô hình cộng tính tổng quát. Những năm cuối thập niên 80 và đầu thập niên 90, mô hình mạng nơ-ron được giới thiệu đến cộng đồng nghiên cứu nhưng chưa có được nhiều sự quan tâm vì dữ liệu chưa đủ phong phú và sự phổ biến của các mô hình học máy khác.Giai đoạn cuối thế kỷ XX và đầu thế kỷ XXI là giai đoạn chiếm ưu thế hoàn toàn của các mô hình học máy rừng ngẫu nhiên và thuật toán học tăng cường. Thuật toán học tăng cường với các biến thể như XGBoost hay LightGBM chiến thắng trong hầu hết các cuộc thi về Khoa học dữ liệu.Từ năm 2010, với sự bùng nổ của các thiết bị thông minh và kết nối internet, dữ liệu trở nên phong phú và đa dạng hơn cũng là thời điểm quay trở lại của mô hình mạng nơ-ron, hay còn được gọi với tên gọi khác là mô hình mạng học sâu (deep learning). Mô hình mạng học sâu vượt trội hoàn toàn các mô hình học máy thông thường khi làm việc với dữ liệu kiểu hình ảnh, video, ngôn ngữ tự nhiên bao gồm cả văn bản và giọng nói. Sự kiện đánh dấu sự phát triển vượt bậc của các mô hình mạng học sâu là sự ra đời của ChatGPT vào cuối những năm 2022 là một mô hình ngôn ngữ lớn cho phép người dùng tương tác, hỏi đáp và trò chuyện một cách hoàn toàn tự nhiên theo định hướng của người sử dụng như phong cách, mức độ chi tiết, hình thức ngôn ngữ. ChatGPT nhanh chóng đạt đến 100 triệu người sau hơn hai tháng phát hành và giúp cho công ty phát hành OpenAI được định giá khoảng 30 tỷ USD. Cho đến thời điểm cuối năm 2023 khi nhóm tác giả bắt đầu viết cuốn sách này, ChatGPT đã được cập nhật đến phiên bản 3.5.","code":""},{"path":"giới-thiệu-chung.html","id":"tại-sao-lại-sử-dụng-phần-mềm-r","chapter":"Chương 2 Giới thiệu chung","heading":"2.3 Tại sao lại sử dụng phần mềm R","text":"Trong thế giới ngày nay, hầu hết các cơ quan, tổ chức, tập đoàn, doanh nghiệp từ lớn đến nhỏ đều sử dụng một số lượng dữ liệu nhất định để phân tích các sự kiện trong quá khứ và cố gắng dự đoán xu hướng trong tương lai để đưa ra các quyết định có lợi cho mình. Tuy nhiên khi dữ liệu ngày càng tăng lên cả về số lượng và sự phức tạp, thì các cơ quan tổ chức cần một công cụ giúp họ thực hiện được các phân tích trên dữ liệu một cách nhanh hơn và chính xác hơn. Một trong các công cụ hiệu quả nhất tại thời điểm hiện tại là phần mềm R.","code":""},{"path":"giới-thiệu-chung.html","id":"phần-mềm-r-là-gì","chapter":"Chương 2 Giới thiệu chung","heading":"2.3.1 Phần mềm R là gì?","text":"Trước tiên, R là ngôn ngữ lập trình được xây dựng để phục vụ cho toán học và thống kê, đồng thời R cũng là một môi trường phần mềm mã nguồn mở miễn phí cho người sử dụng. R được giới thiệu lần đầu tiên vào năm 1992 bởi các giáo sư Ross Ihaka và Robert Gentleman như một ngôn ngữ lập trình để dạy thống kê tại Đại học Auckland. Tên của ngôn ngữ, R, xuất phát từ chữ cái đầu tiên của các tác giả là Ross và Robert.Trước khi trở thành ngôn ngữ lập trình cho Khoa học dữ liệu, R thường được coi là ngôn ngữ lập trình cho các nhà toán học và thống kê. Sau nhiều năm phát triển, R luôn được coi vẫn là một trong những ngôn ngữ lập trình phổ biến nhất trong giới học thuật vì độ tin cậy. Mỗi thư viện của R đều được phát triển một cách hoàn chỉnh và trải qua quá trình kiểm soát chặt chẽ. Tạp chí R (R Journal) là tạp chí học thuật về các phương pháp tính toán trong toán học và thống kê sử dụng phầm mềm R luôn trong danh sách các tạp chí khoa học uy tín (Science Citation Index Expanded hay SCIE) của Web Science. Chính vì sự uy tín trong học thuật nên đa số các trường đại học và viện nghiên cứu hàng đầu trên thế giới sử dụng như là một ngôn ngữ chính trong đào tạo về tính toán, toán học và thống kê.","code":""},{"path":"giới-thiệu-chung.html","id":"tại-sao-r-lại-được-sử-dụng-trong-khdl","chapter":"Chương 2 Giới thiệu chung","heading":"2.3.2 Tại sao R lại được sử dụng trong KHDL","text":"Trong khoảng hơn 10 năm trở lại đây, R không còn chỉ là ngôn ngữ lập trình thông thường như trước đây nữa. Mặc dù vẫn là một công cụ mạnh mẽ trong tính toán toán học và thống kê, nhưng còn có rất nhiều công cụ tuyệt vời khác mà bạn đọc có thể làm với R, đặc biệt là những ứng dụng trong KHDL. Nguyên nhân chính giúp cho phần mềm R nhanh chóng trở thành ngôn ngữ phổ biến trong KHDL là nền tảng quan trọng nhất của KHDL chính là toán học và thống kê. Đồng thời, ngôn ngữ lập trình R cũng đủ linh hoạt để người sử dụng viết các chương trình yêu cầu tính toán phức tạp trong Khoa học máy tính. Một cách tự nhiên, những nhà toán học, thống kê học và các tổ chức sử đang dụng R như là một ngôn ngữ chính sẽ tìm cách phát triển R để đáp ứng được với yêu cầu xử lý dữ liệu của chính họ. Một nguyên nhân khác khiến cho R phổ biến trong KHDL là đặc thù mã nguồn mở của phần mềm này. Những người làm việc trong lĩnh vực KHDL sử dụng R có thể chia sẻ kiến thức và kinh nghiệm một cách nhanh chóng và rộng rãi.","code":""},{"path":"giới-thiệu-chung.html","id":"r-có-thể-làm-những-gì","chapter":"Chương 2 Giới thiệu chung","heading":"2.3.3 R có thể làm những gì?","text":"Danh sách những việc bạn có thể làm trong R là không thể liệt hết bởi vì phần mềm này vẫn đang được phát triển không ngừng. Dưới đây là một số ứng dụng phổ biến mà R vượt trội hơn với các ngôn ngữ khác:Lập trình trong toán học, tính toán tối ưu, giải tối ưu bằng phương pháp sốLập trình trong toán học, tính toán tối ưu, giải tối ưu bằng phương pháp sốTính toán liên quan đến lý thuyết xác suất.Tính toán liên quan đến lý thuyết xác suất.Thực hiện các kiểm định thống kê.Thực hiện các kiểm định thống kê.Phát triển phần mềm thống kê.Phát triển phần mềm thống kê.Xây dựng mô hình kinh tế lượng.Xây dựng mô hình kinh tế lượng.Mô phỏng ngẫu nhiên.Mô phỏng ngẫu nhiên.Các tính năng của R dành cho Khoa học dữ liệu được liệt kê dưới đây:Thu thập tập dữ liệu, bao gồm cả dữ liệu lớn và không có cấu trúc,Thu thập tập dữ liệu, bao gồm cả dữ liệu lớn và không có cấu trúc,Khai phá dữ liệu,Khai phá dữ liệu,Xử lý, sắp xếp, biến đổi dữ liệu,Xử lý, sắp xếp, biến đổi dữ liệu,Phân tích dữ liệu,Phân tích dữ liệu,Trực quan hóa dữ liệu,Trực quan hóa dữ liệu,Xây dựng các mô hình học máy từ đơn giản đến phức tạpXây dựng các mô hình học máy từ đơn giản đến phức tạp","code":""},{"path":"giới-thiệu-chung.html","id":"tại-sao-cuốn-sách-này-lại-sử-dụng-r","chapter":"Chương 2 Giới thiệu chung","heading":"2.3.4 Tại sao cuốn sách này lại sử dụng R","text":"Mặc dù có một số phần mềm khác có thể được sử dụng thay thế cho R trong phân tích dữ liệu, nhưng chúng tôi lựa chọn ngôn ngữ R vìR là một phần mềm uy tín và đáng tin cậy được sử dụng bởi các trường đại học và các viện nghiên cứu hàng đầu trên thế giới. R cũng được sử dụng rộng rãi trong các công ty công nghệ hàng đầu như Microsoft, Facebook, Google, IBM.R là một phần mềm uy tín và đáng tin cậy được sử dụng bởi các trường đại học và các viện nghiên cứu hàng đầu trên thế giới. R cũng được sử dụng rộng rãi trong các công ty công nghệ hàng đầu như Microsoft, Facebook, Google, IBM.R là ngôn ngữ lập trình rất dễ hiểu cho người mới bắt đầu kể cả với những người không có kinh nghiệm lập trình. Còn nếu bạn đã có kinh nghiệm về một ngôn ngữ lập trình, sẽ chỉ cần một khoảng thời gian ngắn để bạn có thể viết các chương trình với R.R là ngôn ngữ lập trình rất dễ hiểu cho người mới bắt đầu kể cả với những người không có kinh nghiệm lập trình. Còn nếu bạn đã có kinh nghiệm về một ngôn ngữ lập trình, sẽ chỉ cần một khoảng thời gian ngắn để bạn có thể viết các chương trình với R.Phần mềm R là một phần mềm mã nguồn mở, nghĩa là bạn đọc có thể sử dụng R và hơn 12000 thư viện mở rộng mà không cần phải bỏ ra bất kỳ một chi phí nào. Điều này giúp cho R trở nên rất dễ tiếp cận đối với những sinh viên và người học không sẵn sàng chi trả một khoản chi phí để học về KHDL. Đồng thời, giảng viên cũng có thể tận dụng tối đa môi trường phần mềm này khi giảng dạy cho sinh viên.Phần mềm R là một phần mềm mã nguồn mở, nghĩa là bạn đọc có thể sử dụng R và hơn 12000 thư viện mở rộng mà không cần phải bỏ ra bất kỳ một chi phí nào. Điều này giúp cho R trở nên rất dễ tiếp cận đối với những sinh viên và người học không sẵn sàng chi trả một khoản chi phí để học về KHDL. Đồng thời, giảng viên cũng có thể tận dụng tối đa môi trường phần mềm này khi giảng dạy cho sinh viên.Cuối cùng và cũng không kém phần quan trọng, đó là sự hỗ trợ từ công đồng. Với số lượng người dùng lên đến hàng triệu người, nhiều người trong đó là những nhà toán học, thống kê học, giáo sư tại các trường đại học, bạn sẽ luôn tìm thấy sự hỗ trợ mỗi khi gặp bất kỳ vấn đề khi làm việc với R.Cuối cùng và cũng không kém phần quan trọng, đó là sự hỗ trợ từ công đồng. Với số lượng người dùng lên đến hàng triệu người, nhiều người trong đó là những nhà toán học, thống kê học, giáo sư tại các trường đại học, bạn sẽ luôn tìm thấy sự hỗ trợ mỗi khi gặp bất kỳ vấn đề khi làm việc với R.","code":""},{"path":"giới-thiệu-chung.html","id":"các-lựa-chọn-thay-thế-và-bổ-sung-cho-r-trong-khdl","chapter":"Chương 2 Giới thiệu chung","heading":"2.3.5 Các lựa chọn thay thế và bổ sung cho R trong KHDL","text":"Như chúng ta đã thấy, R là một trong những ngôn ngữ lập trình tốt nhất cho người mới bắt đầu bước chân vào lĩnh vực KHDL. Tuy nhiên, bạn đọc cũng có thể tìm thấy các phần mềm/ngôn ngữ có thể sử dụng để thay thế cho R trong quá trình học tập và làm việc:Python - vào thời điểm chúng tôi đang hoàn thành cuốn sách này, Python là một ngôn ngữ lập trình được sử dụng nhiều nhất trong KHDL. Python được giới thiệu lần đầu tiên vào năm 1991 và đã không ngừng tiến hóa và phát triển. Cũng như R, cũng có mã nguồn mở và hoàn toàn miễn phí cho người sử dụng. Câu lệnh của Python cũng rất dễ học và đặc biệt mạnh mẽ trong lập trình hướng đối tượng.Python - vào thời điểm chúng tôi đang hoàn thành cuốn sách này, Python là một ngôn ngữ lập trình được sử dụng nhiều nhất trong KHDL. Python được giới thiệu lần đầu tiên vào năm 1991 và đã không ngừng tiến hóa và phát triển. Cũng như R, cũng có mã nguồn mở và hoàn toàn miễn phí cho người sử dụng. Câu lệnh của Python cũng rất dễ học và đặc biệt mạnh mẽ trong lập trình hướng đối tượng.Julia - xuất hiện lần đầu tiên vào năm 2012, Julia là một trong những ngôn ngữ lập trình được phát hành gần đây nhất và là lựa chọn tối ưu cho các nhà khoa học dữ liệu. Ngôn ngữ lập trình cấp cao và hiệu suất cao này rất năng động và phù hợp để viết bất kỳ loại ứng dụng nào. Mặc dù Python và R vẫn được ưu tiên cho KHDL và học máy nhưng dự báo là Julia sẽ vượt qua cả hai trong tương lai gần. Thật vậy, mặc dù đây là ngôn ngữ lập trình tổng quát hơn nhưng nó có tất cả các đặc điểm cần thiết để xử lý phân tích, thống kê và dữ liệu lớn.Julia - xuất hiện lần đầu tiên vào năm 2012, Julia là một trong những ngôn ngữ lập trình được phát hành gần đây nhất và là lựa chọn tối ưu cho các nhà khoa học dữ liệu. Ngôn ngữ lập trình cấp cao và hiệu suất cao này rất năng động và phù hợp để viết bất kỳ loại ứng dụng nào. Mặc dù Python và R vẫn được ưu tiên cho KHDL và học máy nhưng dự báo là Julia sẽ vượt qua cả hai trong tương lai gần. Thật vậy, mặc dù đây là ngôn ngữ lập trình tổng quát hơn nhưng nó có tất cả các đặc điểm cần thiết để xử lý phân tích, thống kê và dữ liệu lớn.MATLAB - được phát triển bởi MathWorks, ngôn ngữ lập trình này là một môi trường điện toán được phát triển đặc biệt để phân tích số và thống kê. Nhờ có số lượng lớn các thư viện có sẵn cho người dùng, MATLAB cho phép các lập trình viên truy cập dữ liệu, xử lý dữ liệu và tạo các mô hình học máy từ đơn giản đến phức tạp. Mặc dù MATLAB là một hệ thống hiệu suất cao nhưng lại không phải là nguồn mở hoặc miễn phí. Thay vào đó, nó được xây dựng bởi các nhà phát triển chuyên nghiệp vàMATLAB - được phát triển bởi MathWorks, ngôn ngữ lập trình này là một môi trường điện toán được phát triển đặc biệt để phân tích số và thống kê. Nhờ có số lượng lớn các thư viện có sẵn cho người dùng, MATLAB cho phép các lập trình viên truy cập dữ liệu, xử lý dữ liệu và tạo các mô hình học máy từ đơn giản đến phức tạp. Mặc dù MATLAB là một hệ thống hiệu suất cao nhưng lại không phải là nguồn mở hoặc miễn phí. Thay vào đó, nó được xây dựng bởi các nhà phát triển chuyên nghiệp vàJava là một trong những ngôn ngữ lập trình phổ biến nhất và cũng là một lựa chọn cho những người khi mới bước vào lĩnh vực KHDL. Mặc dù vẫn có thể tải xuống miễn phí nhưng một số ứng dụng chỉ có sẵn trong phiên bản trả phí. Cú pháp của Java cũng tương đối dễ học đối với người mới bắt đầu. Nhìn chung Java vẫn là ngôn ngữ có mục đích chung và nó vẫn được các nhà khoa học dữ liệu coi là một lựa chọn bổ sung cho R hoặc Python.Java là một trong những ngôn ngữ lập trình phổ biến nhất và cũng là một lựa chọn cho những người khi mới bước vào lĩnh vực KHDL. Mặc dù vẫn có thể tải xuống miễn phí nhưng một số ứng dụng chỉ có sẵn trong phiên bản trả phí. Cú pháp của Java cũng tương đối dễ học đối với người mới bắt đầu. Nhìn chung Java vẫn là ngôn ngữ có mục đích chung và nó vẫn được các nhà khoa học dữ liệu coi là một lựa chọn bổ sung cho R hoặc Python.Ngoài việc thành thạo R hoặc một phần mềm chuyên dùng trong KHDL, bạn nên bổ sung cho mình các ngôn ngữ lập trình khác để đạt hiệu suất công việc tốt nhất:SQL hay ngôn ngữ truy vấn dữ liệu có cấu trúc. SQL xuất hiện từ năm 1974 đã không ngừng được cải tiến và sửa đổi để giúp cho ngôn ngữ này luôn nằm trong nhóm những ngôn ngữ lập trình được lựa chọn hàng đầu trong KHDL. SQL có cả các phiên bản miễn phí và các phiên bản thương mại mà người sử dụng phải trả chi phí.C và C++ là các ngôn ngữ lập trình hiệu suất cao có thể giúp bạn tăng hiệu quả của các chương trình. Hầu như tất cả các ứng dụng trên hệ điều hành máy tính và điện thoại di động hiện nay đều sử dụng C và C++. Viết các chương trình dưới ngôn ngữ C hoặc C++ sẽ hiệu quả về mặt thời gian hơn nhiều với các ngôn ngữ khác.","code":""},{"path":"giới-thiệu-chung.html","id":"cài-đặt-r-và-rstudio","chapter":"Chương 2 Giới thiệu chung","heading":"2.3.6 Cài đặt R và RStudio","text":"Bạn đọc sẽ bắt đầu bằng cài đặt phần mềm R và sau đó là cài đặt RStudio - một môi trường phát triển phổ biến dành cho R. Phần mềm R dành cho các hệ điều hành MAC OS, Windows, và Linux đều có sẵn để tải xuống từ trang web chính thức:https://cran.r-project.org/Tại thời điểm chúng tôi viết cuốn sách này, R đang là phiên bản 4.1.0. Sau khi tải xuống, chúng ta chỉ cần cài đặt R giống như tất cả các phần mềm khác với tất cả các tùy chọn mặc định.Sau khi cài đặt phần mềm R, bạn đọc cài đặt Rstudio. Chúng ta hoàn toàn có thể sử dụng R mà không cần có Rstudio. Tuy nhiên, Rstudio sẽ hỗ trợ bạn rất nhiều trong quá trình sử dụng R, đó, lời khuyên của chúng tôi là hãy sử dụng Rstudio cùng với R. Để tải xuống Rstudio, bạn truy cập vào trang web chính thức:https://posit.co/download/rstudio-desktop/RStudio là một công cụ linh hoạt giúp bạn tạo các phân tích dễ đọc và giữ mã, hình ảnh, nhận xét và sơ đồ của bạn ở cùng một nơi. Sử dụng RStudio để lập trình và phân tích dữ liệu trong R mang lại nhiều lợi ích. Dưới đây là một vài ví dụ về những gì RStudio cung cấp:Giao diện trực quan cho phép chúng ta theo dõi các đối tượng, tập lệnh và số liệu đã lưu,Giao diện trực quan cho phép chúng ta theo dõi các đối tượng, tập lệnh và số liệu đã lưu,Trình soạn thảo văn bản có các tính năng như tự động gợi ý câu lệnh, hiển thị màu giúp cho việc viết các câu lệnh rõ ràng,Trình soạn thảo văn bản có các tính năng như tự động gợi ý câu lệnh, hiển thị màu giúp cho việc viết các câu lệnh rõ ràng,Hiển thị mô tả hàm số, dữ liệu bằng thao tác đơn giản,Hiển thị mô tả hàm số, dữ liệu bằng thao tác đơn giản,Thanh công cụ có đầy đủ các tính năng trực quan để bạn đọc sử dụng thay vì phải viết câu lệnh,Thanh công cụ có đầy đủ các tính năng trực quan để bạn đọc sử dụng thay vì phải viết câu lệnh,Mỗi khi bạn đọc mở RStudio, R cũng được khởi chạy tự động. Giao diện RStudio rất trực quan và dễ sử dụng. Các cửa sổ quan trọng bao gồm:Cửa số Console là nơi chúng ta có thể chạy các câu lệnh R.Cửa số Console là nơi chúng ta có thể chạy các câu lệnh R.Cửa sổ Environment là chúng ta theo dõi các đối tượng, tập lệnh và số liệu đã lưu.Cửa sổ Environment là chúng ta theo dõi các đối tượng, tập lệnh và số liệu đã lưu.Cửa sổ File là nơi hiển thị địa chỉ thư mục đang làm việc, hiển thị đồ thị trực quan, hoặc hiển thị mô tả dữ liệu, hàm số, thư viện.Cửa sổ File là nơi hiển thị địa chỉ thư mục đang làm việc, hiển thị đồ thị trực quan, hoặc hiển thị mô tả dữ liệu, hàm số, thư viện.Bạn đọc sẽ làm quen dần với giao diện và các cửa sổ làm việc khác nhau của RStudio trong quá trình thực hành trên các câu lệnh và dữ liệu cụ thể. Chúng tôi sẽ không đi quá sâu vào chi tiết tại đây.","code":""},{"path":"giới-thiệu-chung.html","id":"về-cuốn-sách-và-tác-giả","chapter":"Chương 2 Giới thiệu chung","heading":"2.4 Về cuốn sách và tác giả","text":"Cuốn sách được viết với mục tiêu là để thành sách tham khảo chính cho các môn học \\(\\textbf{Phân tích và dự báo}\\) và \\(\\textbf{Khoa học dữ liệu cơ bản}\\) cho sinh viên và học viên cao học ngành Toán Kinh tế tại Đại học Kinh tế Quốc dân. Chúng tôi tin rằng những kiến thức và công cụ được giới thiệu trong cuốn sách này sẽ là những hành trang quan trọng cho những nhà kinh tế và kinh doanh tương lai trước khi bước chân vào thế giới việc làm đầy tính cạnh tranh như hiện nay.","code":""},{"path":"giới-thiệu-chung.html","id":"đôi-lời-từ-tác-giả","chapter":"Chương 2 Giới thiệu chung","heading":"2.4.1 Đôi lời từ tác giả","text":"Tôi không phải là một nhà kinh tế, cũng không phải là một chuyên gia dữ liệu, và cũng không được đào tạo bài bản về máy tính hay lập trình, tôi là một Actuary. Cuốn sách được viết dựa trên kinh nghiệm làm việc và giảng dạy của tôi trong những lĩnh vực khoa học tính toán (actuarial science). Tôi bắt đầu sử dụng R như một phần mềm thống kê khi còn là một sinh viên đại học. Ấn tượng đầu tiên của tôi về R là khi sử dụng phần mềm này để mô phỏng các chuyển động Brown hình học vô cùng bắt mắt. Và R vẫn tiếp tục đồng hành với tôi cho đến nay trong cả môi trường doanh nghiệp và học thuật:Trong thời gian nghiên cứu sinh từ 2011 đến 2014, tôi sử dụng R là công cụ chính để thực hiện các tính toán cho luận án của mình. Với nội dung nghiên cứu tập trung vào tính toán và mô phỏng xác suất của các sự kiện cực hiếm, R là lựa chọn tối ưu vào thời điểm đó. Trong một vài tính toán mà chưa có thư viện hỗ trợ, chẳng hạn như tính toán với độ chính xác siêu nhỏ (dưới \\(10^{-100}\\)), tôi đã tìm đến Python là một giải pháp bổ sung.Trong thời gian nghiên cứu sinh từ 2011 đến 2014, tôi sử dụng R là công cụ chính để thực hiện các tính toán cho luận án của mình. Với nội dung nghiên cứu tập trung vào tính toán và mô phỏng xác suất của các sự kiện cực hiếm, R là lựa chọn tối ưu vào thời điểm đó. Trong một vài tính toán mà chưa có thư viện hỗ trợ, chẳng hạn như tính toán với độ chính xác siêu nhỏ (dưới \\(10^{-100}\\)), tôi đã tìm đến Python là một giải pháp bổ sung.Công việc đầu tiên trong môi trường doanh nghiệp của tôi liên quan đến dữ liệu là xây dựng các thuật toán để đầu tư trên thị trường tài chính tại một quỹ đầu tư. Tôi được làm quen với một kho dữ liệu khổng lồ bao gồm dữ liệu hỗ trợ phân tích kỹ thuật, dữ liệu hỗ trợ phân tích cơ bản, và dữ liệu kiểu tin tức của tất cả các công cụ tài chính có thể sử dụng để giao dịch, bao gồm cổ phiếu, trái phiếu, hợp đồng tương lai, quyền chọn. Các mô hình trên dữ liệu được chúng tôi - những người nghiên cứu thị trường - xây dựng bằng nhiều phương pháp để tìm ra các chiến lược mang lại lợi nhuận cho quỹ đầu tư. Mặc dù không có cơ hội sử dụng R để phân tích dữ liệu các yêu cầu liên quan đến bảo mật, nhưng tôi lại được học những kỹ năng lập trình C++ mà tôi nhận ra là vô cùng quan trọng cho công việc của mình sau này.Công việc đầu tiên trong môi trường doanh nghiệp của tôi liên quan đến dữ liệu là xây dựng các thuật toán để đầu tư trên thị trường tài chính tại một quỹ đầu tư. Tôi được làm quen với một kho dữ liệu khổng lồ bao gồm dữ liệu hỗ trợ phân tích kỹ thuật, dữ liệu hỗ trợ phân tích cơ bản, và dữ liệu kiểu tin tức của tất cả các công cụ tài chính có thể sử dụng để giao dịch, bao gồm cổ phiếu, trái phiếu, hợp đồng tương lai, quyền chọn. Các mô hình trên dữ liệu được chúng tôi - những người nghiên cứu thị trường - xây dựng bằng nhiều phương pháp để tìm ra các chiến lược mang lại lợi nhuận cho quỹ đầu tư. Mặc dù không có cơ hội sử dụng R để phân tích dữ liệu các yêu cầu liên quan đến bảo mật, nhưng tôi lại được học những kỹ năng lập trình C++ mà tôi nhận ra là vô cùng quan trọng cho công việc của mình sau này.Khi bắt đầu công việc như một chuyên gia tính toán, tôi làm việc thường xuyên với dữ liệu trong bảo hiểm. Với những dữ liệu nhỏ, tôi nhận thấy rằng Microsoft Excel kết hợp với lập trình VBA là vừa đủ để xử lý. Khi dữ liệu trở nên ngày càng lớn và phức tạp, Excel và VBA không còn đáp ứng được nhu cầu, đó là lúc tôi quay lại sử dụng R trong công việc của mình. Ngoài sử dụng R như là một công cụ chính để định phí, đánh giá hợp đồng bảo hiểm, tôi còn sử dụng R để thực hiện các công việc liên quan đến dữ liệu như\nThu thập dữ liệu nhận được từ các phòng ban như tài chính, kế toán, nghiệp vụ, , kiểm tra, làm sạch và cập nhập dữ liệu lên cơ sở dữ liệu để phục vụ tính toán. R cho phép xử lý và tính toán những dữ liệu hàng chục triệu dòng với hiệu quả thực sự đáng kinh ngạc.\nTrích xuất dữ liệu từ cơ sở dữ liệu, biến đổi và tính toán để thực hiện các nghiệp vụ như tái bảo hiểm, quản lý tài sản nợ/có, tính toán báo cáo kinh nghiệm.\nXây dựng các dashboard để cập nhật tình hình bồi thường bảo hiểm y tế với thời gian thực.\nXây dựng mô hình để phân loại rủi ro, dự báo những chủ hợp đồng, người được bảo hiểm có khả năng cao là trục lợi trong bảo hiểm y tế.\nKhi bắt đầu công việc như một chuyên gia tính toán, tôi làm việc thường xuyên với dữ liệu trong bảo hiểm. Với những dữ liệu nhỏ, tôi nhận thấy rằng Microsoft Excel kết hợp với lập trình VBA là vừa đủ để xử lý. Khi dữ liệu trở nên ngày càng lớn và phức tạp, Excel và VBA không còn đáp ứng được nhu cầu, đó là lúc tôi quay lại sử dụng R trong công việc của mình. Ngoài sử dụng R như là một công cụ chính để định phí, đánh giá hợp đồng bảo hiểm, tôi còn sử dụng R để thực hiện các công việc liên quan đến dữ liệu nhưThu thập dữ liệu nhận được từ các phòng ban như tài chính, kế toán, nghiệp vụ, , kiểm tra, làm sạch và cập nhập dữ liệu lên cơ sở dữ liệu để phục vụ tính toán. R cho phép xử lý và tính toán những dữ liệu hàng chục triệu dòng với hiệu quả thực sự đáng kinh ngạc.Thu thập dữ liệu nhận được từ các phòng ban như tài chính, kế toán, nghiệp vụ, , kiểm tra, làm sạch và cập nhập dữ liệu lên cơ sở dữ liệu để phục vụ tính toán. R cho phép xử lý và tính toán những dữ liệu hàng chục triệu dòng với hiệu quả thực sự đáng kinh ngạc.Trích xuất dữ liệu từ cơ sở dữ liệu, biến đổi và tính toán để thực hiện các nghiệp vụ như tái bảo hiểm, quản lý tài sản nợ/có, tính toán báo cáo kinh nghiệm.Trích xuất dữ liệu từ cơ sở dữ liệu, biến đổi và tính toán để thực hiện các nghiệp vụ như tái bảo hiểm, quản lý tài sản nợ/có, tính toán báo cáo kinh nghiệm.Xây dựng các dashboard để cập nhật tình hình bồi thường bảo hiểm y tế với thời gian thực.Xây dựng các dashboard để cập nhật tình hình bồi thường bảo hiểm y tế với thời gian thực.Xây dựng mô hình để phân loại rủi ro, dự báo những chủ hợp đồng, người được bảo hiểm có khả năng cao là trục lợi trong bảo hiểm y tế.Xây dựng mô hình để phân loại rủi ro, dự báo những chủ hợp đồng, người được bảo hiểm có khả năng cao là trục lợi trong bảo hiểm y tế.Khi tôi quay trở lại với công việc học thuật vào đầu năm 2018, cũng là lúc mà làn sóng về khoa học dữ liệu bắt đầu ảnh hưởng đến đa số các lĩnh vực khác, bao gồm cả ngành khoa học tính toán. Các chứng chỉ về khoa học dữ liệu là điều kiện bắt buộc đối với những người muốn trở thành thành viên của các Hiệp hội chuyên gia tính toán. Tôi cùng với các đồng nghiệp của mình đưa môn học \\(\\textbf{Phân tích và dự báo}\\) vào trong chương trình đào tạo Định phí bảo hiểm và quản trị rủi ro để cung cấp cho sinh viên các kiến thức và kỹ năng cần thiết khi làm việc với dữ liệu và có thể lấy được chứng chỉ nghề nghiệp của Hiệp hội. Môn học \\(\\textbf{Phân tích và dự báo}\\) sau đó chính thức được giảng dạy cho sinh viên ngành Toán Kinh tế vào năm 2021 với tên gọi \\(\\textbf{Khoa học dữ liệu trong Kinh tế và Kinh doanh}\\). Nội dung của cuốn sách xoay quanh các kiến thức mà tôi đã đang và sẽ giảng dạy cho sinh viên và học viên của mình.Khi tôi quay trở lại với công việc học thuật vào đầu năm 2018, cũng là lúc mà làn sóng về khoa học dữ liệu bắt đầu ảnh hưởng đến đa số các lĩnh vực khác, bao gồm cả ngành khoa học tính toán. Các chứng chỉ về khoa học dữ liệu là điều kiện bắt buộc đối với những người muốn trở thành thành viên của các Hiệp hội chuyên gia tính toán. Tôi cùng với các đồng nghiệp của mình đưa môn học \\(\\textbf{Phân tích và dự báo}\\) vào trong chương trình đào tạo Định phí bảo hiểm và quản trị rủi ro để cung cấp cho sinh viên các kiến thức và kỹ năng cần thiết khi làm việc với dữ liệu và có thể lấy được chứng chỉ nghề nghiệp của Hiệp hội. Môn học \\(\\textbf{Phân tích và dự báo}\\) sau đó chính thức được giảng dạy cho sinh viên ngành Toán Kinh tế vào năm 2021 với tên gọi \\(\\textbf{Khoa học dữ liệu trong Kinh tế và Kinh doanh}\\). Nội dung của cuốn sách xoay quanh các kiến thức mà tôi đã đang và sẽ giảng dạy cho sinh viên và học viên của mình.","code":""},{"path":"giới-thiệu-chung.html","id":"ai-nên-đọc-cuốn-sách-này","chapter":"Chương 2 Giới thiệu chung","heading":"2.4.2 Ai nên đọc cuốn sách này","text":"","code":""},{"path":"giới-thiệu-chung.html","id":"cấu-trúc-của-cuốn-sách","chapter":"Chương 2 Giới thiệu chung","heading":"2.4.3 Cấu trúc của cuốn sách","text":"","code":""},{"path":"giới-thiệu-chung.html","id":"các-ký-hiệu-thông-dụng","chapter":"Chương 2 Giới thiệu chung","heading":"2.4.4 Các ký hiệu thông dụng","text":"","code":""},{"path":"giới-thiệu-chung.html","id":"các-dữ-liệu-sử-dụng-trong-cuốn-sách","chapter":"Chương 2 Giới thiệu chung","heading":"2.4.5 Các dữ liệu sử dụng trong cuốn sách","text":"","code":""},{"path":"real-introduction.html","id":"real-introduction","chapter":"Chương 3 Real Introduction","heading":"Chương 3 Real Introduction","text":"","code":"\nlibrary(dplyr)## \n## Attaching package: 'dplyr'## The following objects are masked from 'package:stats':\n## \n##     filter, lag## The following objects are masked from 'package:base':\n## \n##     intersect, setdiff, setequal, union\nlibrary(kableExtra)## \n## Attaching package: 'kableExtra'## The following object is masked from 'package:dplyr':\n## \n##     group_rows"},{"path":"real-introduction.html","id":"các-khái-niệm-cơ-bản-trong-khoa-học-dữ-liệu","chapter":"Chương 3 Real Introduction","heading":"3.1 Các khái niệm cơ bản trong khoa học dữ liệu","text":"Khoa học dữ liệu là ngành khoa học kết hợp toán học và thống kê, lập trình chuyên biệt, kiến thức chuyên môn cụ thể để khám phá những thông tin hữu ích ẩn trong dữ liệu của các cơ quan, tổ chức. Những thông tin hữu ích này có thể được sử dụng để hướng dẫn việc ra quyết định và lập kế hoạch chiến lược cho cơ quan tổ chức đó.Để bắt đầu với Khoa học dữ liệu, chúng ta sẽ đi từ một vài ví dụ đơn giản nhất. Đặt trong trường hợp chúng ta là người phân tích khoa học dữ liệu tại một bệnh viện, một bài toán đơn giản thường được đặt ra là dự đoán khả năng một người bất kỳ nào đó bị bệnh dựa trên các thông tin mà ta biết về người đó. Ví dụ, để tính xác suất của một người bị bệnh tim lần 2, sau có thông tin là người đó đã bị bệnh tim lần 1, dựa vào các thông tin khác như chủng tộc, giới tính, tuổi, tiền sử bệnh lý, lối sống của bệnh nhân đó.Quyển sách này sẽ dạy bạn cách để học từ dữ liệu. Thông thường, chúng ta sẽ có một thước đo kết quả (hay còn gọi là các biến phụ thuộc), thường là theo biến định lượng (ví dụ như giá cổ phiếu, doanh thu của công ty), hoặc biến định tính (ví dụ như có hay không chuyện người đó bị bệnh tim), và chúng ta sẽ dự đoán nó từ các đặc trưng (hay còn gọi là các biến độc lập).Để xây dựng được những mô hình tốt, nghĩa là mô hình có thể đưa ra dự đoán với độ chính xác cao, chúng ta cần phải sử dụng dữ liệu sao cho thật hợp lý. Thử tưởng tượng, mục đích của dự đoán là dự báo những điều chưa được biết. nếu như bạn xây dựng một hàm f để dự đoán kết quả trên toàn bộ dữ liệu bạn có, thì làm sao bạn có thể biết được hàm f đó có hoạt động tốt trên thực tế để chọn ra hàm f phù hợp nhất? Khi hàm f được xây dựng trên toàn bộ tập dữ liệu, các tham số của hàm đó sẽ được tính toán để phù hợp với dữ liệu đó nhất, nhưng có khả năng chúng sẽ lại không phù hợp với các dữ liệu được đưa vào từ bên ngoài.Vậy nên, trong thực tế, chúng ta thường chia dữ liệu ra làm hai hoặc ba phần tuỳ thuộc vào lượng dữ liệu và các đặc tính về dữ liệu đó. Thông thường, nếu dữ liệu đủ lớn, ta sẽ chia dữ liệu ra làm ba thành phần chính: dữ liệu học (train data), dữ liệu xác thực (validation data) và dữ liệu kiểm thử (test data).Các hàm f sẽ được huấn luyện trên dữ liệu học, sau đó các hàm f khác nhau sẽ được đưa vào dữ liệu xác thực để thử dự đoán, và hàm f có những dự đoán chính xác nhất trên dữ liệu xác thực sẽ được chọn để đưa vào dữ liệu kiểm thử. Nếu hàm f dự đoán trên dữ liệu kiểm thử đạt yêu cầu, nghĩa là vượt qua một tiêu chuẩn nhất định. Tiêu chuẩn nhất định cũng sẽ thường phụ thuộc vào bài toán mà hàm f đó được xây dựng để xử lý. Đối với kết quả dự báo là kết quả định lượng, người ta thường đặt tiêu chuẩn là như sai số dự báo nhỏ hơn k cho trước. Còn đối với kết quả dự báo là kết quả định tính, người ta thường đặt tiêu chuẩn là tỉ lệ dự đoán đúng cao hơn một số k cho trước.Trong phương pháp học thống kê, ta thường xây dựng một hàm f dựa trên dữ liệu gồm có nhiều biến khác nhau. Thông thường, biến đầu ra (output) còn được gọi là biến phụ thuộc (dependent variable), thường được ký hiệu là X. Tương tự, biến đầu vào là các biến được sử dụng để dự đoán biến phụ thuộc, loại biến này còn được gọi là biến dự đoán, hay biến độc lập, thường được ký hiệu là Y.Một định nghĩa nữa mà ta cần phải biết đó là định nghĩa về các loại biến khác nhau. Trong quyển sách này, chúng ta sẽ nhắc tới hai loại biến cơ bản nhất là Biến định lượng và biến định tính.\nBiến định lượng là loại biến mà ta có thể sánh các giá trị của biến đó với nhau, và nếu hai giá trị định lượng gần nhau, thì thường chúng cũng sẽ gần nhau trong thực tế. Lấy ví dụ về biến định lượng, ta có thể kể đến chỉ số đường trong máu của một người. Nếu như hai người có chỉ số glucose trong máu chỉ chênh nhau 0.1 mg/dl, thì có nghĩa là mức độ đường trong máu của họ thực tế sẽ giống nhau hơn là hai người có chỉ số glucose trong máu chênh nhau 10 mg/dl. Hoặc đơn giản như là một nhân viên Nói chung là sự khác biệt giữa các giá trị của hai biến định lượng với nhau ta hoàn toàn có thể quan sát được. Biến định lượng cũng sẽ có thể có hai dạng, đó là biến liên tục (continuous) với giá trị quan sát thuộc tập số thực (ví dụ như tốc độ của các xe ô tô trên đường cao tốc), và biến rời rạc (discrete) với các giá trị quan sát thuộc tập số nguyên (ví dụ như tổng số sản phẩm mà một khách hàng mua của công ty).Việc xác định đúng loại biến là rất quan trọng, phục vụ chính xác . Trong thực tế, ta có thể sẽ có trường hợp như sau. Một công ty đưa ra một khảo sát cho khách hàng, khảo sát đó có câu hỏi để đánh giá mức độ hài lòng với chất lượng dịch vụ của công ty với các lựa chọn như sau: “Rất Kém”, “Kém”, “Bình thường”, “Tốt”, “Rất tốt” hoặc đơn thuần là các lựa chọn từ 1 đến 5. Tuy nhiên, trong khảo sát đó, khách hàng cũng có câu hỏi về loại thức ăn mà họ yêu thích, với các lựa chọn là “Cơm”, “Bánh mì”, “Mỳ Ý”. Đối với câu trả lời cho hai câu hỏi trên của khách hàng, thì câu trả lời nào là biến định tính, câu trả lời nào là biến định lượng?Đáp án là không có câu trả lời nào là biến định lượng, toàn bộ các câu trả lời đều là biến định tính. Tại sao lại như vậy? Trong trường hợp hỏi về “loại thức ăn yêu thích”, các câu trả lời không thể sánh được với nhau, ta không thể nói rằng khách hàng thích ăn “Cơm” là tốt hơn, hay kém hơn với khách hàng ăn “Mỳ Ý” hay “Bánh mì”. Nên đương nhiên đáp án ở câu hỏi này là biến định tính. Còn đối với câu hỏi thứ hai, tưởng như ta có thể sánh các lựa chọn của khách hàng với nhau, nhưng sự thực là ta không thể dựa vào các đánh giá trên để kết luận rằng khách hàng này thích dịch vụ của công ty hơn bao nhiêu lần với khách hàng kia. Lấy ví dụ đơn giản, khách hàng có câu trả lời là 3, khách hàng B có câu trả lời là 2, khách hàng C có câu trả lời là 5, ta không thể nói rằng khách hàng thích dịch vụ của công ty hơn gấp rưỡi với khách hàng B, hay khách hàng B thích dịch vụ của công ty bằng 2,5 lần khách hàng C. Điều đó là không hợp lý, ta chỉ có thể nói rằng khách hàng thích dịch vụ của công ty hơn khách hàng B, hay khách hàng B không thích dịch vụ của công ty bằng khách hàng C.Vậy thì có cách nào để ta xác định những trường hợp như vậy là biến định tính, hay biến định lượng? Đối với biến định tính, ta có phải đặt ra tiêu chuẩn phân loại gì không? Ta thường chia ra làm hai loại là biến định lượng có thứ tự (ordinal variable) và biến định lượng không có thứ tự (nominal variable). Biến định lượng có thứ tự là loại biến mà có sự sắp xếp thứ bậc giữa các giá trị trong biến, nhưng không có một thước đo nào phù hợp. Biến định tính không có thứ tự là loại biến có các giá trị mà không có thước đo nào phù hợp giữa chúng, đồng thời cũng không có thước đo nào phù hợp, tương tự như trường hợp “Cơm”, “Bánh mì”, “Mỳ Ý” ở trên.Đối với biến định tính, thường ta có thể chuyển các giá trị của biến định tính sang thành dạng số. Có một phương pháp cơ bản để làm điều này đó được gọi là one-hot encoding hoặc còn gọi là dummy encoding.\nTable 3.1: Original Dataset\n\nTable 3.2: One-Hot Encoded Dataset\nDummy variable hay one-hot encoding có cách gán giá trị đơn giản hơn, khi chúng đưa một biến định tính gồm có N giá trị phân loại thành N-1 biến nhị phân khác nhau.Trong trường hợp biến định tính chỉ có hai giá trị duy nhất, chúng ta gọi giá trị đó là biến nhị phân (binary). Các ví dụ cho trường hợp biến nhị phân có thể kể tới như: “Có đi xe đạp” – “Không đi xe đạp”, “Đang đi học” – “Đang không đi học”…. Những trường hợp này ta có thể gán giá trị số 0,1 lần lượt cho hai trường hợp.Trong phần tiếp theo, ta sẽ đi sâu hơn một chút về lý thuyết xây dựng mô hình hồi quy. Để dễ tưởng tượng hơn, ta sẽ bắt đầu với bài toán mà biến dự đoán là biến định lượng. Cho biến Y là biến phụ thuộc, và có các biến độc lập \\(X_1,X_2,X_3,…,X_p\\). Chúng ta giả sử là có sự liên hệ giữa biến Y và biến \\(X = (X_1,X_2,X_3,…,X_p)\\), ta có thể viết chúng dưới dạng tổng quát:\\[\\begin{align}\nY = f(X) + \\epsilon\n\\end{align}\\]Trong đó, f là một hàm cố định, nhưng chưa xác định của \\(X_1,X_2,X_3,…,X_p\\), và bên cạnh đó \\(\\epsilon\\) là random error term, độc lập với X và có giá trị kỳ vọng bằng 0. Trong công thức trên, f thể hiện systematic information mà X cung cấp về Y.Ta có thể lấy ví dụ đơn giản về một hàm f như sau. Giả sử chúng ta muốn tìm mối liên hệ giữa mức lương của một người với số năm kinh nghiệm làm việc và trình độ học vấn, chúng ta có thể viết lại mối liên hệ đó bằng hàm f như sau:\\[\\begin{align}\nMức lương = f(\\text{Số năm kinh nghiệm}, \\text{Trình độ học vấn}) + \\epsilon\n\\end{align}\\]Trong ví dụ này, chúng ta có thể thấy rằng mức lương phụ thuộc một cách có hệ thống vào trình độ học vấn và số năm kinh nghiệm. Theo lẽ tư duy thông thường, một người có số năm kinh nghiệm và trình độ học vấn càng cao thì mức lương họ được hưởng sẽ cao hơn người khác. Tuy nhiên, sự thật là không phải ai cũng được hưởng một mức lương giống hệt nhau, bởi vì có rất nhiều lý khác có thể ảnh hưởng đến mức lương của mỗi người, có thể kể đến như kỹ năng cá nhân, ngành nghề làm việc, vị trí địa lý, cơ cấu lương của doanh nghiệpm,…. Để thể hiện sự khác biệt về mức lương giữa những người khác nhau, chúng ta sử dụng \\(\\epsilon\\), đây có thể được coi là sự chênh lệch một cách ngẫu nhiên về mức lương của những người khác nhau. Khi chúng ta xây dựng một mô hình (hay nói chính xác hơn là hàm f) để thể hiện mối liên hệ giữa biến Y và biến X, thì phần ngẫu nhiên \\(\\epsilon\\) được thêm vào công thức là cần thiết để đảm bảo mô hình của chúng ta được chính xác.","code":""},{"path":"real-introduction.html","id":"tại-sao-cần-hàm-f","chapter":"Chương 3 Real Introduction","heading":"3.2 Tại sao cần hàm f?","text":"Có hai lý chính để chúng ta tìm ra hàm f, đó là phục vụ cho mục đích dự báo hoặc suy diễn thống kê. Trước tiên, chúng ta sẽ đi sâu vào mục đích thứ nhất, dự báo.","code":""},{"path":"real-introduction.html","id":"dự-báo","chapter":"Chương 3 Real Introduction","heading":"3.2.1 Dự báo","text":"Trong thực tế, có rất nhiều trường hợp, chúng ta có đầy đủ thông tin về biến độc lập X, nhưng chúng ta lại rất khó có đầy đủ thông tin về biến phụ thuộc Y. Trong hệ thống mô hình dự báo của chúng ta, dựa trên việc biến sai số \\(\\epsilon\\) có giá trị trung bình bằng 0, chúng ta có thể dự đoán Y bằng công thức:\\[\\begin{align}\n\\hat{Y} = \\hat{f}(X)\n\\end{align}\\]trong đó \\(\\hat{f}\\) là ước lượng của hàm f, và \\(\\hat{Y}\\) thể hiện kết quả dự báo cho Y. Trong hệ thống này, \\(\\hat{f}\\) thường là dạng hộp đen (black box), có nghĩa là chúng ta thường không quan tâm đến việc hàm \\(\\hat{f}\\) có dạng chính xác như thế nào, mà chỉ quan tâm đến độ chính xác của hàm \\(\\hat{f}\\) khi dự đoán giá trị của Y.Một ví dụ cho việc sử dụng hàm f với biến dự báo là biến định lượng có thể được trình bày như sau. Giả dụ chúng ta đang có các dữ liệu về thông tin cá nhân của một khách hàng mua bảo hiểm của công ty bảo hiểm (thể hiện bằng các biến \\(X_1, X_2, ... X_p\\)), Y là số tiền mô hình dự đoán công ty bảo hiểm sẽ phải chi trả cho khách hàng đó trong tương lai. Việc dự đoán Y bằng việc sử dụng X là khá hợp lý, khi chúng ta có thể dự đoán số tiền bảo hiểm mà một khách hàng sẽ đòi công ty bảo hiểm trong tương lai bằng chính những thông tin cá nhân của người đó, giúp cho công ty bảo hiểm nắm được thông tin để chuẩn bị đủ tiền trả cho các khách hàng.Độ chính xác của \\(\\hat{Y}\\) trong việc dự báo Y dựa vào hai yếu tố: Đó là sai số có thể giảm (reducible error) và sai số không thể giảm (irreducible error). Cần phải hiểu rằng \\(\\hat{f}\\) không dự đoán một cách chính xác hoàn toàn giá trị của f, mà chúng luôn luôn tồn tại sai số trong đó.Sai số này có thể giảm thiểu được, bởi vì chúng ta có thể tăng sự chính xác của mô hình \\(\\hat{f}\\) bằng cách sử dụng những mô hình thống kê tiên tiến, phù hợp với dữ liệu để ước lượng hàm f. Tuy nhiên, nếu chúng ta có thể dự đoán đúng giá trị chính xác của f, nghĩa là ước lượng của chúng ta sẽ có dạng \\(\\hat{Y} = f(X)\\), thì chúng ta vẫn có một vài sai số trong đó. Bởi vì Y là hàm có chứa \\(\\epsilon\\), mà theo định nghĩa, ta không thể dự đoán \\(\\epsilon\\) theo X. Vậy nên, sự biến động liên quan tới \\(\\epsilon\\) cũng vẫn gây ảnh hưởng đến độ chính xác của dự báo. \\(\\epsilon\\) được biết đến như là sai số không thể giảm, bởi vì dù cho chúng ta có ước lượng hàm f tốt đến mức nào, ta vẫn không thể giảm sai số được thể hiện bởi \\(\\epsilon\\).Vậy tại sao sai số không thể giảm lại lớn hơn 0? Giá trị \\(\\epsilon\\) có thể chứa những thông tin chưa được thu thập, nhưng có giá trị trong việc dự đoán Y. Bởi vì chúng ta không có số liệu của những biến \\(X_{p+1}, X_{p+2},...\\) đó, hàm f không thể sử dụng chúng trong việc dự đoán Y. Trong trường hợp khác, ta không thể giảm \\(\\epsilon\\) bởi vì thực sự chúng có chứa những biến động không thể đo lường được. Ví dụ như khi ta xây mô hình để dự đoán rủi ro khi sử dụng một loại thuốc lên người bệnh nhân, thì rủi ro đó có thể thay đổi tuỳ vào chất lượng sản xuất của viên thuốc đó, hoặc đơn giản là thể trạng bệnh nhân vào ngày hôm đó, thường đó là những rủi ro mà ta gần như không thể xác định trước được.Cho một hàm ước lượng \\(\\hat{f}\\) và một bộ các biến dự báo (predictors), mà từ đó ta sẽ có được những giá trị dự báo \\(\\hat{Y} = \\hat{f}(X)\\). Giả sử rằng \\(\\hat{f}\\) và X đều giữ nguyên, và sai số chỉ đến từ \\(\\epsilon\\), ta có công thức:\\[\\begin{align}\nE(Y-\\hat{y})^2 &= E[f(X) + \\epsilon - \\hat{f}(X)]^2 \\\\\n&= \\underbrace{[f(X) - \\hat{f}(X)]^2}_{Sai \\ số \\ có \\ thể \\ giảm} \\quad + \\underbrace{Var(\\epsilon)}_{Sai \\ số \\ không \\ thể \\ giảm}\n\\label{eq:re_irre_error}\n\\end{align}\\]trong đó E(Y-\\(\\hat{y}\\))^2 thể hiện giá trị trung bình, hoặc giá trị kỳ vọng của giá trị bình phương của chênh lệch giữa giá trị dự đoán và giá trị thực tế. \\(Var(\\epsilon)\\) thể hiện phương sai của \\(\\epsilon\\).Quyển sách này sẽ tập trung vào các kỹ thuật thống kê giúp ước lượng hàm f với mục tiêu là giảm tối đa sai số có thể giảm. Tuy nhiên, chúng ta luôn cần chú ý rằng sai số không thể giảm sẽ vẫn luôn luôn tạo ra một giới hạn trên cho sự chính xác của dự báo. Và giới hạn trên này luôn luôn là một ẩn số trong thực tế.","code":""},{"path":"real-introduction.html","id":"suy-diễn","chapter":"Chương 3 Real Introduction","heading":"3.2.2 Suy diễn","text":"Ngoài mục đích xây dựng hàm f để dự đoán các giá trị của Y. Người ta còn muốn hiểu hơn về mối liên hệ giữa Y và \\(X_1, X_2, X_3,...,X_p\\). Trong trường hợp này, chúng ta vẫn sẽ phải ước lượng hàm f, tuy nhiên mục tiêu của chúng ta không phải là dự báo giá trị của Y. Bây giờ, hàm \\(\\hat{f}\\) không thể được coi là hộp đen (black box) nữa, mà chúng ta cần phải biết chính xác dạng của hàm \\(\\hat{f}\\) đó là gì. Khi thực hiện các phương pháp thống kê suy diễn, chúng ta thường tập trung vào trả lời các câu hỏi như sau:Biến độc lập (predictor) nào có mối quan hệ với biến phụ thuộc? Câu hỏi này thường được đặt ra trong trường hợp dữ liệu của chúng ta chỉ có một lượng nhỏ biến độc lập X có liên quan đến biến Y. Việc tìm ra một số lượng nhỏ các biến độc lập X liên hệ với biến Y giữa một số lượng biến độc lập X lớn hơn có rất nhiều tác dụng trong thực tế. Ví dụ như trong bài toán tính toán khả năng bị sốc phản vệ của bệnh nhân khi dùng thuốc, khi đó biến Y thể hiện khả năng bệnh nhân đó bị sốc phản vệ, các biến X chứa thông tin cá nhân của bệnh nhân như tuổi, cân nặng, tiền sử bệnh; trong trường hợp này nếu như chúng ta tìm ra được yếu tố (ở đây được thể hiện bởi biến X) nào có ảnh hưởng nhiều đến biến Y thì có thể các bác sĩ sẽ tập trung tìm ra cách cải thiện những yếu tố đó, giúp cho bệnh nhân giảm khả năng bị sốc phản vệ.Mối liên hệ giữa biến phụ thuộc và các biến độc lập là gì? Trong thực tế, một vài biến độc lập X sẽ có mối quan hệ đồng biến với biến Y, có nghĩa là nếu giá trị các biến độc lập đó càng lớn, thì giá trị của Y sẽ càng lớn. Một vài biến độc lập khác cũng có thể có mối quan hệ nghịch biến với Y. Phụ thuộc vào sự phức tạp của hàm f và của dữ liệu, mối liên hệ giữa biến phụ thuộc Y và một biến độc lập bất kỳ có thể cũng phụ thuộc vào giá trị của các biến độc lập khác.Liệu mối quan hệ giữa biến phụ thuộc Y và các biến độc lập có thể được biểu diễn dưới dạng tuyến tính, hay là còn ở các dạng khác phức tạp hơn? Xuyên suốt lịch sử của các mô hình thống kê, hầu hết các phương pháp ước lượng hàm \\(f\\) đều có dạng tuyến tính, đi kèm với đó là những giả thuyết nhất định về dữ liệu, mà đôi khi chúng rất phù hợp với thực tế dữ liệu. Tuy nhiên trong một vài trường hợp, những giả thuyết của mô hình dạng tuyến tính lại không còn phù hợp nữa, vì dữ liệu thực tế không tuân theo những giả thuyết đó mà nó lại phức tạp hơn. Khi ấy, mô hình tuyến tính có thể sẽ đưa ra những kết quả không chính xác về mối liên hệ giữa biến độc lập và biến phụ thuộc. Một vài mô hình khác với mô hình tuyến tính mà ta có thể tham khảo như: Mô hình logit, mô hình probit, mô hình hồi quy nghịch đảo, mô hình hồi quy mũ…Một ví dụ cho việc tạo mô hình thống kê suy diễn trong lĩnh vực kinh tế là mô hình giữa chi phí tiêu dùng (Consumption) và Mức thu nhập (Income), Số lượng người phụ thuộc. Theo suy nghĩ thông thường, những gia đình có mức thu nhập cao thường có xu hướng chi tiêu nhiều hơn mức bình thường. Tuy nhiên, hoàn toàn có khả năng những người đó sẽ chi tiêu tiết kiệm với mức thu nhập. Để có thể xác nhận thiên kiến này, người ta thường sẽ phải xây dựng những mô hình dựa trên dữ liệu. Ví dụ người ta nghĩ rằng khi mức thu nhập của một người tăng lên 10 triệu, người đó sẽ có xu hướng tiêu thêm 7 triệu nữa, và người ta thu thập thông tin về chi phí tiêu dùng, mức thu nhập và số lượng thành viên trong gia đình, kết quả của mô hình ra như sau:\\[\\begin{align}\nChi phí tiêu dùng = 2.5 + 2.3 * Mức thu nhập + 5 * Số lượng thành viên gia đình\n(Đơn vị của chi phí tiêu dùng và mức thu nhập là (triệu đồng))\n\\end{align}\\]Công thức bên trên có nghĩa là nếu mức thu nhập của một người tăng lên 10 triệu đồng, thì chi phí tiêu dùng của người đó sẽ tăng lên khoảng 2.3 triệu đồng. Mô hình thống kê này thể hiện sự khác biệt với quan điểm trước đó là chi phí tiêu dùng sẽ tăng lên 7 triệu nếu thu nhập tăng lên 10 triệu, vậy nên ta có thể thấy rằng mô hình toán học dựa trên dữ liệu, với định hướng thống kê suy diễn đã giúp chúng ta tránh những sai sót khi tìm cách áp dụng một quan điểm nào đó.","code":""},{"path":"real-introduction.html","id":"cách-ước-lượng-hàm-f","chapter":"Chương 3 Real Introduction","heading":"3.3 Cách ước lượng hàm f","text":"Trong quyển sách này, chúng ta sẽ được học những phương pháp tuyến tính và phi tuyến tính để ước lượng hàm f. Tuy các phương pháp này khác nhau, nhưng chúng đều có một vài điểm chung nhất định, và tôi sẽ trình bày các điểm chung đó trong phần này. Trước tiên, giả sử chúng ta có một bộ số liệu gồm n quan sát khác nhau. Các quan sát này được gọi là dữ liệu huấn luyện (training data) bởi vì chúng ta sẽ sử dụng những dữ liệu này cùng với các phương pháp thống kê để ước lượng hàm f. \nĐể thuận tiện, ta có thể coi \\(x_{ij}\\) là giá trị biến độc lập thứ j của quan sát thứ , với \\(= 1,2,...,n\\) và \\(j = 1,2,...,p\\). Tương tự, ta cũng coi \\(y_i\\) là biến phụ thuộc của quan sát thứ . Vậy nên, dữ liệu huấn luyện sẽ bao gồm \\({(x_1, y_1), (x_2, y_2), ... ,(x_n, y_n)}\\) với \\(x = (x_{i1}, x_{i2}, ... , x_{ip})^T\\).Mục tiêu cao nhất của húng ta là sử dụng các phương pháp thống kê với dữ liệu đào tạo để ước lượng hàm f, hay nói cách khác là tìm một hàm \\(\\hat{f}\\) sao cho \\(Y \\approx \\hat{f}(X)\\) với mọi quan sát (X,Y). Các phương pháp ước lượng với dữ liệu này, nói một cách tổng quát hơn, có thể chia ra làm hai loại là các phương pháp sử dụng tham số (parametric methods) và các phương pháp không sử dụng tham số (non-parametric methods).","code":""},{"path":"real-introduction.html","id":"ước-lượng-tham-số-parametric-methods","chapter":"Chương 3 Real Introduction","heading":"3.3.1 Ước lượng tham số (parametric methods)","text":"Phương pháp ước lượng tham số có thể được chia ra làm hai bước chính:Trước tiên, chúng ta phải đưa ra một giả thiết về dạng của hàm f. Ví dụ như hàm f là hàm tuyến tính, hoặc là hàm nghịch đảo, ở đây tác giả sẽ lấy ví dụ hàm f là hàm tuyến tính của X có dạng:\\[\\begin{align}\nf(X) = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + ... + \\beta_p X_p\n\\end{align}\\]Một khi chúng ta giả định rằng hàm f là hàm tuyến tính, thì vấn đề ước lượng hàm f lại đơn giản hơn rất nhiều. Đó là thay vì chúng ta phải ước lượng một hàm f(X) bất kỳ có p chiều, vốn sẽ làm chậm tốc độ tính toán của máy tính đi rất nhiều, thì chúng ta chỉ cần phải ước lượng p+1 tham số \\(\\beta_0, \\beta_1, \\beta_2, ... , \\beta_p\\).Sau khi chúng ta lựa chọn được kiểu hàm mà chúng ta muốn, chúng ta sẽ cần thực hiện thêm các quy trình kỹ thuật thống kê khác nhau với dữ liệu đào tạo để cho ra mô hình phù hợp với dữ liệu nhất.Hàm bên trên là hàm tuyến tính, một loại hàm vô cùng phổ biến và cơ bản mà hầu hết người xây dựng mô hình phải biết khi xây dựng hàm f. Người ta hướng đến việc tham số \\(\\beta_0, \\beta_1, \\beta_2, ... , \\beta_p\\) rất hiệu quả với mục tiêu là tìm ra các tham số sao cho:\\[\\begin{align}\nY \\approx \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + ... + \\beta_p X_p\n\\end{align}\\]Một khi chúng ta giả thiết rằng hàm f là hàm tuyến tính, việc ước lượng các tham số của hàm f khá là đơn giản và các nhà nghiên cứu đã chỉ ra rất nhiều phương pháp để ước lượng, chúng ta sẽ tìm hiểu kỹ hơn về cách ước lượng các tham số \\(\\beta\\) trong phần sau.Những phương pháp dựa vào việc giả thiệt mô hình trước khi thực hiện tính toán như vừa mô tả thường được gọi là phương pháp ước lượng có tham số. Phương pháp này giúp làm giảm độ phức tạp của bài toán bởi vì việc ước lượng một bộ tham số \\(\\beta_0, \\beta_1, \\beta_2, ... , \\beta_p\\) nhất định thường dễ dàng hơn là ước lượng một hàm f bất kỳ cho phù hợp với dữ liệu. Tuy nhiên, phương pháp ước lượng có tham số này vẫn có điểm hạn chế, đó là khi hàm số chúng ta giả thiết thường không trùng với hàm f thực tế (không bao giờ biết được hàm f thực tế là gì), được thể hiện qua các giá trị sai số lớn khi chúng ta thử mô hình giả thiệt trên các tập dữ liệu. Để hạn chế những sai số lớn này, người ta sẽ sử dụng những mô hình với ít điều kiện hơn là hàm tuyến tính (hàm tuyến tính là hàm yêu cầu nhiều giả thiết khá khó đạt được trong thực tế), từ đó có thể giảm thiểu sai lệch trong những ước lượng của mình. Những mô hình với ít điều kiện hơn thông thường sẽ phù hợp với nhiều dạng hàm f thực tế hơn. Tuy nhiên, nếu như chúng ta sử dụng những mô hình phức tạp, thường thì chúng ta sẽ phải mất nhiều thời gian và tài nguyên để ước lượng ra các tham số. Ngoài ra, ta cũng có thể gặp phải trường hợp quá khớp (overfitting), có nghĩa là mô hình cho ra sai số rất nhỏ trên tập dữ liệu đào tạo bởi vì mô hình đã tự điều chỉnh tham số sao cho khớp với cả nhiễu trong dữ liệu đào tạo, nhưng chúng lại cho ra sai số rất lớn trên tập dữ liệu kiểm thử, hoặc khi ứng dụng ngoài thực tế.","code":""},{"path":"real-introduction.html","id":"uớc-lượng-phi-tham-số-non---parametric-methods","chapter":"Chương 3 Real Introduction","heading":"3.4 Uớc lượng phi tham số (Non - parametric methods)","text":"Khác với phương pháp ước lượng tham số, phương pháp ước lượng phi tham số không đưa ra bất kỳ giả thiết nào về dạng hàm của f. Thay vào đó, phương pháp này sẽ đưa ra một giá trị ước lượng của f sao cho giá trị đó càng gần với các giá trị quan sát (hay còn gọi là các điểm dữ liệu) càng tốt, nhưng các giá trị ước lượng đó vẫn được tính toán sao cho chúng không quá khớp (overfitting) hay ít khớp (underfitting). Cách tiếp cận phi tham số này có lợi thế khá lớn ở chỗ chúng có thể khớp các dạng hàm khác nhau của hàm f trong thực tế một cách chính xác hơn với cách tiếp cận tham số, chúng không đặt ra trước bất kỳ giả thuyết nào về hàm f, nên có thể tránh việc các tính toán ước lượng của chúng ta bị sai lệch mô hình không phù hợp. Khi người ta sử dụng phương pháp phi tham số, có nghĩa là họ đã cho rằng dữ liệu của họ (thể hiện một phần hàm f thực tế) có nhiều khả năng sẽ không phù hợp với các mô hình thông thường đã được biết trước (tuyến tính, nghịch đảo, hồi quy mũ). Phương pháp ước lượng phi tham số hoàn toàn tránh được rủi ro ước lượng sai giả thiết về hàm f không chính xác, tuy nhiên chúng cũng có những hạn chế nhất định khiến người ta phải cân nhắc trước khi áp dụng vào thực tế. Hạn chế đó nằm ở khả năng tính toán, bởi vì các bài toán ước lượng hàm f theo phương pháp phi tham số không được chuyển thành bài toán ước lượng một dãy tham số, có nghĩa là phương pháp này phải sử dụng một lượng quan sát lớn hơn nhiều với phương pháp tham số để có thể ước lượng hàm f với độ chính xác cao.Thêm một ví dụ nói về việc overfitting khi sử dụng ước lượng phi tham sốTrên đây là những điểm mạnh, điểm yếu của hai phương pháp ước lượng có tham số và ước lượng phi tham số, chúng ta sẽ tìm hiểu sâu thêm về một vài điển hình của các phương pháp đó trong quyển sách này.","code":""},{"path":"real-introduction.html","id":"đánh-đổi-giữa-khả-năng-dự-báo-chính-xác-prediction-accuracy-và-khả-năng-diễn-giải-mô-hình-model-interpretability","chapter":"Chương 3 Real Introduction","heading":"3.4.1 Đánh đổi giữa khả năng dự báo chính xác (prediction accuracy) và khả năng diễn giải mô hình (model interpretability)","text":"Trong các phương pháp được tác giả sử dụng ở trong quyển sách này, có những phương pháp linh hoạt (flexible, định nghĩa flexible chưa được nhắc đến), nhưng cũng có phương pháp hạn chế (restrictive) hơn thông thường, có nghĩa là các mô hình sử dụng sẽ giả thiết một dạng hàm f nào đó linh hoạt, hoặc khắt khe hơn thông thường. Ví dụ, hàm tuyến tính là một dạng hàm khá đợn giản, dễ giải thích, còn những mô hình mạng neural (được sử dụng nhiều trong trí tuệ nhân tạo) lại rất phức tạp vì bản chất của mô hình mạng neural cho phép người sử dụng tạo ra một hàm f với các hình dạng linh hoạt hơn rất nhiều và khiến con người khó giải thích sự liên quan giữa các biến trong dữ liệu. Một câu hỏi thường được đặt ra là: Tại sao người ta lại phải sử dụng mô hình linh hoạt, có khả năng giải thích kém hơn thay vì những mô hình hạn chế, nhưng lại dễ giải thích hơn?. Lý là bởi vì, dựa trên thực nghiệm trên rất nhiều dữ liệu khác nhau, hàm tuyến tính lại thường đưa ra những dự đoán kém chính xác hơn với mô hình mạng neural, vậy nên đôi khi người ta phải đánh đổi giữa khả năng dự báo chính xác và khả năng diễn giải khi xây dựng mô hình dựa trên dữ liệu. Chúng ta cũng cần phải nhớ rằng, tuỳ thuộc vào mục đích xây dựng mô hình, người ta sẽ có những lựa chọn ưu tiên khả năng dự báo hoặc khả năng diễn giải. Đối với mục đích mô hình là suy diễn, tìm sự liên hệ giữa các biến, thì người ta sẽ lựa chọn phương pháp hạn chế để dễ diễn giải hơn. Ngược lại, nếu như ta sử dụng mô hình với mục đích dự đoán, thì người ta sẽ không quan tâm đến khả năng diễn giải nữa mà hoàn toàn có thể ưu tiên sử dụng mô hình linh hoạt hơn, với kỳ vọng mang lại dự đoán chính xác hơn.","code":""},{"path":"real-introduction.html","id":"phương-pháp-học-có-giám-sát-và-phương-pháp-học-không-giám-sát","chapter":"Chương 3 Real Introduction","heading":"3.4.2 Phương pháp học có giám sát và phương pháp học không giám sát","text":"Hầu hết các phương pháp học thống kê hiện nay đều chia ra làm hai loại là phương pháp học có giám sát và và phương pháp học không giám sát. Ta có thể miêu tả phương pháp học có giám sát như sau: Cho một dữ liệu, đối với mỗi quan sát chúng ta sẽ có các biến dự báo (predictors) \\(x_i\\), = 1,2,… và biến mục tiêu (response) y tương ứng với mỗi quan sát. Chúng ta muốn tìm một mô hình cho thấy sự liên quan giữa biến mục tiêu (response) và biến dự báo (predictors), với mục tiêu là đoán các giá trị tương lai (dự báo) hoặc để hiểu rõ hơn mối quan hệ giữa biến mục tiêu và biến dự báo (suy diễn). Ta có thể liệt kê ra rất nhiều phương pháp học có giám sát như: hồi quy tuyến tính, hồi quy logistic, GAM, boosting, Support Vector Machine (SVM).Còn đối với phương pháp học không giám sát, phương pháp này có chút khó khăn hơn với phương pháp học có giám sát bởi dữ liệu của chúng không hề chỉ rõ ra đâu là biến mục tiêu \\(y_i, = 1,2,...\\) mà chỉ có các biến \\(x_i\\). Trong trường hợp này, chúng ta không thể sử dụng các phép biến đổi mô hình tuyến tính như trên vì không xác định được biến mục tiêu. Nói một cách đơn giản, khi sử dụng phương pháp học không giám sát, chúng ta xây dựng các phương pháp thống kê mà không có sự rõ ràng từ đầu, giống như là chúng ta đi trong một khu rừng dữ liệu nhưng mà bị bịt mắt và không nhìn thấy đường đi. Câu hỏi đặt ra là: Những phương pháp phân tích thống kê nào là khả dĩ trong trường hợp này? Một hướng đi phổ biến đó là sử dụng dữ liệu để tìm ra mối liên hệ giữa các biến trong dữ liệu đó, và một phương pháp rất cơ bản mà chúng ta cần phải biết đó là phương pháp phân cụm (cluster analysis hoặc clustering). Mục tiêu của phương pháp phân cụm là phương pháp sử dụng thống kê để tăng sự chắc chắn khi phân nhóm các quan sát vào các nhóm khác nhau, dựa trên những biến và quan sát mà chúng ta đang có. Trong bài toán phân loại khách hàng, thông thường, chúng ta có một nhóm các khách hàng, và chúng ta biết được các thông tin cơ bản của các vị khách đó như tuổi, nghề nghiệp, lương, sở thích…, thì chúng ta có thể sử dụng phương pháp phân cụm để phân nhóm các khách hàng đó vào các loại khác nhau, chẳng hạn như Khách hàng giá trị cao, Khách hàng giá trị trung bình, Khách hàng giá trị thấp. Cần chú ý rằng, với phương pháp học không giám sát này, chúng ta đang giả định các khách hàng đó thuộc ba nhóm Khách hàng giá trị cao, Khách hàng giá trị trung bình, Khách hàng giá trị thấp bởi vì chúng ta không có thông tin chính xác về mức độ chi tiêu của các khách hàng này. Nếu chúng ta có những thông tin đó, chúng ta có thể sử dụng phương pháp học có giám sát với biến mục tiêu (response) là biến “Loại khách hàng”. Tuy nhiên, bởi vì những thông tin đó bị ẩn đi, nên chúng ta phải sử dụng phương pháp phân cụm để phân loại các nhóm. Việc phân loại này rất có ích bởi vì chúng ta có thể đưa ra những chiến lược kinh doanh khác nhau cho từng đối tượng, phục vụ cho mục đích của chúng ta, thường là để tối đa hoá lợi nhuận thu được.Phương pháp phân cụm có thể được thể hiện qua hình [Điền tên hình]. Như các bạn có thể thấy, chúng ta chỉ có thể biểu diễn phương pháp phân cụm với hai trục (2 biến), hoặc tối đa là ba trục (ba biến). Để thực hiện phương pháp phân cụm bằng mắt với một dữ liệu nhiều chiền hơn, ta không thể sử dụng phương pháp phân cụm bằng đồ thị. Trong đồ thị [Điền tên đồ thị], giả sử chúng ta có một tập dữ liệu có p biến, thì chúng ta sẽ cần vẽ \\(\\dfrac{p(p-1)}{2}\\) đồ thị để xác định các cụm bằng mắt. Tuy nhiên, con người thường không thể phân cụm được các quan sát chỉ bằng cách nhìn vào các đồ thị. Vậy nên, việc phát triển các phương pháp phân cụm tự động, tiên tiến hơn là điều rất cần thiết và quan trọng. Trong phần [Điền tên phần], quyển sách này sẽ giới thiệu kỹ hơn về các phương pháp phân cụm nói riêng, và các phương pháp học thống kê phi tham số nói chung.Rất nhiều bài toán ứng dụng thực tế rơi vào bối cảnh của thuật toán học có giám sát hoặc không giám sát. Tuy nhiên, có một vài trường hợp bài toán có thể không được phân loại rõ ràng vào loại học có giám sát hay không giám sát. Đơn giản như ta có một tập dữ liệu với n quan sát, với m (m < n) quan sát đầu tiên, chúng ta có cả biến mục tiêu (response) và biến dự báo (predictors), nhưng với n - m quan sát còn lại, chúng ta chỉ có các biến dự báo (predictors) mà không còn biến mục tiêu (response) nữa. Những dữ liệu như vậy có thể xuất hiện trong trường hợp chi phí để thu thập dữ liệu cho các biến dự báo khá rẻ, trong khi đó các biến mục tiêu để thu thập thì cần tốn kém hơn rất nhiều. Chúng ta có thể gọi trường hợp này là bài toán học bán giám sát (semi-supervised learning problem). Đối với trường hợp này, chúng ta sẽ cần một phương pháp xử lý dữ liệu thống kê hiệu quả cho cả m quan sát có biến mục tiêu và n - m quan sát không có biến mục tiêu, tuy nhiên trong quyển sách này chúng tôi sẽ không giới thiệu các phương pháp đó.","code":"\n# Load necessary libraries\nlibrary(MASS)## \n## Attaching package: 'MASS'## The following object is masked from 'package:dplyr':\n## \n##     select\nlibrary(ggplot2)\nlibrary(dplyr)\n\n# Generate synthetic data\nset.seed(123) # For reproducibility\n\n# Parameters for the clusters\nmeans <- list(\n  c(2, 3, 4, 5, 6, 7),   # Mean for cluster 1\n  c(5, 7, 6, 5, 4, 3),   # Mean for cluster 2\n  c(8, 9, 7, 6, 5, 4),   # Mean for cluster 3\n  c(3, 2, 1, 2, 3, 4)    # Mean for cluster 4\n)\n\nsigma <- matrix(c(1,0.5,0,0,0,0,  0.5,1,0.5,0,0,0,  0,0.5,1,0.5,0,0,  0,0,0.5,1,0.5,0,  0,0,0,0.5,1,0.5,  0,0,0,0,0.5,1), 6) # Covariance matrix\n\n# Generate observations\ndata_list <- lapply(means, function(mu) mvrnorm(n = 25, mu = mu, Sigma = sigma))\ndata <- do.call(rbind, data_list)\ndata <- as.data.frame(data)\ndata$cluster <- factor(rep(1:4, each = 25))\n\n# Plotting function\nplot_pair <- function(df, var1, var2) {\n  ggplot(df, aes_string(x = var1, y = var2, color = \"cluster\")) + \n    geom_point() + \n    theme_minimal() + \n    labs(x = var1, y = var2, title = paste(\"Scatter plot of\", var1, \"vs\", var2)) +\n    scale_color_manual(values=c(\"red\", \"green\", \"blue\", \"orange\")) +\n    theme(legend.title = element_blank())\n}\n\n# Create plots\nplot_list <- list()\nvariable_names <- names(data)[1:6]\n\nfor (i in 1:(length(variable_names)-1)) {\n  for (j in (i+1):length(variable_names)) {\n    plot_list[[paste(variable_names[i], variable_names[j], sep = \"_\")]] <- plot_pair(data, variable_names[i], variable_names[j])\n  }\n}## Warning: `aes_string()` was deprecated in ggplot2 3.0.0.\n## ℹ Please use tidy evaluation idioms with `aes()`.\n## ℹ See also `vignette(\"ggplot2-in-packages\")` for more information.\n## This warning is displayed once every 8 hours.\n## Call `lifecycle::last_lifecycle_warnings()` to see where this warning was\n## generated.\n# Display the first plot as an example\nprint(plot_list[[1]])\n# To view other plots, you can print them one by one:\n# print(plot_list[[\"V1_V2\"]]) # For example\n\nlibrary(gridExtra)## \n## Attaching package: 'gridExtra'## The following object is masked from 'package:dplyr':\n## \n##     combine\n# Assuming 'plot_list' contains all 15 ggplot objects from the previous example\n# Arrange plots in a 3x5 grid\ngrid_layout <- do.call(grid.arrange, c(plot_list, ncol=5, nrow=3))"},{"path":"real-introduction.html","id":"bài-toán-hồi-quy-và-bài-toán-phân-loại","chapter":"Chương 3 Real Introduction","heading":"3.4.3 Bài toán hồi quy và bài toán phân loại","text":"Các biến thường được chia ra làm hai loại rõ ràng là biến định lượng và biến định tính. Biến định lượng có thể liệt kê ra một vài ví dụ tiêu biểu như tuổi, thu nhập, số lượng nhà đang sở hữu…. Trong khi đó, biến định tính là các biến nhận một giá trị trong K loại khác nhau, ví dụ như tình trạng hôn nhân (Đã kết hôn/Chưa kết hôn), tình hình tín dụng (Đã vỡ nợ/Chưa vỡ nợ), tình hình bệnh tiểu đường (Không bị/Tuýp 1/Tuýp 2/Tuýp 3). Chúng ta thường phân các bài toán mà biến mục tiêu (response) thuộc loại biến định lượng là bài toán hồi quy, trong khi biến mục tiêu thuộc loại biến định tính thì gọi là bài toán phân loại. Dù trong nhiều trường hợp việc phân loại này hoạt động rất tốt, rất rõ ràng, nhưng đôi khi cũng tồn tại vài trường hợp mà các bài toán không được phân loại thực sự rõ ràng vào hai loại này. Các bài toán không được phân loại hẳn vào một loại ta có thể lấy một vài ví dụ như bài toán hồi quy logistic (chương 4) với biến mục tiêu là biến định tính, vốn luôn được coi là thuộc lớp bài toán phân loại, nhưng vì bản chất chúng tính toán xác suất mà biến mục tiêu rơi vào loại k (ví dụ như xác suất người bị tiểu đường bị tiểu đường Tuýp 2), nên chúng cũng có thể coi là bài toán hồi quy. Hay như các phương pháp như K - mean neighboring (KNN) hoặc Boosting đều cũng đều có thể sử dụng cho cả bài toán phân loại và bài toán hồi quy.Thông thường, chúng ta sẽ dựa vào các biến mục tiêu là biến định tính hay biến định lượng để xác định bài toán của chúng ta là bài toán phân loại hay bài toán hồi quy, còn những biến dự báo (predictors) thuộc loại gì thì thường không ảnh hưởng đến cách chúng ta phân loại bài toán. Thực tế, hầu hết những phương pháp thống kê trong quyển sách này đều sử dụng được bất kể biến dụ báo là biến định lượng hay biến định tính, nếu là biến định tính chúng ta thường có phương pháp mã hoá biến trước khi thực hiện xây dựng mô hình thống kê.","code":""},{"path":"real-introduction.html","id":"đánh-giá-sự-chính-xác-của-mô-hình","chapter":"Chương 3 Real Introduction","heading":"3.5 Đánh giá sự chính xác của mô hình","text":"Mục đích chính của quyển sách này là đưa người đọc đi sâu hơn vào thế giới của thống kê, nơi mà những phương pháp đã được xây dựng vượt xa hoàn toàn phương pháp hồi quy tuyến tính truyền thống. Tại sao chúng ta cần phải học nhiều phương pháp thống kê thay vì chỉ học một phương pháp thống kê tốt nhất? Bởi vì không bao giờ có gì là miễn phí hoàn toàn (free lunch) trong thống kê, có nghĩa là không có một mô hình nào vượt trội hoàn toàn với các mô hình khác trong mọi loại dữ liệu. Trong một bộ dữ liệu, có một phương pháp thống kê nào đó cho ra kết quả tốt nhất, nhưng cũng có thể sẽ có một phương pháp thống kê khác cho ra kết quả tốt hơn trên một bộ dữ liệu khác gần như tương tự. Vậy nên, một trong những công việc quan trọng của các nhà thống kê đó là chọn lựa phương pháp thống kê tốt nhất khi gặp phải một bộ dữ liệu bất kỳ, đây là một trong những công việc khó và phức tạp nhất trong thực tế. Trong phần này, chúng tôi sẽ thảo luân về một vài khái niệm sẽ xuất hiện khi chúng ta lựa chọn một mô hình thống kê nào đó phù hợp với dữ liệu của chúng ta, và chúng tôi cũng sẽ lồng ghép các khái niệm được thể hiện ở đây vào các bài toán thực tế trong phần sau của quyển sách này.","code":""},{"path":"real-introduction.html","id":"đo-độ-khớp-của-mô-hình","chapter":"Chương 3 Real Introduction","heading":"3.5.1 Đo độ khớp của mô hình","text":"Để đánh giá hiệu quả của một phương pháp thống kê trên một tập dữ liệu nhất định, chúng ta cần phải theo dõi xem các dự đoán của mô hình khớp với các giá trị dữ liệu quan sát được đến đâu. Từ đó, người ta đã nghĩ ra các phương pháp để lượng hoá sự gần sát của các dự đoán và các giá trị dữ liệu quan sát thực tế. Trong các bài toán hồi quy, giá trị đo thường được người ta sử dụng nhất là trung bình bình phương sai số (MSE), được cho bởi công thức\\[\\begin{align}\nMSE = \\dfrac{1}{n} \\sum\\limits_{= 1}^{n} \\left[y_i - \\hat{f}(x_i)\\right]^2\n\\tag{3.1}\n\\end{align}\\]trong đó \\(\\hat{f}(x_i)\\) là dự đoán của hàm đối với quan sát thứ . Giá trị trung bình bình phương sai số (MSE) sẽ càng trở nên nhỏ nếu như giá trị dự đoán càng gần với giá trị dữ liệu quan sát thực tế, và càng lớn nếu như có một vài quan sát có kết quả dự đoán cách xa với giá. trị dữ liệu quan sát thực tế. Phương trình (3.1) được tính dựa trên tập dữ liệu huấn luyện mà được sử dụng để tạo ra hàm khớp với mô hình. Vậy nên nó thường được gọi bằng cái tên chính xác hơn là trung bình bình phương sai số của tập dữ liệu huấn luyện (training MSE). Dù vậy, trong thực tế, chúng ta không quá quan tâm đến sự chính xác của mô hình trên dữ liệu huấn luyện mà câu hỏi quan trọng hơn là: Sự chính xác của mô hình khi chúng ta áp dụng nó vào các quan sát trong tập dữ liệu kiểm thử liệu có còn tốt hay không?\nChúng ta quan tâm đến các số liệu này bởi vì những người xây dựng mô hình để dự báo luôn muốn xây dựng một mô hình có khả năng dự đoán tương lai tốt nhất. Ta lấy ví dụ những chuyên gia muốn xây dựng một mô hình để dự đoán giá cổ phiếu trong tương lai, dựa vào dữ liệu giá cổ phiếu trong 6 tháng trước. Khi đó, sẽ không có nhiều ý nghĩa nếu mô hình cho ra kết quả dự đoán dữ liệu giá cổ phiếu 2 tuần trước gần với giá trị đã được ghi nhận, bởi vì đó là thông tin chúng ta và cả mô hình đều được biết rồi, mà quan trọng là mô hình đó phải dự báo được khá chính xác giá cổ phiếu trong tương lai (thông tin chưa ai biết) để các nhà đầu tư có thể đưa ra chiến lược phù hợp. Tương tự, đối với bài toán dự đoán khả năng bị tiểu đường của các bệnh nhân, chúng ta cũng thường chỉ quan tâm đến khả năng mô hình được xây dựng dự báo chính xác những bệnh nhân trong tương lai, chứ không phải là dự đoán xem những bệnh nhân trước đó bị tiểu đường loại nào. Thực tế, ta không hề có dữ liệu về những quan sát ở trong tương lai, nên để kiểm tra mô hình ta thường chia dữ liệu của chúng ta thành ít nhất hai tập dữ liệu huấn luyện và tập dữ liệu kiểm thử. Bởi vì khi tập dữ liệu kiểm thử không được đưa vào để huấn luyện mô hình, có nghĩa là những dữ liệu đó sẽ được coi như là dữ liệu mới chưa bao giờ gặp trong mô hình, nên nếu như mô hình có thể dự đoán với độ chính xác cao những quan sát trong tập dữ liệu kiểm thử thì người ta kỳ vọng rằng nó cũng sẽ chạy tốt khi áp dụng với những quan sát trong tương lai.Để diễn tả một cách toán học, giả sử chúng ta đang muốn khớp một phương pháp thống kê nào đó dựa trên tập dữ liệu huấn luyện \\({(x_1, y_1), (x_2, y_2),...,(x_n, y_n)}\\), kết quả chúng ta sẽ thu được một hàm \\(\\hat{f}\\). Từ đó chúng ta có thể tính các giá trị \\(\\hat{f}(x_1), \\hat{f}(x_2),..., \\hat{f}(x_n)\\). Nếu những giá trị đó mà gần với các giá trị \\(y_1, y_2,...,y_n\\) thì trung bình bình phương sai số của tập dữ liệu huấn luyện (training MSE) được cho bởi công thức (3.1) sẽ rất nhỏ. Nhưng chúng ta lại quan tâm hơn đến liệu giá trị \\(\\hat{f}(x_0)\\) có xấp xỉ bằng \\(y_0\\) hay không, với \\((x_0, y_0)\\) là một quan sát thuộc tập dữ liệu kiểm thử, chưa được đưa vào để tạo ra hàm \\(\\hat{f}\\). Các chuyên gia thường muốn lựa chọn được mô hình có trung bình bình phương sai số trên tập dữ liệu kiểm thử (test MSE) nhỏ nhất. Nói cách khác, nếu chúng ta có một lượng lớn các dữ liệu trong tập dữ liệu kiểm thử, chúng ta có thể tính\\[\\begin{align}\nAverage\\left[y_0 - \\hat{f}(x_0)\\right]^2\n\\end{align}\\]giá trị trung bình bình phương sai số trên tập dữ liệu kiểm thử với các quan sát \\((x_0, y_0)\\), và chúng ta thường lựa chọn mô hình sao cho giá trị này càng nhỏ càng tốt.","code":""},{"path":"real-introduction.html","id":"sự-đánh-đổi-giữa-độ-chệch-và-phương-sai","chapter":"Chương 3 Real Introduction","heading":"3.6 Sự đánh đổi giữa độ chệch và phương sai","text":"Dáng đường cong chữ U (U-shape) của đường sai số bình phương trung bình trên tập dữ liệu kiểm thử thể hiện hai tính chất rất quan trọng của một phươung pháp học thống kê hiện đại. Cần phải nhắc lại rằng trong quyển sách này chúng tôi sẽ không đưa ra những chứng minh toán học quá phức tạp, nhưng để thể hiện tính chất đánh đổi giữa độ chệch và phương sai này, chúng ta vẫn có thể đi từ một công thức đơn giản liên quan đến giá trị kỳ vọng của sai số bình phương trung bình của một giá trị \\(x_0\\). Chúng ta có thể tách giá trị kỳ đó ra thành ba phần chính: phương sai của \\(\\hat{f}(x_0)\\), độ chệch bình phương của \\(\\hat{f}(x_0)\\), và phương sai của sai số \\(\\epsilon\\) như sau:\\[\\begin{equation}\nE\\left(y_0 - \\hat{f}(x_0)\\right)^2 = Var\\left(\\hat{f}(x_0)\\right) + \\left[Bias(\\hat{f}(x_0))\\right]^2 + Var(\\epsilon)\n\\label{eq:bias_variance}\n\\end{equation}\\]Trong đó, \\(E\\left(y_0 - \\hat{f}(x_0)\\right)^2\\) được định nghĩa là giá trị kỳ vọng của trung bình bình phương sai số trên tập dữ liệu kiểm thử với quan sát \\(x_0\\). Nếu chúng ta lặp đi lặp lại quá trình ước lượng các hàm f này với một lượng lớn tập dữ liệu huấn luyện, và test mỗi mô hình với quan sát \\(x_0\\). Giá trị kỳ vọng tổng thể của trung bình bình phương sai số (overall expected test MSE) có thể được tính bằng cách lấy trung bình của \\(E\\left(y_0 - \\hat{f}(x_0)\\right)^2\\) với tất cả các quan sát \\(x_0\\) trong tập dữ liệu kiểm thử.Phương trình \\(\\ref{eq:bias_variance}\\) cho chúng ta biết rằng nếu muốn giảm giá trị sai số kỳ vọng trên tập dữ liệu kiểm thử, chúng ta cần chọn phương pháp học thống kê sao cho đồng thời đạt được cả hai mục tiêu là phương sai thấp và độ chệch thấp. Chúng ta cũng cần lưu ý rằng giá trị MSE kỳ vọng không bao giờ thấp hơn Var(\\(\\epsilon\\)), đó là phần sai số không thể giảm trong công thức \\(\\ref{eq:re_irre_error}\\).","code":""},{"path":"real-introduction.html","id":"phương-sai-của-mô-hình","chapter":"Chương 3 Real Introduction","heading":"3.6.1 Phương sai của mô hình","text":"Vậy thì rốt cuộc, chúng ta nên hiểu phương sai (variance) và độ chệch (bias) của phương pháp học thống kê như thế nào cho đúng? Phương sai (variance) nói đến lượng thay đổi của hàm \\(\\hat{f}\\) khi chúng ta ước lượng hàm \\(\\hat{f}\\) bằng một dữ liệu huấn luyện khác. Bởi vì dữ liệu huấn luyện được sử dụng để ước lượng hàm \\(\\hat{f}\\), nên việc chúng ta sử dụng dữ liệu huấn luyện khác nhau sẽ cho ra hàm \\(\\hat{f}\\) khác nhau. Tuy nhiên, trong trường hợp lý tưởng, ước lượng cho hàm f không nên chênh lệch quá nhiều giữa các dữ liệu huấn luyện khác nhau. Nếu một phương pháp học thống kê nào đó có sự thay đổi lớn về hàm \\(\\hat{f}\\) dù chúng ta chỉ thay đổi dữ liệu đầu vào một chút, có nghĩa là phương pháp học thống kê đó có phương sai cao (high variance). Nhìn chung, các phương pháp học thống kê càng linh hoạt (ví dụ như neural networks, decision tree, random forests) thường có phương sai càng cao.Ở đây tiếp tục simulate ra một dữ liệu nữa để thể hiện phương sai cao của phương pháp flexible","code":"\n# Define the number of variables\nnumber_of_variables <- 1:15\n\n# Train MSE decreases exponentially from 7 to below 0.3\ntrain_mse <- 7 * exp(-0.5 * number_of_variables)\n# Manually set the last value to ensure it's just below 0.3\ntrain_mse[length(train_mse)] <- 0.3\n\n# Test MSE: starts at 7.5, decreases to a point, then increases again\ntest_mse <- numeric(length(number_of_variables))\ntest_mse[1] <- 7.5 # Starting point for Test MSE\n\n# Calculate Test MSE values\nfor (i in 2:length(test_mse)) {\n  if (i <= 4) {\n    # Decreasing phase for the first 4 variables\n    test_mse[i] <- test_mse[i-1] - 0.5\n  } else {\n    # Increasing phase after the 4th variable\n    test_mse[i] <- 6 + 0.1 * (i - 4)^2\n  }\n}\n\n# Ensure Test MSE is always at least 2 units greater than Train MSE\ntest_mse <- pmax(test_mse, train_mse + 2)\n\n# Plotting\nplot(number_of_variables, train_mse, type='l', pch=19, col='blue', ylim=c(0, max(test_mse)), xlim = c(1,14), ylab='MSE', xlab='Number of Variables', main='Train vs Test MSE')\nlines(number_of_variables, test_mse, type='l', pch=19, col='red', lty=2)\n#points(number_of_variables, test_mse, pch=19, col='red') # Adding points for test MSE\n\nlegend(\"topright\", legend=c(\"Train MSE\", \"Test MSE\"), col=c(\"blue\", \"red\"), pch=19, lty=1:2)\n# Required libraries\nlibrary(caret)## Loading required package: lattice\nset.seed(42)\n\n# Simulating data with 15 independent variables\nn <- 200 # Number of observations\np <- 15 # Number of variables\n\n# Simulate relevant variables with stronger signal\nnum_relevant <- 5\nX_relevant <- matrix(rnorm(n * num_relevant), ncol = num_relevant)\nbeta_relevant <- runif(num_relevant, -2, 2) # Random coefficients for relevant variables\n\n# Simulate irrelevant variables with weaker signal (noise)\nnum_irrelevant <- p - num_relevant\nX_irrelevant <- matrix(rnorm(n * num_irrelevant), ncol = num_irrelevant)\nbeta_irrelevant <- runif(num_irrelevant, -0.1, 0.1) # Random coefficients for irrelevant variables\n\n# Combine relevant and irrelevant variables\nX <- cbind(X_relevant, X_irrelevant)\nbeta <- c(beta_relevant, beta_irrelevant)\n\n# Noise\nepsilon <- rnorm(n)\n\n# Response variable\nY <- X %*% beta + epsilon\n\n# Splitting the data into training and testing sets\nset.seed(42)\ntrainIndex <- createDataPartition(Y, p = .8, list = FALSE)\nX_train <- X[trainIndex, ]\nY_train <- Y[trainIndex]\nX_test <- X[-trainIndex, ]\nY_test <- Y[-trainIndex]\n\n# Initialize vectors to store metrics\ntrain_mse <- numeric(p)\ntest_mse <- numeric(p)\n\n# Fit models and calculate MSE for training and testing sets\nfor (i in 1:p) {\n  model <- lm(Y_train ~ X_train[, 1:i])\n  train_predictions <- predict(model, data.frame(X_train[, 1:i]))\n  test_predictions <- predict(model, data.frame(X_test[, 1:i]))\n  \n  # Calculating MSE\n  train_mse[i] <- mean((Y_train - train_predictions)^2)\n  test_mse[i] <- mean((Y_test - test_predictions)^2)\n}## Warning: 'newdata' had 40 rows but variables found have 160 rows\n\n## Warning: 'newdata' had 40 rows but variables found have 160 rows\n\n## Warning: 'newdata' had 40 rows but variables found have 160 rows\n\n## Warning: 'newdata' had 40 rows but variables found have 160 rows\n\n## Warning: 'newdata' had 40 rows but variables found have 160 rows\n\n## Warning: 'newdata' had 40 rows but variables found have 160 rows\n\n## Warning: 'newdata' had 40 rows but variables found have 160 rows\n\n## Warning: 'newdata' had 40 rows but variables found have 160 rows\n\n## Warning: 'newdata' had 40 rows but variables found have 160 rows\n\n## Warning: 'newdata' had 40 rows but variables found have 160 rows\n\n## Warning: 'newdata' had 40 rows but variables found have 160 rows\n\n## Warning: 'newdata' had 40 rows but variables found have 160 rows\n\n## Warning: 'newdata' had 40 rows but variables found have 160 rows\n\n## Warning: 'newdata' had 40 rows but variables found have 160 rows\n\n## Warning: 'newdata' had 40 rows but variables found have 160 rows\n# Plotting\nplot(1:p, train_mse, type = \"b\", pch = 19, col = \"blue\", xlab = \"Number of Variables\", ylab = \"MSE\", ylim = c(min(c(train_mse, test_mse)), max(c(train_mse, test_mse))), main = \"MSE, Bias, and Variance vs Model Complexity\")\npoints(1:p, test_mse, type = \"b\", pch = 23, col = \"red\", bg = \"red\")\nlegend(\"topright\", legend = c(\"Train MSE\", \"Test MSE\"), col = c(\"blue\", \"red\"), pch = c(19, 23))\nset.seed(42)\nlibrary(MASS) # For true randomness in simulation\n\n# Simulating data\nn <- 1000 # Number of observations\np <- 15 # Number of independent variables\nX <- matrix(rnorm(n * p), ncol = p)\ntrue_beta <- rnorm(p, mean = 0.2, sd = 0.5) # True coefficients\nY <- X %*% true_beta + rnorm(n) # Response with noise\n\n# Train-test split\ntrain_proportion <- 0.5\ntrain_sample <- sample(1:n, size = floor(n * train_proportion))\nX_train <- X[train_sample, ]\nY_train <- Y[train_sample]\nX_test <- X[-train_sample, ]\nY_test <- Y[-train_sample]\n\n# Calculate squared bias, variance, and MSE\ncalc_metrics <- function(i, X_train, Y_train, X_test, Y_test, true_beta) {\n  # Bootstrap predictions for variance estimation\n  n_bootstraps <- 100\n  n_test <- length(Y_test)\n  predictions_test <- matrix(nrow = n_bootstraps, ncol = n_test)\n  \n  for (b in 1:n_bootstraps) {\n    sample_indices <- sample(nrow(X_train), nrow(X_train), replace = TRUE)\n    X_train_b <- X_train[sample_indices, , drop = FALSE]\n    Y_train_b <- Y_train[sample_indices]\n    model_b <- lm(Y_train_b ~ X_train_b[, 1:i, drop = FALSE])\n    # Ensuring that newdata has the correct number of columns\n    predictions_test[b, ] <- predict(model_b, newdata = as.data.frame(X_test[, 1:i, drop = FALSE]))\n  }\n  \n  # Bias^2 = (E[Ŷ] - Y_true)^2 where Y_true is the true response\n  Y_true <- X_test[, 1:i, drop = FALSE] %*% true_beta[1:i]\n  expected_predictions <- rowMeans(predictions_test)\n  bias_squared <- mean((mean(predictions_test) - Y_true)^2, na.rm = TRUE)\n  \n  # Variance = E[(Ŷ - E[Ŷ])^2]\n  variance <- mean((predictions_test - mean(predictions_test, na.rm = TRUE)^2))\n  \n  # Test MSE\n  mse_test <- mean((expected_predictions - Y_test)^2, na.rm = TRUE)\n  \n  return(c(bias_squared, variance, mse_test))\n}\n\n# Initialize matrices to store metrics\nresults <- matrix(ncol = 3, nrow = p)\ncolnames(results) <- c(\"Bias^2\", \"Variance\", \"MSE\")\n\n# Calculate metrics for models with increasing complexity\nfor (i in 1:p) {\n  results[i, ] <- calc_metrics(i, X_train, Y_train, X_test, Y_test, true_beta)\n}\n\n# Adding the irreducible error to MSE\nirreducible_error <- var(Y - X %*% true_beta)\nresults[, \"MSE\"] <- results[, \"Bias^2\"] + results[, \"Variance\"] + irreducible_error## Warning in results[, \"Bias^2\"] + results[, \"Variance\"] + irreducible_error: Recycling array of length 1 in vector-array arithmetic is deprecated.\n##   Use c() or as.vector() instead.\n# Plotting the results\nplot(1:p, results[, \"Bias^2\"], type = \"l\", col = \"blue\", ylim = c(0, max(results)), ylab = \"Metric\", xlab = \"Number of Variables\", main = \"Model Complexity vs Error Components\")\nlines(1:p, results[, \"Variance\"], col = \"orange\")\nlines(1:p, results[, \"MSE\"], col = \"red\")\nabline(h = irreducible_error, lty = 2, col = \"black\")\nlegend(\"topright\", legend = c(\"Squared Bias\", \"Variance\", \"Test MSE\", \"Irreducible Error\"), col = c(\"blue\", \"orange\", \"red\", \"black\"), lty = c(1, 1, 1, 2))"},{"path":"real-introduction.html","id":"độ-chệch-của-mô-hình","chapter":"Chương 3 Real Introduction","heading":"3.6.2 Độ chệch của mô hình","text":"Ở một khía cạnh khác, độ chệch (bias) là từ được dùng để nhắc tới sai số được tạo ra khi ước lượng một vấn đề thực tế (có thể rất phức tạp), bằng một mô hình hàm \\(\\hat{f}\\) đơn giản. Ví dụ, mô hình hồi quy tuyến tính thường giả định rằng có mối liên hệ tuyến tính giữa \\(Y\\) và các giá trị \\(X_1, X_2, \\cdot, X_p\\). Tuy nhiên, hầu như không có hiện tượng nào trong thực tế đều có một mối quan hệ hồi quy tuyến tính hoàn toàn, vậy nên mô hình hồi quy tuyến tính sẽ luôn có độ chệch khi ước lượng hàm f. Nhìn chung, phương pháp càng linh hoạt (flexible) thường cho ra kết quả có độ chệch càng thấp.","code":""},{"path":"real-introduction.html","id":"sự-đánh-đổi-giữa-độ-chệch-và-phương-sai-1","chapter":"Chương 3 Real Introduction","heading":"3.6.3 Sự đánh đổi giữa độ chệch và phương sai","text":"Sự đánh đổi này xuất hiện như một quy luật mà chúng ta cần phải nhớ, rằng khi chúng ta sử dụng những mô hình linh hoạt hơn, thì phương sai (variance) sẽ tăng và độ chệch (bias) sẽ giảm. Sự thay đổi của hai giá trị liên quan tới nhau, và sẽ xác định giá trị MSE của tập dữ liệu kiểm thử tăng hay giảm. Khi chúng ta tăng sự phức tạp (flexibility) của mô hình, độ chệch (bias) có xu hướng giảm nhanh hơn là phương sai (variance), dẫn đến giá trị kỳ vọng của MSE trên tập dữ liệu kiểm thử giảm. Tuy nhiên, đến một thời điểm nào đó, việc tăng sự phức tạp (flexibility) của mô hình chỉ giảm độ chệch (bias) đi một chút, nhưng lại làm tăng phương sai (variance) nhanh hơn nhiều, và kết quả là giá trị kỳ vọng của MSE trên tập dữ liệu kiểm thử tăng.Mối liên hệ giữa độ chệch và phương sai khi thực hiện các mô hình học thống kê còn được gọi là sự đánh đổi giữa độ chệch và phương sai (bias-variance trade ). Thông thường, những phương pháp học thống kê cho ra hiệu quả cao trên tập dữ liệu kiểm thử thường là những phương pháp cho ra độ chệch thấp và phương sai thấp. Hiện tượng này được gọi là đánh đổi (trade ), bởi vì rất dễ để chúng ta có một mô hình với độ chệch thấp (low bias) và phương sai cao (high variance), hoặc một mô hình với độ chệch cao (high bias) và phương sai thấp (low variance). Vấn đề khó nhất là tìm ra phương pháp học thống kê nào mà cho ra được phương sai (variance) và độ chệch bình phương (squared bias) thấp nhất. Đó là mục tiêu chính của quyển sách này.Trong những vấn đề thực tế, chúng ta không bao giờ quan sát được hàm f thật, vậy nên là nhìn chung chúng ta không thể tính ra giá trị MSE của tập dữ liệu kiểm thử, độ chệch (bias) và phương sai (variance) của một phương pháp học thống kê nào đó. Tuy nhiên, bạn đọc hãy luôn phải nhớ rằng có sự đánh đổi giữa độ chệch và phương sai (bias-variance trade ) trong các phương pháp thống kê. Trong quyển sách này, chúng tôi sẽ giới thiệucác phương pháp thống kê rất phức tạp (flexible) và có thể giúp chúng ta giảm rất nhiều độ chệch (bias). Tuy nhiên, điều đó, không có nghĩa rằng những phương pháp của chúng tôi sẽ tốt hơn những phương pháp đơn giản (hồi quy tuyến tính chẳng hạn). Một ví dụ đơn giản có thể chỉ ra như …Trong phần tiếp theo, tác giả sẽ giới thiệu về phương pháp cross-validation, là một cách để ước lượng giá trị MSE của tập dữ liệu kiểm thử trên tập dữ liệu huấn luyện, đây là một phương pháp rất hứa hẹn để xử lý các vấn đề liên quan đến sự đánh đổi giữa độ chệch (bias) và phương sai (variance).Tạo ra một tập dữ liệu nữa illustrate cái sự tăng và giảm MSE theo Bias và Variance này","code":""},{"path":"real-introduction.html","id":"sự-đánh-đổi-giữa-độ-chệch-và-phương-sai-của-phương-pháp-k-fold-cross-validation","chapter":"Chương 3 Real Introduction","heading":"3.6.4 Sự đánh đổi giữa độ chệch và phương sai của phương pháp k-fold cross validation","text":"Ở phần bên trên, chúng ta đã nói về việc phương pháp k-fold CV có lợi thế hơn phương pháp LOOCV về tốc độ tính toán số phép tính mà phương pháp k-fold CV phải thực hiện là ít hơn. Ngoài lợi thế về tốc độ tính toán, phương pháp k-fold CV còn được ưa chuộng hơn bởi phương pháp này cho ra kết quả ước lượng sai số trên tập dữ liệu kiểm thử chính xác hơn phương pháp LOOCV. Đây là một ví dụ điển hình cho hiện tượng đánh đổi giữa độ chệch và phương sai của mô hình.Như chúng ta đã nhắc đến ở phần trước (viết trang 205 nhưng trong quyển sách này chưa nhắc đến cái gì ở trước), phương pháp sử dụng tập dữ liệu xác thực (validation set approach) có xu hướng overestimate sai số trên tập dữ liệu kiểm thử, nghĩa là độ chệch sẽ tương đối lớn, bởi vì phương pháp này chỉ sử dụng một nửa lượng dữ liệu để ước lượng. Cùng với một logic đó, không khó để chúng ta nhận ra rằng phương pháp LOOCV sẽ cho ra một ước lượng gần như không chệch (approximately unbiased estimates) của sai số trên tập dữ liệu kiểm thử, bởi vì phương pháp LOOCV sử dụng n - 1 trên n quan sát trong dữ liệu, nghĩa là gần như toàn bộ dữ liệu chúng ta có. Khi chúng ta thực hiện phương pháp k-fold CV, giả sử với trường hợp k = 5 và k = 10, kết quả cho ra một mô hình với độ chệch nằm giữa hai phương pháp vừa liệt kê bên trên, phương pháp này chỉ sử dụng \\(\\frac{(k-1)n}{k}\\) quan sát cho tập dữ liệu huấn luyện, ít hơn phương pháp LOOCV nhưng nhiều hơn phương pháp sử dụng tập dữ liệu xác thực (validation set approach). Vậy nên, từ góc nhìn của các chuyên gia, nếu mục tiêu là làm giảm độ chệch, rõ ràng phương pháp LOOCV là phương pháp vượt trội hơn hẳn với phương pháp k-fold CV.Tuy nhiên, chúng ta đều biết rằng độ chệch không phải là mối quan tâm duy nhất trong quá trình chúng ta ước lượng mô hình; chúng ta cũng cần phải quan tâm đến cả phương sai của mô hình nữa. Những nghiên cứu cho thấy rằng phương pháp LOOCV có phương sai cao hơn phương pháp k-fold CV với k<n. Tại sao lại như vậy? Khi chúng ta thực hiện phương pháp LOOCV, chúng ta thực ra đang tính trung bình sai số từ n mô hình ước lượng từ dữ liệu, và mỗi mô hình được huấn luyện trên một tập dữ liệu gần như y hệt nhau; điều này tạo ra những kết quả sai số có tương quan dương (highly positively correlated) lớn. Ở chiều ngược lại, khi chúng ta sử dụng phương pháp k-fold CV với k<n, chúng ta sẽ tính trung bình sai số của k mô hình ước lượng từ dữ liệu, nhưng các sai số này sẽ tương quan với nhau ít hơn, các tập dữ liệu huấn luyện của k mô hình này ít trùng nhau hơn. trung bình của các phần tử có tương quan lớn sẽ có phương sai lớn hơn trung bình của các phần tử có tương quan nhỏ hơn, nên giá trị sai số trên tập dữ liệu kiểm thử từ phương pháp LOOCV sẽ có xu hướng cho ra phương sai lớn hơn là với sai số trên tập dữ liệu kiểm thử từ phương pháp k-fold CV.Tóm lại, chúng ta có sự đánh đổi giữa độ chệch và phương sai liên quan tới sự lựa chọn k trong k-fold CV. Từ những kết quả thực nghiệm trên rất nhiều dữ liệu, thường các chuyên gia xây dựng mô hình sẽ chọn thực hiện phương pháp k-fold CV với k = 5 hoặc k = 10, để tránh trường hợp sai số trên tập dữ liệu kiểm thử gặp phải tình trạng độ chệch quá cao hoặc phương sai quá cao.","code":""},{"path":"real-introduction.html","id":"lớp-bài-toán-phân-loại","chapter":"Chương 3 Real Introduction","heading":"3.6.4.1 Lớp bài toán phân loại","text":"Từ đầu quyển sách, chúng ta đã thảo luận về độ chính xác của mô hình trong lớp bài toán mô hình hồi quy. Tuy nhiên, rất nhiều thuật ngữ, phương pháp vừa được giới thiệu bên trên, ví dụ như sự đánh đổi giữa độ chệch và phương sai, cũng có thể được áp dụng cho lớp bài toán phân loại. Lớp bài toán phân loại có một sự khác biệt cơ bản với lớp bài toán hồi quy đó là biến phụ thuộc \\(y_i\\) của bài toán phân loại thường là biến định tính, còn biến \\(y_i\\) của lớp bài toán hồi quy là biến định lượng. Trong bài toán phân loại, giả sử chúng ta phải tìm hàm f dựa trên các quan sát \\({(x_1, y_1),...,(x_n, y_n)}\\), với \\(y_1,...,y_n\\) là giá trị định tính. Phương pháp đơn giản nhất để để đánh giá độ chính xác của hàm ước lượng \\(\\hat{f}\\) là sử dụng tỷ lệ sai sót trên tập dữ liệu huấn luyện, được tính bằng công thức:\\[\\begin{align*}\n\\text{Training Error Rate} = \\dfrac{1}{n} \\sum\\limits_{= 1}^{n} (y_i \\neq \\hat{y_i})\n\\tag{3.2}\n\\end{align*}\\]Với \\(\\hat{y_i}\\) là kết quả phân loại quan sát sử dụng hàm \\(\\hat{f}\\), và \\((y_i \\neq \\hat{y_i})\\) là biến phân loại nhận giá trị 1 nếu \\(y_i \\neq \\hat{y_i}\\) và 0 nếu \\(y_i = \\hat{y_i}\\). Nếu ($y_i ) = 0, có nghĩa là hàm \\(\\hat{f}\\) đã phân loại chính xác quan sát thứ , và bằng 1 nếu hàm \\(\\hat{f}\\) phân loại sai quan sát thứu . Công thức (3.2) cho\nthấy tỷ lệ sai sót trên tập dữ liệu huấn luyện bằng số quan sát bị phân loại sai chia cho tổng các quan sát được phân loại.Biểu thức (3.2) được gọi là tỷ lệ sai sót trên mô hình huấn luyện bởi vì giá trị sai số được tính trên dữ liệu được sử dụng để huấn luyện hàm \\(\\hat{f}\\). Tương tự như lớp bài toán hồi quy, trong lớp bài toán phân loại chúng ta cũng quan tâm cả tỷ lệ những quan sát trên tập dữ liệu kiểm thử mà mô hình dự đoán sai. Tỷ lệ sai sót trên tập dữ liệu kiểm thử với các quan sát có dạng {\\((x_i, y_i\\)} (chú ý những dữ liệu trong tập dữ liệu kiểm thử phải khác tập dữ liệu huấn luyện) được cho bởi công thức:\\[\\begin{align*}\n\\text{Test Error Rate} = \\dfrac{1}{m} \\sum\\limits_{= 1}^{m} (y_i \\neq \\hat{y_i})\n\\tag{3.3}\n\\end{align*}\\]Trong đó, \\(\\hat{y_m}\\) là giá trị phân loại được gán cho quan sát thứ m, với các biến dự báo (predictors) \\(x_m\\), được xác định bởi hàm f. Một hàm phân loại tốt là hàm cho giá trị biểu thức (3.3) nhỏ nhất.","code":""},{"path":"real-introduction.html","id":"phân-loại-bayes","chapter":"Chương 3 Real Introduction","heading":"3.6.4.2 Phân loại Bayes","text":"Ta có thể dễ dàng chứng minh (nhưng việc chứng minh nằm ngoài phạm vi của quyển sách này) rằng tỷ lệ sai sót trên tập dữ liệu kiểm thử trong công thức (3.3) là nhỏ nhất, bởi một hàm phân loại khá đơn giản với ý tưởng là gán cho mỗi quan sát giá trị có khả năng đúng nhất, dựa trên những biến dự báo (predictors) của quan sát đó. Nói cách khác, chúng ta sẽ gán cho quan sát \\((x_m, y_m)\\) trên dữ liệu kiểm thử với biến dự báo (predictors) \\(x_m\\) giá trị phân loại j nếu:\\[\\begin{align*}\nP(y_m = j|x_m)\n\\tag{3.4}\n\\end{align*}\\]là lớn nhất. Công thức (3.4) là công thức xác suất có điều kiện, có nghĩa là xác suất mà \\(y_m\\) được phân loại thành loại j, với điều kiện các biến dự báo (predictors) là \\(x_m\\). Phương pháp phân loại này được gọi là phương pháp phân loại Bayes (Bayes classifier).Phương pháp phân loại Bayes cho ra một trong những tỷ lệ sai sót trên tập dữ liệu kiểm thử thấp nhất có thể, được gọi là Bayes error rate. Thực tế, hàm phân loại Bayes luôn luôn chọn giá trị phân loại sao cho (3.4) là lớn nhất, tỷ lệ sai sót sẽ là \\(1 - max_{j}Pr(y_m = j|X = x_m)\\) tại \\(X = x_m\\). Tổng quát hơn, ta có Bayes error rate được cho bởi công thức:\\[\\begin{align*}\n1 - E(\\underbrace{max}_{j} Pr(Y = j|X))\n\\tag{3.5}\n\\end{align*}\\]với ký hiệu E() thể hiện giá trị kỳ vọng trung bình của xác suất trên tất cả các giá trị có thể của X. Giá trị Bayes error rate này thường luôn lớn hơn 0, và chúng có giá trị khá tương đồng với sai số không thể giảm.","code":""},{"path":"real-introduction.html","id":"k-nearest-neighbors","chapter":"Chương 3 Real Introduction","heading":"3.6.4.2.1 K-Nearest Neighbors","text":"Trên lý thuyết, chúng ta luôn muốn dự đoán biến định tính sử dụng phương pháp Bayes. Tuy nhiên trong thực tế, dữ liệu của chúng ta không cho chúng ta biết phân phối của Y với điều kiện X, vậy nên chúng ta không thể tính toán hàm phân loại Bayes. Vậy nên, trong thế giới thống kê, hàm phân loại Bayes đóng vai trò như một tiêu chuẩn vàng không thể đạt tới, và người ta sẽ sử dụng phương pháp phân loại Bayes để sánh với các phương pháp khác. Rất nhiều phương pháp đã được sử dụng để ước lượng \\(P(Y|X)\\) và sau đó cho gán cho quan sát phân loại được dự đoán có xác suất đúng cao nhất. Một trong những phương pháp đó là, phương pháp phân loại K-nearest neighbors (KNN). Phương pháp này được trình bày như sau: Cho một số dương K và một quan sát \\(x_0\\), hàm KNN trước tiên xác định K điểm trong trong tập dữ liệu huấn luyện gần với quan sát \\(x_0\\) nhất, gọi các điểm đó là \\(N_0\\). Tiếp đó, hàm KNN ước lượng giá trị xác suất có điều kiện của loại j bằng công thức sau:\\[\\begin{align*}\nP(Y = j|X = x_0) = \\dfrac{1}{K} \\sum\\limits_{\\N_0} (y_i = j)\n\\tag{3.6}\n\\end{align*}\\]Ta có thể thấy rằng, xác suất có điều kiện của loại j trong công thức (3.6) được tính bằng cách lấy số điểm trong \\(N_0\\) chia cho tổng số điểm trong dữ liệu, và sau đó hàm KNN phân loại quan sát \\(x_0\\) thành loại nhận giá trị xác suất lớn nhất trong công thức (3.6). Dù cho KNN là một phương pháp khá đơn giản, nhưng chúng lại thể hiện kết quả gần giống với phương pháp Bayes classifier một cách bất ngờ.Tương tự với lớp bài toán hồi quy, trong lớp bài toán phân loại, chúng ta cũng không có mối liên hệ mạnh mẽ giữa sai số trên tập dữ liệu huấn luyện và sai số trên tập dữ liệu kiểm thử. Ví dụ với phương pháp KNN, nếu chúng ta cho K bằng 1, thì sai số trên tập dữ liệu huấn luyện sẽ bằng 0, nhưng sai số trên tập dữ liệu kiểm thử vẫn cao. Nhìn chung, nếu chúng ta sử dụng phương pháp phân loại linh hoạt (flexible) hơn, thì có khả năng sai số trên tập dữ liệu huấn luyện sẽ giảm, nhưng sai số trên tập dữ liệu kiểm thử vẫn tăng. Các phương pháp phân loại cũng gặp vấn đề về hiện tượng quá khớp và chưa khớp tương tự như các phương pháp hồi quy.Tóm lại, trong cả lớp bài toán hồi quy và lớp bài toán phân loại, việc lựa chọn mức độ linh hoạt (flexibility) chính xác có vai trò tối quan trọng đến sự thành công của phương pháp học thống kê của chúng ta. Hiện tượng đánh đổi giữa độ chệch và phương sai khiến cho việc lựa chọn mức độ linh hoạt của mô hình trở nên khó khăn, tuy nhiên những phương pháp sử dụng dữ liệu xác thực và\nđặc biệt là k-fold cross validation đã giải quyết phần nào vấn đề này.","code":""},{"path":"real-introduction.html","id":"k-fold-cross-validation","chapter":"Chương 3 Real Introduction","heading":"3.7 K-fold cross validation","text":"Ta lại đặt ra thêm một câu hỏi nữa, nếu như chúng ta không có dữ liệu kiểm thử thì chúng ta sẽ xây dựng mô hình như thế nào? Câu trả lời là sử dụng chính dữ liệu đào tạo để kiểm tra sự chính xác của mô hình. Tuy nhiên, chúng ta sẽ không sử dụng toàn bộ tập dữ liệu kiểm thử, mà chúng ta sẽ chia tập dữ liệu huấn luyện thành k phần khác nhau. Sau đó, chúng ta sẽ xây dựng mô hình trên k-1 tập dữ liệu huấn luyện con, và sử dụng tập dữ liệu huấn luyện con còn lại (thường gọi là tập dữ liệu xác thực) để kiểm tra sự chính xác của mô hình được xây dựng trên k-1 tập dữ liệu huấn luyện kia. Quá trình này được lặp lại k lần, để bất kỳ phần nào của tập dữ liệu huấn luyện cũng đều được làm tập dữ liệu xác thực. Cuối cùng, chúng ta tính trung bình của sai số trên toàn bộ các tập dữ liệu xác thực và sánh. Mô hình nào cho giá trị trung bình sai số trên các tập xác thực khác nhau này nhỏ nhất thì sẽ là mô hình được chúng ta chọn. Phương pháp này thường được gọi là k-fold cross validation.Diễn giải một cách toán học, giả sử thước đo cho sai số của chúng ta là MSE, và chúng ta có k ước lượng của sai số trên tập dữ liệu xác thực, lần lượt là \\(MSE_1, MSE_2, MSE_3, ... , MSE_k\\). Khi đó ước lượng giá trị k-fold cross validation sẽ được tính bằng công thức:\\[\\begin{align}\nCV_{(k)} = \\dfrac{1}{k} \\sum\\limits_{=1}^{k} MSE_i\n\\end{align}\\]Khi ta để giá trị k bằng với số các quan sát trong dữ liệu, khi đó ta sẽ có n tập dữ liệu xác thực, mỗi tập có duy nhất một quan sát. Trường hợp này gọi là Leave-one-cross validation (LOOCV) và công thức ước lượng giá trị LOOCV trong trường hợp này là\\[\\begin{align}\nCV_{(n)} = \\dfrac{1}{n} \\sum\\limits_{=1}^{n} MSE_i\n\\end{align}\\]Hiện giờ quyển sách này đang giới thiệu hai phương pháp để tính toán giá trị cross-validation, vậy ưu và nhược điểm của hai phương pháp này là gì, và chúng ta sử dụng hai phương pháp này như thế nào trong phù hợp? Trong thực tế, để có thể tính toán ra được những con số này yêu cầu chúng ta phải thực hiện bằng máy tính, và đối với những bộ dữ liệu lớn thì tốc độ tính toán là hết sức quan trọng. Phương pháp LOOCV yêu cầu thuật mô hình thống kê của chúng ta phải chạy n lần, và điều này khả năng gây ra gánh nặng rất lớn về mặt tính toán (trừ trường hợp ta sử dụng mô hình ước lượng bình phương tối thiểu, có công thức tính nhanh chính xác là \\(CV_{(n)} = \\dfrac{1}{n} \\sum\\limits_{=1}^{n} \\left(\\dfrac{y_i - \\hat{y_i}}{1 - h_i}\\right)^2\\).\nPhương pháp cross-validation là phương pháp được áp dụng rất rộng rãi, phù hợp hầu như với bất kỳ các mô hình thống kê đã biết. Trong những phương pháp đó, có những phương pháp yêu cầu cách tính toán rất phức tạp chứ không đơn giản như phương pháp ước lượng bình phương tối thiểu, và nếu như sử dụng LOOCV thì máy tính sẽ xử lý mất thời gian hơn rất nhiều, đặc biệt là nếu số lượng quan sát trong dữ liệu lớn. Ngược lại, việc thực hiện k-fold cross validation (thường là với k = 8 hoặc k = 10) thì quá trình xây dựng mô hình thống kê sẽ chỉ phải thực hiện 8 - 10 lần, điều này là khả thi hơn nếu lương dữ liệu được sử dụng lớn.Ngoài lợi thế về mặt tính toán, phương pháp k-fold cross validation còn có một ưu điêm vượt trội nữa với các phương pháp khác, nằm ở sự đánh đổi giữa bias và variance (bias-variance trade ). Phương pháp k-fold cross validation sẽ cho ra kết quả ước lượng của sai số trên tập dữ liệu kiểm thử chính xác hơn với phương pháp LOOCV. Chúng ta đã từng nhắc đến việc phương pháp chọn mô hình dựa trên tập dữ liệu xác thực (validation test) có thể overestimate sai số trên tập dữ liệu kiểm thử, khi mà phương pháp này chỉ sử dụng lượng dữ liệu huấn luyện bằng khoảng một nửa tổng dữ liệu chúng ta có. Với logic này, rất đơn giản để chúng ta nhận ra rằng phương pháp LOOCV sẽ cho ra ước lượng gần như không chệch (approximately unbiased estimates) của tập dữ liệu kiểm thử, khi mỗi tập dữ liệu huấn luyện có tới n - 1 quan sát (tổng số quan sát trong dữ liệu là n). Tương tự, khi chúng ta chạy mô hình và sử dụng phương pháp k-fold cross validation (với các giá trị k khá nhau, có thể là k = 5 hoặc k = 10), chúng ta sẽ có được mức độ chệch vừa phải đối với sai số trên tập dữ liệu kiểm thử, vì khi đó mỗi tập dữ liệu huấn luyện sẽ có khoảng \\(\\dfrac{(k-1)n}{k}\\) quan sát, ít hơn với phương pháp LOOCV nhưng lại nhiều hơn phương pháp chỉ sử dụng một dữ liệu xác thực. Vậy nên, đứng trên góc độ của một người ưu tiên giảm độ chệch của ước lượng, rõ ràng phương pháp LOOCV được ưa chuộng hơn só với phương pháp k-fold cross validation.Tuy nhiên, giảm độ chệch của ước lượng không phải là tất cả những gì chúng ta quan tâm khi ước lượng một mô hình. Một yếu tố rất quan trọng nữa mà chúng ta phải quan tâm đó là phương sai (variance) của mô hình. Nếu như sánh về phương sai, phương pháp LOOCV hay k-fold cross validation sẽ cho ra kết quả tốt hơn? Câu trả lời là phương pháp k-fold cross validation (với điều kiện k<n). Lý là vì khi chúng ta thực hiện phương pháp LOOCV, thực tế chúng ta đang tính trung bình các kết quả của n mô hình ước lượng, nhưng mỗi mô hình đều được huấn luyện dựa trên một tập dữ liệu gần như giống hệt nhau (chỉ khác nhau nhiều nhất là một quan sát); vậy nên, kết quả ước lượng của các mô hình thường có mối tương quan dương rất cao. Ngược lại, phương pháp k-fold cross validation (với điều kiện k < n) sẽ cho ra giá trị trung bình của kết quả của k mô hình ước lượng, và mức độ tương quan của các kết quả này sẽ ít hơn với phương pháp LOOCV, vì số lượng các quan sát trùng nhau ở tập dữ liệu huấn luyện là nhỏ hơn. Bởi vì khi sánh các chỉ số, thì giá trị trung bình của các chỉ số có mối tương quan cao sẽ luôn có phương sai cao hơn giá trị trung bình của các chỉ số có mối tương quan thấp, vậy nên là ước lượng sai số trên tập dữ liệu kiểm thử của phương pháp LOOCV có phương sai cao hơn với phương pháp k-fold cross validation.Tóm lại, phương pháp k-fold cross validation sẽ thể hiện được sự cân bằng tốt giữa độ chệch và phương sai, nhưng sự cân bằng này liên quan đến việc lựa chọn tham số k. Theo như các kết quả thưucj nghiệm cho thấy, khi chúng ta chọn giá trị k = 5 hoặc k = 10, thì phương pháp k-fold cross validation sẽ cho ra được ước lượng sai số trên tập dữ liệu kiểm thử không gặp phải tình trạng độ chệch hoặc phương sai quá cao.\n(đang viết đến trang 205). Chúng ta sẽ sử dụng một dữ liệu được sinh ngẫu nhiên để thể hiện sự vượt trội của phương pháp k -fold cross-validation. Lý là bởi vì khi chúng ta sử dụng dữ liệu thực tế, chúng ta sẽ không biết giá trị MSE thật của tập dữ liệu kiểm thử là bao nhiêu, và điều đó sẽ gây khó khăn nếu như chúng ta muốn đánh giá độ chính xác ước lượng cross-validation của mô hình. Còn nếu như chúng ta sử dụng dữ liệu sinh ngẫu nhiên, chúng ta có thể tính được giá trị MSE thật của tập dữ liệu kiểm thử.need generate simulation data shows k-fold cross validation better Leave-one--cross-validationKhi chúng ta xây dựng mô hình với sai số nhỏ nhất tính theo cross validation, mục tiêu của chúng ta là xác định xem phương pháp thống kê mà ta lựa chọn được kỳ vọng thể hiện tốt như thế nào trên các dữ liệu mới được đưa vào; trong trường hợp này, chúng ta quan tâm đến ước lượng thực tế (actual estimate) của trung bình bình phương sai số trên tập dữ liệu kiểm thử. Tuy nhiên, trong một vài trường hợp khác, có thể chúng ta chỉ đơn thuần quan tâm đến vị trí của điểm cực tiểu trên đường ước lượng trung bình bình phương sai số trên tập dữ liệu kiểm thử. Lý là bởi vì chúng ta đang thực hiện cross-validation với một vài phương pháp thống kê khác nhau, hoặc là với một phương pháp thống kê nhưng có các mức độ linh hoạt (flexibility) khác nhau để tìm ra phương pháp có sai số trên dữ liệu kiểm thử thấp nhất trong các phương pháp được thử. Với mục đích này, vị trí trên đường sai số bình phương trung bình mang lại giá trị sai số bình phương trung bình nhỏ nhất được quan tâm hơn là giá trị của sai số bình phương trung bình đó.Lấy ví dụ đồ thị cho thấy rằng dù các mô hình cho ra test error khác nhau, thì correct level flexibility cũng thường khá gần nhau","code":"\n# # Load necessary libraries\n# library(caret) # For cross-validation\n# library(MASS)  # For synthetic data generation\n# library(ggplot2) # For visualization\n# \n# # Step 1: Data Simulation\n# # Generate a synthetic dataset\n# set.seed(123) # Set seed for reproducibility\n# n <- 250 # Number of observations\n# p <- 5 # Number of predictors\n# X <- matrix(rnorm(n * p), nrow = n, ncol = p) # Generate predictors\n# beta <- runif(p, -2, 2) # Random coefficients\n# epsilon <- rnorm(n, 0, 1) # Noise\n# y <- X %*% beta + epsilon # Generate target variable\n# data <- as.data.frame(cbind(X, y))\n# \n# # Step 2: Model Selection\n# # Linear regression model for this simulation\n# model_formula <- as.formula(paste(\"V1 ~\", paste(\"V\", 2:(p+1), sep=\"\", collapse=\"+\")))\n# \n# # Step 3: Cross-validation Implementations\n# # k-fold cross-validation\n# set.seed(123)\n# k_fold_results <- train(model_formula, data=data, method=\"lm\",\n#                         trControl=trainControl(method=\"cv\", number=10, \n#                                                savePredictions = TRUE))\n# \n# # Leave-one-out cross-validation (LOOCV)\n# set.seed(123)\n# loocv_results <- train(model_formula, data=data, method=\"lm\",\n#                        trControl=trainControl(method=\"LOOCV\", \n#                                               savePredictions = TRUE))\n# \n# # Step 4: Performance Comparison\n# # Extracting RMSE from each fold/iteration\n# k_fold_rmse <- k_fold_results$resample$RMSE\n# \n# # Variance of the RMSE\n# k_fold_variance <- var(k_fold_rmse)\n# \n# # Comparing computational efficiency\n# k_fold_time <- k_fold_results$times$total\n# loocv_time <- loocv_results$times$total\n# \n# # Step 5: Visualization and Interpretation\n# # Plotting the distribution of RMSE for both methods\n# rmse_data <- data.frame(Method = rep(c(\"K-Fold\", \"LOOCV\"), each=nrow(data)),\n#                         RMSE = c(k_fold_rmse, rep(mean(loocv_rmse), nrow(data))))\n# \n# ggplot(rmse_data, aes(x=Method, y=RMSE, fill=Method)) +\n#   geom_boxplot() +\n#   ggtitle(\"Distribution of RMSE for K-Fold vs LOOCV\") +\n#   xlab(\"Cross-Validation Method\") +\n#   ylab(\"RMSE\") +\n#   theme_minimal()\n# \n# # Displaying computational times\n# print(paste(\"K-Fold Computational Time:\", k_fold_time, \"seconds\"))\n# print(paste(\"LOOCV Computational Time:\", loocv_time, \"seconds\"))\n# \n# # Conclusion: This script should help illustrate the differences in variance and\n# # computational efficiency between k-fold and LOOCV cross-validation techniques."},{"path":"real-introduction.html","id":"cross-validation-cho-lớp-bài-toán-phân-loại","chapter":"Chương 3 Real Introduction","heading":"3.8 Cross-validation cho lớp bài toán phân loại","text":"Cross-validation không chỉ phù hợp với lớp bài toán hồi quy, phương pháp này còn rất phù hợp với lớp bài toán phân loại. Khác với lớp bài toán hồi quy, nơi mà người ta thường sử dụng MSE, MAE là giá trị đánh giá sai số trên tập dữ liệu kiểm thử, lớp bài toán phân loại sử dụng số lượng các quan sát bị phân loại sai khi áp dụng mô hình vào dữ liệu. Ví dụ, trong lớp bài toán phân loại, sai số LOOCV có dạng như sau:\\[\\begin{align*}\nCV_{(n)} = \\dfrac{1}{n} \\sum\\limits_{= 1}^{n} Err_i\n\\label{eq:class_cv}\n\\end{align*}\\]với \\(Err_i = (y_i \\neq \\hat{y_i})\\). Sai số của phương pháp k-fold cross validation và phương pháp sử dụng tập dữ liệu xác thực cũng được tính tương tự như sai số LOOCV.","code":""},{"path":"real-introduction.html","id":"phương-pháp-bootstrap","chapter":"Chương 3 Real Introduction","heading":"3.9 Phương pháp Bootstrap","text":"Phương pháp Bootstrap là một phương pháp thống kê rất mạnh, được ứng dụng rộng rãi để lượng hoá sự không chắc chắn của một ước lượng hoặc một phương pháp học thống kê. Một ví dụ đơn giản của phương pháp bootstrap là để ước lượng sai số chuẩn (standard errors) của tham số trong mô hình hồi quy tuyến tính. Trong trường hợp hồi quy tuyến tính (linear regression) thông thường, phương pháp này thường không có ích lắm, khi hầu như mọi gói lệnh thống kê đã tự động tính ra ước lượng sai số chuẩn (standard errors). Tuy nhiên, sức mạnh của phương pháp bootstrap đến từ việc nó có thể dễ dàng được áp dụng với rất nhiều phương pháp học thống kê khác nhau, trong đó có cả những phương pháp mà việc đo phương sai (measure variablitiy) là rất khó khăn, và không được tính sẵn bởi những gói lệnh thống kê.Trong phần này, chúng ta sẽ miêu tả về các bước thực hiện quá trình lấy mẫu để thực hiện phương pháp bootstrap bằng một ví dụ đơn giản như sau:Bước 1: Chọn ngẫu nhiên một quan sát từ dữ liệu gốc vào dữ liệu mẫu.Bước 2: Đưa lại quan sát được chọn ở bước 1 vào dữ liệu gốc.Bước 3: Lặp lại bước 1 và bước 2 n lần (n là số lượng quan sát trong dữ liệu gốc), ta thu được dữ liệu mẫu có n quan sát.Phương pháp boostrap thường cho chúng ta một kết quả ước lượng phù hợp cho phương sai của tham số hoặc một thước đo nào đó cho một tập dữ liệu. Ở đây, chúng ta sẽ tạm gọi giá trị tham số/thước đo cần được ước lượng phương sai là \\(\\hat{\\theta}_n = g(X_1, ... , X_n)\\), với \\(X_i\\) là giá trị của quan sát thứ trong mẫu VÀ g() là hàm để tính tham số/thước đo từ mẫu.Thuật toán ước lượng phương sai bằng phương pháp bootstrap có thể được trình bày như sau:Bước 1: Lấy ra các quan sát mẫu \\(X_1^*, ... , X_n^*\\), chúng ta tính \\(\\hat{\\theta}_n^* = g(X_1^*, ... , X_n^*)\\)Bước 2: Lặp lại bước 1 B lần, chúng ta thu được các ước lượng \\(\\hat{\\theta_{n,1}^*},...,\\hat{\\theta_{n,B}^*}\\)Bước 3: Tínhvới \\(\\bar{\\theta} = \\dfrac{1}{B} \\sum\\limits_{j = 1}^B \\hat{\\theta}_{n,j}^*\\)Bước 4: Thu được kết quả .Khi thoả mãn các điều kiện thông thường, chúng ta có \\(\\dfrac{s^2}{Var(\\hat{\\theta}_n)}\\) hội tụ xác suất đến 1, khi n \\(\\rightarrow \\infty\\). Khi đó, giá trị \\(\\hat{s}^2\\) được chứng minh xấp xỉ bằng phương sai của _n. Thông thường, giả sử ta tính ra giá trị \\(\\hat{s}\\) bằng 0.15, chúng ta có thể nói rằng, chúng ta kỳ vọng \\(\\hat{\\theta}\\) sẽ cách \\(\\theta\\) một khoảng bằng 0.15. Trong xấp xỉ này, chúng ta có hai nguồn gây ra sai số. Nguồn sai số đầu tiên đến từ việc n là hữu hạn, và sai số thứ hai đến từ việc B là hữu hạn. Chúng ta không thể thay đổi giá trị của n, nhưng chúng ta có thể thay đổi giá trị của B. Nếu chúng ta cho giá trị B đủ lớn (trong thực tế ta thường lấy B = 10000), chúng ta có thể bỏ qua sai số sinh ra B là hữu hạn.Khi đã tìm ra phương sai của tham số/thước đo cho tập dữ liệu, chúng ta có thể xác định khoảng tin cậy của tham số/thước đo đó theo các bước sau:Bước 1: Lấy ra các quan sát mẫu \\(X_1^*, ... , X_n^*\\), chúng ta tính \\(\\hat{\\theta}_n^* = g(X_1^*, ... , X_n^*)\\)Bước 2: Lặp lại bước 1 B lần, chúng ta thu được các ước lượng \\(\\hat{\\theta_{n,1}^*},...,\\hat{\\theta_{n,B}^*}\\)Bước 3: Ta tính giá trị:\n\\[\\begin{align*}\n\\hat{F}(t) = \\dfrac{1}{B} \\sum\\limits_{j = 1}^{B} (\\sqrt{n}(\\hat{\\theta}_{n,j}^* - \\hat{\\theta}_n) \\leq t)\n\\end{align*}\\]Bước 4: Sử dụng hàm (t), ta tính được khoảng tin cậy của \\(\\hat{\\theta}_n\\) theo công thức sau:\n\\[\\begin{align*}\nC_n = \\left[ \\hat{\\theta}_n - \\dfrac{t_{1- \\alpha/2}}{\\sqrt{n}}, \\hat{\\theta}_n - \\dfrac{t_{\\alpha/2}}{\\sqrt{n}}  \\right]\n\\end{align*}\\]\nvới \\(t_{\\alpha/2} = \\hat{F}^{-1}(\\alpha/2)\\) và \\(t_{1 - \\alpha/2} = \\hat{F}^{-1}(1 - \\alpha/2)\\).Bước 5: Trả kết quả \\(C_n\\), đây chính là khoảng tin cậy của tham số/thước đo được tính từ dữ liệu.Điểm mạnh của phương pháp bootstrap nằm ở chỗ chúng ta có thể lấy được mẫu dữ liệu B lần (với B không giới hạn) để đánh giá độ chính xác của ước lượng \\(\\theta\\). Có một phương pháp tương đương với cách chọn dữ liệu bootstrap, đó là chúng ta sinh ra B bộ dữ liệu mới khác nhau với các đặc điểm giống với dữ liệu thực tế. Tuy nhiên, với dữ liệu thực tế, chúng ta lại không thể sinh ra một tập mẫu mới từ dữ liệu gốc, vậy nên ta không thể sử dụng được phương pháp kể trên. Với mục tiêu đạt được giá trị \\(\\hat{s}\\) tốt nhất, phương pháp bootstrap vẫn được đánh giá là một phương pháp cho kết quả chính xác, khả năng ứng dụng tốt với các loại mô hình khác nhau.","code":"## \n## Attaching package: 'dplyr'## The following object is masked from 'package:pryr':\n## \n##     where## The following object is masked from 'package:gridExtra':\n## \n##     combine## The following object is masked from 'package:kableExtra':\n## \n##     group_rows## The following objects are masked from 'package:stats':\n## \n##     filter, lag## The following objects are masked from 'package:base':\n## \n##     intersect, setdiff, setequal, union## \n## Attaching package: 'lubridate'## The following objects are masked from 'package:base':\n## \n##     date, intersect, setdiff, union"},{"path":"nhập-dữ-liệu-vào-r.html","id":"nhập-dữ-liệu-vào-r","chapter":"Chương 4 Nhập dữ liệu vào R","heading":"Chương 4 Nhập dữ liệu vào R","text":"Trong phần này của cuốn sách, bạn sẽ tìm hiểu về các kỹ thuật phân tích dữ liệu, bao gồm có tiền xử lý dữ liệu, sắp xếp dữ liệu và trực quan hóa dữ liệu.Tiền xử lý dữ liệu bao gồm tất cả các kỹ thuật biến đổi dữ liệu thô thành định dạng để có thể thực hiện phân tích. Dữ liệu thô bao gồm dữ liệu nhận được từ người khác hoặc dữ liệu mà người phân tích tự tìm kiếm từ các nguồn khác nhau.Tiền xử lý dữ liệu bao gồm tất cả các kỹ thuật biến đổi dữ liệu thô thành định dạng để có thể thực hiện phân tích. Dữ liệu thô bao gồm dữ liệu nhận được từ người khác hoặc dữ liệu mà người phân tích tự tìm kiếm từ các nguồn khác nhau.Sắp xếp dữ liệu bao gồm các bước biến đổi, chuyển hóa dữ liệu thành định dạng để có thể trực quan hóa, thực hiện phân tích tính toán và xây dựng mô hình trên dữ liệu.Sắp xếp dữ liệu bao gồm các bước biến đổi, chuyển hóa dữ liệu thành định dạng để có thể trực quan hóa, thực hiện phân tích tính toán và xây dựng mô hình trên dữ liệu.Trực quan hóa dữ liệu là một nghệ thuật biến đổi dữ liệu (thường được hiển thị dưới dạng các con số, chuỗi ký tự,…) thành các biểu đồ, đồ thị hay hình ảnh sử dụng hình dạng, màu sắc, khoảng cách để con người dễ dàng nhận thức và hiểu về dữ liệu. Trực quan hóa dữ liệu còn có thể giúp người phân tích tìm ra những giá trị ẩn chứa trong dữ liệu.Trực quan hóa dữ liệu là một nghệ thuật biến đổi dữ liệu (thường được hiển thị dưới dạng các con số, chuỗi ký tự,…) thành các biểu đồ, đồ thị hay hình ảnh sử dụng hình dạng, màu sắc, khoảng cách để con người dễ dàng nhận thức và hiểu về dữ liệu. Trực quan hóa dữ liệu còn có thể giúp người phân tích tìm ra những giá trị ẩn chứa trong dữ liệu.Tuy nhiên, trước khi có thể bắt đầu bước tiền xử lý, chúng ta cần nhập dữ liệu vào R trước. Trong một vài trường hợp, nhập dữ liệu chỉ đơn giản là nhập từ một bảng excel có sẵn. Trong một số trường hợp khác, quá trình nhập dữ liệu có thể phức tạp hơn, chẳng hạn như từ một (vài) phần nào đó trong một hoặc nhiều bảng excel, hoặc từ một cơ sở dữ liệu được lưu trữ trong các máy tính server, hoặc đôi khi cần có thể cần viết các vòng lặp để lấy dữ liệu từ các trình duyệt web. Đây là chủ đề mà chúng ta sẽ thảo luận trong chương đầu của phần phân tích dữ liệu.","code":""},{"path":"nhập-dữ-liệu-vào-r.html","id":"đối-tượng-dùng-để-lưu-dữ-liệu-trong-r","chapter":"Chương 4 Nhập dữ liệu vào R","heading":"4.1 Đối tượng dùng để lưu dữ liệu trong R","text":"Hai kiểu đối tượng thường được dùng để lưu dữ liệu trong R là data.frame và tibble. Chúng ta sẽ thảo luận về data.frame trước vì đây là kiểu lưu dữ liệu phổ biến và xuất hiện trước. Đối tượng kiểu tibble, với một vài ưu điểm hơn data.frame, sẽ được thảo luận trong phần tiếp theo.","code":""},{"path":"nhập-dữ-liệu-vào-r.html","id":"data.frame-là-gì","chapter":"Chương 4 Nhập dữ liệu vào R","heading":"4.1.1 data.frame là gì?","text":"Data.frame là đối tượng phổ biến nhất để lưu trữ dữ liệu trên môi trường làm việc của R. Hiểu một cách đơn giản, một data.frame giống như một bảng excel mà mỗi cột tương ứng với một véc-tơ và mỗi dòng tương ứng với một quan sát. Ngay khi cài đặt R, đã có nhiều đối tượng là dữ liệu kiểu data.frame đã được lưu trữ trong R và đã sẵn sàng được sử dụng mà không cần gọi thư viện bổ sung. Để biết trên cửa sổ Rstudio đang sử dụng có những dữ liệu nào, bạn đọc sử dụng câu lệnh data()Bạn đọc có thể thấy trên cửa sổ \\(script\\) xuất hiện một cửa sổ mới với danh sách tất cả các dữ liệu sẵn có trong R và những dữ liệu sẵn có trong các thư viện được cài đặt thêm đang được gọi ra trên cửa sổ làm việc. Để biết trong một thư viện đang được gọi ra trên cửa sổ Rstudio có những dữ liệu nào, bạn đọc có thể sử dụng lệnh data() kèm với tùy chọn package như sau:Trong danh sách dữ liệu của thư viện \\(\\textf{dslabs}\\), bạn đọc có thể thấy một đối tượng có tên \\(\\textbf{murders}\\). Đây là một data.frame. Thật vậy, bạn đọc có thể kiểm tra kiểu của đối tượng này bằng hàm class():Thông thường để có hiểu biết ban đầu về một đối tượng kiểu data.frame, bạn đọc nên bắt đầu bằng đọc mô tả về dữ liệu (nếu có) bằng cách sử dụng ? nếu đây là một dữ liệu có sẵn trong các thư viện của R:Nhóm các câu lệnh dưới đây giúp bạn đọc hiểu được cấu trúc của dữ liệu trong một data.frame:Hàm head() hiển thị nhanh các dòng đầu tiên của dữ liệu cho bạn đọc cái nhìn ban đầu, tuy nhiên hàm head() không hiệu quả khi dữ liệu có nhiều cột.Hàm head() hiển thị nhanh các dòng đầu tiên của dữ liệu cho bạn đọc cái nhìn ban đầu, tuy nhiên hàm head() không hiệu quả khi dữ liệu có nhiều cột.Hàm View() cho hiển thị về dữ liệu dễ nhìn nhất. Hàm View() có hạn chế khi dữ liệu có quá nhiều dòng hoặc nhiều cột và thời gian hiển thị lâu hơn với head().Hàm View() cho hiển thị về dữ liệu dễ nhìn nhất. Hàm View() có hạn chế khi dữ liệu có quá nhiều dòng hoặc nhiều cột và thời gian hiển thị lâu hơn với head().Hàm str() là cách hiển thị dữ liệu một cách tổng quát và hiệu quả hơn với head() hoặc View(). Kết quả từ hàm str() với dữ liệu \\(\\textbf{murders}\\) cho thấy đây là một dữ liệu dạng bảng với 5 cột (còn gọi là variables) và 51 dòng (còn gọi là observations). Ngoài ra, sử dụng hàm str() bạn đọc có thể thấy được kiểu dữ liệu của từng cột; chẳng hạn như cột state là cột chứa dữ liệu kiểu chuỗi ký tự; cột region có kiểu dữ liệu là factor,…Hàm str() là cách hiển thị dữ liệu một cách tổng quát và hiệu quả hơn với head() hoặc View(). Kết quả từ hàm str() với dữ liệu \\(\\textbf{murders}\\) cho thấy đây là một dữ liệu dạng bảng với 5 cột (còn gọi là variables) và 51 dòng (còn gọi là observations). Ngoài ra, sử dụng hàm str() bạn đọc có thể thấy được kiểu dữ liệu của từng cột; chẳng hạn như cột state là cột chứa dữ liệu kiểu chuỗi ký tự; cột region có kiểu dữ liệu là factor,…Một hàm số hiệu quả khác thường được sử dụng để bạn đọc có cái nhìn tổng quan về dữ liệu là hàm summary(). Chúng ta có thể quan sát kết quả khi sử dụng hàm summary() với dữ liệu \\(\\textbf{murders}\\) như sauHàm summary() cho biết thông chi tiết hơn về giá trị trong mỗi cột.Cột state và cột abb là cột mà giá trị trong đó là kiểu chuỗi ký tự;Cột state và cột abb là cột mà giá trị trong đó là kiểu chuỗi ký tự;Cột region là kiểu factor, có thể nhận một trong bốn giá trị là Northeast, South, North Central, hoặc West và cho biết mỗi giá trị xuất hiện bao nhiêu lần trong cột dữ liệu.Cột region là kiểu factor, có thể nhận một trong bốn giá trị là Northeast, South, North Central, hoặc West và cho biết mỗi giá trị xuất hiện bao nhiêu lần trong cột dữ liệu.Các cột population và total là các cột kiểu số. Chúng ta có thể thấy các giá trị lớn nhất, nhỏ nhất, giá trị trung bình và các giá trị tứ phân vị. Bạn đọc có thể hình dung ra phân phối của các giá trị trong cột giá trị kiểu số.Các cột population và total là các cột kiểu số. Chúng ta có thể thấy các giá trị lớn nhất, nhỏ nhất, giá trị trung bình và các giá trị tứ phân vị. Bạn đọc có thể hình dung ra phân phối của các giá trị trong cột giá trị kiểu số.Trong trường hợp cột có giá trị không quan sát được, hàm summary() cũng sẽ cho biết có bao nhiêu giá trị này trong mỗi cột.Trong trường hợp cột có giá trị không quan sát được, hàm summary() cũng sẽ cho biết có bao nhiêu giá trị này trong mỗi cột.Để lấy ra một cột dữ liệu của một data.frame chúng ta sử dụng \\(\\$\\). Chẳng hạn như để lấy giá trị cột population của dữ liệu murders:Như chúng tôi đã đề cập, kiểu dữ liệu của cột \\(region\\) là kiểu \\(factor\\). Về bản chất, véc-tơ kiểu \\(factor\\) là một véc-tơ kiểu chuỗi ký tự nhưng được lưu theo một cách hiệu quả hơn, tiết kiệm bộ nhớ, và thuận lợi cho người sử dụng khi phân tích dữ liệu.Véc-tơ kiểu factor sẽ tương ứng với véc-tơ kiểu chuỗi ký tự nhưng được lưu dưới dạng vec-tơ số tự nhiên, bắt đầu từ 1 đến số lượng chuỗi ký tự khác biệt xuất hiện trong véc-tơ và mỗi chuỗi ký tự sẽ được cho tương ứng với một số tự nhiên. Các lưu này hiệu quả hơn về bộ nhớ khi làm việc với các véc-tơ kiểu chuỗi ký tự nếu có nhiều chuỗi ký tự bị lặp lại trong véc-tơ. Để biết một vec-tơ dạng factor có bao nhiêu giá trị riêng biệt, mỗi giá trị riêng biệt được cho tương ứng với số tự nhiên nào, và mỗi giá trị riêng biệt được lặp lại bao nhiêu lần trong véc-tơ, bạn đọc sử dụng hàm summary() hoặc hàm table()Kết quả từ hàm table() cho thấy cột \\(region\\) có 4 giá trị; cách cho tương ứng mỗi chuỗi ký tự với các số lần lượt là \\(Northeast \\rightarrow 1\\) ; \\(South \\rightarrow 2\\); \\(North Central \\rightarrow 3\\), và \\(West \\rightarrow 4\\); tần suất xuất hiện của mỗi giá trị cũng được cho trong bảng: có 9 giá trị \\(Northeast\\), có 17 giá trị \\(South\\), có 12 giá trị \\(North Central\\), và 13 giá trị \\(West\\).Một lưu ý khác khi sử dụng véc-tơ kiểu factor thay vì kiểu chuỗi ký tự nghĩa là bạn đọc đang định nghĩa dữ liệu là kiểu biến rời rạc hay các biến định tính. Các biến này có thể trực tiếp đưa vào các mô hình và không cần thực hiện thêm biến đổi nào khác.Trong hầu hết các trường hợp, bạn đọc sẽ dùng R để xử lý dữ liệu từ nguồn ngoài vào. Chúng ta sẽ sử dụng các hàm có sẵn trong R đọc dữ liệu và kết quả đầu ra của hàm này sẽ là các \\(data.frame\\). Trong một vài trường hợp, bạn đọc sẽ phải tự tạo \\(data.frame\\). Câu lệnh để tạo một \\(data.frame\\) (tên \\(df\\)) với các cột có tên lần lượt là \\(id\\), \\(names\\), \\(grades\\), và \\(result\\) được viết như sau:Đối tượng kiểu \\(data.frame\\) có một vài nhược điểm khi sử dụng để lưu dữ liệu từ các nguồn khác nhau vào R. đó kiểu đối tượng mới được phát triển để khắc phục các nhược điểm này, đó là \\(tibble\\). Phần tiếp theo chúng ta sẽ thảo luận về đối tượng này.","code":"\ndata()\nlibrary(dslabs) # Gọi thư viện dslabs \ndata(package = \"dslabs\") # Liệt kê những data trong dslabs\nclass(murders) # Trả lại kết quả là một data frame## [1] \"data.frame\"\n? murders # Cửa sổ help sẽ hiển thị mô tả về murders\nView(murders) # Hiển thị data.frame dưới dạng bảng \nhead(murders,k = 5) # Hiển thị k dòng đầu tiên của data.frame\nstr(murders) # Hiển thị cấu trúc của data.frame\nsummary(murders)##     state               abb                      region     population      \n##  Length:51          Length:51          Northeast    : 9   Min.   :  563626  \n##  Class :character   Class :character   South        :17   1st Qu.: 1696962  \n##  Mode  :character   Mode  :character   North Central:12   Median : 4339367  \n##                                        West         :13   Mean   : 6075769  \n##                                                           3rd Qu.: 6636084  \n##                                                           Max.   :37253956  \n##      total       \n##  Min.   :   2.0  \n##  1st Qu.:  24.5  \n##  Median :  97.0  \n##  Mean   : 184.4  \n##  3rd Qu.: 268.0  \n##  Max.   :1257.0\nmurders$population # in ra màn hình cột population của data.frame murders\n# summary(murders$region) # Tổng hợp thông tin của vec-tơ dạng factor\ntable(murders$region) # cho kết quả tương tự như summary## \n##     Northeast         South North Central          West \n##             9            17            12            13\ndf<-data.frame( # Hàm data.frame() dùng để tạo data.frame tên df\n      id = paste(\"SV\",1:5), # Cột có tên là ID nhận giá trị \"SV1\",...,\"SV5\"\n      names = c(\"You\", \"Me\", \"Him\", \"Her\", \"John\"), # Cột names\n      grades = c(5.5, 1.5, 10.0, 9.0, 7.6), # Cột grades\n      result = c(TRUE, FALSE,TRUE, TRUE, TRUE)) # Cột result"},{"path":"nhập-dữ-liệu-vào-r.html","id":"tibble-là-một-cải-tiến-của-data.frame","chapter":"Chương 4 Nhập dữ liệu vào R","heading":"4.1.2 \\(tibble\\) là một cải tiến của \\(data.frame\\)?","text":"Về cơ bản một \\(tibble\\) là cũng có thể hiểu là một \\(data.frame\\) với một vài điều chỉnh để giúp việc lấy dữ liệu từ nguồn bên ngoài vào và phân tích trở nên dễ dàng hơn. Thực tế thì ở mức độ phân tích dữ liệu thông thường, sự khác khác nhau giữa \\(tibble\\) và \\(data.frame\\) là không đáng kể. Để liệt kê ra sự khác nhau cơ bản giữu hai đối tượng này thì có thể kể đến:Thứ nhất: khi một \\(tibble\\) ra màn hình sẽ chỉ có 10 dòng đầu được hiển thị và số lượng cột của một \\(tibble\\) luôn luôn khớp với kích thước cửa sổ R Console, đồng thời kiểu dữ liệu của mỗi cột sẽ được hiển thị ngay dưới tên cộtThứ hai: khi lấy dữ liệu từ bên ngoài vào trong R, \\(tibble\\) không đổi tên cột dù tên cột không phải là kiểu tên được phép trong R. Đồng thời, khi tạo một \\(tibble\\), bạn đọc có thể đặt tên cột là một kiểu tên không được phép sử dụng với tên biến thông thường.Thứ hai: khi lấy dữ liệu từ bên ngoài vào trong R, \\(tibble\\) không đổi tên cột dù tên cột không phải là kiểu tên được phép trong R. Đồng thời, khi tạo một \\(tibble\\), bạn đọc có thể đặt tên cột là một kiểu tên không được phép sử dụng với tên biến thông thường.Cuối cùng, khi dữ liệu từ bên ngoài được lưu vào một \\(tibble\\), kiểu dữ liệu sẽ không thay đổi.Cuối cùng, khi dữ liệu từ bên ngoài được lưu vào một \\(tibble\\), kiểu dữ liệu sẽ không thay đổi.Để tạo một \\(tibble\\), bạn đọc có thể sử dụng hàm tibble(). Bạn đọc có thể tạo ra một dữ liệu có 3 cột mà tên các cột đều không thể được sử dụng làm tên biến trong R như sauNếu thay thế đoạn lệnh trên bằng hàm data.frame() thì hàm \\(data.frame\\) được tạo thành sẽ tự động thay đổi tên cộtBạn đọc có thể thấy rằng dữ liệu có tên \\(df\\) nếu được lưu dưới dạng \\(data.frame\\) thì tên các cột đã được tự động thay đổi cho thích hợp với tên biến. Những điểm khác nhau giữa \\(tibble\\) và \\(data.frame\\) sẽ được tiếp tục thảo luận ở các phần tiếp theo khi chúng tôi giới thiệu về các hàm dùng để nhập dữ liệu vào R.","code":"\nlibrary(tibble)\ntrump_tweets # in một data frame ra màn hình sẽ không hiệu quả\n# Hàm as_tibble đổi data.frame sang tibble\nas_tibble(trump_tweets) # Hiển thị 1 tibble hiệu quả hơn.\ntib<-tibble( # hàm tibble dùng để tạo tibble\n  \":D\" = c(1,2,3), # có thể dùng tên cột là \":D\"\n  \":p\" = c(\"X\",\"Y\",\"Z\"), # có thể dùng tên cột là \":p\"\n  \"1\" = 2 # có thể dùng tên cột là \"1\"\n)\ntib## # A tibble: 3 × 3\n##    `:D` `:p`    `1`\n##   <dbl> <chr> <dbl>\n## 1     1 X         2\n## 2     2 Y         2\n## 3     3 Z         2\ndf<-data.frame( # tạo data.frame thay vì tibble\n  \":D\" = c(1,2,3), # data.frame sẽ đổi tên cột cho phù hợp\n  \":p\" = c(\"X\",\"Y\",\"Z\"), # data.frame sẽ đổi tên cột cho phù hợp\n  \"1\" = 2 # data.frame sẽ đổi tên cột cho phù hợp\n)\ndf # hãy quan sát xem tên cột của df thay đổi như thế nào##   X.D X.p X1\n## 1   1   X  2\n## 2   2   Y  2\n## 3   3   Z  2"},{"path":"nhập-dữ-liệu-vào-r.html","id":"nhập-dữ-liệu-bằng-hàm-sẵn-có.","chapter":"Chương 4 Nhập dữ liệu vào R","heading":"4.2 Nhập dữ liệu bằng hàm sẵn có.","text":"\n(#tab:tbptdl001 )Danh sách hàm có sẵn để lấy dữ liệu từ nguồn bên ngoài\nKhi lấy dữ liệu từ các nguồn bên ngoài vào bằng các câu lệnh có sẵn, tên của các cột dữ liệu có thể bị thay đổi một số tên cột không thể được dùng để đặt tên của \\(data.frame\\). đó, bạn đọc hãy luôn kiểm tra lại tên các cột dữ liệu sau khi đọc. Hàm names() cho biết tên các cột của một \\(data.frame\\).Để đổi tên của \\(data.frame\\) ở trên, bạn đọc cần gán \\(names(df)\\) bằng một véc-tơ chứa tên các cột. Hãy đảm bảo rằng độ dài của vec-tơ chứa tên cột bằng số cột của \\(data.frame\\)","code":"\ndf<-read.csv(header = TRUE, \n              text = \"@1,@2 \n                      1,2 \n                      3,4\") # sử dụng read.csv để đọc đoạn text\nnames(df) # hiển thị tên của các cột## [1] \"X.1\" \"X.2\"\nnames(df)<-c(\"@1\",\"@2\") # đổi tên 2 cột của data.frame df\ndf # in data.frame df##   @1 @2\n## 1  1  2\n## 2  3  4"},{"path":"nhập-dữ-liệu-vào-r.html","id":"nhập-dữ-liệu-bằng-thư-viện-textbfreadr.","chapter":"Chương 4 Nhập dữ liệu vào R","heading":"4.3 Nhập dữ liệu bằng thư viện \\(\\textbf{readr}\\).","text":"Các câu lệnh để đọc dữ liệu của thư viện \\(readr\\) tương tự như các câu lệnh sẵn có, nhưng đặc biệt hiệu quả hơn về thời gian đọc dữ liệu. Chẳng hạn như hàm số dùng để đọc các file định dạng \\(csv\\) trong thư viện \\(readr\\) là hàm read_csv() được dùng thay thế cho read.csv() khi chúng ta cần đọc các file có dung lượng lớn. Để sánh thời gian đọc dữ liệu của hàm read_csv() và hàm read.csv() bạn đọc có thể thực hiện ví dụ sau: chúng ta sẽ tạo hai file dữ liệu bao gồm test1.csv và test2.csv là các file chứa các số được sinh ngẫu nhiên. Dữ liệu test1.csv có \\(10^2\\) hàng và \\(10^4\\) cột trong khi dữ liệu test2.csv có \\(10^2\\) hàng và \\(10^5\\) cột. Các dữ liệu này có thể được tạo và lưu bằng các câu lệnh dưới đây:Bạn đọc có thể kiểm tra kích thước của các file \\(test1.csv\\) và \\(test2.csv\\) trên máy tính để thấy rằng dung lượng của các file lần lượt là 18 Mega byte và 180 Mega byte. Chúng ta sẽ kiểm tra thời gian mà các hàm read.csv() và read_csv() nhập dữ liệu đối với dữ liệu test1.csv trước:Đối với dữ liệu test1.csv thì thời gian nhập dữ liệu của read_csv() có nhanh hơn nhưng không có sự khác biệt đáng kể. Tuy nhiên sự khác biệt sẽ rõ ràng khi nhập dữ liệu \\(test2.csv\\). Bạn đọc cân nhắc khi dùng hàm read.csv() đọc dữ liệu bởi thời gian nhập dữ liệu với những file có dung lượng hơn 100 Mega bytes có thể lên đến hơn 20 phút.Trên máy tính của chúng tôi, hàm read_csv() sẽ mất khoảng 2 phút để đọc dữ liệu test2.csv, nghĩa là thời gian tiết kiêm lên đến hơn 10 lần. Ngoài hàm read_csv(), thư viện \\(\\textbf{readr}\\) còn có các hàm để đọc các kiểu định dạng file khác nhau. Danh sách các hàm thường hay dùng được liệt kê trong bảng ??\nTable 4.1: Danh sách hàm đọc dữ liệu của thư viện readr\nMột sự khác biệt cơ bản khác của các hàm đọc dữ liệu trong thư viện \\(\\textbf{readr}\\) đó là dữ liệu được lưu vào một tibble thay vì một data.frame. Điều này giúp cho dữ liệu không bị thay đổi định dạng và giữ nguyên tên cột. Các lưu ý khác khi bạn đọc sử dụng các hàm số đọc dữ liệu của thư viện \\(\\textbf{readr}\\) làCác hàm số trong thư viện \\(\\textbf{readr}\\) luôn hiểu hàng đầu tiên của dữ liệu là tên của mỗi cột. đó, bạn đọc cần sử dụng tham số \\(col_names = FALSE\\) nếu không muốn R tự động hiểu hàng đầu tiên là tên của mỗi cột dữ liệu. Ví dụ, hãy quan sát sự khác nhau giữa việc có và không sử dụng col_names = FALSE:bạn đọc có thể thấy rằng khi không sử dụng col_names = FALSE, hàm read_csv() sẽ hiểu hàng đầu tiên, tương ứng với các số 1, 2, và 3, là tên các cột và hàng thứ hai là dữ liệu. Điều này giải thích tại sao tibble chỉ có 1 hàng. Khi chúng ta sử dụng col_names = FALSE, hàm read_csv() sẽ tự động đặt tên các cột là “X1”, “X2”, “X3” và dữ liệu sẽ có 2 hàng.Trong nhiều file dữ liệu, người gửi thường sử dụng các hàng đầu tiên để mô tả về dữ liệu. đó khi sử dụng các hàm trong thư viện \\(\\textbf{readr}\\), bạn đọc có thể sử dụng tham số skip = k để loại bỏ \\(k\\) dòng đầu tiên của file dữ liệu. Ví dụ:Bạn đọc cũng có thể sử dụng tham số \\(col_names\\) để gán giá trị cho tên các cột ngay trong hàm read_csv(). Tuy nhiên để tránh sự phức tạp, chúng tôi khuyên bạn đọc hãy đặt tên cho các cột bằng hàm names() sau khi lưu dữ liệu vào tibble.Cách sử dụng các hàm khác ngoài read_csv() bạn đọc có thể tham khảo trong hướng dẫn của thư viện \\(\\textbf{readr}\\). Sau khi đọc qua hướng dẫn, bạn đọc hãy thử kiểm tra xem các câu lệnh sau có vấn đề gì và nếu có thể, bạn đọc hãy thử lựa chọn hàm hoặc thêm tham số phù hợp để đọc dữ liệu.","code":"\nx<-matrix(rnorm(10^6),10^2,10^4) # Ma trận 100 hàng, 10^4 cột\nwrite.csv(x,\"test1.csv\") # Ma tran thanh file .csv\nx<-matrix(rnorm(10^7),10^2,10^5) # Ma trận 100 hàng, 10^5 cột\nwrite.csv(x,\"test2.csv\") # Ma tran thanh file .csv\nstart<-proc.time() # lưu lại thời điểm trước khi chạy read.csv\ndat<-read.csv(\"test1.csv\") # dùng hàm read.csv để load dữ liệu \nproc.time() - start # tính thời gian hàm read.csv chạy\n\nstart<-proc.time() # lưu lại thời điểm trước khi chạy read_csv\ndat<-read_csv(\"test1.csv\") # dùng hàm read_csv để load dữ liệu \nproc.time() - start # tính thời gian hàm read_csv chạy\nstart<-proc.time() # lưu lại thời điểm trước khi chạy read.csv\ndat<-read.csv(\"test2.csv\") # !!! THỜI GIAN CHẠY CÓ THỂ LÊN ĐẾN 20-25 phút\nproc.time() - start # tính thời gian hàm read.csv chạy\n\nstart<-proc.time() # lưu lại thời điểm trước khi chạy read_csv\ndat<-read_csv(\"test2.csv\") # dùng hàm read_csv để load dữ liệu \nproc.time() - start # tính thời gian hàm read_csv chạy\nlibrary(readr)\n# Kết quả sẽ là một Tibble 1 hàng và 3 cột\nread_csv(\"1,2,3 \n         4,5,6\") # Tên các cột là \"1\", \"2\", và \"3\"## # A tibble: 1 × 3\n##     `1`   `2`   `3`\n##   <dbl> <dbl> <dbl>\n## 1     4     5     6\n# Kết quả sẽ là một Tibble 2 hàng và 3 cột\nread_csv(\"1,2,3 \n         4,5,6\", col_names = FALSE)## # A tibble: 2 × 3\n##      X1    X2    X3\n##   <dbl> <dbl> <dbl>\n## 1     1     2     3\n## 2     4     5     6\n# readr tự động đặt tên các cột X1, X2, X3\n# Kết quả sẽ là một Tibble 2 hàng và 3 cột\nread_csv(\"Trường ĐHKTQD\n        Khoa toán Kinh tế\n         1,2,3 \n         4,5,6\", col_names = FALSE, skip = 2) # readr sẽ không đọc 2 dòng đầu## Rows: 2 Columns: 3\n## ── Column specification ────────────────────────────────────────────────────────\n## Delimiter: \",\"\n## dbl (3): X1, X2, X3\n## \n## ℹ Use `spec()` to retrieve the full column specification for this data.\n## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.## # A tibble: 2 × 3\n##      X1    X2    X3\n##   <dbl> <dbl> <dbl>\n## 1     1     2     3\n## 2     4     5     6\nread_csv(\"x,y\\n1,2,3\\n4,5,6\") # \\n thay cho xuống dòng\nread_csv(\"x,y,z\\n1,2\\n1,2,3,4\") \nread_csv(\"x,y\\n\\\",1,\\n,a,b\",col_names = FALSE) \nread_csv(\"x;y\\n1;2\\nx;y\") # Thử hàm số khác\nread_csv(\"x|y\\n1|2\") # Thử hàm số khác"},{"path":"nhập-dữ-liệu-vào-r.html","id":"tương-tác-giữa-r-với-microsoft-excel","chapter":"Chương 4 Nhập dữ liệu vào R","heading":"4.4 Tương tác giữa R với Microsoft Excel","text":"Microsoft Excel là rất phổ biến trong môi trường làm việc công sở. đó, người phân tích dữ liệu thường xuyên nhận được dữ liệu trong các file có xls, xlsx, xlsb, hoặc xlsm. Tất nhiên, bạn đọc có thể sử dụng trực tiếp Microsoft Excel để thực hiện những phân tích và tính toán đơn giản. Nhưng để thực hiện các yêu cầu phức tạp hơn, chẳng hạn như làm việc với nhiều file Excel, hoặc thực hiện xây dựng các mô hình trên dữ liệu bạn đọc cần phải sử dụng R. Các thư viện cần cài đặt bổ sung mà chúng thôi thường sử dụng để kết nối giữa R với Microsoft Excel là thư viện \\(\\textbf{readxl}\\) và \\(\\textbf{openxlsx}\\). Nếu như thư viện \\(\\textbf{readxl}\\) chỉ đơn thuần là để đọc dữ liệu từ Excel thì thư viện \\(\\textbf{openxlsx}\\) còn cho phép chúng ta điều khiển, tính toán, và định dạng các file có định dạnh ở trên mà không cần mở Microsoft Excel. Cách sử dụng các hàm cơ bản trong hai thư viện này được trình bày trong các phần dưới đây.","code":""},{"path":"nhập-dữ-liệu-vào-r.html","id":"đọc-dữ-liệu-lưu-dưới-định-dạng-của-excel","chapter":"Chương 4 Nhập dữ liệu vào R","heading":"4.4.1 Đọc dữ liệu lưu dưới định dạng của Excel","text":"Hàm read_excel() được sử dụng để đọc các file được lưu bằng Microsoft Excel. Hàm số sẽ hỗ trợ đọc các file có định dạng xlsx, xlsm, xls. Hai biến thể khác của read_excel() là read_xlsx() và read_xls() được sử dụng khi chúng ta biết chính xác định dạng của file cần đọc. Các tham số thường được sử dụng trong các hàm này được liệt kê như sau:sheet cho biết tên của sheet của excel workbook. Sử dụng tham số này có ý nghĩa khi workbook chúng ta cần lấy dữ liệu có nhiều sheet. Nếu không sử dụng tham số này, hàm read_excel() sẽ luôn luôn đọc dữ liệu trong sheet thứ nhất của excel workbook.sheet cho biết tên của sheet của excel workbook. Sử dụng tham số này có ý nghĩa khi workbook chúng ta cần lấy dữ liệu có nhiều sheet. Nếu không sử dụng tham số này, hàm read_excel() sẽ luôn luôn đọc dữ liệu trong sheet thứ nhất của excel workbook.range được sử dụng nếu bạn đọc chỉ muốn đọc dữ liệu từ một phần của của sheet. Chẳng hạn như range = \"A1:E100\" nghĩa là bạn đọc chỉ muốn lấy dữ liệu từ cell A1 đến cell E100 của sheet tương ứng.range được sử dụng nếu bạn đọc chỉ muốn đọc dữ liệu từ một phần của của sheet. Chẳng hạn như range = \"A1:E100\" nghĩa là bạn đọc chỉ muốn lấy dữ liệu từ cell A1 đến cell E100 của sheet tương ứng.col_names nhận một trong hai giá trị là TRUE hoặc FALSE. Giá trị TRUE cho biết có sử dụng hàng đầu tiên làm tên các cột trong dữ liệu trong khi giá trị FALSE cho biết không lấy hàng đầu tiên làm tên cột.col_names nhận một trong hai giá trị là TRUE hoặc FALSE. Giá trị TRUE cho biết có sử dụng hàng đầu tiên làm tên các cột trong dữ liệu trong khi giá trị FALSE cho biết không lấy hàng đầu tiên làm tên cột.skip cho biết số lượng dòng sẽ bỏ qua trước khi bắt đầu đọc dữ liệu. Nếu có sử dụng tham số range thì hàm read_excel() sẽ bỏ qua tham số này.skip cho biết số lượng dòng sẽ bỏ qua trước khi bắt đầu đọc dữ liệu. Nếu có sử dụng tham số range thì hàm read_excel() sẽ bỏ qua tham số này.Ngoài ra hàm read_excel() còn có các tham số khác có thể hữu ích trong nhiều trường hợp như col_types hay n_max. Bạn đọc hãy đọc mô tả hàm số để hiểu chính xác hơn về các tham số. Đoạn lệnh dưới đây được sử dụng để đọc dữ liệu nằm trong range được giới hạn từ cell A1 đến cell E100, trong sheet có tên ws1, và workbook có tên “wb1.xlsx”","code":"\nsetwd(path) # Đặt đường dẫn đến folder chứa file\ndat<-read_xlsx(\"wb1.xlsx\", \n               sheet = \"ws1\",\n               range = \"A1:E100\",\n               col_names = TRUE)"},{"path":"nhập-dữ-liệu-vào-r.html","id":"tương-tác-với-microsoft-excel-bằng-thư-viện-textbfopenxlsx","chapter":"Chương 4 Nhập dữ liệu vào R","heading":"4.4.2 Tương tác với Microsoft Excel bằng thư viện \\(\\textbf{openxlsx}\\)","text":"Ngoài việc lấy dữ liệu từ các file được lưu dưới định dạnh của Excel, bạn đọc cũng có thể sử dụng R để thực hiện tính toán ngay trên các worksheet và lưu lại kết quả mà không cần phải mở file. Thư viện \\(openxlsx\\) sẽ giúp bạn đọc làm được việc này. Các hàm quan trọng sử dụng để làm việc trên các worksheet là: loadWorkbook(), addWorksheet() hàm writeData(), và hàm saveWorkbook().Hàm loadWorkbook() đượpc sử dụng để mở một workbook và lưu thành một đối tượng kiểu workbook trên R. Đoạn lệnh dưới đây dùng để lấy thông tin từ file có tên là “mau bd.xlsx” trong đường dẫn tương ứng và sau đó lưu thông tin vào một đối tượng kiểu workbook trên R có tên \\(wb1\\):Hàm str() có thể sử dụng để xem cấu trúc của đối tượng wb1. Tuy nhiên các workbook lớn có thể làm cho kết quả của hàm str() trở nên khó hiểu. Nhìn chung đối tượng kiểu workbook sẽ hoạt động tương đối giống như một list. Mỗi list con tương ứng với một sheet của workbook Bạn đọc có thể sử dụng hàm names() để biết wb1 có những sheet nào:Hàm addWorksheet() được sử dụng để thêm một worksheet vào trong một đối tượng workbook. Cấu trúc câu lệnh của hàm addWworksheet() khá đơn giản và các tham số được sử dụng chủ yếu là để thiết kế worksheet mới thêm vào nên chúng tôi không liệt kê ở đây. Đoạn câu lệnh sau được sử dụng để thêm vào workbook wb1 một worksheet có tên là “Sheet 2”Hàm writeData() được sử dụng để thay đổi và sửa thông tin trên đối tượng workbook. Các tham số quan trọng với hàm writeData() được liệt kê ở dưới đây:startCol: chỉ số của cột trên worksheet mà chúng ta bắt đầu ghi thông tin lên. Nếu không sử dụng tham số này, chỉ số cột đầu tiên sẽ luôn là 1, nghĩa là cột của workssheet.startCol: chỉ số của cột trên worksheet mà chúng ta bắt đầu ghi thông tin lên. Nếu không sử dụng tham số này, chỉ số cột đầu tiên sẽ luôn là 1, nghĩa là cột của workssheet.statRow: chỉ số của hàng trên worksheet mà chúng ta bắt đầu ghi thông tin lên. Nếu không sử dụng tham số này, chỉ số hàng đầu tiên sẽ luôn là 1.statRow: chỉ số của hàng trên worksheet mà chúng ta bắt đầu ghi thông tin lên. Nếu không sử dụng tham số này, chỉ số hàng đầu tiên sẽ luôn là 1.colNames: nhận giá trị bằng TRUE hoặc FALSE. Giá trị TRUE cho biết có ghi nhận thông tin tên cột của dữ liệu (hoặc véc-tơ) vào hàng đầu tiên. Giá trị FALSE nghĩa là không ghi nhận tên cột của dữ liệu hay véc-tơ vào hàng đầu tiên.colNames: nhận giá trị bằng TRUE hoặc FALSE. Giá trị TRUE cho biết có ghi nhận thông tin tên cột của dữ liệu (hoặc véc-tơ) vào hàng đầu tiên. Giá trị FALSE nghĩa là không ghi nhận tên cột của dữ liệu hay véc-tơ vào hàng đầu tiên.rowNames: : nhận giá trị bằng TRUE hoặc FALSE. Giá trị TRUE cho biết có ghi nhận thông tin tên hàng của dữ liệu (hoặc véc-tơ) vào cột đầu tiên của worksheet. Giá trị FALSE nghĩa là không ghi nhận tên hàng của dữ liệu hay véc-tơ vào hàng đầu tiên của worksheet.rowNames: : nhận giá trị bằng TRUE hoặc FALSE. Giá trị TRUE cho biết có ghi nhận thông tin tên hàng của dữ liệu (hoặc véc-tơ) vào cột đầu tiên của worksheet. Giá trị FALSE nghĩa là không ghi nhận tên hàng của dữ liệu hay véc-tơ vào hàng đầu tiên của worksheet.Đoạn câu lệnh sau sẽ lưu thông tin của một data.frame có tên là \\(\\textbf{women}\\) bắt đầu từ hàng thứ nhất, cột thứ nhất (cell A1) vào sheet có tên là “Sheet 2” của workbook wb1:Hàm số saveWorkbook() được sử dụng để lưu một đối tượng workbook (của R) thành một excel workbook.Bạn đọc có thể sử dụng Microsoft excel để mở workbook có tên “mau bd1.xlsx” và xem thông tin về dữ liệu \\(\\textbf{women}\\) ở trong worksheet có tên là “Sheet 2” của workbook này. Tham số overwrite nhận giá trị TRUE có nghĩa là nếu trong đường dẫn tương ứng đã có file có tên trùng với tên workbook mới thì sẽ lưu workbook mới thay thế cho workbook cũ có trùng tên.","code":"\nwb1<-loadWorkbook(\"../KHDL_KTKD Final/Dataset/mau bd.xlsx\")\nclass(wb1)## [1] \"Workbook\"\n## attr(,\"package\")\n## [1] \"openxlsx\"\nnames(wb1)## [1] \"Sheet\"  \"Sheet1\"\naddWorksheet(wb1, \"Sheet 2\")\nwriteData(wb1, \"Sheet 2\", women, startCol = 1, startRow = 1, colNames = TRUE, rowNames = FALSE)\nsaveWorkbook(wb1, \"mau bd1.xlsx\", overwrite = TRUE)"},{"path":"nhập-dữ-liệu-vào-r.html","id":"kết-nối-r-với-cơ-sở-dữ-liệu","chapter":"Chương 4 Nhập dữ liệu vào R","heading":"4.5 Kết nối R với cơ sở dữ liệu","text":"Có thể sử dụng R như một công cụ để kết nối và thực hiện các câu lệnh truy vấn vào các cơ sở dữ liệu. Để làm được điều này, trước tiên, bạn đọc cần phải cài đặt một Open Database Connectivity (ODBC), được gọi là kết nối cơ sở dữ liệu mở. Kết nối này giúp cho hệ điều hành máy tính tương thích với hệ quản lý cơ sở dữ liệu mà bạn sử dụng. Nhóm tác giả sử dụng hệ điều hành Windows và hệ quản trị cơ sở dữ liệu MySQL nên chúng tôi sẽ lựa chọn ODBC phù hợp. Bạn đọc tham khảo tại địa chỉ https://dev.mysql.com/downloads/connector/odbc/. Tại thời điểm nhóm tác giả viết cuốn sách này ODBC cho hệ điều hành Windows đang là phiên bản 8.0Sau khi cài đặt ODBC lên hệ điều hành, bạn đọc đã có thể sử dụng R để truy cập vào một cơ sở dữ liệu và thực hiện các câu lệnh truy vấn dữ liệu trên cơ sở dữ liệu đó trên R với sự trợ giúp của thư viện \\(DBI\\). Sau khi cài đặt thư viện \\(DBI\\), bạn đọc cần tạo một kết nối giữa R và cơ sở dữ liệu bằng hàm \\(dbConnect\\).Trong đó “ten_serve” là địa chỉ local hoặc server lưu trữ cơ sở dữ liệu, “db_name” là tên của cơ sở dữ liệu, ID và password lần lượt là tên và password mà bạn đọc tự tạo hoặc được cấp để truy cập vào cơ sở dữ liệu. Sau khi đã tạo được kết nối, bạn đọc có thể thực hiện bất kỳ câu lệnh truy vấn dữ liệu nào từ R với hàm “DBI::dbGetQuery()”. Chẳng hạn, bạn đọc muốn lấy ra thông tin của tất cả những người có ngày sinh là ngày 01 tháng 01 năm 2000 từ một bảng có tên là Life_Insured từ một cơ sở dữ liệu tên là \\(tktdb\\), bạn đọc thực hiện như sauKhi làm việc với dữ liệu được trích xuất từ một hệ cơ sở dữ liệu, bạn đọc hãy cố gắng thực hiện các phép biến đổi, sắp xếp dữ liệu bằng SQL thay vì thực hiện biến đổi trên R vì các hệ quản trị cơ sở dữ liệu thực hiện các chức năng này nhanh hơn.","code":"\nlibrary(DBI)\ncon <- dbConnect(odbc::odbc(), .connection_string = \"Driver={MySQL ODBC 8.0 Unicode Driver};\", \n    Server = \"ten_serve\", Database = \"db_name\", UID = \"ID\", PWD = \"password\")\nsql<-\"select * from tktdb.Life_Insured \n      where DOB = '2000-01-01'\" # Viết đúng câu lệnh truy vấn từ MySQL\ndf<-DBI::dbGetQuery(sql) # data.frame df sẽ lưu kết quả của câu lệnh truy vấn"},{"path":"nhập-dữ-liệu-vào-r.html","id":"thu-thập-dữ-liệu-từ-các-website","chapter":"Chương 4 Nhập dữ liệu vào R","heading":"4.6 Thu thập dữ liệu từ các website","text":"","code":""},{"path":"nhập-dữ-liệu-vào-r.html","id":"phụ-lục","chapter":"Chương 4 Nhập dữ liệu vào R","heading":"4.7 Phụ lục","text":"","code":""},{"path":"nhập-dữ-liệu-vào-r.html","id":"bài-tập","chapter":"Chương 4 Nhập dữ liệu vào R","heading":"4.8 Bài tập","text":"","code":""},{"path":"tiền-xử-lý-dữ-liệu.html","id":"tiền-xử-lý-dữ-liệu","chapter":"Chương 5 Tiền xử lý dữ liệu","heading":"Chương 5 Tiền xử lý dữ liệu","text":"Tiền xử lý dữ liệu là một công việc đòi hỏi sự tỉ mỉ cẩn thận và là một trong những bước quan trọng nhất trong một quy trình làm việc trên dữ liệu. Tiền xử lý dữ liệu là tập hợp tất cả các bước kỹ thuật nhằm đảm bảo cho dữ liệu bạn sử dụng phân tích hoặc xây dựng mô hình được đảm bảo về định dạng, giá trị, và ý nghĩa. Hiểu một cách đơn giản, tiền xử lý dữ liệu là quá trình biến dữ liệu thô thành dữ liệu có thể sử dụng được để phân tích và đưa ra kết quả.Khi làm việc với dữ liệu, thực tế là đến hơn 50% các trường hợp bạn đọc sẽ nhận được những dữ liệu ở dạng thô chưa qua tiền xử lý. Thông thường thì đối với những dữ liệu được nhập và xuất ra qua một hệ thống được phát triển đầy đủ, công việc tiền xử lý chỉ cần một vài bước cơ bản để đi đến kết quả. Tuy nhiên, trong trường hợp dữ liệu bạn nhận được là dữ liệu được nhập một cách thủ công, qua tay nhiều người nhập, thì đây thực sự sẽ là một vấn đề lớn. Tiền xử lý dữ liệu trong tình huống như vậy có thể chiếm từ 80% đến 90% thời gian công việc của bạn!","code":""},{"path":"tiền-xử-lý-dữ-liệu.html","id":"tiền-xử-lý-dữ-liệu-là-gì","chapter":"Chương 5 Tiền xử lý dữ liệu","heading":"5.1 Tiền xử lý dữ liệu là gì?","text":"Các vấn đề thường gặp phải khi làm việc với một dữ liệu thô thường xuất phát từ hai nguyên nhân:Thứ nhất là dữ liệu sai định dạng, nghĩa là trong cùng một cột dữ liệu có các biến kiểu khác nhau hoặc kiểu của biến không đúng như quy ước.Thứ nhất là dữ liệu sai định dạng, nghĩa là trong cùng một cột dữ liệu có các biến kiểu khác nhau hoặc kiểu của biến không đúng như quy ước.Thứ hai là dữ liệu chứa giá trị không quan sát được hoặc chứa các giá trị ngoại lai. Các giá trị ngoại lai thường được gọi là các outliers.Thứ hai là dữ liệu chứa giá trị không quan sát được hoặc chứa các giá trị ngoại lai. Các giá trị ngoại lai thường được gọi là các outliers.Hãy quan sát một ví dụ như sau: bạn nhận được dữ liệu về 3 ứng cử viên từ bộ phận nhân sự và bạn muốn xem xét độ tuổi trung bình của những ứng cử viên và tỷ lệ Nam/Nữ trong danh sách ứng tuyển\nTable 5.1: Dữ liệu thô từ nguồn bên ngoài\nĐây là một dữ liệu không thể sử dụng để phân tích bởi vì giá trị trong các cột ngày sinh là không đúng định dạng ngày tháng. Ngoài ra còn có các giá trị không quan sát được ở cột giới tính. Nếu sử dụng dữ liệu này để phân tích mà bỏ qua việc tiền xử lý dữ liệu thì kết quả sẽ sai lệch hoàn toàn với bản chất của dữ liệu:Tính độ tuổi trung bình của các ứng cử viên là không thể thực hiện được với dữ liệu này bởi vì cột ngày sinh đang có dạng chuỗi ký tự và định dạng ngày tháng là không thống nhất.Tính độ tuổi trung bình của các ứng cử viên là không thể thực hiện được với dữ liệu này bởi vì cột ngày sinh đang có dạng chuỗi ký tự và định dạng ngày tháng là không thống nhất.Nếu bỏ qua những giá trị không có quan sát, tỷ lệ giới tính Nam là 100%. Liệu con số này có thực sự đúng ?Nếu bỏ qua những giá trị không có quan sát, tỷ lệ giới tính Nam là 100%. Liệu con số này có thực sự đúng ?Tiền xử lý dữ liệu không chỉ bao gồm các công cụ kỹ thuật mà còn yêu cầu cả kiến thức phổ thông và kiến thức nghiệp vụ của người làm dữ liệu. Khi có vấn đề gây khó hiểu về dữ liệu nhận được, điều trước hết cần làm đó là liên hệ với người chủ dữ liệu để kiểm tra lại thông tin. Khi việc này là không thể thực hiện được, người xử lý dữ liệu sẽ phải đưa ra các phán đoán về dữ liệu đó dựa trên hiểu biết của mình.Giả sử chúng ta không thể có thêm thông tin nào từ nơi cung cấp dữ liệu, chúng ta cần phải đưa ra phán đoán với dữ liệu kể trên. Trước hết, với cột ngày sinh của các nhân viên:Giá trị “01/02/98” có khả năng cao là ngày 01 tháng 02 năm 1998 quy ước phổ biến ở Việt Nam là viết theo thứ tự ngày -> tháng -> năm.Giá trị “01/02/98” có khả năng cao là ngày 01 tháng 02 năm 1998 quy ước phổ biến ở Việt Nam là viết theo thứ tự ngày -> tháng -> năm.Giá trị “12/17/1999” có khả năng cao là ngày 17 tháng 12 năm 1999. Khi gặp các trường hợp này nhiều khả năng người nhập dữ liệu sử dụng format ngày tháng của Microsoft Excel.Giá trị “12/17/1999” có khả năng cao là ngày 17 tháng 12 năm 1999. Khi gặp các trường hợp này nhiều khả năng người nhập dữ liệu sử dụng format ngày tháng của Microsoft Excel.Giá trị “1-1-1992” có khả năng cao là ngày 01 tháng 01 năm 1992.Giá trị “1-1-1992” có khả năng cao là ngày 01 tháng 01 năm 1992.Như vậy với mỗi giá trị trong cột ngày sinh, bạn đọc cần một phép biến đổi khác nhauVéc-tơ DOB được tính toán trong các câu lệnh ở trên chứa giá trị ngày sinh kiểu dạng ngày tháng đúng định dạng của các ứng cử viên và bạn đọc có thể sử dụng các hàm số có sẵn để tính tuổi của các ứng cử viên.Đối với cột giới tính của nhân viên:Giới tính của ứng cử viên Trần Văn Cường là không quan sát được tuy nhiên theo tên của ứng cử viên thì nhiều khả năng đây là Nam.Giới tính của ứng cử viên Trần Văn Cường là không quan sát được tuy nhiên theo tên của ứng cử viên thì nhiều khả năng đây là Nam.Giới tính của ứng cử viên Lê Thị Loan là không quan sát được tuy nhiên theo tên của ứng cử viên thì nhiều khả năng đây là Nữ.Giới tính của ứng cử viên Lê Thị Loan là không quan sát được tuy nhiên theo tên của ứng cử viên thì nhiều khả năng đây là Nữ.Sau những bước xử lý như trên, chúng ta đã có một dữ liệu được định dạng chính xác như ở dưới. Đây là một dữ liệu đã được làm sạch và sẵn sàng để trả lời cho các câu hỏi như độ tuổi trung bình hay tỷ lệ Nam/Nữ của các ứng cử viên.\nTable 5.2: Dữ liệu sau tiền xử lý\nCác bước ở trên mặc dù rất đơn giản nhưng lại là ví dụ điển hình của tiền xử lý dữ liệu. Dữ liệu bạn đọc nhận được sẽ ít khi được định dạng chuẩn và sẵn sàng để phân tích. Để xử lý những giá trị sai định dạng, và điền vào dữ liệu các giá trị không quan sát, loại bỏ các giá trị ngoại lai,…, người làm dữ liệu phải sử dụng kiến thức phổ thông và kiến thức nghiệp vụ để làm sạch và đưa ra những dự đoán tốt nhất có thể.","code":"\nDOB<-rep(as.Date(\"1900-01-01\"),3)\nDOB[1]<-as.Date(\"01/02/98\", format = \"%d/%m/%y\")\nDOB[2]<-as.Date(\"12/17/1999\", format = \"%m/%d/%Y\")\nDOB[3]<-as.Date(\"1-1-1992\", format = \"%d-%m-%Y\")"},{"path":"tiền-xử-lý-dữ-liệu.html","id":"định-dạng-cột-dữ-liệu-sử-dụng-thư-viện-textbfreadr","chapter":"Chương 5 Tiền xử lý dữ liệu","heading":"5.2 Định dạng cột dữ liệu sử dụng thư viện \\(\\textbf{readr}\\)","text":"","code":""},{"path":"tiền-xử-lý-dữ-liệu.html","id":"quy-tắc-định-dạng-tự-động-của-textbfreadr","chapter":"Chương 5 Tiền xử lý dữ liệu","heading":"5.2.1 Quy tắc định dạng tự động của \\(\\textbf{readr}\\)","text":"\nTable 5.3: Nguyên tắc định tự động định dạng kiểu dữ liệu của readr\nLưu ý rằng các giá trị không quan sát được không ảnh hưởng đến việc \\(\\textbf{readr}\\) dự đoán kiểu dữ liệu của một cột, nghĩa là các hàm đọc dữ liệu sẽ bỏ qua các giá trị NA trong 1000 hàng đầu tiên.Khi đọc dữ liệu kiểu số, sự khác nhau trong cách sử dụng dấu thập phân là . và , có thể làm cho giá trị của biến kiểu số thay đổi về bản chất. Chẳng hạn như nếu bạn đọc không sử dụng thêm tham số \\(\\textbf{readr}\\) luôn mặc định dấu thập phân là . khi bạn sử dụng hàm read_csv() và mặc định dấu thập phân là , khi bạn sử dụng read_csv2(). Hãy quan sát ví dụ dưới đây khi đọc dữ liệu bằng hàm read_csv2() với các giá trị kiểu số:Hàm read_csv2() là hàm dùng để đọc dữ liệu mà các cột được phân tách bằng dấu ; và cho đầu ra là một tibble. Từ kết quả ở trên, có thể đưa ra nhận xét như sau:Cột thứ nhất (C1) có các số đặc biệt như 1e-10 hay Inf đều được hiểu là kiểu số như thông thường.Cột thứ nhất (C1) có các số đặc biệt như 1e-10 hay Inf đều được hiểu là kiểu số như thông thường.Cột thứ hai (C2) chứa các giá trị 2.2 và 3,2 khi được đọc bằng read_csv2() cho kết quả là véc-tơ kiểu số với hai phần tử là 22 và 3.2. Điều này có nghĩa là read_csv2() bỏ qua dấu . trong 2.2 và cho kết quả đầu ra là 22, trong khi giá trị 3,2 có dấu , được hiểu là số thập phân.Cột thứ hai (C2) chứa các giá trị 2.2 và 3,2 khi được đọc bằng read_csv2() cho kết quả là véc-tơ kiểu số với hai phần tử là 22 và 3.2. Điều này có nghĩa là read_csv2() bỏ qua dấu . trong 2.2 và cho kết quả đầu ra là 22, trong khi giá trị 3,2 có dấu , được hiểu là số thập phân.Tương tự, trong cột thứ ba (C3) giá trị 1.0 được hiểu là số 10, trong khi 1,000 được hiểu là số 1 vì , được hiểu là số thập phân.Tương tự, trong cột thứ ba (C3) giá trị 1.0 được hiểu là số 10, trong khi 1,000 được hiểu là số 1 vì , được hiểu là số thập phân.Hàm read_csv2() không phân tích được kiểu dữ liệu trong cột thứ tư (C4) và cột thứ năm (C5) nên kiểu dữ liệu của hai cột này trong kết quả là kiểu chuỗi ký tự.Hàm read_csv2() không phân tích được kiểu dữ liệu trong cột thứ tư (C4) và cột thứ năm (C5) nên kiểu dữ liệu của hai cột này trong kết quả là kiểu chuỗi ký tự.Chúng ta tiếp tục xem xét quy tắc tự động định dạng véc-tơ kiểu logic thông qua ví dụ dưới đây:Hàm read_csv() được sử dụng để đọc dữ liệu từ nguồn ngoài có các cột ngăn cách nhau bằng dấu ,. Có thể thấy rằng các giá trị tương ứng với biến logic được liệt kê trong Bảng 5.3 đều được hàm read_csv() hiểu đúng.Chúng ta chuyển sang kiểu biến tiếp theo là kiểu ngày tháng. Như trình bày trong Bảng 5.3, \\(\\textbf{readr}\\) chỉ có thể tự động định dạng đúng véc-tơ kiểu thời gian nếu giá trị từ nguồn bên ngoài vào được viết theo định dạng “yyyy-mm-dd” hoặc “yyyy/mm/dd”:Từ kết quả của hàm read_csv() có thể thấy cách thư viện \\(\\textbf{readr}\\) tự động định dạng dữ liệu kiểu ngày tháng như sau:Giá trị trong cột được viết theo một trong hai kiểu định dạng là “yyyy-mm-dd” hoặc “yyyy/mm/dd” đều được hiểu là kiểu ngày tháng. Cột thứ nhất (C1) và cột thứ hai (C2) được định nghĩa đúng kiểu ngày tháng trong đầu ra.Giá trị trong cột được viết theo một trong hai kiểu định dạng là “yyyy-mm-dd” hoặc “yyyy/mm/dd” đều được hiểu là kiểu ngày tháng. Cột thứ nhất (C1) và cột thứ hai (C2) được định nghĩa đúng kiểu ngày tháng trong đầu ra.Cột thứ ba (C3) mặc dù giá trị hàng thứ hai bị viết ngược ngày - tháng nhưng \\(\\textbf{readr}\\) vẫn ghi nhận cột này là kiểu ngày tháng.Cột thứ ba (C3) mặc dù giá trị hàng thứ hai bị viết ngược ngày - tháng nhưng \\(\\textbf{readr}\\) vẫn ghi nhận cột này là kiểu ngày tháng.Côt thứ tư (C4) và cột thứ năm (C5) không được hiểu là kiểu ngày tháng dữ liệu không được viết theo một trong hai định dạng trong Bảng 5.3Côt thứ tư (C4) và cột thứ năm (C5) không được hiểu là kiểu ngày tháng dữ liệu không được viết theo một trong hai định dạng trong Bảng 5.3Để tìm hiểu chi tiết hơn cách thư viện \\(\\textbf{readr}\\) tự động định dạng kiểu giá trị của dữ liệu đọc từ nguồn bên ngoài, bạn đọc tham khảo hướng dẫn của hàm guess_parse(). Đây là hàm mặc định dùng để dự đoán kiểu giá trị của các biến.Khi thư viện \\(\\textbf{readr}\\) không thể phân tích được định dạng của các biến, kiểu biến mặc định sẽ là kiểu chuỗi ký tự. Trong các phần tiếp theo, chúng ta sẽ thảo luận về các hàm được sử dụng để chuyển đổi véc-tơ kiểu chuỗi ký tự sang kiểu dữ liệu đúng của véc-tơ đó.","code":"\nfile<-\"C1;C2;C3;C4; C5\n         1e-10;2.2;1.0;TRUE; 1.0.0.0\n         Inf;3,2;1,000.0;1;10%\" \n# Dữ liệu có 5 cột và 2 hàng\nread_csv2(file)## # A tibble: 2 × 5\n##        C1    C2    C3 C4    C5     \n##     <dbl> <dbl> <dbl> <chr> <chr>  \n## 1   1e-10  22      10 TRUE  1.0.0.0\n## 2 Inf       3.2     1 1     10%\nfile<-\"C1,C2,C3,C4,C5,C6\n        TRUE,t,True,false,F,true\n        F,F,FALSE,T,f,True\"\nread_csv(file)## # A tibble: 2 × 6\n##   C1    C2    C3    C4    C5    C6   \n##   <lgl> <lgl> <lgl> <lgl> <lgl> <lgl>\n## 1 TRUE  TRUE  TRUE  FALSE FALSE TRUE \n## 2 FALSE FALSE FALSE TRUE  FALSE TRUE\nfile<-\"C1,C2,C3,C4,C5\n        2020/01/12,2020-01-12,2020/01/12,2020/1/1,2020|1|1\n        2021-12-31,2021/12/31,2021/31/12,2021-12-31,2021-12-31\"\nread_csv(file)## # A tibble: 2 × 5\n##   C1         C2         C3         C4         C5        \n##   <date>     <date>     <date>     <chr>      <chr>     \n## 1 2020-01-12 2020-01-12 2020-01-12 2020/1/1   2020|1|1  \n## 2 2021-12-31 2021-12-31 NA         2021-12-31 2021-12-31"},{"path":"tiền-xử-lý-dữ-liệu.html","id":"định-dạng-véc-tơ-bằng-các-hàm-parse_","chapter":"Chương 5 Tiền xử lý dữ liệu","heading":"5.2.2 Định dạng véc-tơ bằng các hàm parse_*()","text":"Với các cột dữ liệu mà không thể xác định được kiểu biến, thư viện \\(\\textbf{readr}\\) sẽ lưu dưới dạng véc-tơ kiểu chuỗi ký tự. Để làm việc được trên dữ liệu, bạn đọc cần định dạng lại các cột cho đúng với mong muốn. Các hàm parse_*() trong thư viện \\(\\textbf{readr}\\) hỗ trợ bạn đọc thực hiện yêu cầu này. Nhóm hàm parse_*() có đầu vào là một véc-tơ kiểu chuỗi ký tự và đầu ra sẽ là kiểu dữ liệu mà bạn đọc mong muốn. Đối với mỗi kiểu dữ liệu, bao gồm dữ liệu kiểu số, kiểu logic, kiểu thời gian, và kiểu chuỗi ký tự, hàm parse_*() tương ứng sẽ có các tham số phù hợp.","code":""},{"path":"tiền-xử-lý-dữ-liệu.html","id":"định-dạng-véc-tơ-kiểu-logic","chapter":"Chương 5 Tiền xử lý dữ liệu","heading":"5.2.2.1 Định dạng véc-tơ kiểu logic","text":"Định dạng lại một véc-tơ kiểu chuỗi ký tự thành kiểu logic là đơn giản nhất bởi các giá trị có thể nhận của biến logic chỉ bao gồm TRUE hoặc FALSE. Hàm số sử dụng trong trường hợp này là parse_logical(). Bạn đọc hãy quan sát ví dụ sau:Bạn đọc có thể thấy rằng tất cả các giá trị nằm trong véc-tơ \\(\\textbf{x}\\), ngoại trừ hai ký tự đặc biệt đã được khai báo trong hàm parse_logical() là \".\" và \"@\", thì còn có giá trị \"2\" là không thể đổi sang biến kiểu logic. Có thể thấy rằng parse_logical() tự động đổi giá trị \"1\" thành TRUE và giá trị \"0\" thành FALSE. Trong trường hợp véc-tơ \\(\\textbf{x}\\) có kích thước lớn, các phần tử không thể đổi sang kiểu logic sẽ được lưu vào một tibble. Bạn đọc sử dụng hàm problems() để xem danh sách các các giá trị này:Trong kết quả của hàm problem(), cột row cho biết vị trí của các phần tử trong véc-tơ x1 không thể đổi sang biến kiểu logic. Giá trị thực của các phần tử này nằm trong cột actual. Bạn đọc có thể quan sát các giá trị trong cột actual để tìm hiểu nguyên nhân tại sao parse_logical() không thể hoạt động trên các giá trị này.","code":"\nx<-c(\"TRUE\",\"True\",\"1\",\"0\",\"2\",\".\",\"@\",\n     \"FALSE\",\"false\",\"f\",\"F\",\"T\",\"t\",\"true\",\"false\")\nparse_logical(x, na = c(\".\", \"@\"))##  [1]  TRUE  TRUE  TRUE FALSE    NA    NA    NA FALSE FALSE FALSE FALSE  TRUE\n## [13]  TRUE  TRUE FALSE\n## attr(,\"problems\")\n## # A tibble: 1 × 4\n##     row   col expected           actual\n##   <int> <int> <chr>              <chr> \n## 1     5    NA 1/0/T/F/TRUE/FALSE 2\nx1<-sample(x, 10^4, replace = TRUE)\ny<-parse_logical(x1)\nproblems(y)## # A tibble: 1,996 × 4\n##      row   col expected           actual\n##    <int> <int> <chr>              <chr> \n##  1     3    NA 1/0/T/F/TRUE/FALSE @     \n##  2     8    NA 1/0/T/F/TRUE/FALSE 2     \n##  3    11    NA 1/0/T/F/TRUE/FALSE 2     \n##  4    20    NA 1/0/T/F/TRUE/FALSE 2     \n##  5    24    NA 1/0/T/F/TRUE/FALSE @     \n##  6    35    NA 1/0/T/F/TRUE/FALSE .     \n##  7    48    NA 1/0/T/F/TRUE/FALSE 2     \n##  8    51    NA 1/0/T/F/TRUE/FALSE .     \n##  9    56    NA 1/0/T/F/TRUE/FALSE .     \n## 10    66    NA 1/0/T/F/TRUE/FALSE @     \n## # ℹ 1,986 more rows"},{"path":"tiền-xử-lý-dữ-liệu.html","id":"định-dạng-véc-tơ-kiểu-số","chapter":"Chương 5 Tiền xử lý dữ liệu","heading":"5.2.2.2 Định dạng véc-tơ kiểu số","text":"Các nguyên nhân dẫn đến việc các hàm đọc dữ liệu trong thư viện \\(\\textbf{readr}\\) không thể tự động định dạng một véc-tơ có kiểu số là:Cách đánh số thập phân của các số trong véc-tơ. Chẳng hạn như tại Việt Nam số thập phân được sử dụng là dấu phẩy (,) trong khi R hiểu số thập phân là dấu chấm (.). Một số quốc gia khác trên thế giới như Pháp cũng sử dụng dấu thập phân là dấu phẩy.Cách đánh số thập phân của các số trong véc-tơ. Chẳng hạn như tại Việt Nam số thập phân được sử dụng là dấu phẩy (,) trong khi R hiểu số thập phân là dấu chấm (.). Một số quốc gia khác trên thế giới như Pháp cũng sử dụng dấu thập phân là dấu phẩy.Cách viết các số sử dụng cùng với các ký tự chấm hoặc phẩy để người đọc dễ dàng đọc số đó. Chẳng hạn như tại Việt Nam, chúng ta viết số 1 tỷ với dấu chấm phân tách các số không như sau: 1.000.000.000. Tại Thụy Sỹ cách phân tách số lại được viết theo cách khác; số 1 tỷ được viết thành 1’000’000’000. Khi gặp các trường hợp này, chúng ta cần cung cấp cho R định dạng đúng của các số đó.Cách viết các số sử dụng cùng với các ký tự chấm hoặc phẩy để người đọc dễ dàng đọc số đó. Chẳng hạn như tại Việt Nam, chúng ta viết số 1 tỷ với dấu chấm phân tách các số không như sau: 1.000.000.000. Tại Thụy Sỹ cách phân tách số lại được viết theo cách khác; số 1 tỷ được viết thành 1’000’000’000. Khi gặp các trường hợp này, chúng ta cần cung cấp cho R định dạng đúng của các số đó.Khi các con số đi kèm theo đơn vị, chẳng hạn như đi kèm với ký hiệu tiền tệ: “100.000 đồng”, “100.000 vnd”, hoặc đi kèm với ký hiệu % như 50%, các hàm đọc dữ liệu của thư viện \\(\\textbf{readr}\\) cũng sẽ không thể tự động chuyển đổi các giá trị này sang kiểu số nếu không có gợi ý thích hợp.Khi các con số đi kèm theo đơn vị, chẳng hạn như đi kèm với ký hiệu tiền tệ: “100.000 đồng”, “100.000 vnd”, hoặc đi kèm với ký hiệu % như 50%, các hàm đọc dữ liệu của thư viện \\(\\textbf{readr}\\) cũng sẽ không thể tự động chuyển đổi các giá trị này sang kiểu số nếu không có gợi ý thích hợp.Bạn đọc có thể sử dụng parse_double() hoặc parse_number() khi gặp phải các vấn đề ở trên. Chẳng hạn như khi gặp vấn đề về dấu phẩy đối với dấu thập phân, nghĩa là dữ liệu từ nguồn bên ngoài viết số thập phân sử dụng dấu phẩy, bạn đọc sử dụng parse_number() với tham số locale = locale(decimal_mark = \",\") để đổi định dạng véc-tơ kiểu chuỗi ký tự sang véc-tơ kiểu số:Khi gặp phải vấn đề về việc sử dụng các ký tự không đúng định dạng để phân tách các giá trị đơn vị hàng nghìn, hàng triệu,…, chúng ta sử tham số grouping_mark trong hàm locate(). Ví dụ dưới đây sử dụng đồng thời hai tham số decimal_mark và grouping_mark của hàm locate() để biến đổi cách viết số thập phân theo kiểu viết của Việt Nam sang kiểu số thông thường:Khi gặp phải chuỗi ký tự chứa biến kiểu số đi kèm với đơn vị tiền tệ, hoặc đơn vị %, hàm parse_number() vẫn cho phép chuyển đổi chuỗi ký tự sang kiểu số mà không cần sử dụng thêm tham số nào cả. Bạn đọc hãy quan sát ví dụ dưới đây:Bạn đọc cần thận trọng khi véc-tơ kiểu số có % ở phía sau. Hàm parse_number() loại bỏ ký tự % theo sau và giữa nguyên giá trị số đó. Áp dụng parse_number() trên giá trị 2000.5% cho kết quả là 2000.5 chứ không phải là 20.005. Để có giá trị đúng kiểu số, chúng ta cần chia kết quả cho 100.","code":"\nx<-c(\"0,5\",\"1,5\") # Dấu thập phân là dấu phẩy\nparse_number(x, locale = locale(decimal_mark = \",\"))## [1] 0.5 1.5\nx<-c(\"1.000,5\",\"1.000.000,5\") \n# véc-tơ chứa các số 1000,5 và 1000000,5; \n# dấu thập phân là dấu \",\"\n# phân tách hàng nghìn, triệu là dấu .\nparse_number(x, locale = locale(decimal_mark = \",\",\n                                grouping_mark = \".\"))## [1]    1000.5 1000000.5\nx<-c(\"1.000,5 đồng\",\"1.000.000,5 vnd\", \"2.000,5%\")\n# số kiểu Việt Nam\n# có đơn vị tiền phía sau\n# có ký hiệu % phía sau\nparse_number(x, locale = locale(decimal_mark = \",\",\n                                grouping_mark = \".\"))## [1]    1000.5 1000000.5    2000.5"},{"path":"tiền-xử-lý-dữ-liệu.html","id":"định-dạng-véc-tơ-kiểu-thời-gian","chapter":"Chương 5 Tiền xử lý dữ liệu","heading":"5.2.2.3 Định dạng véc-tơ kiểu thời gian","text":"Hàm số parse_datetime() có thể sử dụng để chuyển đổi các véc-tơ kiểu chuỗi ký tự sang véc-tơ kiểu thời gian và véc-tơ kiểu ngày tháng.Hai tham số của hàm parse_datetime() mà bạn đọc cần lưu ý là na và format. Tham số na là một véc-tơ chứa các giá trị mà bạn đọc cho rằng đây là các giá trị không quan sát được. Trong véc-tơ x ở trên, giá trị \"01/01/1990\" được gán cho tham số na mặc dù giá trị này là ngày tháng có ý nghĩa. Điều này khiến cho giá trị thứ ba trong véc-tơ kết quả có giá trị là NA. Nếu không sử dụng tham số na, giá trị thứ ba của véc-tơ kết quả sẽ là ngày 01 tháng 01 năm 1990. Đối với một vài hệ thống lưu dữ liệu, có thể xảy ra trường hợp các giá trị không được ghi nhận nhưng vẫn được gán một giá trị mặc định nào đó. Các giá trị mặc định này nếu giữ nguyên giá trị sẽ làm sai lệch phân tích. Giả sử với véc-tơ ở trên, nếu chúng ta biết giá trị mặc định gán cho các giá trị không quan sát được của hệ thống là “01/01/1990”, việc gán giá trị này cho tham số na là cần thiết.Tham số format sử dụng trong hàm parse_datetime() là gợi ý cho R về định dạng của biến kiểu ngày tháng. Khi gán giá trị cho tham số format, bạn đọc cần lưu ý:Mỗi thành phần của biến thời gian (ngày, tháng, năm, giờ, phút, giây,…) được định nghĩa bắt đầu bằng % và theo sau 1 chữ cái, chẳng hạn như bạn đọc sử dụng %Y khi muốn gợi ý rằng biến kiểu thời gian nằm trong chuỗi ký tự được sử dụng 4 chữ số để chỉ định giá trị năm.Mỗi thành phần của biến thời gian (ngày, tháng, năm, giờ, phút, giây,…) được định nghĩa bắt đầu bằng % và theo sau 1 chữ cái, chẳng hạn như bạn đọc sử dụng %Y khi muốn gợi ý rằng biến kiểu thời gian nằm trong chuỗi ký tự được sử dụng 4 chữ số để chỉ định giá trị năm.Các ký tự không liên quan đến các thành phần của thời gian, ngoại trừ các khoảng trắng phía trước và sau biến thời gian, cần phải được khai báo chính xác. Hãy quan sát ví dụ dưới đâyCác ký tự không liên quan đến các thành phần của thời gian, ngoại trừ các khoảng trắng phía trước và sau biến thời gian, cần phải được khai báo chính xác. Hãy quan sát ví dụ dưới đâyTừ ví dụ trên bạn đọc có thể thấy rằngCần khai báo chính xác các ký tự nằm giữa các biến thời gian. Ký tự \"@\" nằm giữa các giá trị ngày, tháng, năm; phân tách giữa ngày tháng với thời gian trong ngày là ký tự \"-\"; phân tách giữa các thành phần của thời gian trong ngày là ký tự \"#\". Tất cả đều cần phải được khai báo chính xác trong tham số format. Điều này giải thích tại sao hai giá trị đầu được chuyển đổi sang dạng biến thời gian. Giá trị thứ ba trong véc-tơ x gặp vấn đề vì phân tách giữa các thành phần của thời gian trong ngày sử dụng dấu \":\".Cần khai báo chính xác các ký tự nằm giữa các biến thời gian. Ký tự \"@\" nằm giữa các giá trị ngày, tháng, năm; phân tách giữa ngày tháng với thời gian trong ngày là ký tự \"-\"; phân tách giữa các thành phần của thời gian trong ngày là ký tự \"#\". Tất cả đều cần phải được khai báo chính xác trong tham số format. Điều này giải thích tại sao hai giá trị đầu được chuyển đổi sang dạng biến thời gian. Giá trị thứ ba trong véc-tơ x gặp vấn đề vì phân tách giữa các thành phần của thời gian trong ngày sử dụng dấu \":\".Các khoảng trắng nằm trước và sau các chuỗi ký tự được bỏ qua và không ảnh hưởng đến kết quả. Các giá trị thứ nhất và thứ hai trong véc-tơ x có khoảng trắng phía trước và phía sau nhưng hàm parse_datetime() bỏ qua các khoảng trắng đo khi chuyển đổi ký tự sang ngày tháng.Các khoảng trắng nằm trước và sau các chuỗi ký tự được bỏ qua và không ảnh hưởng đến kết quả. Các giá trị thứ nhất và thứ hai trong véc-tơ x có khoảng trắng phía trước và phía sau nhưng hàm parse_datetime() bỏ qua các khoảng trắng đo khi chuyển đổi ký tự sang ngày tháng.Để biết một cách chính xác cách gán giá trị cho tham số format, bạn đọc nên tham khảo hướng dẫn sử dụng hàm parse_datetime(). Chúng tôi tóm tắt cách định dạng các thành phần của một biến thời gian trong bảng 5.4\nTable 5.4: Định nghĩa các thành phần của biến thời gian của \\(\\textbf{readr}\\)\nLưu ý rằng khi bạn đọc sử dụng %y để định nghĩa cho giá trị năm, các ký tự từ “00” đến “69” sẽ được chuyển thành năm 2000 đến năm 2069. Trong khi đó, các ký tự từ “70” đến “99” sẽ được chuyển thành năm 1970 đến 1999. Ngoài ra, thành phần tháng của biến thời gian trong nhiều dữ liệu thường được viết dưới dạng chuỗi ký tự thay vì sử dụng số. đó bạn đọc cần các gợi ý %b hoặc %B để gợi ý cho R. Hãy quan sát ví dụ sau:Khi ngày tháng được viết bằng chuỗi ký tự viết tắt, bằng chữ hoa hoặc chữ thường, như \"sep\" hay \"JAN\", gợi ý cần sử dụng là %b. Điều này giải thích tại sao kết quả của hàm parse_datetime() thứ nhất cho kết quả đúng định dạng ngày tháng đối với ba giá trị đầu của véc-tơ, và cho kết quả là NA với phân tử thứ tư khi tháng được viết đầy đủ là \"april\". Ngược lại, hàm parse_datetime() thứ hai cho kết quả ba giá trị đầu của véc-tơ là NA và phần tử thứ tư là giá trị ngày tháng được định dạng đúng là chúng ta sử dụng tham số format với gợi ý cho cách viết tháng là %B.","code":"\nx<-c(\"1/2/2023\", \"23/10/2023 \", \"01/01/1900\")\nparse_datetime(x, format = \"%d/%m/%Y\",\n               na = c(\"01/01/1900\"))## [1] \"2023-02-01 UTC\" \"2023-10-23 UTC\" NA\nx<-c(\" 1@2@2023-23#25#01  \", \"  23@10@2023-01#06#59 \", \"01@01@2023-00:00:00\")\nparse_datetime(x, format = \"%d@%m@%Y-%H#%M#%S\")## [1] \"2023-02-01 23:25:01 UTC\" \"2023-10-23 01:06:59 UTC\"\n## [3] NA\n# Gợi ý cho R là ngày, tháng, năm cách nhau bởi @\n# và giờ phút, giây cách nhau bởi #\nx<-c(\"sep 21, 23 \", \"  JAN 1, 69 \", \"Dec 25, 70\", \"april 3, 99\")\nparse_datetime(x, format = \"%b %d, %y\")## [1] \"2023-09-21 UTC\" \"1969-01-01 UTC\" \"1970-12-25 UTC\" NA\nparse_datetime(x, format = \"%B %d, %y\")## [1] NA               NA               NA               \"1999-04-03 UTC\""},{"path":"tiền-xử-lý-dữ-liệu.html","id":"định-dạng-cột-kiểu-chuỗi-ký-tự","chapter":"Chương 5 Tiền xử lý dữ liệu","heading":"5.2.3 Định dạng cột kiểu chuỗi ký tự","text":"Khi bạn đọc dùng thư viện \\(\\textbf{readr}\\) để đọc dữ liệu từ nguồn bên ngoài, cột dữ liệu không rõ định dạng sẽ được lưu dưới dạng véc-tơ chuỗi ký tự. Vậy tại sao cần định dạng lại thành véc-tơ kiểu chuỗi ký tự ? Nghe có vẻ vô lý nhưng đây lại là vấn đề phức tạp nhất trong định dạng lại cột dữ liệu. Để hiểu vấn đề này bạn đọc cần tìm hiểu một chút về cách máy tính điện tử lưu và mở một chuỗi ký tự. Giả sử bạn đọc muốn gửi một dữ liệu chứa ký tự “” đến một máy tính khác. Sau khi viết ký tự “” lên một phần mềm soạn thảo văn bản nào, bạn đọc sẽ cần lưu ký tự “” lên máy tính của bạn. Tất nhiên máy tính của bạn sẽ không thể ghi nhớ chữ “” một cách tượng hình mà sẽ mã hóa (hay thuật ngữ chuyên ngành gọi là \\(encode\\)) chữ “” thành một đoạn mã nhị phân bao gồm 0 và 1 mà máy tính có thể lưu được. Khi bạn gửi dữ liệu sang một máy tính khác, đoạn mã bao gồm 0 và 1 đó sẽ được gửi đi. Khi máy tính điện tử khác mở dữ liệu, đoạn mã nhị phân sẽ được giải mã (thuật ngữ chuyên ngành gọi là \\(decode\\)) để hiển thị. Sẽ không có vấn đề gì xảy ra nếu quy tắc mã hóa và giải mã được thống nhất và chữ “” sẽ được hiển thị chính xác trên máy tính thứ hai.Thực tế là trước khi có bộ mã hóa và quy tắc mã hóa chung được công nhận rộng rãi như Unicode và UTF-8, rất khó để có sự thống nhất quy tắc mã hóa ký tự. May mắn là đến thời điểm chúng tôi viết cuốn sách này đa số các hệ điều hành, hệ soạn thảo văn bản,… đều sử dụng bảng mã Unicode và bộ mã hóa UTF-8. Giải thích chi tiết về bộ mã hóa hay quy tắc mã hóa là rất phức tạp và vượt quá nội dung của cuốn sách này. Chúng tôi chỉ cần bạn đọc hiểu về Unicode và UTF-8 như sau:Unicode là một bảng mã chuẩn được công nhận rộng rãi cho biết quy tắc cho tương ứng hầu hết các ký tự từ đơn giản đến phức tạp, kể cả các ngôn ngữ sử dụng ký tự tượng hình phức tạp như chữ Hán của tiếng Trung Quốc, tiếng Nhật, chữ Nôm của tiếng Việt, với một số nằm giữa số 0 đến số \\(10FFFF\\) khi viết theo hệ 16. Một số khi viết trong hệ 16 có thể sử dụng (0, 1, …, 9, , B, C, D, E, F) để biểu diễn, đó số các ký tự mà bảng mã Unicode có thể đưa vào là \\(16^4 + 16^5 = 1.114.112\\) ký tự, bao gồm \\(16^5\\) số từ 0 đến FFFFF và \\(16^4\\) số từ 100000 đến 10FFFF. Ví dụ, bạn đọc có thể dễ dàng tìm thấy được qua các công cụ tìm kiếm rằng ký tự “” có mã Unicode là “0041” và “” có mã Unicode là “0061”.Unicode là một bảng mã chuẩn được công nhận rộng rãi cho biết quy tắc cho tương ứng hầu hết các ký tự từ đơn giản đến phức tạp, kể cả các ngôn ngữ sử dụng ký tự tượng hình phức tạp như chữ Hán của tiếng Trung Quốc, tiếng Nhật, chữ Nôm của tiếng Việt, với một số nằm giữa số 0 đến số \\(10FFFF\\) khi viết theo hệ 16. Một số khi viết trong hệ 16 có thể sử dụng (0, 1, …, 9, , B, C, D, E, F) để biểu diễn, đó số các ký tự mà bảng mã Unicode có thể đưa vào là \\(16^4 + 16^5 = 1.114.112\\) ký tự, bao gồm \\(16^5\\) số từ 0 đến FFFFF và \\(16^4\\) số từ 100000 đến 10FFFF. Ví dụ, bạn đọc có thể dễ dàng tìm thấy được qua các công cụ tìm kiếm rằng ký tự “” có mã Unicode là “0041” và “” có mã Unicode là “0061”.UTF-8 là quy tắc lưu các số viết trong hệ 16 của bảng mã Unicode thành các chuỗi nhị phân 0 và 1 mà máy tính có thể phân biệt được. Số 8 ở đây có nghĩa là 8 bit hay một byte là 8 giá trị 0 và 1 đứng liền nhau. Một ký tự bất kỳ trong bảng mã Unicode đều có thể được mã hóa thành 1, 2, 3 hoặc nhiều byte theo quy tắc mã hóa UTF-8. Chữ “” với mã Unicode “0041” sẽ được lưu trong máy tính điện tử dưới dạng một byte là “01000001”, hay “” có mã Unicode “0061” được máy tính điện tử lưu bằng 1 byte có giá trị “01100001”.UTF-8 là quy tắc lưu các số viết trong hệ 16 của bảng mã Unicode thành các chuỗi nhị phân 0 và 1 mà máy tính có thể phân biệt được. Số 8 ở đây có nghĩa là 8 bit hay một byte là 8 giá trị 0 và 1 đứng liền nhau. Một ký tự bất kỳ trong bảng mã Unicode đều có thể được mã hóa thành 1, 2, 3 hoặc nhiều byte theo quy tắc mã hóa UTF-8. Chữ “” với mã Unicode “0041” sẽ được lưu trong máy tính điện tử dưới dạng một byte là “01000001”, hay “” có mã Unicode “0061” được máy tính điện tử lưu bằng 1 byte có giá trị “01100001”.Quay trở lại vấn đề định dạng lại dữ liệu kiểu chuỗi ký tự, sẽ không có vấn đề xảy ra nếu người nhập liệu sử dụng bộ mã hóa UTF-8 bởi \\(\\textbf{readr}\\) luôn sử dụng UTF-8 để giải mã. Trong thực tế thì vẫn còn một số hệ thống, hoặc hệ soạn thảo văn bản sử dụng cách mã hóa khác với UTF-8. Điều này làm cho dữ liệu khi được nhập vào R sẽ hiển thị không đúng như mong muốn. Ví dụ, khi đọc một dữ liệu từ nguồn ngoài vào bằng read_csv() và cho kết quả như sauCột \\(\\) của dữ liệu đã không được lưu bằng bộ mã hóa UTF-8 nên thư viện \\(\\textbf{readr}\\) không hiển thị được các chuỗi ký tự có ý nghĩa. Để định dạng lại cột dữ liệu, bạn đọc sử dụng hàm parse_character() với tham số encoding. Không dễ để biết được dữ liệu đã được mã hóa bằng bộ mã hóa nào. Thư viện \\(\\textbf{readr}\\) cung cấp hàm guess_encoding() hỗ trợ bạn đọc dự đoán một biến kiểu chuỗi ký tự đã được mã hóa bẳng bộ mã hóa nào. Tuy nhiên trải nghiệm của chúng tôi với hàm số này là không tốt! Lời khuyên của chúng tôi là bạn đọc khi có thể hãy tìm hiểu nguồn gốc của dữ liệu: dữ liệu được sinh ra từ đâu, hoặc từ hệ thống nào,…, để đưa ra phán đoán. Nếu không thể tìm kiếm nguồn gốc của dữ liệu, giải pháp duy nhất là thử giải mã đoạn văn bản bằng một số bộ mã hóa thường gặp cho đến khi gặp được kết quả mong muốn! Trong trường hợp dữ liệu ở trên nguồn là tiếng Việt nên chúng ta có thể thử các bộ mã hóa như “Latin1” hay “Latin2”. Cách sử dụng hàm parse_character() như sau:Kết quả khi sử dụng bộ mã \\(Latin2\\) đã cho một vài giá trị có ý nghĩa, chúng ta tiếp tục thử với \\(Latin1\\):May mắn là cột dữ liệu đều đã có thể đọc được với người Việt Nam. Chúng ta có thể suy đoán đây là một dữ liệu về giá của các loại quả, đó cột \\(B\\) của dữ liệu cần được định dạng lại kiểu số. Bạn đọc có thể sử dụng parse_numbder() như đã trình bày ở trên. Dữ liệu sau khi được định dạng lại các cột đã dễ hiểu hơn rất nhiều:","code":"\nx<-read_csv(\"../KHDL_KTKD Final/Dataset/Book1.csv\")\nx## # A tibble: 5 × 2\n##   A              B         \n##   <chr>          <chr>     \n## 1 \"l\\xea\"        20.000 vnd\n## 2 \"t\\xe1o\"       35.000 vnd\n## 3 \"qu\\xfdt\"      30.000 vnd\n## 4 \"c\\xe0 t\\xedm\" 5.500 vnd \n## 5 \"m\\xedt\"       10.000 vnd\nparse_character(x$A, locale = locale(encoding = \"Latin2\"))## [1] \"lę\"     \"táo\"    \"quýt\"   \"cŕ tím\" \"mít\"\nparse_character(x$A, locale = locale(encoding = \"Latin1\"))## [1] \"lê\"     \"táo\"    \"quýt\"   \"cà tím\" \"mít\"\ntibble(Name = parse_character(x$A, locale = locale(encoding = \"Latin1\")), \n      Price = parse_number(x$B, locale = locale(grouping_mark = \".\")))## # A tibble: 5 × 2\n##   Name   Price\n##   <chr>  <dbl>\n## 1 lê     20000\n## 2 táo    35000\n## 3 quýt   30000\n## 4 cà tím  5500\n## 5 mít    10000"},{"path":"tiền-xử-lý-dữ-liệu.html","id":"xử-lý-giá-trị-không-quan-sát-được","chapter":"Chương 5 Tiền xử lý dữ liệu","heading":"5.3 Xử lý giá trị không quan sát được","text":"Giá trị không quan sát được là các giá trị NA xuất hiện trong dữ liệu khi nhập vào R. Có nhiều lý khác nhau dẫn đến việc dữ liệu không quan sát được. Chẳng hạn như thông tin người làm dữ liệu cung cấp không đầy đủ, hoặc người cung cấp dữ liệu từ chối chia sẻ thông tin, hoặc hệ thống quản lý dữ liệu bị lỗi, hoặc cũng có thể người quản lý dữ liệu chủ động xóa dữ liệu vì lý bảo mật. Giá trị không quan sát được ngoài các giá trị NA xuất hiện trong dữ liệu còn có thể là các giá trị không phù hợp với kiểu dữ liệu hoặc miền giá trị của cột dữ liệu. Đối với một vài hệ thống, khi dữ liệu được xuất ra giá trị không quan sát được vẫn được ghi nhận bằng một giá trị nào đó. Bạn đọc cần cẩn trọng khi làm việc với những dữ liệu như vậy.","code":""},{"path":"tiền-xử-lý-dữ-liệu.html","id":"xác-định-giá-trị-không-quan-sát-được","chapter":"Chương 5 Tiền xử lý dữ liệu","heading":"5.3.1 Xác định giá trị không quan sát được","text":"Khi không được xử lý thích hợp, giá trị không quan sát được sẽ làm sai lệch kết quả của các phân tích về dữ liệu, có thể khiến người ra quyết định dựa trên dữ liệu mắc phải sai lầm. Ví dụ, một yêu cầu về phân tích độ tuổi và giới tính của sinh viên được gửi kèm với dữ liệu như sau:Trong dữ liệu ở trên, mặc dù chỉ có 1 giá trị đang là NA ở cột giới tính, nhưng nếu quan sát kỹ bạn đọc sẽ nhận ra rằng:Trong cột Name: giá trị \"12345\" không thể là tên của một sinh viên, đó đây cũng là một giá trị không quan sát được.Trong cột Name: giá trị \"12345\" không thể là tên của một sinh viên, đó đây cũng là một giá trị không quan sát được.Trong cột Age: thứ nhất, giá trị ở hàng thứ hai là kiểu chuỗi ký tự Nhập sai ngày sinh. Thứ hai, tuổi của một sinh viên không thể là số âm, nên giá trị -1 ở hàng thứ ba không phù hợp với miền giá trị của cột này. Như vậy, cột Age có hai giá trị không quan sát được.Trong cột Age: thứ nhất, giá trị ở hàng thứ hai là kiểu chuỗi ký tự Nhập sai ngày sinh. Thứ hai, tuổi của một sinh viên không thể là số âm, nên giá trị -1 ở hàng thứ ba không phù hợp với miền giá trị của cột này. Như vậy, cột Age có hai giá trị không quan sát được.Cột Gender: có giá trị là ký tự N không rõ là thể hiện cho giới tính Nam hay Nữ, giá trị này cũng là không quan sát được.Cột Gender: có giá trị là ký tự N không rõ là thể hiện cho giới tính Nam hay Nữ, giá trị này cũng là không quan sát được.Cột MSV: Giả sử bằng một cách nào đó, bạn đọc biết rằng mã sinh viên phải là một đoạn ký tự có độ dài là 8, bao gồm đoạn ký tự “MSV” và theo sau là 5 chữ số, thì giá trị \"MS34\" ở hàng thứ tư cũng là một giá trị không quan sát được ở cột mã sinh viên.Cột MSV: Giả sử bằng một cách nào đó, bạn đọc biết rằng mã sinh viên phải là một đoạn ký tự có độ dài là 8, bao gồm đoạn ký tự “MSV” và theo sau là 5 chữ số, thì giá trị \"MS34\" ở hàng thứ tư cũng là một giá trị không quan sát được ở cột mã sinh viên.Để xác định dữ liệu có giá trị ngoại lai hay không cần sử dụng các kiến thức chuyên môn về dữ liệu và các kiến thức về xác suất - thống kê toán:Cột Height có giá trị chiều cao ở hàng thứ nhất là 1.76 cm. Giá trị này quá nhỏ để làm chiều cao của một người bình thường. Nhiều khả năng khi đo chiều cao của sinh viên, người nhập dữ liệu đã ghi lại theo đơn vị mét.Cột Height có giá trị chiều cao ở hàng thứ nhất là 1.76 cm. Giá trị này quá nhỏ để làm chiều cao của một người bình thường. Nhiều khả năng khi đo chiều cao của sinh viên, người nhập dữ liệu đã ghi lại theo đơn vị mét.Cột Weight có giá trị cân nặng của hàng thứ tư là 150 kg. Mặc dù dữ liệu có rất ít quan sát để đưa ra kết luận phân phối xác suất của cân nặng của sinh viên là gì, tuy nhiên với kiến thức thực tế chúng ta có thể kết luận rằng 150 kg là một cân nặng lớn bất thường với các giá trị cân nặng còn lại. Đây nhiều khả năng là một giá trị ngoại lai.Cột Weight có giá trị cân nặng của hàng thứ tư là 150 kg. Mặc dù dữ liệu có rất ít quan sát để đưa ra kết luận phân phối xác suất của cân nặng của sinh viên là gì, tuy nhiên với kiến thức thực tế chúng ta có thể kết luận rằng 150 kg là một cân nặng lớn bất thường với các giá trị cân nặng còn lại. Đây nhiều khả năng là một giá trị ngoại lai.Để xác định các giá trị không quan sát được và giá trị ngoại lai tùy thuộc vào từng dữ liệu cụ thể và kiến thức tổng hợp và kiến thức chuyên môn của người xử lý dữ liệu và nằm ngoài phạm vi thảo luận của cuốn sách này. Dữ liệu ở trên chỉ là một dữ liệu nhỏ và đơn giản nên việc xác định các giá trị không quan sát được và giá trị ngoại lai là đơn giản.Khi dữ liệu có giá trị NA, người phân tích dữ liệu luôn luôn là tìm cách dự đoán giá trị thay vì xóa đi quan sát có chứa NA. Trước hết, chúng ta biến đổi các giá trị không quan sát được thành giá trị NA:Đối với các giá trị ngoại lai, chúng ta sẽ đổi giá trị bị ghi nhận sai đơn vị về đúng đơn vị. Với giá trị cân nặng 150 kg, dữ liệu nhỏ, bạn đọc có thể giữ nguyên giá trị này hoặc thay thế giá trị này bằng giá trị lớn nhất của những người có cân nặng thông thường.Dữ liệu sau khi xử lý giá trị ngoại lai và định nghĩa lại các giá trị không quan sát được như sau:Với những dữ liệu nhỏ như trên thì hiển thị trực tiếp dữ liệu cũng cho phép người phân tích xác định vị trí của giá trị không quan sát được trong mỗi cột. Với dữ liệu lớn thì hiển thị dữ liệu không phải là cách xác định NA hiệu quả. Các hàm số cho phép bạn đọc hình dung tốt về giá trị không quan sát được trong dữ liệu bao gồm có summary(), .na().Hàm summary() sử dụng trên dữ liệu cho kết quả là các giá trị thống kê mô tả của các biến trong dữ liệu, bao gồm cả số lượng giá trị không quan sát được của các biến dạng số và dạng factor. Cách sử dụng hàm summary() như sauBạn đọc có thể thấy rằng hàm summary() cho chúng ta biết trong mỗi cột Age và Gender có hai giá trị không quan sát được, trong khi trong các cột Height và Weight không có giá trị không quan sát được. Hạn chế của hàm summary() là không cho chúng ta biết số lượng giá trị không quan sát được trong các biến kiểu chuỗi ký tự.Hàm số thường được sử dụng để xác định vị trí của giá trị không quan sát được là hàm .na(). Hàm .na() áp dụng trên một véc-tơ, một ma trận, một dữ liệu, hay một mảng nhiều chiều sẽ trả lại kết quả tương ứng là một véc-tơ, một ma trận, hay một mảng nhiều chiều kiểu logic có kích thước bằng với kích thước của đầu vào, đồng thời tại vị trí tương ứng của giá trị không quan sát được sẽ có giá trị là TRUE, và giá trị FALSE tại các vị trí còn lại. Ví dụ, chúng ta áp dụng hàm .na() trên dữ liệu df sẽ cho kết quả là một ma trận kiểu logic kích thước \\(4 \\times 6\\) tương ứng với 4 hàng và 6 cột của dữ liệu:Bạn đọc có thể nhận thấy các vị trí nhận giá trị TRUE trong ma trận kết quả tương ứng với giá trị không quan sát được trong các cột MSV, Name, Age, và Gender của dữ liệu df.Khi dữ liệu lớn thì việc hiển thị trực tiếp kết quả của hàm .na() là không hiệu quả. Chúng ta cần kết hợp .na() với các hàm số khác và các kỹ thuật trực quan hóa để đánh giá được tỷ lệ không quan sát được của các biến trong dữ liệu. Ví dụ, khi thực hiện phân tích trên dữ liệu có tên \\(\\textbf{gapminder}\\) có kích thước \\(10545 \\text{ dòng } \\times 9 \\text{ cột }\\) của thư viện \\(\\textbf{dslabs}\\), chúng ta có thể kết hợp .na() với đồ thị dạng cột để mô tả tỷ lệ số giá trị không quan sát được trong mỗi cột như sau:\nFigure 5.1: Mô tả tỷ lệ giá trị không quan sát được của các biến trong dữ liệu gapminder bằng đồ thị dạng cột\nCó thể dễ dàng nhận thấy rằng biến \\(\\textit{gdp}\\) có tỷ lệ giá trị không quan sát được cao nhất lên đến khoảng 28%, sau đó là biến \\(\\textit{infant_mortality}\\) với tỷ lệ giá trị không quan sát được khoảng 14%. Các biến \\(\\textit{population}\\) và \\(\\textit{fertility}\\) có tỷ lệ giá trị không quan sát được là khoảng 2%. Các biến còn lại gần như không có giá trị không quan sát được.Chúng ta có thể trực quan hóa tỷ lệ giá trị không quan sát được của các biến của dữ liệu một cách chi tiết hơn. các hàm số phục vụ cho trực quan hóa dữ liệu được giới thiệu trong phần sau của cuốn sách nên chúng tôi không giới thiệu chi tiết ở phần này. Chúng tôi sẽ mô tả cách xác định giá trị không quan sát được bằng một hàm số Visual_Na() mà chúng tôi tự phát triển. Hàm số có đầu vào là một dữ liệu và tên một biến rời rạc. Kết quả của hàm Visual_Na() sẽ cho bạn đọc thông tin về tỷ lệ giá trị không quan sát được của từng biến khi chia dữ liệu thành các nhóm nhỏ theo từng giá trị của biến rời rạc. Ví dụ, bạn đọc muốn tìm hiểu về tỷ lệ giá trị không quan sát được của các biến trong dữ liệu \\(\\textbf{gapminder}\\) theo từ năm, nghĩa là theo biến \\(\\textit{year}\\), bạn đọc chỉ cần khai báo hàm Visual_Na() sau đó thực thi hàm số này:\nFigure 5.2: Tỷ lệ giá trị không quan sát được của các biến trong dữ liệu gapminder theo năm bắt đầu từ 1960 đến 2016\nHình 5.2 cho biết chi tiết hơn về tỷ lệ giá trị NA xuất hiện trong các biến với hình 5.1. Có thể thấy rằng biến \\(\\textit{gdp}\\) có tỷ lệ NA cao nhất là giai đoạn 2012 đến 2016 tỷ lệ NA là gần 100%. Biến \\(\\textit{infant_mortality}\\) ngoài năm 2016 có tỷ lệ NA là 100% còn có tỷ lệ giá trị không quan sát được khá cáo trong các năm trước năm 1980. Hai biến \\(\\textit{population}\\) và \\(\\textit{fertility}\\) có tỷ lệ giá trị NA là 100% vào 2016, trong khi các năm khác tỷ lệ NA là bằng 0.","code":"## # A tibble: 4 × 6\n##   MSV      Name            Age                Gender `Height (cm)` `Weight (kg)`\n##   <chr>    <chr>           <chr>              <chr>          <dbl>         <dbl>\n## 1 MSV00001 12345           30                 Nam             1.76            68\n## 2 MSV43241 Nguyễn Văn An   Nhập sai ngày sinh N             169               72\n## 3 MSV65432 Lê Thị Loan     -1                 Nữ            155               48\n## 4 MSV34    Trần Mạnh Cường 15                 <NA>          175              150\ndf$MSV[(nchar(df$MSV)!=8)]<-NA # mã sinh viên không có 8 ký tự là không quan sát được\ndf$Name[df$Name==\"12345\"]<-NA\ndf$Age<-parse_number(df$Age, na = c(\"-1\")) # tuổi có giá trị (-1) là không quan sát được\ndf$Gender[df$Gender == \"N\"]<-NA\ndf$Gender<-as.factor(df$Gender)\ndf$`Height (cm)`[1]<-df$`Height (cm)`[1] * 100 # đổi đơn vị đo từ mét sang cm\ndf## # A tibble: 4 × 6\n##   MSV      Name              Age Gender `Height (cm)` `Weight (kg)`\n##   <chr>    <chr>           <dbl> <fct>          <dbl>         <dbl>\n## 1 MSV00001 <NA>               30 Nam              176            68\n## 2 MSV43241 Nguyễn Văn An      NA <NA>             169            72\n## 3 MSV65432 Lê Thị Loan        NA Nữ               155            48\n## 4 <NA>     Trần Mạnh Cường    15 <NA>             175           150\nsummary(df)##      MSV                Name                Age         Gender   Height (cm)   \n##  Length:4           Length:4           Min.   :15.00   Nam :1   Min.   :155.0  \n##  Class :character   Class :character   1st Qu.:18.75   Nữ  :1   1st Qu.:165.5  \n##  Mode  :character   Mode  :character   Median :22.50   NA's:2   Median :172.0  \n##                                        Mean   :22.50            Mean   :168.8  \n##                                        3rd Qu.:26.25            3rd Qu.:175.2  \n##                                        Max.   :30.00            Max.   :176.0  \n##                                        NA's   :2                               \n##   Weight (kg)   \n##  Min.   : 48.0  \n##  1st Qu.: 63.0  \n##  Median : 70.0  \n##  Mean   : 84.5  \n##  3rd Qu.: 91.5  \n##  Max.   :150.0  \n## \nis.na(df)##        MSV  Name   Age Gender Height (cm) Weight (kg)\n## [1,] FALSE  TRUE FALSE  FALSE       FALSE       FALSE\n## [2,] FALSE FALSE  TRUE   TRUE       FALSE       FALSE\n## [3,] FALSE FALSE  TRUE  FALSE       FALSE       FALSE\n## [4,]  TRUE FALSE FALSE   TRUE       FALSE       FALSE\n# Tính tỷ lệ giá trị NA trong mỗi cột\ny<-sapply(gapminder,\n          function(x) sum(is.na(x))/length(x)) \n\n# Dùng đồ thị dạng cột để mô tả tỷ lệ NA\ndf<-data.frame(variable = names(y), NA_rate = y, row.names = NULL)\ndf%>%arrange(NA_rate)%>%\n  ggplot(aes(y = variable, x = NA_rate))+\n  geom_bar(stat = \"identity\",alpha = 0.7, color = \"black\", fill = \"blue\")+\n  theme_minimal()+scale_x_continuous(labels = scales::percent)+\n  ylab(\"\")+\n  xlab(\"Tỷ lệ NA\")\nVisual_Na<-function(df,variable){\n  # Tìm chỉ số của biến variable\n  ind<-names(df)==variable\n  \n  # Tính tỷ lệ NA của từng biến theo từng nhóm\n  # Nhóm đươc xác định theo giá trị của variable\n  df1<-df%>%group_by(df[,ind])%>%\n    group_modify(~summarize(.x, across(everything(), function(x) sum(is.na(x))/length(x) ))) %>%\n    as.data.frame()%>%gather(variables,na_rate,-1)\n  \n  # Đặt lại tên biến nhóm theo\n  names(df1)[1]<-\"variable\"\n  \n  # Biểu diễn đồ thị\n  p<-df1%>%ggplot(aes(x = variable, y = variables, fill = na_rate))+\n    geom_tile(color = \"grey\", height = 1, width = 1)+\n    scale_fill_gradient(low=\"white\", high = \"orange\",\n                        labels = scales::label_percent())+\n    theme_minimal()+ylab(\"\")+xlab(\"\")\n  return(p)\n}\nVisual_Na(gapminder,\"year\")"},{"path":"tiền-xử-lý-dữ-liệu.html","id":"xử-lý-giá-trị-không-quan-sát-được.","chapter":"Chương 5 Tiền xử lý dữ liệu","heading":"5.3.2 Xử lý giá trị không quan sát được.","text":"Khi dữ liệu có giá trị không quan sát được, cách xử lý đơn giản nhất là xóa các quan sát hoặc xóa các biến chứa các giá trị đó. Nếu bạn đọc gặp dữ liệu mà trong đó có một hoặc một số quan sát có đa số các giá trị trong đó là \\(NA\\), trong khi tất cả các quan sát còn lại đều không có chứa \\(NA\\), thì cách xử lý xóa quan sát có giá trị \\(NA\\) là giải pháp hợp lý nhất. Ví dụ như chúng ta có dữ liệu về thông tin của sinh viên của một lớp như sauQuan sát tương ứng với mã sinh viên “MSV33789” ngoài thông tin về tên sinh viên, các thông tin khác đều không quan sát được. Ngoài sinh viên này, các sinh viên còn lại đều có đầy đủ thông tin. Trong trường hợp này phương pháp xử lý hiệu quả nhất là xóa sinh viên “MSV33789” khỏi dữ liệu trước khi phân tích.Khi các giá trị không quan sát được tập trung ở một số quan sát (hàng) hoặc tập trung ở một số biến (cột), chúng ta nói rằng các giá trị không quan sát được một cách không ngẫu nhiên (Missing value random hay MNAR). Dưới đây là một ví dụ khác mà các giá trị không quan sát được tập trung vào một biến:Đây cũng là một ví dụ về việc giá trị không quan sát được xuất hiện một cách không ngẫu nhiên. Có thể thấy rằng trong cột \\(\\textit{GPA}\\) đa số các giá trị là không quan sát được. Mọi phân tích liên quan đến giá trị của biến này sẽ không có ý nghĩa, đó cách tốt nhất là xóa cột này ra khỏi dữ liệu. Để xóa các quan sát (hàng) có chứa giá trị không quan sát được ra khỏi dữ liệu, bạn đọc có thể sử dụng hàm drop_na() của thư viện \\(\\textbf{tidyr}\\). Để xóa một cột khỏi dữ liệu, bạn đọc có thể coi dữ liệu như là một list và gán giá trị của biến đó bằng giá trị NULL:Nếu giá trị không quan sát được tập trung vào một số quan sát hoặc một số cột thì xóa quan sát hay xóa cột sẽ không làm ảnh hưởng đến kết quả phân tích. Tuy nhiên, dữ liệu chúng ta thường gặp sẽ có các giá trị không quan sát được nằm rải rác ở các cột không theo một quy tắc nào. Chúng tôi muốn nói đến trường hợp dữ liệu không quan sát được xuất hiện một cách hoàn toàn ngẫu nhiên (Missing completely random, hay MCAR). Khi gặp trường hợp này nếu xóa đi các quan sát có chứa giá trị NA, tỷ lệ dữ liệu bị xóa đi sẽ là rất đáng kể.Để minh họa rõ hơn cho vấn đề này, và để đánh giá hiệu quả của các phương pháp xử lý giá trị NA trong các phần sau, chúng tôi sẽ sử dụng dữ liệu \\(\\textbf{mpg}\\) của thư viện \\(\\textbf{ggplot2}\\). Đây là dữ liệu có 234 quan sát và 11 biến. Dữ liệu mô tả mức độ tiêu hao nhiên liệu của các loại xe ô tô thương mại đang bán trên thị trường trong hai năm 1999 và 2008. Dữ liệu không có giá trị NA nhưng chúng ta sẽ thêm các giá trị không quan sát được vào dữ liệu một các ngẫu nhiên. Sau đó dữ liệu chính xác sẽ được sử dụng để đánh giá phương pháp xử lý giá trị không quan sát được.Bạn đọc sử dụng đoạn câu lệnh dưới đây để thêm giá trị không quan sát được vào trong dữ liệu một cách ngẫu nhiên. Dữ liệu mới sau khi thêm NA vào sẽ được gọi tên là \\(\\textbf{na.mpg}\\) để phân biệt với dữ liệu ban đầu.Chúng ta thấy rằng có 8 trên tổng số 11 cột có giá trị NA, mỗi cột có 5 giá trị NA xuất hiện một cách ngẫu nhiên trên tổng số 234 giá trị (tỷ lệ khoảng 2%). Tuy nhiên số quan sát có chứa NA lại lớn hơn 2% rất nhiều. Hàm drop_na() của thư viện \\(\\textbf{tidyr}\\) sẽ xóa các quan sát có giá trị không quan sát được ra khỏi dữ liệu. Chúng ta có thể tính được tỷ lệ dữ liệu còn giữ lại là bao nhiêu như sau:Có thể thấy nếu 2% dữ liệu không quan sát được xuất hiện ngẫn nhiên ở mỗi cột thì tỷ lệ dữ liệu còn lại là khoảng 85% nếu chúng ta xóa các quan sát có chứa NA, nghĩa là 15% dữ liệu đã bị xóa. Chúng ta có thể thử tăng tỷ lệ giá trị không quan sát được trên mỗi cột lên thành 3%, 5%, 10%, 20%, 30% và quan sát tỷ lệ dữ liệu còn lại sau khi xóa.\nTable 5.5: Tỷ lệ dữ liệu bị xóa nếu loại bỏ quán sát có NA\nBảng 5.5 cho thấy nếu xử lý giá trị không quan sát được bằng cách xóa quan sát thì tỷ lệ dữ liệu còn lại giảm đi rất nhanh. Khi tỷ lệ NA xuất hiện một cách ngẫu nhiên trên mỗi cột từ 5% trở lên chúng ta phải xóa đi hơn 35% số quan sát. Tỷ lệ dữ liệu xóa như vậy sẽ ảnh hưởng lớn đến kết quả của phân tích dữ liệu.Rõ ràng đây không phải là một giải pháp hiệu quả khi giá trị NA xuất hiện trong hầu hết các cột.Phương pháp xử lý giá trị không quan sát được thường được áp dụng hơn là thay thế giá trị không quan sát được bằng các giá trị thích hợp. Phương pháp đơn giản nhất đó là giả thiết các cột chứa giá trị không quan sát được độc lập với nhau và sử dụng các giá trị đặc trưng của cột dữ liệu đó để thay thế cho giá trị không quan sát được. Các phương pháp phức tạp hơn tính toán mối liên hệ giữa các cột dữ liệu và xây dựng các thuật toán để tìm giá trị thích hợp thay thế cho các giá trị không quan sát được nằm trong tất cả các cột. Mỗi phương pháp đều có ưu nhược điểm và chúng tôi thường thử cả hai hướng tiếp cận sau đó đánh giá hiệu quả của kết quả phân tích. Các phương pháp thay thế giá trị không quan sát được bằng một giá trị thích hợp được trình bày trong các phần tiếp theo.","code":"# Dữ liệu có tên là dat\ndat<-drop_na(dat) \n# Xóa các quan sát (hàng) có giá trị NA ra khỏi dữ liệu\n\ndat$ten_cot<-NULL # \nXóa cột có tên là ten_cot ra khoi du lieu\n# Tạo dữ liệu mới giống như dữ liệu mpg\nna.mpg<-mpg\n\n# Định dạng các cột kiểu biến rời rạc thành kiểu factor\nchiso<- !(names(na.mpg) %in% c(\"displ\", \"cty\", \"hwy\"))\nna.mpg[,chiso]<-lapply(na.mpg[,chiso], as.factor)%>%\n  as.data.frame()\n\n# Viết hàm số để thêm giá trị NA vào một véc-tơ\n## Hàm số thêm vào véc-tơ x các giá trị NA một cách ngẫu nhiên\n## Tỷ lệ giá trị NA được thêm vào là na.rate\nrd.add<-function(x, na.rate){\n  n<-length(x)\n  k<-round(n*na.rate)\n  ind<-sample(1:n,k,replace=FALSE)\n  x[ind]<-NA\n  return(x)\n}\n\n# Thêm giá trị NA vào các cột NGOẠI TRỪ ba cột\n## Cột nhà sản xuất: manufacturer\n## Cột loại xe: model\n## Cột năm sản xuất\n## tỷ lệ thêm NA một cách ngẫu nhiên vào các cột là 2%\nchiso<- !(names(na.mpg) %in% c(\"manufacturer\", \"model\", \"year\"))\nset.seed(12)\nna.mpg[,chiso]<-as.data.frame(lapply(na.mpg[,chiso], \n                                     rd.add, \n                                     na.rate = 0.02))\n\n# Xem mỗi cột có bao nhiêu giá trị NA\nsapply(na.mpg, f<-function(x) sum(is.na(x)))## manufacturer        model        displ         year          cyl        trans \n##            0            0            5            0            5            5 \n##          drv          cty          hwy           fl        class \n##            5            5            5            5            5\nnrow(drop_na(na.mpg))/nrow(na.mpg)## [1] 0.8461538"},{"path":"tiền-xử-lý-dữ-liệu.html","id":"thay-thế-giá-trị-không-quan-sát-được-bằng-các-đại-lượng-đặc-trưng-của-biến","chapter":"Chương 5 Tiền xử lý dữ liệu","heading":"5.3.2.1 Thay thế giá trị không quan sát được bằng các đại lượng đặc trưng của biến","text":"Phương pháp thay thế này dựa trên giả thiết rằng cột chứa giá trị không quan sát được không có mối liên hệ đến các cột còn lại, chúng ta sẽ sử dụng một trong các giá trị đặc trưng của các giá trị quan sát được của cột đó như trung bình (mean), trung vị (median), hoặc mode để thay thế cho các giá trị không quan sát được.Giá trị trung bình thường được sử dụng để thay thế cho các giá trị không quan sát được cho véc-tơ kiểu số liên tục và phân phối của các giá trị không có đuôi dài và không có giá trị bất thường.Giá trị trung bình thường được sử dụng để thay thế cho các giá trị không quan sát được cho véc-tơ kiểu số liên tục và phân phối của các giá trị không có đuôi dài và không có giá trị bất thường.Giá trị trung vị, là giá trị tại ngưỡng xác suất 50%, thường được sử dụng để thay thế cho các giá trị không quan sát được trong véc-tơ kiểu số liên tục và véc-tơ có đuôi dài. Giá trị trung vị có ưu điểm là ít bị ảnh hưởng bởi các giá trị ngoại lai và không bị thay đổi sau các bước biến đổi dữ liệu bằng các hàm đơn điệu.Giá trị trung vị, là giá trị tại ngưỡng xác suất 50%, thường được sử dụng để thay thế cho các giá trị không quan sát được trong véc-tơ kiểu số liên tục và véc-tơ có đuôi dài. Giá trị trung vị có ưu điểm là ít bị ảnh hưởng bởi các giá trị ngoại lai và không bị thay đổi sau các bước biến đổi dữ liệu bằng các hàm đơn điệu.Giá trị mode, là giá trị mà hàm mật độ có xác suất cao nhất, có thể dùng cho cả véc-tơ kiểu số liên tục hoặc véc-tơ kiểu biến rời rạc. Trong trường hợp véc-tơ kiểu số liên tục, bạn đọc cần phải ước lượng hàm mật độ nên giá trị mode sẽ còn phụ thuộc vào phương pháp tiếp cận của người phân tích.Giá trị mode, là giá trị mà hàm mật độ có xác suất cao nhất, có thể dùng cho cả véc-tơ kiểu số liên tục hoặc véc-tơ kiểu biến rời rạc. Trong trường hợp véc-tơ kiểu số liên tục, bạn đọc cần phải ước lượng hàm mật độ nên giá trị mode sẽ còn phụ thuộc vào phương pháp tiếp cận của người phân tích.Để thay thế giá trị không quan sát được bằng một giá trị khác, bạn đọc có thể sử dụng hàm na_if() của thư viện \\(\\textbf{dplyr}\\), hàm replace_na() của thư viện \\(\\textbf{tidyr}\\), hoặc cũng có thể tự xây dựng hàm số của mình. Để đơn giản hóa, chúng ta giả sử rằng sẽ luôn luôn sử dụng giá trị thay thế là giá trị trung vị khi đối với véc-tơ kiểu số liên tục và giá trị mode đối với véc-tơ kiểu biến rời rạc.\nTable 5.6: sánh giá trị đúng và giá trị thay thế của các biến liên tục trong dữ liệu mpg\nGiá trị đúng của các biến rời rạc và giá trị dùng để thay thế được tổng kết trong Bảng 5.7.\nTable 5.7: sánh giá trị đúng và giá trị thay thế của các biến rời rạc trong dữ liệu mpg\nKhông dễ dàng để đưa ra kết luận là thay thế các giá trị không quan sát được của véc-tơ kiểu số liên tục bằng giá trị trung vị như trong Bảng 5.6 là hiệu quả hay không. Chúng ta chỉ có thể thấy rằng giá trị thay thế nằm giữa các giá trị thực và khoảng cách từ giá trị thay thế đến các giá trị đúng không quá lớn. Về lý thuyết, khi thay thế giá trị không quan sát được bằng giá trị trung vị, chúng ta đang cố gắng đưa ra các dự đoán cho giá trị không quan sát được sao cho giá trị trung bình của sai số tuyệt đối (Mean Absoluted Error hay MAE) là nhỏ nhất. Trong trường hợp chúng ta sử dụng giá trung bình để thay thế, chúng ta tối thiệu hóa trung bình bình phương của sai số của giá trị dự đoán (Mean Squared Error).Thay thế các giá trị không quan sát được trong các véc-tơ kiểu biến rời rạc bằng giá trị mode tương đương với nguyên tắc làm giảm thiểu tối đa xác suất dự đoán sai. Tuy nhiên, giá trị thay thế cho giá trị rời rạc trong bảng 5.7 chỉ cho 1-3 lần dự đoán đúng cho mỗi biến. Nguyên nhân là giá trị mode trong các biến rời rạc không chiếm ưu thế với các giá trị khác. Chẳng hạn như biến \\(\\textit{drv}\\) bị dự đoán sai 4/5 kết quả biến này có 2 giá trị mode.","code":"\nmy_mode<-function(x){ # Tự định nghĩa hàm mode\n  names(which.max(table(x)))\n}\nmy_fillna_1<-function(x){ # Tự định nghĩa cách thay thế giá trị NA\n  if(is.numeric(x)){\n    # Nếu x là biến liên tục thì dùng median\n    x[is.na(x)]<-median(x,na.rm=TRUE)\n  } else {\n    # Nếu x là biến rời rạc thì dùng mode\n    x[is.na(x)]<-my_mode(x)\n  }\n  return(x)\n}\nmpg_1<-lapply(na.mpg, my_fillna_1)%>%as.data.frame()"},{"path":"tiền-xử-lý-dữ-liệu.html","id":"thay-thế-giá-trị-không-quan-sát-được-bằng-một-mẫu-ngẫu-nhiên.","chapter":"Chương 5 Tiền xử lý dữ liệu","heading":"5.3.2.2 Thay thế giá trị không quan sát được bằng một mẫu ngẫu nhiên.","text":"Vẫn với giả thiết rằng cột chứa giá trị không quan sát được không có mối liên hệ đến các cột còn lại, chúng ta sẽ sử dụng phép lấy mẫu ngẫu nhiên từ các giá trị quan sát được để thay thế cho các giá trị NA. Hàm sample() là hàm số có sẵn được sử dụng để sinh ngẫu nhiên. Để lấy ra \\(k\\) số ngẫu nhiên từ một véc-tơ \\(\\textbf{x}\\) ban đầu, chúng ta sử dụng câu lệnh như sau:Tham số replace nhận giá trị bằng TRUE có ý nghĩa là giá trị ngẫu nhiên được lấy ra từ véc-tơ \\(\\textbf{x}\\) có thể được lấy lặp lại. Chúng ta tự định nghĩa hàm fill_na_2() để thay thế giá trị ngẫu nhiên trong một véc-tơ \\(\\textbf{x}\\) bằng một mẫu ngẫu nhiên có lặp lại được lấy từ \\(x\\) như sau\nTable 5.8: Biến liên tục, thay thế NA bằng lấy mẫu ngẫu nhiên\nGiá trị thật của các biến kiểu factor và giá trị dùng để thay thế được tổng kết trong Bảng 5.9\nTable 5.9: Biến rời rạc, thay thế NA bằng lấy mẫu ngẫu nhiên\nHiệu quả của phương pháp lấy mẫu ngẫu nhiên với phương pháp sử dụng các giá trị trung vị hoặc mode là không rõ ràng. Phương pháp thay thế giá trị không quan sát được bằng một mẫu ngẫu nhiên chỉ cho hiệu quả khi dữ liệu đủ lớn và phân phối xác suất của các biến quan sát được ổn định. Nhược điểm lớn nhất của phương pháp này đó là giá trị được sử dụng để thay thế được sinh ngẫu nhiên nên có khả năng sẽ làm cho dữ liệu bị sai lệch, đồng thời mỗi lần thực hiện phương pháp sẽ cho các kết quả khác nhau tùy theo hàm sinh ngẫu nhiên.","code":"\nsample(x,size = k, replace = TRUE) \nmy_fillna_2<-function(x){ # Hàm thay thế giá trị NA, phương pháp thứ 2\n  ind<-is.na(x) # véc-tơ kiểu logic, nhận giá trị TRUE tại các vị trí NA\n  k<-sum(ind)\n  x[ind]<-sample(x[!ind],k,replace = TRUE)\n  return(x)\n}\nset.seed(12)\nmpg_1<-lapply(na.mpg, my_fillna_2)%>%as.data.frame()"},{"path":"tiền-xử-lý-dữ-liệu.html","id":"replaceNAbymodel","chapter":"Chương 5 Tiền xử lý dữ liệu","heading":"5.3.2.3 Thay thế giá trị không quan sát được dựa trên các biến khác","text":"Giả thiết cột chứa giá trị không quan sát được không có mối liên hệ đến các cột còn lại là một giả thiết không thực tế. Các cột dữ liệu luôn luôn có mối liên hệ với nhau dù ít hay nhiều. Nói theo khái niệm của xác suất - thống kê thì các cột dữ liệu thường không độc lập với nhau. Làm thế nào để biết hai cột dữ liệu bất kỳ là độc lập hay phụ thuộc ? Đây là một câu hỏi không dễ. Bạn đọc cần có kiến thức về xác suất và thống kê toán để có được câu trả lời triệt để. Có rất nhiều lý thuyết khác nhau nghiên cứu về sự phụ thuộc giữa các biến và đa số các lý thuyết đó vượt quá phạm vi của cuốn sách này. Chúng tôi chỉ trình bày các phương pháp được công nhận rộng rãi ở mức độ vừa đủ để bạn đọc không có nền tảng nâng cao về toán học - xác suất thống kê có thể hiểu được. Nhìn chung, để đưa ra kết luận hai cột dữ liệu có độc lập hay không, bạn đọc có thể sử dụng các kiểm định như sau:Kiểm định Khi-bình phương khi cả hai biến đều là biến rời rạc.Kiểm định Khi-bình phương khi cả hai biến đều là biến rời rạc.Kiểm định hệ số tương quan Person, hoặc hệ số tương quan Spearman, hoặc hệ số tương quan Kendall khi cả hai biến đều là biến liên tục.Kiểm định hệ số tương quan Person, hoặc hệ số tương quan Spearman, hoặc hệ số tương quan Kendall khi cả hai biến đều là biến liên tục.Sử dụng phân tích phương sai (hay còn gọi là anova test) trong trường hợp một biến là rời rạc và một biến là liên tục.Sử dụng phân tích phương sai (hay còn gọi là anova test) trong trường hợp một biến là rời rạc và một biến là liên tục.Chi tiết của các kiểm định này được trình bày ở phần Phụ lục 5.5.1. Chúng ta sẽ sử dụng các phương pháp này để kiểm tra mối liên hệ giữa các cột trong dữ liệu \\(\\textbf{na.mpg}\\).Để thực hiện kiểm định Khi-bình phương trong R, chúng ta sử dụng hàm chisq.test(). Ví dụ, để kiểm ra hai biến \\(\\textit{year}\\) và \\(\\textit{drv}\\) có mối liên hệ hay không, chúng ta thực hiện như sau:Giá trị \\(\\textit{p-value}\\) bằng paste(round((chisq.test(na.mpg$year,na.mpg$drv))$p.value*100,2), \"%\") nghĩa là xác suất bác bỏ giả thiết hai biến \\(year\\) và \\(drv\\) độc lập là paste(round((100-chisq.test(na.mpg$year,na.mpg$drv))$p.value*100,2), \"%\"). Thông thường, mức xác suất bác bỏ giả thiết độc lập thường được chọn ở mức 95% hoặc thậm chí 99%. xác suất bác bỏ giả thiết độc lập là thấp nên trong trường hợp này có thể đưa ra kết luận rằng hai biến \\(\\textit{year}\\) và \\(\\textit{drv}\\) là không có mối liên hệ. Tương tự, để kiểm ra hai biến \\(\\textit{drv}\\) và \\(\\textit{cyl}\\) có mối liên hệ hay không, chúng ta thực hiện như sau:Trong trường hợp này, xác suất bác bỏ giả thiết độc lập là xấp xỉ 100% nên chúng ta có thể đưa ra kết luận rằng hai biến \\(drv\\) và \\(cyl\\) là có mối liên hệ.Để kiểm định hệ số tương quan giữa hai biến liên tục chúng ta sử dụng hàm cor.test(). Tham số method nhận giá trị \"pearson\", \"kendall\", hoặc \"spearman\" tương ứng với các hệ số tương quan Pearson, hệ số tương quan Kendall, hoặc hệ số tương quan Spearman. Chúng ta kiểm định sự độc lập giữa hai biến \\(\\textit{displ}\\) và \\(\\textit{hwy}\\) như sauKiểm định cả ba hệ số tương quan đều cho xác suất bác bỏ giả thiết độc lập là xấp xỉ 100%. Nói một cách khác có thể khẳng định rằng hai biến \\(\\textit{displ}\\) và \\(\\textit{hwy}\\) là có sự phụ thuộc.Sau cùng, để kiểm định sự phụ thuộc giữa một biến rời rạc và một biến liên tục, chúng ta sử dụng phân tích phương sai. Hàm số để thực hiện phân tích phương sai trong R là hàm aov(). Chúng ta kiểm định sự phụ thuộc giữa biến liên tục \\(\\textit{hwy}\\) và biến rời rạc \\(\\textit{cyl}\\) như sau:Xác suất bác bỏ giả thiết giá trị trung bình của biến \\(\\textit{hwy}\\) bằng nhau theo các nhóm của biến \\(\\textit{cyl}\\) là xấp xỉ 100% hay nói một cách khác \\(\\textit{hwy}\\) và \\(\\textit{cyl}\\) là có mối liên hệ.Để xem xét một cách tổng thể mối liên hệ giữa các biến trong dữ liệu \\(\\textbf{na.mpg}\\), bạn đọc có thể sử dụng kiểm định phù hợp với từng cặp biến và lưu xác suất bác bỏ giả thiết độc lập vào một ma trận. Hàm số ind_check() được chúng tôi tự xây dựng với đầu vào là một dữ liệu, dưới dạng một tibble hoặc một data.frame, cho đầu ra là một ma trận cho biết xác suất bác bỏ giả thiết độc lập của từng cặp biến như thế nào.Ma trận thể hiện xác suất bác bỏ giả thiết độc lập giữa từng cặp biến trong dữ liệu \\(\\textbf{na.mpg}\\) ở trong Hình 5.3\nFigure 5.3: Ma trận xác suất bác bỏ giả thuyết độc lập giữa các cặp biến trong dữ liệu na.mpg\nCó thể thấy rằng ngoại trừ biến \\(\\textit{year}\\) ít có mối liên hệ đến các biến khác, còn lại đa số các biến là có mối liên hệ với nhau. Điều này được thể hiện qua xác suất bác bỏ giả thiết độc lập giữa các biến trong ma trận của hình đều xấp xỉ 100%. Khi xây dựng mô hình trên dữ liệu, sự xuất hiện của các biến ít có mối liên hệ đến các biến khác sẽ khiến mô hình bị nhiễu và làm giảm chất lượng dự đoán. đó, chúng tôi sẽ loại bỏ biến \\(\\textit{year}\\) khi dự đoán giá trị không quan sát được của các biến khác.Phương pháp để xây dựng mô hình dự đoán cho các giá trị không quan sát được là thuật toán “rừng ngẫu nhiên”. Đây là một thuật toán nâng cao của mô hình dạng cây quyết định sẽ được trình bày trong chương ??. Còn quá sớm để nói về mô hình này, bạn đọc chỉ cần hiểu rằng chúng ta sẽ dựa vào các giá trị quan sát được để xây dựng mô hình, hay tổng quát hơn là một hàm số \\(f\\), mà biến có chứa giá trị NA phụ thuộc vào các biến không có giá trị NA tại các vị trí tương ứng để đưa ra dự đoán. Thư viện \\(\\textbf{missForest}\\) hỗ trợ chúng ta làm việc này. Bạn đọc có thể cài thư viện sau đó sử dụng hàm missForest(). Quá trình thay thế giá trị NA của dữ liệu \\(\\textbf{na.mpg}\\) bằng thuật toán rừng ngẫu nhiên chỉ cần một dòng lệnh:Giá trị đúng của các biến kiểu số và các giá trị thay thế bằng phương pháp xây dựng mô hình dự đoán ở trong Bảng 5.10\nTable 5.10: Biến liên tục, thay thế NA bằng giá trị dự đoán bằng random forest\nGiá trị đúng của các biến kiểu factor và giá trị thay thế bằng phương pháp xây dựng mô hình dự đoán ở trong Bảng 5.11\nTable 5.11: Biến rời rạc, thay thế NA bằng giá trị dự đoán bằng random forest\nBạn đọc có thể nhận thấy:Với các biến kiểu số, giá trị dùng để thay thế cho NA không khác nhiều với giá trị đúng. Sai số giữa giá trị dự đoán với giá trị đúng nhỏ hơn rất nhiều với sai số của hai phương pháp trước.Với các biến kiểu số, giá trị dùng để thay thế cho NA không khác nhiều với giá trị đúng. Sai số giữa giá trị dự đoán với giá trị đúng nhỏ hơn rất nhiều với sai số của hai phương pháp trước.Với các biến kiểu rời rạc, ngoại trừ biến \\(\\textit{trans}\\) và biến \\(\\textit{fl}\\), các biến còn lại đều được dự đoán chính xác 100% bằng thuật toán rừng ngẫu nhiên.Với các biến kiểu rời rạc, ngoại trừ biến \\(\\textit{trans}\\) và biến \\(\\textit{fl}\\), các biến còn lại đều được dự đoán chính xác 100% bằng thuật toán rừng ngẫu nhiên.Nhìn chung, xây dựng mô hình để dự đoán giá trị không quan sát được dựa trên các biến khác là phương pháp cho hiệu quả vượt trội, đặc biệt đối với dữ liệu có các cột có mối liên hệ mật thiết với nhau. Điểm bất lợi duy nhất của phương pháp này là sự phức tạp trong kỹ thuật xây dựng mô hình. Bạn đọc cần có các hiểu biết cơ bản về xây dựng mô hình, trong trường hợp này là mô hình cây quyết định, và các kỹ thuật thống kê hiện đại như kỹ thuật lấy mẫu lặp, để hiểu được nguyên tắc dự đoán giá trị không quan sát được. Tất nhiên, thực thi hàm missForest() của thư viện cùng tên không cần bạn phải có các kiến thức này. Để hiểu được chính xác cách xây dựng mô hình, bạn đọc tham khảo chương ??.","code":"\nchisq.test(na.mpg$year,na.mpg$drv)## \n##  Pearson's Chi-squared test\n## \n## data:  na.mpg$year and na.mpg$drv\n## X-squared = 1.689, df = 2, p-value = 0.4298\nchisq.test(na.mpg$drv, na.mpg$cyl)## \n##  Pearson's Chi-squared test\n## \n## data:  na.mpg$drv and na.mpg$cyl\n## X-squared = 90.288, df = 6, p-value < 2.2e-16\ncor.test(na.mpg$displ, na.mpg$hwy, method = \"pearson\")## \n##  Pearson's product-moment correlation\n## \n## data:  na.mpg$displ and na.mpg$hwy\n## t = -17.743, df = 223, p-value < 2.2e-16\n## alternative hypothesis: true correlation is not equal to 0\n## 95 percent confidence interval:\n##  -0.8143893 -0.7048317\n## sample estimates:\n##       cor \n## -0.765092\ncor.test(na.mpg$displ, na.mpg$hwy, method = \"kendall\")## \n##  Kendall's rank correlation tau\n## \n## data:  na.mpg$displ and na.mpg$hwy\n## z = -13.857, p-value < 2.2e-16\n## alternative hypothesis: true tau is not equal to 0\n## sample estimates:\n##        tau \n## -0.6534741\ncor.test(na.mpg$displ, na.mpg$hwy, method = \"spearman\")## \n##  Spearman's rank correlation rho\n## \n## data:  na.mpg$displ and na.mpg$hwy\n## S = 3467012, p-value < 2.2e-16\n## alternative hypothesis: true rho is not equal to 0\n## sample estimates:\n##        rho \n## -0.8262809\nsummary(aov(hwy~cyl,data=na.mpg))##              Df Sum Sq Mean Sq F value Pr(>F)    \n## cyl           3   4479  1492.9   101.6 <2e-16 ***\n## Residuals   220   3233    14.7                   \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## 10 observations deleted due to missingness\n### Hàm số `ind_check()`\nind_check<-function(dat){\n  dat<-as.data.frame(dat)\n  dat.name<-names(dat)\n  p<-dim(dat)[2]\n  M<-matrix(0,p,p)\n  for (i in 1:(p-1)){\n    for (j in (i+1):p){\n      x<-dat[,i]\n      y<-dat[,j]\n      if(is.character(x)|is.character(y)){\n        return(NA)\n      } else {\n        if (is.numeric(x)){\n          if (is.numeric(y)){\n            test<-cor.test(x, y, method = \"spearman\")\n            M[i,j]<-1 - test$p.value\n          } else {\n            test<-summary(aov(x ~ y))\n            M[i,j]<-1 - test[[1]][[5]][1]\n          }\n        } else {\n          if (is.numeric(y)){\n            test<-summary(aov(y ~ x))\n            M[i,j]<-1 - test[[1]][[5]][1]\n          } else {\n            test<-chisq.test(x,y)\n            M[i,j]<-1 - test$p.value\n          }\n        }\n      }\n      M[j,i]<-M[i,j]\n    }\n  }\n  colnames(M)<-dat.name\n  rownames(M)<-dat.name\n  diag(M)<-1\n  return(round(M,2))\n}\nlibrary(missForest)\nna.mpg<-as.data.frame(na.mpg)\n### Thời gian chạy mất khoảng 1-2 phút\nmodel<-missForest(select(na.mpg,-year), maxiter = 200, ntree = 100) \nmpg_1<-model$ximp # Dữ liệu mpg_1 là dữ liệu sau khi thay thế NA"},{"path":"tiền-xử-lý-dữ-liệu.html","id":"xử-lý-giá-trị-ngoại-lai","chapter":"Chương 5 Tiền xử lý dữ liệu","heading":"5.4 Xử lý giá trị ngoại lai","text":"","code":""},{"path":"tiền-xử-lý-dữ-liệu.html","id":"ảnh-hưởng-của-giá-trị-ngoại-lai-lên-kết-quả-phân-tích","chapter":"Chương 5 Tiền xử lý dữ liệu","heading":"5.4.1 Ảnh hưởng của giá trị ngoại lai lên kết quả phân tích","text":"Giá trị ngoại lai hay còn được gọi là giá trị bất thường là một (hoặc một số) điểm dữ liệu có giá trị sai khác đáng kể với đa số các quan sát khác. Một giá trị ngoại lai xuất hiện trong dữ liệu có thể là lỗi trong quản lý dữ liệu, sai số trong đo lường hoặc cũng có thể bản chất phân phối của dữ liệu. Tùy theo nguồn gốc của giá trị ngoại lai mà chúng ta có cách xử lý dữ liệu khác nhau. Khi không được xử lý thích hợp, các giá trị ngoại lai có thể làm sai lệch kết luận của tất cả các phân tích về dữ liệu.Giá trị ngoại lai được hiểu là những điểm dữ liệu khác xa tập hợp các điểm còn lại. Không có một định nghĩa chính xác nào cho khái niệm “khác xa các giá trị còn lại”. đó, tùy theo bản chất của dữ liệu, và tùy theo quan điểm của người phân tích dữ liệu, mà một (hay một số) giá trị có khả năng là giá trị ngoại lai hay không. Giá trị ngoại lai thường chỉ được nhắc đến với các dữ liệu có số quan sát đủ lớn để đưa ra kết luận có ý nghĩa thống kê.\nFigure 5.4: Giá trị ngoại lai xuất hiện trong dư liệu có ít và nhiều quan sát. Hình bên trái: Các điểm và B nằm cách xa 8 điểm còn lại nhưng dữ liệu chưa đủ lớn để đưa ra kết luận. Hình bên phải: điểm và B nằm cách xa 98 điểm còn lại, có thể kết luận và B là các điểm ngoại lai.\nHình 5.4 mô tả trực quan các điểm và B có khả năng là giá trị ngoại lai trong hai trường hợp là có ít quan sát và có tương đối nhiều quan sát. Khi dữ liệu có 10 quan sát như hình bên trái, có 8 quan sát màu xanh nằm gần nhau, điểm B nằm xa hơn tập hợp các điểm màu xanh một chút, còn điểm nằm cách xa hơn. Khi gặp dữ liệu như vậy, chúng ta có thể kết luận điểm là giá trị ngoại lai vì điểm này nằm cách rất xa các điểm còn lại. Tuy nhiên còn kết luận điểm B có phải ngoại lai hay không thì còn tùy thuộc vào cách tiếp cận của người phân tích dữ liệu. Hình bên phải với dữ liệu có 100 quan sát. Các điểm màu xanh định hình khá rõ miền giá trị của trung tâm của dữ liệu là nằm xung quanh điểm (3,3) . Chúng ta có thể kết luận một cách khá chắc chắn rằng điểm là một giá trị ngoại lai. Điểm B mặc dù nằm khá xa trung tâm của dữ liệu, nhưng để kết luận rằng có phải giá trị ngoại lai hay không vẫn phụ thuộc vào ý tưởng của người phân tích.Nguồn gốc của giá trị ngoại lai là có thể có nhiều nguyên nhân khác nhau, bao gồm cả nguyên nhân khách quan hoặc nguyên nhân chủ quan. Các nguyên nhân khách quan có thể nguồn sinh dữ liệu, hay hệ thống quản lý dữ liệu gặp sự cố, lỗi trong quá trình truyền hoặc sao chép dữ liệu. Nguyên nhân chủ quan bao gồm có các hành vi gian lận, lỗi nhập và sao chép dữ liệu của con người, hoặc các giá trị được cố tình đưa vào trong dữ liệu với mục đích lấy phản hồi từ người dùng dữ liệu.Nếu không xử lý giá trị ngoại lai, kết quả phân tích sẽ bị sai lệch đáng kể. Và dữ liệu có kích thước càng nhỏ thì ảnh hưởng của giá trị ngoại lai lại càng lớn. Trong ví dụ ở Hình 5.4, giả sử chúng ta cần phân tích sự tác động của biến \\(X\\) lên biến \\(Y\\) bằng một mối quan hệ tuyến tính. Chúng ta xây dựng mô hình tuyến tính trong ba trường hợpGiữ nguyên 10 quan sát và xây dựng mô hình mô tả mối liên hệ tuyến tính.Giữ nguyên 10 quan sát và xây dựng mô hình mô tả mối liên hệ tuyến tính.Loại bỏ điểm trước khi xây dựng mô hình.Loại bỏ điểm trước khi xây dựng mô hình.Loại bỏ điểm và điểm B trước khi xây dựng mô hình.Loại bỏ điểm và điểm B trước khi xây dựng mô hình.Các đường tuyến tính mô tả mối liên hệ giữa biến \\(X\\) và \\(Y\\) được mô tả trong Hình 5.5\nFigure 5.5: Xây dựng mô hình tuyến tính trên dữ liệu vẫn chứa giá trị ngoại lai. Hình bên trái: bao gồm cả hai điểm và B trong xây dựng mô hình. Hình ở giữa: loại điểm và giữ lại điểm B trong xây dựng mô hình. Hình bên phải: loại bỏ cả điểm và điểm B trước khi xây dựng mô hình.\nKhi giữ nguyên 10 điểm dữ liệu để xây dựng mối quan hệ tuyến tính, đường thẳng mô tả mối quan hệ giữa \\(Y\\) và \\(X\\) là nằm trong hình phía bên trái của Hình 5.5. Đường thẳng này có hệ số góc dương, nghĩa là một đường dốc lên khi đi từ trái sang phải, điều này có nghĩa là biến \\(X\\) có tác động cùng chiều lên biến \\(Y\\). Khi \\(X\\) tăng hoặc giảm thì nhiều khả năng \\(Y\\) cũng sẽ tăng hoặc giảm.Khi giữ nguyên 10 điểm dữ liệu để xây dựng mối quan hệ tuyến tính, đường thẳng mô tả mối quan hệ giữa \\(Y\\) và \\(X\\) là nằm trong hình phía bên trái của Hình 5.5. Đường thẳng này có hệ số góc dương, nghĩa là một đường dốc lên khi đi từ trái sang phải, điều này có nghĩa là biến \\(X\\) có tác động cùng chiều lên biến \\(Y\\). Khi \\(X\\) tăng hoặc giảm thì nhiều khả năng \\(Y\\) cũng sẽ tăng hoặc giảm.Sau khi loại bỏ điểm và tính toán lại, đường thẳng mô tả mối quan hệ tuyến tính giữa \\(Y\\) và \\(X\\) ở là đường thẳng trong hình ở giữa của Hình 5.5. Đường thẳng gần như nằm ngang, cho thấy \\(X\\) không có tác động lên biến \\(Y\\). Nghĩa là \\(X\\) có tăng hay giảm cũng không làm thay đổi \\(Y\\).Sau khi loại bỏ điểm và tính toán lại, đường thẳng mô tả mối quan hệ tuyến tính giữa \\(Y\\) và \\(X\\) ở là đường thẳng trong hình ở giữa của Hình 5.5. Đường thẳng gần như nằm ngang, cho thấy \\(X\\) không có tác động lên biến \\(Y\\). Nghĩa là \\(X\\) có tăng hay giảm cũng không làm thay đổi \\(Y\\).Sau cùng, trong hình phía bên phải của Hình 5.5, sau khi loại bỏ các điểm và điểm B, đường thẳng mô tả mối quan hệ tuyến tính giữa \\(X\\) và \\(Y\\) là đường dốc xuống, nghĩa là mối tác động của \\(X\\) lên \\(Y\\) là ngược chiều.Sau cùng, trong hình phía bên phải của Hình 5.5, sau khi loại bỏ các điểm và điểm B, đường thẳng mô tả mối quan hệ tuyến tính giữa \\(X\\) và \\(Y\\) là đường dốc xuống, nghĩa là mối tác động của \\(X\\) lên \\(Y\\) là ngược chiều.Bạn đọc có thể thấy rằng kết luận đưa sau từ kết quả ước lượng mô hình thay đổi hoàn toàn khi chúng ta có các lựa chọn khác nhau về loại bỏ các giá trị được cho là ngoại lai ra khỏi dữ liệu. Sự tác động của \\(X\\) lên \\(Y\\) từ thuận chiều (hình bên trái) sang không có mối liên hệ (hình ở giữa) và sau cùng là sự tác động ngược chiều của \\(X\\) lên \\(Y\\) (hình bên phải). Điều này cho thấy việc xác định và xử lý giá trị ngoại lai là vô cùng quan trọng trước khi xây dựng mô hình.Trong phần tiếp theo chúng ta sẽ thảo luận về các phương pháp dùng để xác định các giá trị ngoại lai trong dữ liệu.","code":""},{"path":"tiền-xử-lý-dữ-liệu.html","id":"phương-pháp-phát-hiện-giá-trị-ngoại-lai","chapter":"Chương 5 Tiền xử lý dữ liệu","heading":"5.4.2 Phương pháp phát hiện giá trị ngoại lai","text":"Không có một định nghĩa chính xác như thế nào là giá trị ngoại lai, chính vì thế không có phương pháp chung để phát hiện giá trị ngoại lai. Với mỗi dữ liệu, với mỗi cách nhìn nhận giá trị ngoại lại khác nhau, mà có phương pháp tiếp cận cụ thể để xác định các giá trị đó. Trong phần này, chúng tôi chỉ trình bày các phương pháp chung được chấp nhận rộng rãi. Đây là các phương pháp đơn giản, dễ hiểu và có thể thực hiện được mà không cần bổ sung thêm kiến thức. Các phương pháp phức tạp hơn, đòi hỏi kiến thức nâng cao về dữ liệu như phân nhóm, phân cụm,…, sẽ được thảo luận trong chương sách học máy không có giám sát.","code":""},{"path":"tiền-xử-lý-dữ-liệu.html","id":"phát-hiện-giá-trị-ngoại-lai-trong-một-véc-tơ","chapter":"Chương 5 Tiền xử lý dữ liệu","heading":"5.4.2.1 Phát hiện giá trị ngoại lai trong một véc-tơ","text":"Để xác định một giá trị là giá trị ngoại lai hay không thường bao gồm hai bước, bước thứ nhất là sử dụng các phương pháp xác suất thống kê để xác định các giá trị có khả năng cao là ngoại lai, sau đó bước thứ hai là sử dụng kiến thức chuyên môn hoặc hỏi ý kiến chuyên gia để khẳng định lại kết quả từ bước thứ nhất.Nếu véc-tơ là một véc-tơ kiểu chuỗi ký tự (không phải kiểu factor) thì không có quy tắc rõ ràng nào để xác định giá trị ngoại lai. Trước hết, việc một biến kiểu chuỗi ký tự có phải là một giá trị ngoại lai hay không phụ thuộc vào bản chất của véc-tơ chuỗi ký tự. Việc này hoàn toàn phụ thuộc vào hiểu biết của người phân tích dữ liệu với véc-tơ đó. Ví dụ, véc-tơ chuỗi ký tự là tên của người bằng tiếng Việt, một giá trị ngoại lai có thể là một tên người với nhiều hơn 6 hay 7 từ, bởi vì theo hiểu biết chung, tên người bình thường bao gồm 2,3 hoặc 4 từ. Ngoài ra, một chuỗi ký tự có thể là ngoại lai nếu chuỗi ký tự có độ dài bất thường, có chứa nhiều ký tự bất thường, hay một chuỗi ký tự không có ý nghĩa trong một véc-tơ bao gồm các chuỗi ký tự có ý nghĩa. Nói một cách khác một giá trị có phải là ngoại lai hay không hoàn toàn phụ thuộc vào cách tiếp cận của người phân tích dữ liệu. Các phương pháp xử lý dữ liệu kiểu chuỗi ký tự hiện nay có khả năng biến đổi một chuỗi ký tự thành một véc-tơ kiểu số. Việc xác định chuỗi ký tự có phải là một giá trị bất thường hay không sẽ liên quan đến việc xác định một véc-tơ kiểu số có phải là một véc-tơ có giá trị bất thường trong một tập hợp các véc-tơ. Các kỹ thuật này vượt quá phạm vi của cuốn sách nên chúng tôi không đề cập ở đây.\nFigure 5.6: Tần suất xuất hiện của các loại đồ uống được bán tại một siêu thị. Loại đồ uống có tần suất xuất hiện thấp có khả năng là giá trị ngoại lai\nKhi gặp trường hợp như hình 5.6, có khả năng đồ uống có tên Collagen là giá trị ngoại lai. Chúng ta chưa thể khẳng định bởi vì nếu siêu thị thực sự có bán loại đồ uống này và việc sản phẩm không được khách hàng ưa chuộng, thì sản phẩm xuất hiện với tần xuất thấp là bình thướng. Tuy nhiên cũng có thể tên sản phẩm xuất hiện trong danh sách bán hàng dù siêu thị bán cũng có thể là lỗi gặp phải trong quản lý hệ thống bán hàng, hoặc người bán hàng đã ghi nhận tên Collagen cho một đồ uống khác.Đối với véc-tơ kiểu số, các giá trị có khả năng là ngoại lai thường là các giá trị nằm ở đuôi của phân phối xác suất. Các giá trị nằm ở đuôi là các giá trị nằm cách xa các giá trị trung bình về phía bên phải hoặc bên trái của giá trị trung bình. Để biết một véc-tơ kiểu số có giá trị ngoại lai hay không, bạn đọc nên sử dụng đồ thị boxplot. Các điểm nằm phía dưới điểm nhỏ nhất (\\(Q_0\\)) và nằm phía trên điểm lớn nhất (\\(Q_4\\)) của đồ thị boxplot có nhiều khả năng là các giá trị ngoại lai. Điểm nhỏ nhất và điểm lớn nhất của đồ thị boxplot được xác định dựa trên mức tứ phân vị thứ nhất (\\(Q_1\\)) và mức tứ phân vị thứ ba (\\(Q_3\\)):\n\\[\\begin{align}\n&\\text{Inter Quartile Range (IQR)} = Q_3 - Q_1 \\\\\n&\\text{Điểm nhỏ nhất } (Q_0) = Q_1 - 1.5 \\times IQR \\\\\n&\\text{Điểm lớn nhất } (Q_4) = Q_3 + 1.5 \\times IQR\n\\end{align}\\]Các giá trị nằm ngoài khoảng \\((Q_1 - 1,5 \\times IQR, Q_3 + 1.5 \\times IQR)\\) có nhiều khả năng là giá trị ngoại lai. Giá trị càng thấp hơn \\(Q_0\\) và càng cao hơn \\(Q_4\\) thì khả năng là giá trị ngoại lai lại càng cao. Ví dụ dưới đây mô tả việc sử dụng đồ thị boxplot để phát hiện giá trị ngoại lai trong một dữ liệu thực tế. Hình 5.7 mô tả phân phối xác suất của véc-tơ chứa khối lượng giao dịch, tính bằng triệu cổ phiếu/ngày, của cổ phiếu tập đoàn FLC. Cổ phiếu được niêm yết trên sàn giao dịch chứng khoán Thành phố Hồ Chí Minh từ ngày 6 tháng 10 năm 2011 đến ngày 8 tháng 9 năm 2022. Dữ liệu có 2719 quan sát.\nFigure 5.7: Sử dụng đồ thị boxplot để mô tả lịch sử khối lượng giao dịch cổ phiếu FLC. Các giá trị có khả năng là giá trị ngoại lai nằm trên điểm Q4 của phân phối xác suất.\nChúng ta có thể thấy trên đồ thị boxplot không có điểm nằm dưới \\(Q_0\\). Có 8 quan sát có giá trị lớn hơn \\(Q_4\\); các giá trị này có khả năng là các giá trị ngoại lai. Có 3 quan sát với giá trị lớn hơn 100 triệu, nghĩa là có ba ngày mà có hơn 100 triệu cổ phiếu FLC được giao dịch. Nếu có một chút kinh nghiệm về giao dịch thị trường chứng khoán Việt Nam, bạn đọc có thể kiểm chứng được đây là số lượng cổ phiếu giao dịch lớn bất thường.Ba phiên giao dịch có khối lượng giao dịch lớn hơn 100 triệu cổ phiếu là các phiên giao dịch ngày 10 tháng 1 năm 2022, ngày 11 tháng 1 năm 2022 và phiên giao dịch ngày 1 tháng 4 năm 2022. Thực tế cho thấy đây là ba phiên giao dịch mà cổ phiếu FLC đã bị thao túng giá và dẫn đến việc cố phiếu FLC bị cấm giao dịch trên sàn giao dịch HOSE kể từ tháng 09 năm 2022.Từ khoảng tháng 10 năm 2021 giá cổ phiếu FCL bắt đầu tăng nhanh. Đến đầu tháng 01 năm 2022, giá cổ phiếu đã tăng lên gấp 2 lần. Ngày 10 và ngày 11 tháng 01 năm 2022, các cổ đông chính của FLC bán ra khối lượng rất lớn các cổ phiếu mà không đăng ký với Ủy ban chứng khoán theo quy định. Sau hai phiên giao dịch này giá cổ phiếu FLC giảm mạnh về đến mức trước đó vài tháng.Từ khoảng tháng 10 năm 2021 giá cổ phiếu FCL bắt đầu tăng nhanh. Đến đầu tháng 01 năm 2022, giá cổ phiếu đã tăng lên gấp 2 lần. Ngày 10 và ngày 11 tháng 01 năm 2022, các cổ đông chính của FLC bán ra khối lượng rất lớn các cổ phiếu mà không đăng ký với Ủy ban chứng khoán theo quy định. Sau hai phiên giao dịch này giá cổ phiếu FLC giảm mạnh về đến mức trước đó vài tháng.Ngày 31 tháng 03 năm 2022 các thông tin giả mạo về nhu cầu mua cổ phiếu FLC với khối lượng lớn được đưa ra sau nhiều ngày giá cổ phiếu FLC giảm hết biên độ làm cho nhu cầu mua FLC trong ngày 01 tháng 04 năm 2022 cao đột biến.Ngày 31 tháng 03 năm 2022 các thông tin giả mạo về nhu cầu mua cổ phiếu FLC với khối lượng lớn được đưa ra sau nhiều ngày giá cổ phiếu FLC giảm hết biên độ làm cho nhu cầu mua FLC trong ngày 01 tháng 04 năm 2022 cao đột biến.Việc thao túng giá và đưa thông tin giả mạo khiến cho số lượng cố phiếu FLC tăng lên đột biến đã bị các cơ quan chức năng phát hiện và đưa ra lệnh cấm giao dịch với cổ phiếu này. Đây là ví dụ điển hình về dữ liệu có giá trị ngoại lai có nguyên nhân chủ quan từ con người.Ngoài đồ thị boxplot, bạn đọc có thể sử dụng các đồ thị mô tả phân phối của biến liên tục như đồ thị histogram hay đồ thị density để xác định giá trị ngoại lai trong véc-tơ kiểu số. Ví dụ, Hình 5.8 mô tả phân phối của chiều cao của 245 nam giới là nhân viên của một công ty. Đơn vị đo chiều cao là cm.\nFigure 5.8: Kết hợp boxplot và histogram để xác định giá trị ngoại lai trong dữ liệu. Hình bên trái: đồ thị boxplot cho có điểm nằm phía dưới điểm Q0. Hình bên phải: đồ thị histogram cho thấy có giá trị bất thường nằm ở đuôi bên trái\nCả hai đồ thị boxplot và histogram trong Hình 5.8 đều cho thấy trong dữ liệu có các giá trị chiều cao của nam giới xấp xỉ giá trị 0 và nhiều khả năng đây là các giá trị ngoại lai. Đồ thị histogram còn cho thấy có nhiều hơn 1 giá trị có giá trị như vậy. Lọc các giá trị đó ra khỏi véc-tơ chúng ta sẽ thu được 5 giá trị là 1,52; 1,74; 1,70; 1,62; và 1,80. Đây không thể là chiều cao của nam giới đo bằng đơn vị cm. Có nhiều khả năng là khi ghi lại chiều cao của các nhân viên này, người nhập dữ liệu đã sử dụng đơn vị là mét thay vì cm. Chúng ta có thể sửa các giá trị ngoại lai này bằng cách đổi từ đơn vị mét sang cm. Phân phối của chiều cao sau khi sửa lại dữ liệu được mô tả như hình dưới đây:\nFigure 5.9: Kết hợp boxplot và histogram để xác định giá trị ngoại lai trong dữ liệu. Hình bên trái: đồ thị boxplot sau khi điều chỉnh lại chiều cao từ đơn vị mét sang cm. Hình bên phải: đồ thị histogram sau khi điều chỉnh lại chiều cao từ đơn vị mét sang cm\nVéc-tơ kiểu số là đơn vị đo lường hay đơn vị tiền tệ rất thường xuyên gặp vấn đề như kể trên. Ngay khi gặp giá trị ngoại lai trong véc-tơ kiểu số như trên bạn đọc hãy nghĩ đến sai đơn vị đo lường là nguyên nhân đầu tiên.Ngoài việc sử dụng các tứ phân vị để phát hiện giá trị ngoại lai, một phương pháp định lượng khác cũng thường được đề cập đến trong nhiều tài liệu là sử dụng \\(Z-Score\\). \\(Z-Score\\) được tính bằng khoảng cách từ 1 điểm đến giá trị trung bình của dữ liệu sau đó chia cho độ lệch chuẩn của dữ liệu\n\\[\\begin{align}\nZ-Score(x_i) = \\cfrac{|x_i - \\bar{x}|}{\\sigma(x)}\n\\end{align}\\]\nvới \\(x_i\\) là giá trị thứ \\(\\) trong véc-tơ \\(\\textbf{x}\\), \\(\\bar{x}\\) là giá trị trung bình của véc-tơ \\(\\textbf{x}\\), và \\(\\sigma(x)\\) là độ lệch chuẩn của các số trong véc-tơ \\(\\textbf{x}\\). \\(Z-Score\\) dựa trên giả thiết là dữ liệu có phân phối chuẩn, đó các điểm dữ liệu có \\(Z-Score\\) lớn, thường sử dụng ngưỡng bằng 3, được coi là các giá trị ngoại lai. Chẳng hạn như khi vẽ \\(Z-Score\\) của tất cả các điểm dữ liệu trong dữ liệu về chiều cao của nhân viên trong ví dụ được mô tả trong Hình 5.8, chúng ta sẽ có giá trị \\(Z-Score\\) của chiều cao của tất cả các nhân viên như trong Hình 5.10\nFigure 5.10: Giá trị Z-Score chiều cao của tất cả các nhân viên. Các quan sát có Z-Score lớn hơn 3 có nhiều khả năng là giá trị ngoại lai\nCác điểm có \\(Z-score\\) lớn hơn 3 trong Hình 5.10 là các điểm bị ghi nhận sai đơn vị đo lường từ \\(cm\\) sang \\(mét\\) và có \\(Z-Score\\) lên đến hơn 6. Trong trường hợp này \\(Z-Score\\) cũng là phương pháp định lượng hiệu quả để xác định giá trị ngoại lai. Tuy nhiên, \\(Z-Score\\) có điểm bất lợi là giá trị này được tính toán dựa trên giá trị trung bình và độ lệch tiêu chuẩn của dữ liệu trong khi chính các giá trị đó lại bị tác động rất mạnh bởi các giá trị ngoại lai. Một cách đề giảm thiểu tác động của giá trị ngoại lai lên \\(Z-Score(x_i)\\) là không tính đến \\(x_i\\) khi tính toán trung bình \\(\\bar{x}\\) và \\(\\sigma(x)\\).Đa số các phương pháp xác định giá trị ngoại lai ở trên đều dựa trên giả thiết là véc-tơ dữ liệu có phân phối chuẩn. Tuy nhiên, không phải lúc nào phân phối chuẩn cũng phù hợp với véc-tơ kiểu số. Dữ liệu về bồi thường bảo hiểm là một điển hình của dữ liệu không có phân phối chuẩn. Hình 5.11 mô tả số liệu về tiền bồi thường bảo hiểm sức khỏe của hơn 1.000 khách hàng tại một công ty bảo hiểm:\nFigure 5.11: Phân phối xác suất của tiền bồi thường bảo hiểm y tế tại của hơn 1.000 khách hàng tại một công ty bảo hiểm. Hình bên trái: đồ thị histogram cho thấy phân phối của số tiền bồi thường không phải là phân phối chuẩn. Hình bên phải: có rất nhiều điểm có Z-Score lớn hơn ngưỡng 3\nNhiều điểm dữ liệu được xác định là ngoại lai mặc dù thực tế thì đây vẫn là các giá trị thông thường. Nguyên nhân là phân phối của số tiền bảo hiểm y tế không phải là phân phối chuẩn. Trong trường hợp bạn đọc gặp dữ liệu không có phân phối chuẩn, hãy biến đổi dữ liệu về phân phối chuẩn hoặc gần phân phối chuẩn nhất có thể trước khi thực hiện tính \\(Z-Score\\). Phép biến đổi thường được sử dụng nhất là biến đổi Box-Cox được trình bày trong Phụ lục 5.5.2.\nFigure 5.12: Dữ liệu bồi thường bảo hiểm sau khi đã được biến đổi về gần với phân phối chuẩn. Hình bên trái: đồ thị histogram cho thấy phân phối của các điểm dữ liệu đã gần với phân phối chuẩn hơn với dữ liệu ban đầu. Hình bên phải: các điểm có nhiều khả năng là giá trị ngoại lai là các giá trị có Z-score trên ngưỡng 3\nHình 5.12 mô tả dữ liệu bồi thường bảo hiểm sau khi đã được biến đổi về phân phối chuẩn. Có thể thấy rằng sau phép biến đổi, các giá trị có Z-Score lớn hơn ngưỡng 3 đã giảm đi rất nhiều. Các điểm này nhiều khả năng là những khoản tiền bồi thường lớn bất thường và cần được kiểm tra lại. Ngoài biến đổi Box-Cox, một phương pháp khác để biến đổi véc-tơ về phân phối chuẩn là sử dụng hàm ngược của hàm phân phối chuẩn. Phương pháp này được trình bày trong Phụ lục ??.Trong thực tế nếu chỉ quan sát trên các cột dữ liệu riêng lẻ thì không thể xác định được giá trị ngoại lai. Giống như điểm B trong hình bên phải của Hình 5.4, nếu chúng ta chỉ quan sát vị trí của điểm này trên trục X hoặc trục Y một cách riêng biệt thì không thể xác định được đây là giá trị ngoại lai. Điều này có nghĩa là một quan sát có thể không phải là ngoại lai nếu như chỉ quan sát trên từng cột dữ liệu, nhưng lại là giá trị ngoại lai khi quan sát đồng thời các thành phần của quan sát đó. Các kỹ thuật xác định giá trị ngoại lai trong không gian nhiều chiều sẽ được thảo luận trong phần tiếp theo.","code":""},{"path":"tiền-xử-lý-dữ-liệu.html","id":"giá-trị-ngoại-lai-trong-không-gian-nhiều-chiều","chapter":"Chương 5 Tiền xử lý dữ liệu","heading":"5.4.2.2 Giá trị ngoại lai trong không gian nhiều chiều","text":"Xác định giá trị ngoại lai trong không gian nhiều chiều phức tạp hơn trong không gian một chiều. Trong không gian một chiều, chúng ta cần xác định những số nào là giá trị ngoại lai của một véc-tơ. Trong khi trong không gian nhiều chiều, chúng ta cần phải xác định các quan sát nào là giá trị ngoại lai trong một dữ liệu. Ngoài việc xem xét giá trị trong từng cột dữ liệu, chúng ta cần phải xem xét cả mối liên hệ giữa các véc-tơ (cột) đó.Các phương pháp để xác định giá giá trị ngoại lai trong không gian nhiều chiều vẫn dựa trên nguyên tắc cơ bản áp dụng trong không gian một chiều, đó là các quan sát càng xa điểm trung tâm của dữ liệu thì quan sát đó càng có khả năng cao là giá trị ngoại lai. Khái niệm xa hay gần trong một không gian nhiều chiều luôn gắn liền với một khái niệm về khoảng cách. Khoảng cách thường được sử dụng nhiều nhất trong không gian nhiều chiều là khoảng cách Euclid. Tuy nhiên khoảng cách Euclid có nhược điểm là không tính đến mối liên hệ giữa các cột dữ liệu. Khoảng cách thường được sử dụng để xác định giá trị ngoại lai là khoảng cách Mahalanobis.Cho \\(\\textbf{x}_i = x_{i1}, x_{i2}, \\cdots, x_{ip}\\) là quan sát thứ \\(\\) và \\(\\boldsymbol{\\mu} = \\mu_{1}, \\mu_{2}, \\cdots, \\mu_{p}\\) là véc-tơ các giá trị trung bình của các véc-tơ cột. Khoảng cách Euclid và khoảng cách Mahalanobis từ điểm \\(\\textbf{x}_i\\) đến \\(\\boldsymbol{\\mu}\\) được định nghĩa như sau:\n\\[\\begin{align}\nD^{Euclid}(\\textbf{x}_i,\\boldsymbol{\\mu}) & = \\sqrt{(\\textbf{x}_i - \\boldsymbol{\\mu})^T (\\textbf{x}_i - \\boldsymbol{\\mu}} \\\\\nD^{Mahalanobis}(\\textbf{x}_i,\\boldsymbol{\\mu})& = \\sqrt{(\\textbf{x}_i - \\boldsymbol{\\mu})^T \\ \\Sigma^{-1} \\  (\\textbf{x}_i - \\boldsymbol{\\mu})} \\\\\n\\end{align}\\]\ntrong đó \\(D^{Euclid}(\\textbf{x}_i,\\boldsymbol{\\mu})\\) và \\(D^{Mahalanobis}(\\textbf{x}_i,\\boldsymbol{\\mu})\\) lần lượt là khoảng cách Euclid và khoảng cách Mahalanobis từ quan sát \\(\\textbf{x}_i\\) đến điểm trung bình \\(\\boldsymbol{\\mu}\\). Trong công thức tính khoảng cách Mahalanobis, \\(\\Sigma^{-1}\\) là ma trận nghịch đảo của ma trận hiệp phương sai của các biến trong dữ liệu. Có thể thấy rằng khoảng cách Euclid là trường hợp riêng của khoảng cách Mahalanobis khi các cột dữ liệu có phương sai bằng 1 và đôi một độc lập với nhau.Bạn đọc có thể tự lập trình hàm số tính khoảng cách Euclid và hàm số tính khoảng cách Mahalanobis giữa 2 véc-tơ bất kỳ. Các hàm có tên là Dis.Euc() và hàm Dis.Mah() được định nghĩa như sauChúng ta quay trở lại ví dụ về dữ liệu bao gồm 10 quan sát với hai giá trị ngoại lai là điểm và điểm B trong Hình 5.4. Chúng ta tính toán khoảng cách Euclid của mỗi điểm đến trung tâm của dữ liệu và sắp xếp các điểm theo thứ tự khoảng cách Euclid đến điểm trung tâm giảm dần. Kết quả được cho trong Bảng @ref(tag:tbptdl010)\nTable 5.12: Điểm dữ liệu sắp xếp theo khoảng cách Euclid đến trung tâm giảm dần trong trường hợp có 10 điểm dữ liệu\nCó thế thấy rằng khi chỉ có 10 quan sát, khoảng cách Euclid có thể sử dụng để phát hiện được giá trị ngoại lai là điểm và điểm B vì hai điểm này có khoảng cách đến trung tâm xa hơn với các điểm còn lại. Khoảng cách Mahalanobis cũng cho kết quả tương tự. Tuy nhiên khoảng cách Euclid sẽ gặp vấn đề khi số lượng quan sát nhiều hơn và mối liên hệ giữa \\(X\\) và \\(Y\\) rõ ràng hơn. Thật vậy, chúng ta thực hiện tính toán khoảng cách từ các điểm , B và các điểm còn lại đến điểm trung tâm của dữ liệu trong trường hợp có 100 quat sát, sau đó sắp xếp các điểm theo thứ tự khoảng cách Euclid giảm dần giống như khi có 10 quan sát. 10 điểm có khoảng cách Euclid đến trung tâm lớn nhất được liệt kê trong Bảng 5.13.\nTable 5.13: 10 điểm dữ liệu có khoảng cách Euclid xa nhất trong trường hợp có 100 điểm\nBạn đọc có thể thấy khoảng cách Euclid không cho kết quả tốt như khoảng cách Malahanobis trong trường hợp dữ liệu có 100 quan sát. Khi đo bằng khoảng cách Euclid, điểm vẫn là điểm xa trung tâm dữ liệu nhất. Tuy nhiên khoảng cách từ điểm B đến trung tâm dữ liệu là nhỏ hơn một số điểm khác, mặc dù các điểm đó không phải là các giá trị ngoại lai. Ngược lại, khi tính bằng khoảng cách Mahalanobis, chúng ta có thể thấy rằng điểm là điểm có khoảng cách xa nhất, sau đó đến điểm B với khoảng cách Malahanobis là 4.675. Các điểm khác đều có khoảng cách Mahalanobis nhỏ hơn 2,5.Các kỹ thuật phát hiện giá trị ngoại lai phức tạp hơn dựa trên nguyên lý phân nhóm và phân cụm sẽ được trình bày trong chương học máy không có giám sát. Nguyên tắc xác định một quan sát ngoại lai là phân chia dữ liệu thành các cụm sao cho các quan sát trong cùng một cụm có tính chất tương tự nhau. Các quan sát không nằm trong cụm nào, hoặc trong các cụm có rất ít quan sát, là các điểm dữ liệu có nhiều khả năng là giá trị ngoại lai.","code":"\nDis.Euc<-function(x,y) sum((x-y)^2)^0.5\nDis.Mah<-function(x,y,Sigma) (t(x-y)%*% solve(Sigma) %*%(x-y))^0.5 "},{"path":"tiền-xử-lý-dữ-liệu.html","id":"xử-lý-giá-trị-ngoại-lai.","chapter":"Chương 5 Tiền xử lý dữ liệu","heading":"5.4.3 Xử lý giá trị ngoại lai.","text":"Có nhiều phương pháp để xử lý giá trị ngoại lai trong dữ liệu. Tùy thuộc vào tình huống và dữ liệu cụ thể, phương pháp nào cũng có thể đúng hoặc sai. Điều quan trọng là bạn đọc phải phân tích các tình huống có thể liên quan đến giá trị ngoại lai. Đôi khi việc phân tích các giá trị ngoại lai này còn giúp bạn có những hiểu biết hơn về dữ liệu và tối ưu công việc phân tích của bạn.Phương pháp đơn giản nhất và nhưng kém hiệu quả nhất là loại bỏ các quan sát, hoặc biến có chứa giá trị ngoại lai. Phương pháp này chỉ có ý nghĩa khi bạn có số lượng quan sát đủ lớn và các giá trị bị coi là ngoại lai không có có ý nghĩa trong xác định phân phối xác suất của từng biến.Phương pháp đơn giản nhất và nhưng kém hiệu quả nhất là loại bỏ các quan sát, hoặc biến có chứa giá trị ngoại lai. Phương pháp này chỉ có ý nghĩa khi bạn có số lượng quan sát đủ lớn và các giá trị bị coi là ngoại lai không có có ý nghĩa trong xác định phân phối xác suất của từng biến.Phương pháp thứ hai là thay thế giá trị ngoại lai bằng một giá trị khác: bạn đọc có thể thay thế giá trị ngoại lai bằng giá trị có ý nghĩa hơn như giá trị \\(Q_0\\) hoặc \\(Q_4\\) của phân phối xác suất, hoặc cũng có thể thay thế giá trị ngoại lai bằng giá trị trung bình, trung vị, hoặc mode của phân phối. Đây là phương pháp đơn giản, dễ sử dụng và thường cho hiệu quả tốt hơn với phương pháp xóa quan sát.Phương pháp thứ hai là thay thế giá trị ngoại lai bằng một giá trị khác: bạn đọc có thể thay thế giá trị ngoại lai bằng giá trị có ý nghĩa hơn như giá trị \\(Q_0\\) hoặc \\(Q_4\\) của phân phối xác suất, hoặc cũng có thể thay thế giá trị ngoại lai bằng giá trị trung bình, trung vị, hoặc mode của phân phối. Đây là phương pháp đơn giản, dễ sử dụng và thường cho hiệu quả tốt hơn với phương pháp xóa quan sát.Phương pháp sau cùng và cũng là phương pháp đòi hỏi kỹ thuật phức tạp nhất đó là coi giá trị ngoại lai như một giá trị không quan sát được, sau đó xây dựng mô hình để dự đoán cho giá trị ngoại lai. Các phương pháp thay thế giá trị ngoại lai bằng giá trị dự đoán dựa trên các mô hình tương tự như các phương pháp xử lý dữ liệu không quan sát được. Bạn đọc có thể tham khảo phần 5.3.2.3 của cuốn sách.Phương pháp sau cùng và cũng là phương pháp đòi hỏi kỹ thuật phức tạp nhất đó là coi giá trị ngoại lai như một giá trị không quan sát được, sau đó xây dựng mô hình để dự đoán cho giá trị ngoại lai. Các phương pháp thay thế giá trị ngoại lai bằng giá trị dự đoán dựa trên các mô hình tương tự như các phương pháp xử lý dữ liệu không quan sát được. Bạn đọc có thể tham khảo phần 5.3.2.3 của cuốn sách.","code":""},{"path":"tiền-xử-lý-dữ-liệu.html","id":"phụ-lục-1","chapter":"Chương 5 Tiền xử lý dữ liệu","heading":"5.5 Phụ lục","text":"","code":""},{"path":"tiền-xử-lý-dữ-liệu.html","id":"appenptdl01","chapter":"Chương 5 Tiền xử lý dữ liệu","heading":"5.5.1 Kiểm định sự độc lập của hai biến","text":"","code":""},{"path":"tiền-xử-lý-dữ-liệu.html","id":"pearsons-chi-squared-tests","chapter":"Chương 5 Tiền xử lý dữ liệu","heading":"5.5.1.1 Pearson’s Chi-squared tests","text":"","code":""},{"path":"tiền-xử-lý-dữ-liệu.html","id":"appenptdl02","chapter":"Chương 5 Tiền xử lý dữ liệu","heading":"5.5.2 Box-Cox transformation","text":"Biến đổi Box-Cox là một phương pháp biến đổi để đưa một véc-tơ có phân phối khác với phân phối chuẩn thành một véc-tơ có phân phối gần với phân phối chuẩn. Phép biến đổi Box-Cox chỉ có duy nhất một tham số \\(\\lambda\\).\n\\[\\begin{align}\nx(\\lambda) = \\begin{cases}\n\\cfrac{y^\\lambda-1}{\\lambda} \\text{ nếu } \\lambda \\neq 0 \\\\\nlog(y) \\text{ nếu } \\lambda = 0\n\\end{cases}\n\\end{align}\\]giá trị \\(\\lambda\\) thường được lựa chọn trong khoảng (-5,5) sao cho khoảng cách giữa dữ liệu sau khi biến đổi đến phân phối chuẩn là nhỏ nhất. Khoảng cách giữa hai phân phối được đo bằng khoảng cách Kolmogorov-Smirnov. Hình\nFigure 5.13: Lựa chọn tham số để thực hiện biến đổi Box-Cox. Giá trị tham số lambda tối thiểu hóa khoảng cách của phân phối của dữ liệu đến phân phối chuẩn được lựa chọn\nTham số \\(\\lambda\\) tối thiểu hóa khoảng cách từ phân phối của dữ liệu đến phân phối chuẩn là 0.21, chúng ta có phân phối của dữ liệu sau khi biến đổi trong Hình 5.14\nFigure 5.14: Biến đổi số tiền bồi thường thành phân phối chuẩn bằng cách sử dụng biến đổi Box-Cox với tham số 0.21\n","code":""},{"path":"tiền-xử-lý-dữ-liệu.html","id":"phương-pháp-chuẩn-hóa-véc-tơ-bằng-hàm-ngược","chapter":"Chương 5 Tiền xử lý dữ liệu","heading":"5.5.3 Phương pháp chuẩn hóa véc-tơ bằng hàm ngược","text":"Phương pháp biến đổi này dựa trên hai kết quả cơ bản như sau:Nếu \\(X\\) là một biến ngẫu nhiên liên tục có hàm phân phối \\(F\\) thì biến ngẫu nhiên \\(F(X)\\) sẽ có phân phối \\(\\textit{Uniform(0,1)}\\)Nếu \\(X\\) là một biến ngẫu nhiên liên tục có hàm phân phối \\(F\\) thì biến ngẫu nhiên \\(F(X)\\) sẽ có phân phối \\(\\textit{Uniform(0,1)}\\)Nếu \\(U\\) là một biến ngẫu nhiên phân phối \\(\\textit{Uniform(0,1)}\\) thì biến ngẫu nhiên \\(\\Phi^{-1}(U)\\) sẽ có phân phối chuẩn với trung bình bằng 0 và phương sai bằng 1, trong đó \\(\\Phi^{-1}\\) là hàm ngược của hàm phân phối của biến ngẫu nhiên phân phối chuẩn với trung bình bằng 0 và phương sai bằng 1.Nếu \\(U\\) là một biến ngẫu nhiên phân phối \\(\\textit{Uniform(0,1)}\\) thì biến ngẫu nhiên \\(\\Phi^{-1}(U)\\) sẽ có phân phối chuẩn với trung bình bằng 0 và phương sai bằng 1, trong đó \\(\\Phi^{-1}\\) là hàm ngược của hàm phân phối của biến ngẫu nhiên phân phối chuẩn với trung bình bằng 0 và phương sai bằng 1.Cả hai kết quả này đều có thể được chứng minh bằng kiến thức xác suất cơ bản. Thứ nhất, \\(F(X)\\) có phân phối \\(\\textit{Uniform(0,1)}\\) vì \\(F(X)\\) nhận giá trị trên \\([0,1]\\), đồng thời\n\\[\\begin{align}\n\\mathbb{P}\\left(F(X) < x\\right) &= \\mathbb{P}\\left(X < F^{-1}(x)\\right) \\\\\n& = F(F^{-1}(x)) \\\\\n& = x\n\\end{align}\\]\nlà hàm phân phối xác suất của biến ngẫu nhiên \\(\\textit{Uniform(0,1)}\\)Thứ hai, biến ngẫu nhiên \\(\\Phi^{-1}(U)\\) có phân phối chuẩn với trung bình bằng 0 và phương sai bằng 1 vì\n\\[\\begin{align}\n\\mathbb{P}\\left(\\Phi^{-1}(U) < x\\right) & = \\mathbb{P}\\left(U < \\Phi(x)\\right) \\\\\n& = \\Phi(x)\n\\end{align}\\]Như vậy, biến ngẫu nhiên \\(X\\) bất kỳ có thể được biến đổi thành biến ngẫu nhiên phân phối chuẩn với trung bình bằng 0 và phương sai bằng 1 bằng phép biến đổi \\(N = \\Phi^{-1}(F(X))\\) với \\(F\\) là hàm phân phối của \\(X\\).Khó khăn lớn nhất trong phép biến đổi này là tìm ra phân phối \\(F\\) của biến ngẫu nhiên \\(X\\) vì chúng ta chỉ có một véc-tơ quan sát được của \\(X\\). Việc này đòi hỏi các kiến thức liên quan đến thống kê toán.Quay trở lại với ví dụ về dữ liệu về số tiền bồi thường bảo hiểm trong hình 5.11, chúng ta có thể sử dụng phân phối \\(Pareto(\\alpha,\\beta)\\) cho số tiền bảo hiểm. Các tham số của phân phối \\(Pareto\\) ước lượng cho dữ liệu là \\(\\alpha = 2.17\\), và \\(\\beta = 3.34\\). Chúng ta biến đổi số tiền bồi thường thành phân phối chuẩn như sau:\n\\[\\begin{align}\n& N = \\Phi^{-1}(F(X))\n\\end{align}\\]\nvới \\(F(x)\\) được xác định bởi\n\\[\\begin{align}\nF(x) = 1 - \\left(\\cfrac{\\beta}{x+\\beta} \\right)^\\alpha\n\\end{align}\\]Hình 5.15 mô tả dữ liệu trước và sau khi biến đổi\nFigure 5.15: Biến đổi số tiền bồi thường có phân phối Pareto thành phân phối chuẩn bằng cách sử dụng hàm ngược\nCó thể thấy dữ liệu sau khi biến đổi đã gần với phân phối chuẩn hơn với dữ liệu gốc.","code":""},{"path":"tiền-xử-lý-dữ-liệu.html","id":"bài-tập-1","chapter":"Chương 5 Tiền xử lý dữ liệu","heading":"5.6 Bài tập","text":"","code":""},{"path":"biến-đổi-và-sắp-xếp-dữ-liệu.html","id":"biến-đổi-và-sắp-xếp-dữ-liệu","chapter":"Chương 6 Biến đổi và sắp xếp dữ liệu","heading":"Chương 6 Biến đổi và sắp xếp dữ liệu","text":"","code":""},{"path":"biến-đổi-và-sắp-xếp-dữ-liệu.html","id":"biến-đổi-dữ-liệu-bằng-thư-viện-textbfdplyr","chapter":"Chương 6 Biến đổi và sắp xếp dữ liệu","heading":"6.1 Biến đổi dữ liệu bằng thư viện \\(\\textbf{dplyr}\\)","text":"Dữ liệu trước khi đưa vào thực hiện các phân tích, thực hiện trực quan hóa, hoặc xây dựng mô hình, thường không có được định dạng đúng như chúng ta mong muốn. Thông thường, chúng ta sẽ cần tạo thêm một số biến, hoặc có thể chỉ muốn đổi tên các biến, hoặc sắp xếp lại các quan sát, hoặc tổng hợp và nhóm dữ liệu vào các nhóm để dễ dàng hơn cho các bước tiếp theo. Các bước biến đổi dữ liệu như vậy sẽ được trình bày chi tiết trong phần này. Đa số các hàm thực hiện các phép biến đổi dữ liệu được lấy từ thư viện \\(\\textbf{dplyr}\\). Dữ liệu để minh họa trực tiếp cho các phép biến đổi là tập dữ liệu \\(\\textbf{gapminder}\\) của thư viện \\(\\textbf{dslabs}\\), dữ liệu về sức khỏe và thu nhập của các quốc gia trên thế giới từ năm 1960 đến năm 2016. Chúng tôi lựa chọn dữ liệu này vì đây là dữ liệu dễ hiểu và có nhiều ý tưởng để phân tích.Thư viện \\(\\textbf{dplyr}\\) là một thư viện nằm trong thư viện tổng hợp \\(\\textbf{tidyverse}\\), bạn đọc có thể gọi thư viện \\(\\textbf{dplyr}\\) hoặc trực tiếp thư viện \\(\\textbf{dplyr}\\) lên cửa sổ làm việc. Bạn đọc cũng cần lưu ý rằng trong thư viện \\(\\textbf{dplyr}\\) có một số hàm trùng tên với các hàm có sẵn trong R, chẳng hạn như hàm filter(), hàm select(), hoặc hàm lag(). Bạn đọc cần lưu ý thứ tự ưu tiên của các thư viện khi sử dụng các hàm kể trên, hoặc gọi tên thư viện đính kèm với tên hàm để tránh gặp lỗi khi thực thi các câu lệnh.Dữ liệu \\(\\textbf{gapminder}\\) là đã được đề cập trong phần tiền xử lý dữ liệu. Bạn đọc nên tham khảo mô tả dữ liệu này trước khi đi đến các phần tiếp theo của cuốn sách. Trong các phần tiếp theo, chúng tôi sẽ lần lượt giới thiệu các hàm quan trọng để thực hiện các phép biến đổi dữ liệu từ thư viện \\(\\textbf{dplyr}\\) và các tham số quan trọng của hàm số đó. Chúng tôi cũng sẽ giới thiệu với bạn đọc về cách kết hợp các hàm với nhau thành một câu lệnh duy nhất viết câu lệnh bằng cách sử dụng toán tử pipe (%>%).","code":""},{"path":"biến-đổi-và-sắp-xếp-dữ-liệu.html","id":"thêm-biến-bằng-hàm-mutate","chapter":"Chương 6 Biến đổi và sắp xếp dữ liệu","heading":"6.1.1 Thêm biến bằng hàm mutate()","text":"Khi bạn đọc muốn thêm các cột khác vào một dữ liệu được tính toán từ các cột hiện có, bạn có thể sử dụng hàm mutate(). Hàm mutate() luôn thêm cột vào sau cột cuối cùng trong dữ liệu hiện có. Khi bạn muốn thêm cột vào một vị trí cụ thể, bạn có thể sử dụng các tham số sau:Tham số .sử dụng để gán giá trị cho tên biến (cột) phía trước cột mà bạn đọc muốn thêm vào.Tham số .sử dụng để gán giá trị cho tên biến (cột) phía trước cột mà bạn đọc muốn thêm vào.Tham số .sử dụng để gán giá trị cho tên biến (cột) phía sau cột mà bạn đọc muốn thêm vào.Tham số .sử dụng để gán giá trị cho tên biến (cột) phía sau cột mà bạn đọc muốn thêm vào.Cách sử dụng mutate() như sau:Câu lệnh trên có ý nghĩa là thêm cột có tên là \\(gdp\\_per\\_capita\\), được tính bằng tổng thu nhập quốc dân (\\(gdp\\)) chia cho dân số (\\(population\\)) của quốc gia trong năm tương ứng. Nếu bạn đọc muốn cột mới được thêm vào ngay sau cột \\(gdp\\), hãy sử dụng thêm tham số .như sau:và để thêm cột thu nhập bình quân đầu người vào phía trước cột \\(gdp\\), chúng ta sử dụng tham số .beforeHàm số được sử dụng tương tự như hàm mutate() là hàm \\(transmute()\\). Hàm \\(transmute()\\) khác \\(mutate()\\) ở chỗ là trong dữ liệu mới được tạo thành chỉ có các cột mới được tạo thành. Ví dụ,Chúng ta có thể thấy rằng trong dữ liệu mới được tạo thành, chỉ có một cột duy nhất là cột thu nhập bình quân đầu người (\\(gdp\\_per\\_capita\\)).","code":"\nmytib<-as.tibble(gapminder) # mytib là dữ liệu kiểu tibble\nmutate(mytib, gdp_per_capita = gdp/population) # thêm cột có tên là gdp_per_capita## # A tibble: 10,545 × 10\n##    country   year infant_mortality life_expectancy fertility population      gdp\n##    <fct>    <int>            <dbl>           <dbl>     <dbl>      <dbl>    <dbl>\n##  1 Albania   1960            115.             62.9      6.19    1636054 NA      \n##  2 Algeria   1960            148.             47.5      7.65   11124892  1.38e10\n##  3 Angola    1960            208              36.0      7.32    5270844 NA      \n##  4 Antigua…  1960             NA              63.0      4.43      54681 NA      \n##  5 Argenti…  1960             59.9            65.4      3.11   20619075  1.08e11\n##  6 Armenia   1960             NA              66.9      4.55    1867396 NA      \n##  7 Aruba     1960             NA              65.7      4.82      54208 NA      \n##  8 Austral…  1960             20.3            70.9      3.45   10292328  9.67e10\n##  9 Austria   1960             37.3            68.8      2.7     7065525  5.24e10\n## 10 Azerbai…  1960             NA              61.3      5.57    3897889 NA      \n## # ℹ 10,535 more rows\n## # ℹ 3 more variables: continent <fct>, region <fct>, gdp_per_capita <dbl>\nmutate(mytib, gdp_per_capita = gdp/population, .after = gdp)## # A tibble: 10,545 × 10\n##    country   year infant_mortality life_expectancy fertility population      gdp\n##    <fct>    <int>            <dbl>           <dbl>     <dbl>      <dbl>    <dbl>\n##  1 Albania   1960            115.             62.9      6.19    1636054 NA      \n##  2 Algeria   1960            148.             47.5      7.65   11124892  1.38e10\n##  3 Angola    1960            208              36.0      7.32    5270844 NA      \n##  4 Antigua…  1960             NA              63.0      4.43      54681 NA      \n##  5 Argenti…  1960             59.9            65.4      3.11   20619075  1.08e11\n##  6 Armenia   1960             NA              66.9      4.55    1867396 NA      \n##  7 Aruba     1960             NA              65.7      4.82      54208 NA      \n##  8 Austral…  1960             20.3            70.9      3.45   10292328  9.67e10\n##  9 Austria   1960             37.3            68.8      2.7     7065525  5.24e10\n## 10 Azerbai…  1960             NA              61.3      5.57    3897889 NA      \n## # ℹ 10,535 more rows\n## # ℹ 3 more variables: gdp_per_capita <dbl>, continent <fct>, region <fct>\nmutate(mytib, gdp_per_capita = gdp/population, .before = gdp)## # A tibble: 10,545 × 10\n##    country            year infant_mortality life_expectancy fertility population\n##    <fct>             <int>            <dbl>           <dbl>     <dbl>      <dbl>\n##  1 Albania            1960            115.             62.9      6.19    1636054\n##  2 Algeria            1960            148.             47.5      7.65   11124892\n##  3 Angola             1960            208              36.0      7.32    5270844\n##  4 Antigua and Barb…  1960             NA              63.0      4.43      54681\n##  5 Argentina          1960             59.9            65.4      3.11   20619075\n##  6 Armenia            1960             NA              66.9      4.55    1867396\n##  7 Aruba              1960             NA              65.7      4.82      54208\n##  8 Australia          1960             20.3            70.9      3.45   10292328\n##  9 Austria            1960             37.3            68.8      2.7     7065525\n## 10 Azerbaijan         1960             NA              61.3      5.57    3897889\n## # ℹ 10,535 more rows\n## # ℹ 4 more variables: gdp_per_capita <dbl>, gdp <dbl>, continent <fct>,\n## #   region <fct>\ntransmute(mytib, gdp_per_capita = gdp/population)## # A tibble: 10,545 × 1\n##    gdp_per_capita\n##             <dbl>\n##  1            NA \n##  2          1243.\n##  3            NA \n##  4            NA \n##  5          5254.\n##  6            NA \n##  7            NA \n##  8          9393.\n##  9          7415.\n## 10            NA \n## # ℹ 10,535 more rows"},{"path":"biến-đổi-và-sắp-xếp-dữ-liệu.html","id":"lựa-chọn-biến-bằng-hàm-select","chapter":"Chương 6 Biến đổi và sắp xếp dữ liệu","heading":"6.1.2 Lựa chọn biến bằng hàm select()","text":"Khi dữ liệu có quá nhiều cột và chúng ta chỉ muốn sử dụng một số cột nhất định để phân tích, bạn đọc có thể sử dụng hàm select() để lựa chọn các cột mà chúng ta sẽ sử dụng trong các bước tiếp theo. Điều quan trọng khi sử dụng hàm select() là bạn đọc cần gọi tên đúng các biến mà chúng ta muốn lựa chọn. Hàm select() được sử dụng như sau:Hàm select() cũng có thể được sử dụng để lựa chọn cột và thay đổi tên cột. Ví dụ, khi bạn đọc muốn tên các biến mới tương ứng với các biến \\(year\\), \\(gdp\\), và \\(population\\) thành \\(Year\\), \\(Gdp\\) và \\(Population\\), chúng ta viết câu lệnh với hàm select() như sau:Khi dữ liệu có quá nhiều cột và việc gọi tên chính xác tất cả các cột làm cho câu lệnh select() quá phức tạp, bạn đọc có thể sử dụng select() để lựa chọn ra các cột đứng liền nhau bằng cách sử dụng dấu :,Câu lệnh trên có ý nghĩa là lấy ra biến \\(year\\) và tất cả các biến nằm giữa biến \\(gdp\\) và biến \\(population\\).Hàm select() còn cho phép bạn đọc lấy ra các cột mà chúng ta không nhớ chính xác tên bằng cách sử dụng các tham số dưới đây:Tham số starts_with() được sử dụng để lựa chọn các cột có tên bắt đầu bằng một chuỗi ký tự nào đó. Ví dụ như khi chúng ta không nhớ chính xác tên của biến mô tả tỷ lệ tử vong của trẻ sơ sinh trong dữ liệu \\(\\textbf{gapminder}\\), chúng ta chỉ chắc chắn rằng tên cột bắt đầu bằng \"infant\", chúng ta lựa chọn biến này như sau:Tương tự như starts_with(), các tham số ends_with() và contains() được sử dụng để lựa chọn các cột kết thúc bởi một chuỗi ký tự và chứa một chuỗi ký tự nào đó.Tương tự như starts_with(), các tham số ends_with() và contains() được sử dụng để lựa chọn các cột kết thúc bởi một chuỗi ký tự và chứa một chuỗi ký tự nào đó.Tham số matches() có thể được sử dụng trong trường hợp tổng quát hơn contains() các cột được lựa chọn có tên được biểu diễn qua các biểu thức chính quy.Tham số matches() có thể được sử dụng trong trường hợp tổng quát hơn contains() các cột được lựa chọn có tên được biểu diễn qua các biểu thức chính quy.Khi số lượng cột được lựa chọn nhiều hơn số cột không được lựa chọn, hoặc khi bạn đọc muốn loại một số cột không sử dụng ra khỏi dữ liệu, hàm select() cũng cho phép chúng ta thực thi yêu cầu bằng cách thêm dấu - vào tên các cột mà chúng ta muốn loại ra khỏi dữ liệu. Hãy quan sát ví dụ dưới đây:Hàm select() ỏ trên có ý nghĩa là loại đi ra khỏi dữ liệu tất cả các biến có tên bắt đầu bằng “gdp”##$ Lọc dữ liệu bằng hàm filter()Hàm filter() cho phép chúng ta lọc các quan sát dựa trên giá trị của các cột. có một số thư viện trong R sử dụng hàm filter() với mục đích khác nhau và chúng ta có thể không chắc chắn về thứ tự ưu tiên của các thư viện đang sẵn sàng trên môi trường làm việc hiện tại nên tốt nhất là chúng ta định nghĩa lại tên hàm filter() cho hàm có tên tương ứng của thư viện \\(\\textbf{dplyr}\\)Cách hoạt động của hàm \\(filter()\\) như sau: ví dụ chúng ta muốn lấy ra dữ liệu của riêng năm 2010 trong dữ liệu \\(\\textbf{gapminder}\\), hàm filter() được viết như sau như sauSau khi thực thi câu lệnh ở trên, một tibble mới được tạo thành chỉ bao gồm các quan sát có giá trị cột \\(year\\) là 2010. Lưu ý rằng nếu chúng ta muốn lưu lại giá trị sau mỗi lần thực hiện biến đổi dữ liệu, hãy gán giá trị kết quả của các hàm trong thư viện \\(\\textbf{dplyr}\\) vào một đối tượng. Hàm filter() có thể thực hiện việc lọc dữ liệu trên nhiều cột cùng một lúc. Ví dụ như chúng ta muốn lọc ra các quan sát của năm 2010 của các quốc gia thuộc lục địa Châu Âu, chúng ta sử dụng hai phép sánh trong hàm filter()","code":"\nselect(mytib, year, gdp, population) # lựa chọn các cột year, gdp, population## # A tibble: 10,545 × 3\n##     year          gdp population\n##    <int>        <dbl>      <dbl>\n##  1  1960           NA    1636054\n##  2  1960  13828152297   11124892\n##  3  1960           NA    5270844\n##  4  1960           NA      54681\n##  5  1960 108322326649   20619075\n##  6  1960           NA    1867396\n##  7  1960           NA      54208\n##  8  1960  96677859364   10292328\n##  9  1960  52392699681    7065525\n## 10  1960           NA    3897889\n## # ℹ 10,535 more rows\nselect(mytib, Year = year, Gdp =  gdp,  Population = population)## # A tibble: 10,545 × 3\n##     Year          Gdp Population\n##    <int>        <dbl>      <dbl>\n##  1  1960           NA    1636054\n##  2  1960  13828152297   11124892\n##  3  1960           NA    5270844\n##  4  1960           NA      54681\n##  5  1960 108322326649   20619075\n##  6  1960           NA    1867396\n##  7  1960           NA      54208\n##  8  1960  96677859364   10292328\n##  9  1960  52392699681    7065525\n## 10  1960           NA    3897889\n## # ℹ 10,535 more rows\nselect(mytib, year, gdp:population)## # A tibble: 10,545 × 3\n##     year          gdp population\n##    <int>        <dbl>      <dbl>\n##  1  1960           NA    1636054\n##  2  1960  13828152297   11124892\n##  3  1960           NA    5270844\n##  4  1960           NA      54681\n##  5  1960 108322326649   20619075\n##  6  1960           NA    1867396\n##  7  1960           NA      54208\n##  8  1960  96677859364   10292328\n##  9  1960  52392699681    7065525\n## 10  1960           NA    3897889\n## # ℹ 10,535 more rows\nselect(mytib, starts_with(\"infant\"))## # A tibble: 10,545 × 1\n##    infant_mortality\n##               <dbl>\n##  1            115. \n##  2            148. \n##  3            208  \n##  4             NA  \n##  5             59.9\n##  6             NA  \n##  7             NA  \n##  8             20.3\n##  9             37.3\n## 10             NA  \n## # ℹ 10,535 more rows\nmytib1<-mutate(mytib, gdp_per_capita = gdp/population)\nselect(mytib1, contains(\"gdp\")) # Lấy các cột có tên chứa \"gdp\"## # A tibble: 10,545 × 2\n##             gdp gdp_per_capita\n##           <dbl>          <dbl>\n##  1           NA            NA \n##  2  13828152297          1243.\n##  3           NA            NA \n##  4           NA            NA \n##  5 108322326649          5254.\n##  6           NA            NA \n##  7           NA            NA \n##  8  96677859364          9393.\n##  9  52392699681          7415.\n## 10           NA            NA \n## # ℹ 10,535 more rows\nselect(mytib1, - starts_with(\"gdp\"))## # A tibble: 10,545 × 8\n##    country  year infant_mortality life_expectancy fertility population continent\n##    <fct>   <int>            <dbl>           <dbl>     <dbl>      <dbl> <fct>    \n##  1 Albania  1960            115.             62.9      6.19    1636054 Europe   \n##  2 Algeria  1960            148.             47.5      7.65   11124892 Africa   \n##  3 Angola   1960            208              36.0      7.32    5270844 Africa   \n##  4 Antigu…  1960             NA              63.0      4.43      54681 Americas \n##  5 Argent…  1960             59.9            65.4      3.11   20619075 Americas \n##  6 Armenia  1960             NA              66.9      4.55    1867396 Asia     \n##  7 Aruba    1960             NA              65.7      4.82      54208 Americas \n##  8 Austra…  1960             20.3            70.9      3.45   10292328 Oceania  \n##  9 Austria  1960             37.3            68.8      2.7     7065525 Europe   \n## 10 Azerba…  1960             NA              61.3      5.57    3897889 Asia     \n## # ℹ 10,535 more rows\n## # ℹ 1 more variable: region <fct>\nfilter<-function(...) dplyr::filter(...)\nfilter(mytib, year == 2010) # chỉ lấy các quan sát có year là 2010## # A tibble: 185 × 9\n##    country   year infant_mortality life_expectancy fertility population      gdp\n##    <fct>    <int>            <dbl>           <dbl>     <dbl>      <dbl>    <dbl>\n##  1 Albania   2010             14.8            77.2      1.74    2901883  6.14e 9\n##  2 Algeria   2010             23.5            76        2.82   36036159  7.92e10\n##  3 Angola    2010            110.             57.6      6.22   21219954  2.61e10\n##  4 Antigua…  2010              7.7            75.8      2.13      87233  8.37e 8\n##  5 Argenti…  2010             13              75.8      2.22   41222875  4.34e11\n##  6 Armenia   2010             16.1            73        1.55    2963496  4.10e 9\n##  7 Aruba     2010             NA              75.1      1.7      101597 NA      \n##  8 Austral…  2010              4.1            82        1.89   22162863  5.63e11\n##  9 Austria   2010              3.6            80.5      1.44    8391986  2.24e11\n## 10 Azerbai…  2010             33.9            70.1      1.97    9099893  2.12e10\n## # ℹ 175 more rows\n## # ℹ 2 more variables: continent <fct>, region <fct>\nfilter(mytib, year == 2010, continent == \"Europe\") ## # A tibble: 39 × 9\n##    country    year infant_mortality life_expectancy fertility population     gdp\n##    <fct>     <int>            <dbl>           <dbl>     <dbl>      <dbl>   <dbl>\n##  1 Albania    2010             14.8            77.2      1.74    2901883 6.14e 9\n##  2 Austria    2010              3.6            80.5      1.44    8391986 2.24e11\n##  3 Belarus    2010              4.7            70.2      1.46    9492122 2.60e10\n##  4 Belgium    2010              3.6            80.1      1.84   10929978 2.67e11\n##  5 Bosnia a…  2010              6.4            77.9      1.24    3835258 8.21e 9\n##  6 Bulgaria   2010             11.2            73.7      1.49    7407297 1.92e10\n##  7 Croatia    2010              4.6            76.7      1.47    4316425 2.80e10\n##  8 Czech Re…  2010              3.4            77.5      1.5    10506617 8.21e10\n##  9 Denmark    2010              3.3            79.4      1.88    5550959 1.69e11\n## 10 Estonia    2010              3.6            76.4      1.63    1332089 8.01e 9\n## # ℹ 29 more rows\n## # ℹ 2 more variables: continent <fct>, region <fct>"},{"path":"biến-đổi-và-sắp-xếp-dữ-liệu.html","id":"sắp-xếp-dữ-liệu-bằng-hàm-arrange","chapter":"Chương 6 Biến đổi và sắp xếp dữ liệu","heading":"6.1.3 Sắp xếp dữ liệu bằng hàm arrange()","text":"Hàm arrange() sắp xếp các quan sát của dữ liệu theo thứ tự tăng dần. Nguyên tắc sắp xếp cũng tương tự như quy tắc sắp xếp của hàm sort() khi làm trên véc-tơ, nghĩa là hàm bạn đọc có thể sắp xếp dữ liệu dựa theo kiểu số, kiểu logic, kiểu thời gian, hay kiểu chuỗi ký tự. Lưu ý rằng khi véc-tơ kiểu chuỗi ký tự được định nghĩa dưới dạng factor thì thứ tự tăng dần sẽ được hiểu theo cách đánh số thứ tự của factor bắt đầu từ 1.Chúng ta sắp xếp dữ liệu \\(\\textbf{gapmider}\\) theo thứ tự tăng dần cuả biến \\(year\\) như sauĐể sắp xếp dữ liệu theo thứ tự giảm dần của một biến, nếu biến dữ liệu dùng để sắp xếp là kiểu số, bạn đọc chỉ cần thêm dấu - trước tên biến đó trong hàm arrange(). Trong trường hợp tổng quát, khi dữ liệu có thể có kiểu chuỗi ký tự, ngày tháng, …, chúng ta sử dụng hàm desc() để sắp xếp dữ liệu theo thứ tự giảm dần:Việc sắp xếp dữ liệu có thể thực hiện dựa trên nhiều cột dữ liệu. Ví dụ, để sắp xếp dữ liệu \\(\\textbf{gapminder}\\) theo thứ tự tăng dần theo năm (\\(year\\)), tiếp theo là theo Châu lục (\\(continent\\)), và sau cùng là theo vùng (\\(region\\)), bạn đọc viết câu lệnh như sau:Lưu ý rằng thứ tự sắp xếp cũng có thể là tăng theo một biến và giảm theo các biến khác:Khi trong cột dữ liệu sử dụng để sắp xếp có chứa giá trị không quan sát được (NA) thì các giá trị này luôn được sắp xếp xuống phía dưới của dữ liệu bất kể chúng ta sắp xếp dữ liệu theo thứ tự tăng dần hay giảm dần. Ví dụ, cột \\(gdp\\) của dữ liệu \\(\\textbf{gapminder\\) có tỷ lệ biến nhận giá trị NA khá cao. Khi quan sát phần đuôi của dữ liệu được sắp xếp theo \\(gdp\\), chúng ta sẽ luôn thấy các giá trị NA được đẩy xuống phía dưới:Bạn đọc có thể quan sát thấy rằng phần đuôi của kết quả không thay đổi dù chúng ta có thực hiện sắp xếp theo thứ tự \\(gdp\\) tăng dần hay \\(gdp\\) giảm dần.","code":"\narrange(mytib, year) ## # A tibble: 10,545 × 9\n##    country   year infant_mortality life_expectancy fertility population      gdp\n##    <fct>    <int>            <dbl>           <dbl>     <dbl>      <dbl>    <dbl>\n##  1 Albania   1960            115.             62.9      6.19    1636054 NA      \n##  2 Algeria   1960            148.             47.5      7.65   11124892  1.38e10\n##  3 Angola    1960            208              36.0      7.32    5270844 NA      \n##  4 Antigua…  1960             NA              63.0      4.43      54681 NA      \n##  5 Argenti…  1960             59.9            65.4      3.11   20619075  1.08e11\n##  6 Armenia   1960             NA              66.9      4.55    1867396 NA      \n##  7 Aruba     1960             NA              65.7      4.82      54208 NA      \n##  8 Austral…  1960             20.3            70.9      3.45   10292328  9.67e10\n##  9 Austria   1960             37.3            68.8      2.7     7065525  5.24e10\n## 10 Azerbai…  1960             NA              61.3      5.57    3897889 NA      \n## # ℹ 10,535 more rows\n## # ℹ 2 more variables: continent <fct>, region <fct>\narrange(mytib, desc(year)) ## # A tibble: 10,545 × 9\n##    country      year infant_mortality life_expectancy fertility population   gdp\n##    <fct>       <int>            <dbl>           <dbl>     <dbl>      <dbl> <dbl>\n##  1 Albania      2016               NA            78.1        NA         NA    NA\n##  2 Algeria      2016               NA            76.5        NA         NA    NA\n##  3 Angola       2016               NA            60          NA         NA    NA\n##  4 Antigua an…  2016               NA            76.5        NA         NA    NA\n##  5 Argentina    2016               NA            76.7        NA         NA    NA\n##  6 Armenia      2016               NA            74.9        NA         NA    NA\n##  7 Aruba        2016               NA            75.8        NA         NA    NA\n##  8 Australia    2016               NA            82.3        NA         NA    NA\n##  9 Austria      2016               NA            81.4        NA         NA    NA\n## 10 Azerbaijan   2016               NA            73.3        NA         NA    NA\n## # ℹ 10,535 more rows\n## # ℹ 2 more variables: continent <fct>, region <fct>\narrange(mytib, year, continent, region) ## # A tibble: 10,545 × 9\n##    country    year infant_mortality life_expectancy fertility population     gdp\n##    <fct>     <int>            <dbl>           <dbl>     <dbl>      <dbl>   <dbl>\n##  1 Burundi    1960            145.             40.6      6.95    2786740  3.41e8\n##  2 Comoros    1960            200              44.0      6.79     188732 NA     \n##  3 Djibouti   1960             NA              45.8      6.46      83636 NA     \n##  4 Eritrea    1960             NA              39.0      6.9     1407631 NA     \n##  5 Ethiopia   1960            162              37.7      6.88   22151218 NA     \n##  6 Kenya      1960            119.             47.4      7.95    8105440  2.12e9\n##  7 Madagasc…  1960            112              42.0      7.3     5099371  2.09e9\n##  8 Malawi     1960            218.             38.5      6.91    3618604  3.48e8\n##  9 Mauritius  1960             67.8            58.7      6.17     660023 NA     \n## 10 Mozambiq…  1960            183              38.2      6.6     7493278 NA     \n## # ℹ 10,535 more rows\n## # ℹ 2 more variables: continent <fct>, region <fct>\narrange(mytib, year, desc(continent), desc(region), -gdp) # tăng dần theo năm, giảm dần theo continent, region, gdp## # A tibble: 10,545 × 9\n##    country    year infant_mortality life_expectancy fertility population     gdp\n##    <fct>     <int>            <dbl>           <dbl>     <dbl>      <dbl>   <dbl>\n##  1 French P…  1960              NA             56.3      5.66      78083 NA     \n##  2 Samoa      1960              92             51.4      7.65     108645 NA     \n##  3 Tonga      1960              NA             61.2      7.36      61600 NA     \n##  4 Kiribati   1960              NA             45.8      6.95      41234 NA     \n##  5 Micrones…  1960              NA             56.8      6.93      44539 NA     \n##  6 Papua Ne…  1960             135.            38.6      6.28    1966957  8.37e8\n##  7 Fiji       1960              54             55.7      6.46     393383  4.37e8\n##  8 New Cale…  1960              NA             56.4      5.22      78058 NA     \n##  9 Solomon …  1960             132.            50.6      6.39     117869 NA     \n## 10 Vanuatu    1960             107.            46.0      7.2       63701 NA     \n## # ℹ 10,535 more rows\n## # ℹ 2 more variables: continent <fct>, region <fct>\ntail(arrange(mytib, gdp))## # A tibble: 6 × 9\n##   country       year infant_mortality life_expectancy fertility population   gdp\n##   <fct>        <int>            <dbl>           <dbl>     <dbl>      <dbl> <dbl>\n## 1 Venezuela     2016               NA            74.8        NA         NA    NA\n## 2 West Bank a…  2016               NA            74.7        NA         NA    NA\n## 3 Vietnam       2016               NA            75.6        NA         NA    NA\n## 4 Yemen         2016               NA            64.9        NA         NA    NA\n## 5 Zambia        2016               NA            57.1        NA         NA    NA\n## 6 Zimbabwe      2016               NA            61.7        NA         NA    NA\n## # ℹ 2 more variables: continent <fct>, region <fct>\ntail(arrange(mytib, desc(gdp)))## # A tibble: 6 × 9\n##   country       year infant_mortality life_expectancy fertility population   gdp\n##   <fct>        <int>            <dbl>           <dbl>     <dbl>      <dbl> <dbl>\n## 1 Venezuela     2016               NA            74.8        NA         NA    NA\n## 2 West Bank a…  2016               NA            74.7        NA         NA    NA\n## 3 Vietnam       2016               NA            75.6        NA         NA    NA\n## 4 Yemen         2016               NA            64.9        NA         NA    NA\n## 5 Zambia        2016               NA            57.1        NA         NA    NA\n## 6 Zimbabwe      2016               NA            61.7        NA         NA    NA\n## # ℹ 2 more variables: continent <fct>, region <fct>"},{"path":"biến-đổi-và-sắp-xếp-dữ-liệu.html","id":"kết-hợp-các-phép-biến-đổi-bằng-toán-tử-pipe","chapter":"Chương 6 Biến đổi và sắp xếp dữ liệu","heading":"6.1.4 Kết hợp các phép biến đổi bằng toán tử pipe (%>%)","text":"Trước khi giới thiệu các hàm số khác sử dụng để biến đổi dữ liệu của thư viện \\(\\textbf{dplyr}\\), chúng tôi giới thiệu đến bạn đọc toán tử pipe (%>%). Đây là một công cụ rất hiệu quả khi chúng ta thực hiện một chuỗi các phép biến đổi dữ liệu. Thuật ngữ “toán tử pipe” được mượn từ toán học khi nói đến việc sử dụng các hàm số nối tiếp nhau. Pipe trong thư viện \\(\\textbf{dplyr}\\) cũng có ý nghĩa tương tự khi bạn đọc sẽ sử dụng một chuỗi các hàm của thư viện nhằm biến đổi dữ liệu. Toán tử pipe giúp cho việc viết các câu lệnh biến đổi dữ liệu phức tạp trở nên đơn giản và tránh bị nhầm lẫn. Ví dụ, khi chúng ta muốn thực hiện một phân tích trên dữ liệu \\(textbf{gapminder}\\), đó là tìm ra ba quốc gia có thu nhập bình quân đầu người cao nhất của năm 2000. Để thực hiện được việc này, bạn đọc sẽ cần thực hiện các phép biến đổi sau theo thứ tự như sau:Thứ nhất: tính toán thêm cột thu nhập bình quân đầu người, sử dụng hàm mutate().Thứ nhất: tính toán thêm cột thu nhập bình quân đầu người, sử dụng hàm mutate().Thứ hai: lọc dữ liệu theo năm, chỉ lấy dữ liệu của năm 2000, sử dụng hàm filter().Thứ hai: lọc dữ liệu theo năm, chỉ lấy dữ liệu của năm 2000, sử dụng hàm filter().Thứ ba: lựa chọn cột tên quốc gia (\\(country\\)) và cột thu nhập bình quân đầu người vừa tính toán, sử dụng hàm select().Thứ ba: lựa chọn cột tên quốc gia (\\(country\\)) và cột thu nhập bình quân đầu người vừa tính toán, sử dụng hàm select().Thứ tư: sắp xếp dữ liệu theo cột thu nhập bình quân đầu người, thứ tự sắp xếp là giảm dần, sử dụng hàm arrange().Thứ tư: sắp xếp dữ liệu theo cột thu nhập bình quân đầu người, thứ tự sắp xếp là giảm dần, sử dụng hàm arrange().Thứ năm: lấy ra ba hàng đầu tiên của dữ liệu sau khi sắp xếp, sử dụng hàm head().Thứ năm: lấy ra ba hàng đầu tiên của dữ liệu sau khi sắp xếp, sử dụng hàm head().Nếu viết các câu lệnh một cách thông thường, sau mỗi bước ở trên bạn đọc sẽ phải lưu kết quả và gọi lại kết quả vào bước kế tiếp:Cách viết các câu lệnh như trên sẽ gặp khó khăn và dễ bị nhầm lần chúng ta phải liên tục lưu kết quả của từng câu lệnh và gọi lại kết quả đó trong bước tiếp theo. Để tránh gặp phải vấn đề này, toán tử pipe %>% giúp chúng ta kết nối các câu lệnh lại với nhau trong một câu lệnh duy nhất. Cùng một yêu cầu như ở trên, cách viết các câu lệnh biến đổi dữ liệu sử dụng toán tử pipe như sau:Bạn đọc có thể thấy rằng kết quả thu được hoàn toàn giống như phía trên. Ngoài ra, cách viết câu lệnh sử dụng toán tử pipe có ưu điểm là rất rõ ràng, ngắn gọn, và không gây nhầm lẫn. Để bạn đọc làm quen với cách viết câu lệnh sử dụng pipe, từ phần này của cuốn sách, mọi phép biến đổi trên dữ liệu đều được ưu tiên sử dụng cách viết này.Chúng ta sẽ tiếp tục làm quen với các hàm số sử dụng để biến đổi dữ liệu của thư viện \\(\\textbf{dplyr}\\) trong các phần tiếp theo.","code":"\nmytib1<-mutate(mytib,gdp_per_capita = gdp/population) # bước thứ nhất\nmytib1<-filter(mytib1, year == 2010) # bước thứ hai\nmytib1<-select(mytib1, country, gdp_per_capita) # bước thứ ba\nmytib1<-arrange(mytib1, desc(gdp_per_capita)) # bước thứ tư\nhead(mytib1,3) # bước thứ năm## # A tibble: 3 × 2\n##   country    gdp_per_capita\n##   <fct>               <dbl>\n## 1 Luxembourg         52210.\n## 2 Japan              40013.\n## 3 Norway             39954.\nmytib%>%mutate(gdp_per_capita = gdp/population)%>%\n  filter(year == 2010)%>%\n  select(country, gdp_per_capita)%>%\n  arrange(desc(gdp_per_capita)) %>%\n  head(3)## # A tibble: 3 × 2\n##   country    gdp_per_capita\n##   <fct>               <dbl>\n## 1 Luxembourg         52210.\n## 2 Japan              40013.\n## 3 Norway             39954."},{"path":"biến-đổi-và-sắp-xếp-dữ-liệu.html","id":"tổng-hợp-dữ-liệu-bằng-group_by-và-summarise","chapter":"Chương 6 Biến đổi và sắp xếp dữ liệu","heading":"6.1.5 Tổng hợp dữ liệu bằng group_by và summarise()","text":"Hàm group_by() là một công cụ mạnh mẽ sử dụng để tổng hợp dữ liệu và tính toán theo nhóm. Ví dụ, tổng thu nhập quốc nội của Việt Nam năm 2000 là hơn 31 tỷ USD. Chúng ta muốn biết nền kinh tế của Việt Nam với mức trung bình của Châu Á vào năm 2000 là như thế nào. Để thực hiện được việc đó, chúng ta cần tính tổng thu nhập quốc dân trung bình của các quốc gia Châu Á. Một cách tự nhiên, bạn đọc có thể sử dụng hàm filter() để lọc dữ liệu của Châu Á vào năm 2000 sau đó tính trung bình biến tổng thu nhập quốc dân. Tuy nhiên, nếu bạn muốn thực hiện phép sánh tổng thu nhập quốc dân của tất cả các quốc gia với thu nhập trung bình của Châu lục tương ứng trong năm 2000, sử dụng hàm filter() như trên sẽ không hiệu quả vì chỉ cho phép thực hiện sánh với từng quốc gia cụ thể trong một năm cụ thể.Hàm group_by() sẽ giúp chúng ta thực hiện các tính toán theo các nhóm trên toàn bộ dữ liệu. Để sánh \\(gdp\\) của một nước với \\(gdp\\) trung bình của Châu lục trong từng năm, chúng ta sẽ cần thực hiện các bước như sau:Bước 1: Nhóm dữ liệu lại theo châu lục và theo năm.Bước 1: Nhóm dữ liệu lại theo châu lục và theo năm.Bước 2: Tính giá trị trung bình thu nhập quốc dân của châu lục trong mỗi năm. Khi tính giá trị trung bình sẽ bỏ qua các quốc gia không có quan sát về \\(gdp\\).Bước 2: Tính giá trị trung bình thu nhập quốc dân của châu lục trong mỗi năm. Khi tính giá trị trung bình sẽ bỏ qua các quốc gia không có quan sát về \\(gdp\\).Bước 3: sánh gdp của quốc gia với gdp trung bình của châu lục trong nămBước 3: sánh gdp của quốc gia với gdp trung bình của châu lục trong nămNhững bước như trên có thể được thực hiện một cách đơn giản thông qua hàm group_by() như sauCác tính toán được sử dụng sau hàm group_by() và trước hàm ungroup() đều được thực hiện theo các nhóm được định nghĩa bởi hàm group_by(). Trong câu lệnh ở trên, sau khi sử dụng hàm group_by(), các quan sát có cùng giá trị của cột \\(continents\\) và cột \\(year\\) sẽ được hiểu là một nhóm. Các tính toán sau đó cần phải được thực hiện bằng các hàm hoạt động trên véc-tơ. Thật vậy, hàm mean(,na.rm = TRUE) tính giá trị trung bình của cột \\(gdp\\) cho từng châu lục theo từng năm. Chúng ta có thể kiểm tra giá trị ở các hàng đầu tiên của cột \\(gdp\\_year\\_continent\\) từ kết quả ở trên hoàn toàn giống với cách tính sử dụng hàm filter() cho một châu lục cụ thể vào một năm cụ thể:Cả hai cách tính đều cho biết tổng thu nhập quốc dân trung bình của các nước Châu Âu trong năm 1960 là 110 tỷ USD.Hàm summarise() được sử dụng tiếp sau hàm group_by() thay cho ungroup() khi chúng ta muốn tạo thành một dữ liệu mới mà mỗi quan sát tương ứng với một nhóm được quy định bởi group_by(). Ví dụ, nếu chúng ta muốn tạo ra một dữ liệu mới mà mỗi quan sát tương ứng với một châu lục trong một năm, và cột \\(gdp\\_year\\_continent\\) cho biết tổng thu nhập quốc dân trung bình của châu lục trong năm đó, câu lệnh được viết như sau:Bạn đọc có thể thấy rằng dữ liệu mới được tạo thành dưới dạng một tibble mà mỗi quan sát là một châu lục trong một năm. Dữ liệu có ba biến, trong đó hai biến đầu tiên là \\(continent\\) và \\(year\\) được quy định bởi hàm group_by(), còn biến thứ ba là \\(gdp\\_year\\_continent\\) mới được tạo thành từ hàm summarise().","code":"\nmytib%>%group_by(continent, year) %>% # nhóm dữ liệu theo continent và year\n  mutate(gdp_year_continent = mean(gdp,na.rm=TRUE))%>% # thêm cột gdp bình quân theo nhóm\n  ungroup()%>% # để dữ liệu trở lại trạng thái ban đầu\n  mutate(gdp_level = ifelse(gdp > gdp_year_continent, \"High\", \"Low\"))%>%\n  select(country,continent,year,contains(\"gdp\"))## # A tibble: 10,545 × 6\n##    country             continent  year          gdp gdp_year_continent gdp_level\n##    <fct>               <fct>     <int>        <dbl>              <dbl> <chr>    \n##  1 Albania             Europe     1960           NA      110891884444. <NA>     \n##  2 Algeria             Africa     1960  13828152297        3652247577. High     \n##  3 Angola              Africa     1960           NA        3652247577. <NA>     \n##  4 Antigua and Barbuda Americas   1960           NA      114730852582  <NA>     \n##  5 Argentina           Americas   1960 108322326649      114730852582  Low      \n##  6 Armenia             Asia       1960           NA       61181554907. <NA>     \n##  7 Aruba               Americas   1960           NA      114730852582  <NA>     \n##  8 Australia           Oceania    1960  96677859364       32650664881. High     \n##  9 Austria             Europe     1960  52392699681      110891884444. Low      \n## 10 Azerbaijan          Asia       1960           NA       61181554907. <NA>     \n## # ℹ 10,535 more rows\nmytib%>%filter(year == 1960, continent == \"Europe\")%>%\n  select(gdp)%>%sapply(function(x) mean(x,na.rm=TRUE))##          gdp \n## 110891884444\nmytib%>%group_by(continent, year) %>%\n  summarise(gdp_year_continent = mean(gdp,na.rm=TRUE)) %>%\n  arrange(year, continent)## # A tibble: 285 × 3\n## # Groups:   continent [5]\n##    continent  year gdp_year_continent\n##    <fct>     <int>              <dbl>\n##  1 Africa     1960        3652247577.\n##  2 Americas   1960      114730852582 \n##  3 Asia       1960       61181554907.\n##  4 Europe     1960      110891884444.\n##  5 Oceania    1960       32650664881.\n##  6 Africa     1961        3642863976.\n##  7 Americas   1961      118059881733.\n##  8 Asia       1961       62764504767.\n##  9 Europe     1961      123997131430.\n## 10 Oceania    1961       33430553232.\n## # ℹ 275 more rows"},{"path":"biến-đổi-và-sắp-xếp-dữ-liệu.html","id":"sắp-xếp-dữ-liệu-bằng-thư-viện-textbftidyr","chapter":"Chương 6 Biến đổi và sắp xếp dữ liệu","heading":"6.2 Sắp xếp dữ liệu bằng thư viện \\(\\textbf{tidyr}\\)","text":"Một dữ liệu trước khi được đưa vào trực quan hóa hay xây dựng mô hình cần phải được sắp xếp và trình bày một cách nhất quán. Các dữ liệu có sẵn trong R hoặc trong thư viện của R nhìn chung đều đã được sắp xếp hoàn chỉnh nên chúng ta không gặp phải vấn đề này trong các ví dụ ở trên. Tuy nhiên, dữ liệu lấy từ các nguồn khác nhau thường gặp phải các vấn đề về sự nhất quán. Hãy quan sát các ví dụ sau, các dữ liệu cùng thể hiện thông tin về tổng thu nhập quốc dân và dân số của các quốc gia trên thế giới từ năm 2000 đến 2010 nhưng lại được trình bày khác nhau:Tất cả dữ liệu kể trên đều trình bày chung một nguồn thông tin nhưng chỉ có mytib1 là được sắp xếp một cách nhất quán. Trong khi các dữ liệu khác chưa sẵn sàng để thực hiện các bước tiếp theo. Thật vậy:Dữ liệu lưu trong mytib2 có biến \\(gdp\\_per\\_capita\\) trình bày dưới dạng chuỗi ký tự, với số tổng thu nhập quốc dân và dân số được lưu trong cùng một cột và cách nhau bởi dấu /. Không thể thực hiện các phân tích hay tính toán với cột dữ liệu như vậy.Dữ liệu lưu trong mytib2 có biến \\(gdp\\_per\\_capita\\) trình bày dưới dạng chuỗi ký tự, với số tổng thu nhập quốc dân và dân số được lưu trong cùng một cột và cách nhau bởi dấu /. Không thể thực hiện các phân tích hay tính toán với cột dữ liệu như vậy.Dữ liệu lưu trong mytib3 có cột \\(type\\) cho biết số tương ứng trong cột \\(value\\) là giá trị của tổng thu nhập quốc dân hay dân số của nước đó. Giá trị trong cột \\(value\\) không nhất quán vì vừa có thể là số tiền, vừa là số người.Dữ liệu lưu trong mytib3 có cột \\(type\\) cho biết số tương ứng trong cột \\(value\\) là giá trị của tổng thu nhập quốc dân hay dân số của nước đó. Giá trị trong cột \\(value\\) không nhất quán vì vừa có thể là số tiền, vừa là số người.Dữ liệu lưu trong mytyb4 cũng là một điển hình cho dữ liệu chưa được sắp xếp một cách nhất quán. Giá trị của biến \\(year\\) không được lưu dưới dạng véc-tơ cột mà được lưu dưới dạng tên các cột nên rất khó khăn trong thực hiện tính toán.Dữ liệu lưu trong mytyb4 cũng là một điển hình cho dữ liệu chưa được sắp xếp một cách nhất quán. Giá trị của biến \\(year\\) không được lưu dưới dạng véc-tơ cột mà được lưu dưới dạng tên các cột nên rất khó khăn trong thực hiện tính toán.Một dữ liệu được sắp xếp nhất quán cần phải đảm bảo hai yêu cầu:Thứ nhất: mỗi quan sát là một dòng dữ liệu.Thứ nhất: mỗi quan sát là một dòng dữ liệu.Thứ hai: mỗi biến nằm ở một cột. Một biến mô tả một thuộc tính, một tính chất của quan sát tương ứng. Giá trị trong cột phải đồng nhất và đúng định dạng.Thứ hai: mỗi biến nằm ở một cột. Một biến mô tả một thuộc tính, một tính chất của quan sát tương ứng. Giá trị trong cột phải đồng nhất và đúng định dạng.Lý chính khiến dữ liệu không được sắp xếp nhất quán là dữ liệu thường được tổ chức để thuận lợi cho một số mục tiêu khác với mục tiêu phân tích, chẳng hạn như mục tiêu nhập dữ liệu hoặc mục tiêu để hiển thị trực quan hơn với người quan sát dữ liệu.Quá trình sắp xếp dữ liệu là quá trình biến đổi, chuyển hóa dữ liệu từ các định dạng như mytib2, mytib3, hoặc mytib4 về định dạng như mytib1. Nhìn chung, sắp xếp dữ liệu là công việc tương đối đơn giản với các quy trình khác. Trong phần này, bạn đọc chỉ cần nắm vững nguyên tắc của hai phép biến đổi là kéo dài dữ liệu bằng hàm gather() (hoặc pivot_longer() trong các phiên bản mới của \\(\\textbf{tidyr}\\)) và mở rộng dữ liệu bằng hàm spread() (hoặc pivot_wider() trong các phiên bản mới của \\(\\textbf{tidyr}\\)).","code":"\nmytib1## # A tibble: 2,035 × 4\n##    country              year           gdp population\n##    <fct>               <int>         <dbl>      <dbl>\n##  1 Albania              2000   3686649387     3121965\n##  2 Algeria              2000  54790058957    31183658\n##  3 Angola               2000   9129180361    15058638\n##  4 Antigua and Barbuda  2000    802526701.      77648\n##  5 Argentina            2000 284203745280    37057453\n##  6 Armenia              2000   1911563665     3076098\n##  7 Aruba                2000   1858659293       90858\n##  8 Australia            2000 416887521196    19107251\n##  9 Austria              2000 192070749954     8050884\n## 10 Azerbaijan           2000   5272617196     8117742\n## # ℹ 2,025 more rows\nmytib2## # A tibble: 2,035 × 3\n##    country              year gdp_per_capita       \n##    <fct>               <int> <chr>                \n##  1 Albania              2000 3686649387/3121965   \n##  2 Algeria              2000 54790058957/31183658 \n##  3 Angola               2000 9129180361/15058638  \n##  4 Antigua and Barbuda  2000 802526700.6/77648    \n##  5 Argentina            2000 284203745280/37057453\n##  6 Armenia              2000 1911563665/3076098   \n##  7 Aruba                2000 1858659293/90858     \n##  8 Australia            2000 416887521196/19107251\n##  9 Austria              2000 192070749954/8050884 \n## 10 Azerbaijan           2000 5272617196/8117742   \n## # ℹ 2,025 more rows\nmytib3## # A tibble: 4,070 × 4\n##    country              year type               value\n##    <fct>               <int> <chr>              <dbl>\n##  1 Albania              2000 gdp          3686649387 \n##  2 Albania              2000 population      3121965 \n##  3 Algeria              2000 gdp         54790058957 \n##  4 Algeria              2000 population     31183658 \n##  5 Angola               2000 gdp          9129180361 \n##  6 Angola               2000 population     15058638 \n##  7 Antigua and Barbuda  2000 gdp           802526701.\n##  8 Antigua and Barbuda  2000 population        77648 \n##  9 Argentina            2000 gdp        284203745280 \n## 10 Argentina            2000 population     37057453 \n## # ℹ 4,060 more rows\nmytib4## # A tibble: 370 × 13\n##    country type   `2000`  `2001`  `2002`  `2003`  `2004`  `2005`  `2006`  `2007`\n##    <fct>   <chr>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>\n##  1 Albania gdp   3.69e 9 3.94e 9 4.06e 9 4.29e 9 4.54e 9 4.79e 9 5.03e 9 5.33e 9\n##  2 Albania popu… 3.12e 6 3.12e 6 3.12e 6 3.12e 6 3.10e 6 3.08e 6 3.05e 6 3.01e 6\n##  3 Algeria gdp   5.48e10 5.62e10 5.89e10 6.29e10 6.62e10 6.96e10 7.10e10 7.31e10\n##  4 Algeria popu… 3.12e 7 3.16e 7 3.20e 7 3.24e 7 3.28e 7 3.33e 7 3.37e 7 3.43e 7\n##  5 Angola  gdp   9.13e 9 9.42e 9 1.08e10 1.11e10 1.24e10 1.46e10 1.77e10 2.17e10\n##  6 Angola  popu… 1.51e 7 1.56e 7 1.61e 7 1.67e 7 1.73e 7 1.79e 7 1.85e 7 1.92e 7\n##  7 Antigu… gdp   8.03e 8 8.20e 8 8.41e 8 8.84e 8 9.46e 8 9.85e 8 1.12e 9 1.01e 9\n##  8 Antigu… popu… 7.76e 4 7.90e 4 8.00e 4 8.09e 4 8.17e 4 8.26e 4 8.35e 4 8.44e 4\n##  9 Argent… gdp   2.84e11 2.72e11 2.42e11 2.63e11 2.87e11 3.14e11 3.40e11 3.70e11\n## 10 Argent… popu… 3.71e 7 3.75e 7 3.79e 7 3.83e 7 3.87e 7 3.91e 7 3.96e 7 4.00e 7\n## # ℹ 360 more rows\n## # ℹ 3 more variables: `2008` <dbl>, `2009` <dbl>, `2010` <dbl>"},{"path":"biến-đổi-và-sắp-xếp-dữ-liệu.html","id":"kéo-dài-dữ-liệu-bằng-gather","chapter":"Chương 6 Biến đổi và sắp xếp dữ liệu","heading":"6.2.1 Kéo dài dữ liệu bằng gather()","text":"Dữ liệu được lưu dưới dạng của mytib4 có thông tin về năm quan sát được lưu dưới dạng cột. Đây là trường hợp rất hay gặp phải khi lấy dữ liệu từ các nguồn bên ngoài vào. Các dữ liệu này được lưu dưới dạng các ma trận với ít dòng và nhiều cột để thuận lợi cho người sử dụng dữ liệu có quan sát trực quan. Để đưa dữ liệu như mytib4 về định dạng có thể phân tích được, mà mỗi dòng là một quốc gia, được quan sát trong 1 năm, chúng ta cần thêm vào dữ liệu một cột có tên \\(year\\). Giá trị của biến \\(year\\) là tên tất cả các cột, từ cột 2000 đến cột 2010. Thêm cột \\(year\\) và kéo dài dữ liệu mytib4, chúng ta sử dụng hàm gather() như sau:Bạn đọc có thể thấy rằng kết quả thu được đã có thêm cột \\(year\\) cho biết thông tin mỗi quốc gia được quan sát trong năm bao nhiêu va cột \\(`gdp/population`\\) cho biết giá trị của tổng thu nhập quốc dân hoặc dân số của quốc gia đó. Qua cách sử dụng hàm gather(), có thể thấy rằngTham số key trong hàm gather() cho biết tên của cột trong dữ liệu mới được tạo thành chứa giá trị là tên các cột được tập hợp lại. Tham số value cho biết tên cột trong dữ liệu mới được tạo thành chứa tất cả các giá trị trong các cột được tập hợp.Tham số key trong hàm gather() cho biết tên của cột trong dữ liệu mới được tạo thành chứa giá trị là tên các cột được tập hợp lại. Tham số value cho biết tên cột trong dữ liệu mới được tạo thành chứa tất cả các giá trị trong các cột được tập hợp.Sau khi khai báo hai tham số key và value, bạn đọc cần khai báo chính xác thông tin các biến không bị tác động bởi hàm gather(). Trong ví dụ ở trên, tất các các giá trị chúng ta muốn tập hợp lại là các cột bắt đầu từ cột có tên 2000 (tương ứng với các quan sát trong năm 2000) đến cột có tên 2010 (tương ứng với các quan sát của năm 2010). Khai báo -country và -type trong hàm gather() có ý nghĩa là các cột được tập hợp thông tin lại thành một cột duy nhất là tất cả các cột, ngoại trừ hai cột \\(country\\) và \\(type\\).Sau khi khai báo hai tham số key và value, bạn đọc cần khai báo chính xác thông tin các biến không bị tác động bởi hàm gather(). Trong ví dụ ở trên, tất các các giá trị chúng ta muốn tập hợp lại là các cột bắt đầu từ cột có tên 2000 (tương ứng với các quan sát trong năm 2000) đến cột có tên 2010 (tương ứng với các quan sát của năm 2010). Khai báo -country và -type trong hàm gather() có ý nghĩa là các cột được tập hợp thông tin lại thành một cột duy nhất là tất cả các cột, ngoại trừ hai cột \\(country\\) và \\(type\\).Trong các phiên bản mới của thư viện \\(\\textbf{tidyr}\\), hàm pivot_long() thường được sử dụng thay thế cho gather(). Ưu điểm của pivot_longer() là câu lệnh dễ hiểu hơn và cho phép chúng ta quản lý tên cột bằng các hàm sử dụng cùng với các biểu thức chính quy. Ví dụ, chúng ta có thể sử dụng pivot_longer() để tập hợp thông tin của tất cả các cột có tên cột bắt đầu bằng ký tự \"2\" như sauHàm pivot_longer() cho phép kéo dài các bảng mà giá trị của nhiều biến được tích hợp trong tên của các cột. Ví dụ, dữ liệu trong mytib5 dưới đây lưu điểm của hai sinh viên theo hai kỳ học của các năm 2023 và năm 2024:Bạn đọc có thể thấy rằng tên các cột có bao gồm hai thông tin là năm học (2023 hoặc 2024) và thông tin về kỳ học (S1 hoặc S2). Chúng ta có thể sử dụng pivot_longer() để tạo thành dữ liệu mới, với hai biến mới tương ứng với hai thông tin: \\(year\\) tương ứng với năm học và \\(semester\\) tương ứng với kỳ học của hai sinh viên:Trong các trường hợp phức tạp hơn, trong tên biến của dữ liệu ban đầu vừa chứa tên biến trong dữ liệu mới vừa chứa giá trị của biến trong dữ liệu mới như dữ liệu trong mytib6 dưới đây:Bạn đọc có thể thấy rằng dữ liệu chứa thông tin về 8 người, giữ các chức chức vụ lớp trưởng và bí thư tại 4 lớp với thông tin tương ứng với từng người là tên và điểm. Các cột dữ liệu 'Ten_LopTruong' và 'Ten_BiThu' vừa chứa tên của một biến là tên của người, vừa chứa giá trị của biến chức vụ của người đó. Tương tự, các cột dữ liệu 'Diem_LopTruong' và 'Diem_BiThu' vừa chứa tên của biến là điểm của bạn sinh viên tương ứng, vừa chứa giá trị của biến là chức vụ của người đó. Dữ liệu có thể được sắp xếp lại bằng pivot_longer() như sau:Trong câu lệnh ở trên, chúng tôi sử dụng tham số names_to với phần tử đầu tiên là \".value\" thay vì sử dụng tham số values_to như trên. Mục đích là để khai báo rằng thành phần thứ nhất trong tên của các cột là tên biến, bao gồm các biến \\(Ten\\) và biến \\(Diem\\). Phần tử thứ hai của tham số names_to là \"Chuc_vu\" tương ứng với biến \\(Chuc\\_vu\\) trong dữ liệu mới.","code":"\nmytib4%>%gather(key = \"year\", value = \"gdp_population\", -country, -type)## # A tibble: 4,070 × 4\n##    country             type       year  gdp_population\n##    <fct>               <chr>      <chr>          <dbl>\n##  1 Albania             gdp        2000     3686649387 \n##  2 Albania             population 2000        3121965 \n##  3 Algeria             gdp        2000    54790058957 \n##  4 Algeria             population 2000       31183658 \n##  5 Angola              gdp        2000     9129180361 \n##  6 Angola              population 2000       15058638 \n##  7 Antigua and Barbuda gdp        2000      802526701.\n##  8 Antigua and Barbuda population 2000          77648 \n##  9 Argentina           gdp        2000   284203745280 \n## 10 Argentina           population 2000       37057453 \n## # ℹ 4,060 more rows\nmytib4%>%pivot_longer(cols = starts_with(\"2\"), \n                      names_to = \"year\",\n                      values_to = \"gdp_population\")## # A tibble: 4,070 × 4\n##    country type  year  gdp_population\n##    <fct>   <chr> <chr>          <dbl>\n##  1 Albania gdp   2000      3686649387\n##  2 Albania gdp   2001      3944714844\n##  3 Albania gdp   2002      4059111575\n##  4 Albania gdp   2003      4290480934\n##  5 Albania gdp   2004      4543619309\n##  6 Albania gdp   2005      4793518372\n##  7 Albania gdp   2006      5033194290\n##  8 Albania gdp   2007      5330152753\n##  9 Albania gdp   2008      5740574515\n## 10 Albania gdp   2009      5930013474\n## # ℹ 4,060 more rows\nmytib5## # A tibble: 2 × 5\n##   Name  `2023 S1` `2023 S2` `2024 S1` `2024 S2`\n##   <chr>     <dbl>     <dbl>     <dbl>     <dbl>\n## 1 SV1         8.5       6.9       8.1       8.5\n## 2 SV2         8         8.2       8.8       8\nmytib5%>%pivot_longer(cols = !Name, # Không tập hợp cột Names\n                      names_to = c(\"year\", \"semester\"),\n                      names_sep = \" \",\n                      values_to = \"GPA\")## # A tibble: 8 × 4\n##   Name  year  semester   GPA\n##   <chr> <chr> <chr>    <dbl>\n## 1 SV1   2023  S1         8.5\n## 2 SV1   2023  S2         6.9\n## 3 SV1   2024  S1         8.1\n## 4 SV1   2024  S2         8.5\n## 5 SV2   2023  S1         8  \n## 6 SV2   2023  S2         8.2\n## 7 SV2   2024  S1         8.8\n## 8 SV2   2024  S2         8\nmytib6## # A tibble: 4 × 5\n##   Lop   Ten_LopTruong Ten_BiThu Diem_LopTruong Diem_BiThu\n##   <chr> <chr>         <chr>              <int>      <int>\n## 1 Act61 LT1           BT1                    6          7\n## 2 Act62 LT2           BT2                    7          8\n## 3 Act63 LT3           BT3                    8          9\n## 4 Act64 LT4           BT4                    9         10\nmytib6%>%pivot_longer(cols = !Lop,\n                      names_to = c(\".value\", \"Chuc_vu\"),\n                      names_sep = \"_\")## # A tibble: 8 × 4\n##   Lop   Chuc_vu   Ten    Diem\n##   <chr> <chr>     <chr> <int>\n## 1 Act61 LopTruong LT1       6\n## 2 Act61 BiThu     BT1       7\n## 3 Act62 LopTruong LT2       7\n## 4 Act62 BiThu     BT2       8\n## 5 Act63 LopTruong LT3       8\n## 6 Act63 BiThu     BT3       9\n## 7 Act64 LopTruong LT4       9\n## 8 Act64 BiThu     BT4      10"},{"path":"biến-đổi-và-sắp-xếp-dữ-liệu.html","id":"mở-rộng-dữ-liệu-với-speard","chapter":"Chương 6 Biến đổi và sắp xếp dữ liệu","heading":"6.2.2 Mở rộng dữ liệu với speard()","text":"Ngược lại với gather(), chúng ta sử dụng speard() để mở rộng dữ liệu, nghĩa là biến đổi tên biến thành tên các cột. Dữ liệu cần được mở rộng khi giá trị trong các cột dữ liệu không được đồng nhất, giống như thông tin về tổng thu nhập quốc dân và dân số của các quốc gia trên thế giới trong dữ liệu được lưu trong mytib3Bạn đọc có thể thấy rằng trong cột \\(gdp\\_population\\) vừa có cả thông tin về tổng thu nhập quốc dân (\\(gdp\\)) và thông tin về dân số (\\(population\\)) của quốc gia đó và cột \\(type\\) cho biết giá trị đó là \\(gdp\\) hay \\(population\\). Dữ liệu như trên có thể được sắp xếp lại bằng cách sử dụng hàm speard() như sau:Có thể thấy rằng dữ liệu mới đã được sắp xếp nhất quán và đã có thể được sử dụng trong phân tích và xây dựng mô hình. Trong hàm spread() ở trên, tham số key cho biết cột nào trong dữ liệu ban đầu là cột chứa giá trị là tên các biến mới. cột \\(type\\) của dữ liệu mytib3 chỉ có chứa hai giá trị là gdp và population nên trong dữ liệu mới có hai cột mới được hình thành tương ứng với hai giá trị trong cột \\(type\\). Tham số value được sử dụng để cho biết giá trị của biến nào trong dữ liệu ban đầu sẽ được lấy vào các cột mới được hình thành. Trong câu lệnh ở trên, cột \\(value\\) của mytib3 chứa giá trị tương ứng với tổng thu nhập quốc dân và dân số của mỗi nước được sử dụng để gán cho tham số value trong hàm spread().Trong các phiên bản mới của thư viện \\(\\textbf{tidyr}\\), hàm pivot_wider() được sử dụng để thay thế cho hàm spread() có nhiều ưu điểm hơn. cách sử dụng pivot_wider() tương tự như pivot_longer() nên chúng tôi không đi vào thảo luận chi tiết về hàm số này. Bạn đọc tự tham khảo cách sử dụng hàm pivot_wider() và thực hành trong phần bài tập của chương.","code":"\nmytib3## # A tibble: 4,070 × 4\n##    country              year type               value\n##    <fct>               <int> <chr>              <dbl>\n##  1 Albania              2000 gdp          3686649387 \n##  2 Albania              2000 population      3121965 \n##  3 Algeria              2000 gdp         54790058957 \n##  4 Algeria              2000 population     31183658 \n##  5 Angola               2000 gdp          9129180361 \n##  6 Angola               2000 population     15058638 \n##  7 Antigua and Barbuda  2000 gdp           802526701.\n##  8 Antigua and Barbuda  2000 population        77648 \n##  9 Argentina            2000 gdp        284203745280 \n## 10 Argentina            2000 population     37057453 \n## # ℹ 4,060 more rows\nmytib3%>%spread(key = type, value = value)## # A tibble: 2,035 × 4\n##    country  year        gdp population\n##    <fct>   <int>      <dbl>      <dbl>\n##  1 Albania  2000 3686649387    3121965\n##  2 Albania  2001 3944714844    3124093\n##  3 Albania  2002 4059111575    3123112\n##  4 Albania  2003 4290480934    3117045\n##  5 Albania  2004 4543619309    3103758\n##  6 Albania  2005 4793518372    3082172\n##  7 Albania  2006 5033194290    3050741\n##  8 Albania  2007 5330152753    3010849\n##  9 Albania  2008 5740574515    2968026\n## 10 Albania  2009 5930013474    2929886\n## # ℹ 2,025 more rows"},{"path":"biến-đổi-và-sắp-xếp-dữ-liệu.html","id":"phụ-lục-2","chapter":"Chương 6 Biến đổi và sắp xếp dữ liệu","heading":"6.3 Phụ lục","text":"","code":"\n# A simple formula to success\nwhile (result != success){\n  try <- try * 2 }"},{"path":"biến-đổi-và-sắp-xếp-dữ-liệu.html","id":"bài-tập-2","chapter":"Chương 6 Biến đổi và sắp xếp dữ liệu","heading":"6.4 Bài tập","text":"","code":"## \n## Attaching package: 'dplyr'## The following object is masked from 'package:pryr':\n## \n##     where## The following object is masked from 'package:gridExtra':\n## \n##     combine## The following object is masked from 'package:kableExtra':\n## \n##     group_rows## The following objects are masked from 'package:stats':\n## \n##     filter, lag## The following objects are masked from 'package:base':\n## \n##     intersect, setdiff, setequal, union## \n## Attaching package: 'lubridate'## The following objects are masked from 'package:base':\n## \n##     date, intersect, setdiff, union## \n## Attaching package: 'plotly'## The following object is masked from 'package:ggplot2':\n## \n##     last_plot## The following object is masked from 'package:stats':\n## \n##     filter## The following object is masked from 'package:graphics':\n## \n##     layout## ── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n## ✔ purrr   1.0.2     ✔ stringr 1.5.1\n## ✔ readr   2.1.4     \n## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n## ✖ dplyr::combine()    masks gridExtra::combine()\n## ✖ purrr::compose()    masks pryr::compose()\n## ✖ plotly::filter()    masks dplyr::filter(), stats::filter()\n## ✖ dplyr::group_rows() masks kableExtra::group_rows()\n## ✖ dplyr::lag()        masks stats::lag()\n## ✖ purrr::partial()    masks pryr::partial()\n## ✖ dplyr::where()      masks pryr::where()\n## ℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors"},{"path":"trực-quan-hóa-dữ-liệu.html","id":"trực-quan-hóa-dữ-liệu","chapter":"Chương 7 Trực quan hóa dữ liệu","heading":"Chương 7 Trực quan hóa dữ liệu","text":"Trực quan hóa dữ liệu là nghệ thuật mô tả dữ liệu thông qua việc sử dụng đồ họa và hình ảnh như các biểu đồ, sơ đồ, hình vẽ, bao gồm cả hình ảnh động hoặc hình ảnh tương tác. Đây là phương pháp truyền đạt thông tin một cách trực quan và dễ hiểu từ người quản lý dữ liệu đến người tiếp nhận. Trực quan hóa giúp mô tả một cách hiệu quả các mối quan hệ dữ liệu phức tạp, các thông tin chuyên sâu, và các vấn đề bất thường ẩn chứa trong dữ liệu.Tại sao lại cần trực quan hóa dữ liệu ? Thứ nhất là não bộ của con người sẽ cho phản ứng đối với hình ảnh, màu sắc, kích thước, hay khoảng cách, tốt hơn nhiều với các ký hiệu, chuỗi ký tự, hay các con số. Thứ hai là dữ liệu đang ngày càng trở nên lớn hơn và phức tạp hơn. Trực quan hóa là phương pháp hiệu quả nhất để tìm ra các giá trị ẩn chứa bên trong dữ liệu. Đây chính là nguyên nhân khiến kỹ năng trực quan hóa dữ liệu được đánh giá là kỹ năng quan trọng nhất đối với những người phân tích dữ liệu.Có nhiều công cụ để trực quan hóa dữ liệu một cách chuyên nghiệp. Tiêu biểu phải kể đến hai công cụ quen thuộc là Power BI và Tableau. Đây là hai công cụ thân thiện với người dùng, cho phép người dùng tạo bảng điều khiển và báo cáo tương tác một cách nhanh chóng và dễ dàng. Cả hai đều có giao diện kiểu kéo và thả con trỏ giúp dễ dàng tạo hình ảnh trực quan mà không cần bất kỳ kỹ năng lập trình nào.Khác với Power BI hay Tableau, R sử dụng thư viện \\(\\textbf{ggplot2}\\) để trực quan hóa dữ liệu. Sẽ là không dễ dàng cho người mới bắt đầu vẽ được đồ thị bằng các câu lệnh của \\(\\textbf{ggplot2}\\). Điểm mạnh của thư viện \\(\\textbf{ggplot2}\\) với các công cụ như Power BI hay Tableau là cho phép người dùng tạo các hình ảnh có khả năng tùy biến cao. \\(\\textbf{ggplot2}\\) là lựa chọn phù hợp dành cho các nhà phân tích dữ liệu, những người cảm thấy hứng thú với việc viết các câu lệnh để tạo ra các hình ảnh trực quan phức tạp, và quan trọng nhất là đúng theo ý muốn của mình. Với một chút kinh nghiệm về Power BI và Tableau, cùng với nhiều hơn một chút kinh nghiệm về \\(\\textbf{ggplot2}\\), chúng tôi cho rằng bạn đọc nên làm quen với cả hai cách trực quan hóa dữ liệu. Khi bạn phải tạo các báo cáo trực quan trong một thời gian ngắn, Power BI hay Tableau sẽ là lựa chọn tối ưu. Khi bạn muốn vẽ những hình ảnh phức tạp, có tính cá nhân cao, và bạn có thời gian để làm việc đó, hãy sử dụng R và thư viện \\(\\textbf{ggplot2}\\).","code":""},{"path":"trực-quan-hóa-dữ-liệu.html","id":"ggplot_intro","chapter":"Chương 7 Trực quan hóa dữ liệu","heading":"7.1 Tổng quan về \\(\\textbf{ggplot2}\\)","text":"Như chúng tôi đã giới thiệu, \\(\\textbf{ggplot2}\\) là một thư viện để trực quan hóa dữ liệu trong R. Ngoài \\(\\textbf{ggplot2}\\), bạn đọc cũng có thể sử dụng các đồ thị cơ bản của R, hoặc sử dụng các thư viện khác như \\(\\textbf{lattice}\\) để vẽ đồ thị. Tuy nhiên, không giống như hầu hết các công cụ khác, \\(\\textbf{ggplot2}\\) trực quan hóa dữ liệu dựa trên Ngữ pháp của đồ thị (Wilkinson 2005). Hai chữ \\(\\textbf{gg}\\) bắt đầu có nghĩa là “Grammar Graphic” hay Ngữ pháp của đồ thị. Ngữ pháp cho phép bạn đọc vẽ đồ thị bằng cách kết hợp các cấu phần độc lập lại với nhau. Đây chính là điểm mạnh của \\(\\textbf{ggplot2}\\). Thay vì bị giới hạn ở các bộ đồ thị đã được xác định trước, bạn đọc có thể tạo đồ thị mới phù hợp với mục tiêu của mình. Ý tưởng phải học ngữ pháp để vẽ đồ thị có thể làm cho bạn đọc cảm thấy nản chí, nhưng sự thật là ngữ pháp của \\(\\textbf{ggplot2}\\) thực sự dễ học. Chỉ có một số nguyên tắc cốt lõi đơn giản và có rất ít trường hợp đặc biệt. Khi đã thông thạo Ngữ pháp của đồ thị, ngoài tạo ra những đồ thị quen thuộc, bạn đọc còn có thể tạo ra những đồ thị mới hơn, đẹp hơn và mang tính cách riêng. Bạn đọc có thể gặp khó khăn một chút thời gian ban đầu nhưng chúng tôi tin rằng khi đã quen với thư viện \\(\\textbf{ggplot2}\\) thì sẽ rất ít bạn đọc muốn quay lại với các công cụ trực quan hóa dữ liệu khác.Chúng ta hãy thử xem một ví dụ để hình dung về cách thư viện \\(\\textbf{ggplot2}\\) trực quan hóa dữ liệu. Chúng ta sẽ bắt đầu với một dữ liệu có tên là \\(\\textbf{murders}\\) trong thư viện \\(\\textbf{dslabs}\\). Đây là dữ liệu FBI cung cấp về số vụ sát nhân bằng súng tại các bang của nước Mỹ vào năm 2010. Giả sử bạn muốn du lịch đến Mỹ nhưng bạn lo ngại về việc cho phép sử dụng súng ở quốc gia này và bạn muốn biết ở những bang nào có tỷ lệ số vụ sát nhân bằng súng cao. Chúng ta sẽ bắt đầu bằng việc tìm hiểu thông tin sơ bộ về dữ liệu này. Bạn đọc đã có kiến thức về data.frame, nên bạn đọc cũng đã quen với các câu lệnh giúp tìm hiểu về dữ liệu bằng như head(), str(), view():Dù dữ liệu chỉ có hơn 51 dòng và 5 cột nhưng thật khó để có thể có được cái nhìn tổng thể về dữ liệu nếu chỉ nhìn vào các bảng, các con số, các véc-tơ kiểu chuỗi ký tự như trên. Nếu thay vì nhìn vào các con số, ký tự, bạn đọc có thể trình bày dữ liệu \\(\\textbf{murders}\\) dưới dạng một đồ thị phân tán như Hình 7.1\nFigure 7.1: Đồ thị phân tán mô tả số vụ sát nhân bằng súng tại 51 bang của Mỹ vào năm 2010. Có mối liên hệ cùng chiều giữa dân số của các bang và số vụ sát nhân bằng súng tại mỗi bang.\nChúng tôi đã sử dụng một vài kỹ thuật biến đổi dữ liệu để vẽ đồ thị ở trên:biến \\(total\\) (tổng số vụ sát nhân) và biến \\(population\\) (dân số của mỗi bang) đều có đuôi dài, nghĩa là có nhiều điểm tập trung ở khu vực trung tâm, và một số ít điểm tập trung ở phía đuôi bên phải, đó thay vì sử dụng chính xác giá trị của các biến trên đồ thị, các điểm của đồ thị rải điểm sẽ phân bố không đồng đều. Giá trị hiển thị trên đồ thị đã được điều chính lại theo hàm log() cơ số 10. Bạn đọc có thể thấy rằng trên trục y khoảng cách từ 10 đến 100 sẽ bằng khoảng cách từ 100 đến 1000.biến \\(total\\) (tổng số vụ sát nhân) và biến \\(population\\) (dân số của mỗi bang) đều có đuôi dài, nghĩa là có nhiều điểm tập trung ở khu vực trung tâm, và một số ít điểm tập trung ở phía đuôi bên phải, đó thay vì sử dụng chính xác giá trị của các biến trên đồ thị, các điểm của đồ thị rải điểm sẽ phân bố không đồng đều. Giá trị hiển thị trên đồ thị đã được điều chính lại theo hàm log() cơ số 10. Bạn đọc có thể thấy rằng trên trục y khoảng cách từ 10 đến 100 sẽ bằng khoảng cách từ 100 đến 1000.Chúng tôi thêm vào một đường thẳng tuyến tính (đường kẻ màu xám đi qua trung tâm) để mô tả mối quan hệ chung giữa hai biến \\(total\\) và \\(population\\). Đây có thể hiểu là đường thẳng mô tả mối quan hệ cùng chiều giữa biến \\(population\\) và biến \\(total\\). Điều này có nghĩa là ở các bang có dân số càng đông thì tổng số vụ sát nhân bằng súng càng cao.Chúng tôi thêm vào một đường thẳng tuyến tính (đường kẻ màu xám đi qua trung tâm) để mô tả mối quan hệ chung giữa hai biến \\(total\\) và \\(population\\). Đây có thể hiểu là đường thẳng mô tả mối quan hệ cùng chiều giữa biến \\(population\\) và biến \\(total\\). Điều này có nghĩa là ở các bang có dân số càng đông thì tổng số vụ sát nhân bằng súng càng cao.Dựa trên đồ thị rải điểm ở trên, bạn đọc có thể đưa ra được các nhận xét như sauBang nào có dân số càng cao thì số vụ sát nhân bằng súng càng nhiều.Bang nào có dân số càng cao thì số vụ sát nhân bằng súng càng nhiều.Hầu hết các bang nằm phía trên đường trung bình là các bang ở miền Nam.Hầu hết các bang nằm phía trên đường trung bình là các bang ở miền Nam.Các vùng còn lại không có sự phân biệt rõ ràng về tỷ lệ số vụ sát nhân bằng súng.Các vùng còn lại không có sự phân biệt rõ ràng về tỷ lệ số vụ sát nhân bằng súng.Bang “District Columbia” là bang nằm cao hơn hẳn với đường trung bình, và cũng là bang có tỷ lệ số vụ sát nhân bằng súng cao nhất.Bang “District Columbia” là bang nằm cao hơn hẳn với đường trung bình, và cũng là bang có tỷ lệ số vụ sát nhân bằng súng cao nhất.Bang California có tổng số vụ sát nhân bằng súng lớn nhất, nhưng tỷ lệ số vụ sát nhân bằng súng trên đầu người chỉ bằng mức trung bình chung.Bang California có tổng số vụ sát nhân bằng súng lớn nhất, nhưng tỷ lệ số vụ sát nhân bằng súng trên đầu người chỉ bằng mức trung bình chung.Không dễ dàng để đưa ra được các nhận xét như trên nếu chỉ dựa trên quan sát con số và dữ liệu. Thay vào đó chúng ta có thể đưa ra nhiều phân tích có ý nghĩa về dữ liệu khi sử dụng đồ thị như trên.Wilkinson (2005) giới thiệu khái niệm Ngữ pháp đồ thị để mô tả các thành phần cơ bản làm nền tảng cho tất cả các đồ thị sử dụng và cách các thành phần tương tác trong mô tả dữ liệu. Ngữ pháp đồ thị là mô tả chính xác nhất cho câu hỏi đồ thị trực quan hóa dữ liệu là gì? Thư viện \\(\\textbf{ggplot2}\\) được Wickham giới thiệu vào 2009 và xây dựng dựa trên Ngữ pháp đồ thị mà Wilkinson đã đề cập bằng cách tập trung vào việc xây dựng các đồ thị trực quan dựa trên nhiều lớp (layer). Nhìn chung, ngữ pháp đồ thị cho chúng ta biết quy tắc cho tương ứng các biến của dữ liệu đến các thuộc tính thẩm mỹ (các aesthetic attributions) của đối tượng hình ảnh xuất hiện (các geometries). Đồ thị được vẽ bằng thư viện \\(\\textbf{ggplot2}\\) cũng có thể bao gồm các mô hình thống kê của dữ liệu và hệ tọa độ mà đồ thị sử dụng. Bạn đọc cũng có thể chia dữ liệu thành các tập hợp con dựa trên các biến rời rạc và mô tả dữ liệu thông qua một nhóm các đồ thị con thông qua kỹ thuật facetting. Sự kết hợp của các thành phần độc lập kể trên tạo nên một đồ thị mô tả dữ liệu.Bạn đọc không cần phải lo lắng nếu khái niệm Ngữ pháp đồ thị chúng tôi vừa giải thích ở trên không có ý nghĩa ngay lập tức. Trong phần sau của cuốn sách, chúng tôi sẽ nói về ngữ pháp đồ thị một cách chi tiết hơn. Bạn sẽ có nhiều cơ hội hơn để tìm hiểu về Ngữ pháp và các sử dụng ngữ pháp để các cấu phần độc lập của một đồ thị hoạt động cùng nhau. Trong phần giới thiệu này, chúng tôi muốn bạn đọc hãy ghi nhớ \\(\\textbf{bảy}\\) thành phần độc lập tạo nên một đồ thị cơ bản trong thư viện \\(\\textbf{ggplot2}\\):Dữ liệu (\\(Data\\)) là dữ liệu hay tập hợp các dữ liệu mà bạn đọc muốn trực quan hóa. Thông thường thì chỉ có một dữ liệu chính mà bạn đọc muốn minh họa cho người tiếp nhận dữ liệu, trong khi các dữ liệu khác được sử dụng với mục đích để mô tả và minh họa cho dữ liệu chính. Một ví dụ điển hình của dữ liệu phụ là dữ liệu kiểu bản đồ. Chẳng hạn như khi bạn đọc muốn mô tả về dữ liệu \\(\\textbf{murders}\\) ở trên, bạn đọc có thể sử dụng dữ liệu về bản đồ nước Mỹ để có mô tả tốt hơn.Dữ liệu (\\(Data\\)) là dữ liệu hay tập hợp các dữ liệu mà bạn đọc muốn trực quan hóa. Thông thường thì chỉ có một dữ liệu chính mà bạn đọc muốn minh họa cho người tiếp nhận dữ liệu, trong khi các dữ liệu khác được sử dụng với mục đích để mô tả và minh họa cho dữ liệu chính. Một ví dụ điển hình của dữ liệu phụ là dữ liệu kiểu bản đồ. Chẳng hạn như khi bạn đọc muốn mô tả về dữ liệu \\(\\textbf{murders}\\) ở trên, bạn đọc có thể sử dụng dữ liệu về bản đồ nước Mỹ để có mô tả tốt hơn.Hình dạng đồ họa (được gọi là các \\(geometries\\) hay viết tắt là các \\(geoms\\)) là những hình dạng đồ họa mà chúng ta muốn nhìn thấy trên đồ thị. Các hình dạng này có thể là các điểm, các thanh, các đường.Hình dạng đồ họa (được gọi là các \\(geometries\\) hay viết tắt là các \\(geoms\\)) là những hình dạng đồ họa mà chúng ta muốn nhìn thấy trên đồ thị. Các hình dạng này có thể là các điểm, các thanh, các đường.Các ánh xạ thẩm mỹ (được gọi là \\(Aesthetic\\) \\(mapping\\)) là các quy tắc cho tương ứng từ các biến (cột của dữ liệu) đến các thuộc tính thẩm mỹ (aesthetic attribution) của các hình dạng đồ họa. Các thuộc tính thẩm mỹ có thể là hình dạng, màu sắc, độ đậm nhạt, …Các ánh xạ thẩm mỹ (được gọi là \\(Aesthetic\\) \\(mapping\\)) là các quy tắc cho tương ứng từ các biến (cột của dữ liệu) đến các thuộc tính thẩm mỹ (aesthetic attribution) của các hình dạng đồ họa. Các thuộc tính thẩm mỹ có thể là hình dạng, màu sắc, độ đậm nhạt, …Các mô hình hay biến đổi thống kê (các \\(statistics\\) hay viết tắt là \\(stats\\)) là các quy tắc tóm tắt dữ liệu, các mô hình xây dựng trên dữ liệu được mô tả dưới dạng một hình dạng đồ họa nhằm tăng tính dễ hiểu cho đồ thị hoặc làm nổi bật một xu thế nào đó. Ví dụ như trong đồ thị phân tán mô tả dữ liệu \\(\\textbf{murders}\\), chúng tôi đã sử dụng một mô hình tuyến tính mô tả mối quan hệ giữa biến \\(total\\) và biến \\(population\\) với mục đích phân loại ra các bang có tỷ lệ số vụ sát nhân bằng súng thấp hơn trung bình và các bang có tỷ lệ số vụ sát nhân bằng súng cao hơn với mức trung bình.Các mô hình hay biến đổi thống kê (các \\(statistics\\) hay viết tắt là \\(stats\\)) là các quy tắc tóm tắt dữ liệu, các mô hình xây dựng trên dữ liệu được mô tả dưới dạng một hình dạng đồ họa nhằm tăng tính dễ hiểu cho đồ thị hoặc làm nổi bật một xu thế nào đó. Ví dụ như trong đồ thị phân tán mô tả dữ liệu \\(\\textbf{murders}\\), chúng tôi đã sử dụng một mô hình tuyến tính mô tả mối quan hệ giữa biến \\(total\\) và biến \\(population\\) với mục đích phân loại ra các bang có tỷ lệ số vụ sát nhân bằng súng thấp hơn trung bình và các bang có tỷ lệ số vụ sát nhân bằng súng cao hơn với mức trung bình.Hệ tọa độ (\\(Cordinate\\)) mô tả cách dữ liệu được trực quan hóa trên mặt phẳng của đồ họa. Đa số các trường hợp chúng ta sẽ sử dụng hệ tọa độ Descartes, nhưng cũng có một số hệ tọa độ khác có thể sử dụng bao gồm tọa độ cực và bản đồ.Hệ tọa độ (\\(Cordinate\\)) mô tả cách dữ liệu được trực quan hóa trên mặt phẳng của đồ họa. Đa số các trường hợp chúng ta sẽ sử dụng hệ tọa độ Descartes, nhưng cũng có một số hệ tọa độ khác có thể sử dụng bao gồm tọa độ cực và bản đồ.Một thành phần mô tả cách dữ liệu được hiển thị là chia nhỏ dữ liệu để mô tả bằng một nhóm các đồ thị thay vì một đồ thị duy nhất được gọi là \\(facetting\\). Thành phần này thường được sử dụng để mô tả dữ liệu có kích thước lớn và hoặc chúng ta muốn sánh trực quan dữ liệu ở các nhóm khác nhau.Một thành phần mô tả cách dữ liệu được hiển thị là chia nhỏ dữ liệu để mô tả bằng một nhóm các đồ thị thay vì một đồ thị duy nhất được gọi là \\(facetting\\). Thành phần này thường được sử dụng để mô tả dữ liệu có kích thước lớn và hoặc chúng ta muốn sánh trực quan dữ liệu ở các nhóm khác nhau.Thành phần cuối cùng của đồ thị là ngữ cảnh của đồ thị, còn được gọi là các \\(themes\\). Theme quy định khung hoặc nền mà đồ thị được hiển thị chẳng hạn như kích thước phông chữ hoặc màu nền. Mặc dù các giá trị mặc định trong thư viện \\(\\textbf{ggplot2}\\) đã được lựa chọn hợp lý nhưng bạn đọc cũng có thể cần tham khảo các tài liệu tham khảo khác để tạo ra một ngữ cảnh phù hợp hơn cho đồ thị của mình.Thành phần cuối cùng của đồ thị là ngữ cảnh của đồ thị, còn được gọi là các \\(themes\\). Theme quy định khung hoặc nền mà đồ thị được hiển thị chẳng hạn như kích thước phông chữ hoặc màu nền. Mặc dù các giá trị mặc định trong thư viện \\(\\textbf{ggplot2}\\) đã được lựa chọn hợp lý nhưng bạn đọc cũng có thể cần tham khảo các tài liệu tham khảo khác để tạo ra một ngữ cảnh phù hợp hơn cho đồ thị của mình.Mỗi khi vẽ một đồ thị sử dụng \\(\\textbf{ggplot2}\\), bạn đọc cần tự định nghĩa ít nhất ba thành phần: 1. Dữ liệu; 2. Các hình dạng đồ họa; và 3. Các ánh xạ thẩm mỹ. Các thành phần 5. Hệ tọa độ; và 7. Ngữ cảnh; sẽ được tự động gán cho các giá trị mặc định nếu bạn đọc không quy định trong câu lệnh. Và các thành phần 4. Mô hình; và 6. Facetting; chỉ xuất hiện khi bạn đọc gọi lên trong câu lệnh của mình.Trước khi đi vào giới thiệu chi tiết các cách tạo nên một đồ thị trực quan hóa, bạn đọc cũng cần biết được các hạn chế khi trực quan hóa dữ liệu bằng \\(\\textbf{ggplot2}\\):\\(\\textbf{ggplot2}\\) là một thư viện của R nên bạn đọc cần có kỹ năng viết câu lệnh R tương đối thành thạo.\\(\\textbf{ggplot2}\\) là một thư viện của R nên bạn đọc cần có kỹ năng viết câu lệnh R tương đối thành thạo.Thư viện \\(\\textbf{ggplot2}\\) không gợi ý bạn đọc nên sử dụng đồ thị nào khi gặp một dữ liệu cụ thể. Điều đó cũng có nghĩa là bạn đọc cần có một chút kinh nghiệm về trực quan hóa dữ liệu trước khi sử dụng thư viện này.Thư viện \\(\\textbf{ggplot2}\\) không gợi ý bạn đọc nên sử dụng đồ thị nào khi gặp một dữ liệu cụ thể. Điều đó cũng có nghĩa là bạn đọc cần có một chút kinh nghiệm về trực quan hóa dữ liệu trước khi sử dụng thư viện này.Thư viện \\(ggplot2\\) không được phát triển để vẽ các đồ thị động hay đồ thị tương tác mà chỉ tập trung vào vẽ các đồ thị tĩnh. Muốn vẽ các đồ thị tương tác hay đồ thị động, bạn đọc phải sử dụng các thư viện đi kèm như \\(\\textbf{gganimate}\\) hay \\(\\textbf{ggplotly}\\).Thư viện \\(ggplot2\\) không được phát triển để vẽ các đồ thị động hay đồ thị tương tác mà chỉ tập trung vào vẽ các đồ thị tĩnh. Muốn vẽ các đồ thị tương tác hay đồ thị động, bạn đọc phải sử dụng các thư viện đi kèm như \\(\\textbf{gganimate}\\) hay \\(\\textbf{ggplotly}\\).Để kết thúc phần giới thiệu, chúng tôi sẽ sử dụng thư viện \\(\\textbf{ggplot2}\\) kết hợp với thư viện vẽ hình ảnh động \\(\\textbf{gganimate}\\) để kể một câu chuyện về sự phát triển về sự tiến bộ y tế của các quốc gia trên thế giới từ năm 1960 đến năm 2010 thông qua hai khía cạnh là tuổi thọ trung bình và tỷ lệ tử . Dữ liệu chính được sử dụng là dữ liệu \\(\\textbf{gapminder}\\) trong thư viện \\(\\textbf{dslabs}\\) mà bạn đọc đã làm quen trong phần phân tích dữ liệu. Sự thay đổi của tuối thọ trung bình và thu nhập bình quân đầu người của các quốc gia được mô tả lại một cách sinh động qua Hình 7.2\nFigure 7.2: Sự thay đổi trong tỷ lệ tử vong trẻ sơ sinh, tính trên 1000 trẻ, và tuổi thọ trung bình của cá quốc gia trên thế giới từ năm 1960 đến năm 2010\nDựa trên đồ thị ở trên, bạn đọc hãy quan sát và trả lời các câu hỏi dưới đây:Xu hướng di chuyển chung của các điểm dữ liệu là như thế nào ? Điều này cho biết tỷ lệ tử vong của trẻ sơ sinh và tuổi thọ bình quân của các quốc gia trên thế giới từ năm 1960 đến năm 2010 thay đổi như thế nào ? Điều này cho thấy việc phát triển về y tế nhìn chung là như thế nào ?Xu hướng di chuyển chung của các điểm dữ liệu là như thế nào ? Điều này cho biết tỷ lệ tử vong của trẻ sơ sinh và tuổi thọ bình quân của các quốc gia trên thế giới từ năm 1960 đến năm 2010 thay đổi như thế nào ? Điều này cho thấy việc phát triển về y tế nhìn chung là như thế nào ?Trong xu thế phát triển chung đó, sự khác biệt của các Châu lục là như thế nào ?Trong xu thế phát triển chung đó, sự khác biệt của các Châu lục là như thế nào ?Kích thước của các điểm trong đồ thị phân tán cho biết dân số quốc gia đó, bạn có nhận thấy sự thay đổi của kích thước của các điểm theo thời gian là như thế nào không ? Bạn có nhận ra các quốc gia đông dân như Trung Quốc, Ấn Độ, Mỹ, Nhật Bản từ đồ thị trên hay không ?Kích thước của các điểm trong đồ thị phân tán cho biết dân số quốc gia đó, bạn có nhận thấy sự thay đổi của kích thước của các điểm theo thời gian là như thế nào không ? Bạn có nhận ra các quốc gia đông dân như Trung Quốc, Ấn Độ, Mỹ, Nhật Bản từ đồ thị trên hay không ?Một vài quốc gia Châu Phi có quỹ đạo di chuyển bất thường khi điểm dữ liệu bị “rơi” xuống theo trục \\(y\\) trong một vài năm, theo bạn đó là nguyên nhân gì ?Một vài quốc gia Châu Phi có quỹ đạo di chuyển bất thường khi điểm dữ liệu bị “rơi” xuống theo trục \\(y\\) trong một vài năm, theo bạn đó là nguyên nhân gì ?","code":"\nlibrary(dslabs)\nhead(murders)##        state abb region population total\n## 1    Alabama  AL  South    4779736   135\n## 2     Alaska  AK   West     710231    19\n## 3    Arizona  AZ   West    6392017   232\n## 4   Arkansas  AR  South    2915918    93\n## 5 California  CA   West   37253956  1257\n## 6   Colorado  CO   West    5029196    65\n# str(murders)\n# View(murders)"},{"path":"trực-quan-hóa-dữ-liệu.html","id":"tạo-một-đồ-thị-cơ-bản","chapter":"Chương 7 Trực quan hóa dữ liệu","heading":"7.2 Tạo một đồ thị cơ bản","text":"Trước khi giới thiệu chi tiết về các thành phần độc lập của đồ thị và cách sử dụng ngữ pháp của đồ thị, chúng tôi nghĩ rằng sẽ tốt hơn nếu bạn đọc bắt đầu vẽ các đồ thị đơn giản bằng cách sao chép và dán các câu lệnh vẽ đồ thị. Sau khi thực thi một vài lần, bạn đọc sẽ có cảm nhận được phần nào cách mà một đồ thị của thư viện \\(\\textbf{ggplot2}\\) được xây dựng. Dữ liệu chúng tôi sử dụng để trực quan hóa trong suốt chương sách này là dữ liệu \\(\\textbf{gapminder}\\), dữ liệu về sức khỏe và thu nhập của tất cả các quốc gia trên thế giới bắt đầu từ năm 1960 đến năm 2016. Trong phần giới thiệu chúng ta đã làm quen với dữ liệu này. Hình 7.3 mô tả tỷ lệ biến không dữ liệu không quan sát được của các biến qua các năm\nFigure 7.3: Tỷ lệ giá trị không quan sát được của các biến trong dữ liệu gapminder theo năm bắt đầu từ 1960 đến 2016\nBạn đọc có thể thấy rằng dữ liệu \\(gapminder\\) có nhiều giá trị không quan sát được trong năm 2016. Hai cột có tỷ lệ không quan sát được qua các năm lớn là \\(infant\\_mortality\\) và \\(gdp\\). Riêng biến \\(gdp\\) là gần như không quan sát được từ năm 2012 đến 2016. chỉ sử dụng dữ liệu với mục đích trực quan hóa nên chúng tôi sẽ xử lý dữ liệu một cách đơn giản là xóa các quan sát của các năm 2012 đến 2016. Giá trị không quan sát được từ năm 1960 đến 2011 sẽ được thay thế bằng cách nội suy tuyến tính theo chuỗi thời gian. Thư viện để nội suy tuyến tính các giá trị không quan sát được theo chuỗi thời gian là thư viện \\(\\textbf{imputeTS}\\).Hàm số sử dụng vẽ đồ thị thư viện \\(\\textbf{ggplot2}\\) là hàm ggplot(). Ba thành phần bắt buộc phải có của một đồ thị là 1. Dữ liệu; 2. Ít nhất một hình dạng đồ họa; và 3. Ánh xạ thẩm mỹ. Đồ thị dưới đây mô tả hai biến gdp bình quân đầu người và tuổi thọ trung bình của các quốc gia trên thế giới vào năm 2011. Bạn đọc có thể sao chép các câu lệnh ở dưới vào cửa sổ Script và thực thi giống như các câu lệnh thông thường\nFigure 7.4: Thu nhập bình quân đầu người và tuổi thọ trung bình của các quốc gia trên thế giới năm 2011\nTrong câu lệnh ggplot() ở trên, dữ liệu được đưa vào là \\(\\textbf{dat}\\) - dữ liệu được biến đổi từ dữ liệu \\(\\textbf{gapminder}\\) sau khi thêm cột thu nhâp bình quân đầu người và lọc theo năm 2011. Hình dạng đồ họa là các điểm trên trục tọa độ Descartes. Hình dạng đồ họa này được gọi bằng hàm geom_point(). Ánh xạ thẩm mỹ được khai báo thông qua hàm aes() nằm trong hàm ggplot(). Trong ánh xạ thẩm mỹ ở trên, chúng ta đã cho tương ứng (ánh xạ) biến \\(life\\_expectancy\\) với giá trị trên trục \\(x\\) của các điểm và biến \\(gdp\\_per\\_capita\\) tương ứng (ánh xạ) với giá trị trên trục \\(y\\) của các điểm.Mặc dù đồ thị trên Hình 7.4 còn đơn giản, nhưng chúng ta đã có thể nhận thấy được một vài thông tin về thu nhập binCó mối liên hệ đồng biến giữa tuổi thọ trung bình và thu nhập bình quân đầu người. Quốc gia nào có thu nhập bình quân đầu người cao thì tuổi thọ trung bình cũng sẽ cao. Điều này khá hợp lý bởi các quốc gia có thu nhập trung bình cao thường là các nước phát triển có hệ thống chăm sóc sức khỏe tốt, đó tuổi thọ trung bình cũng sẽ cao.Có mối liên hệ đồng biến giữa tuổi thọ trung bình và thu nhập bình quân đầu người. Quốc gia nào có thu nhập bình quân đầu người cao thì tuổi thọ trung bình cũng sẽ cao. Điều này khá hợp lý bởi các quốc gia có thu nhập trung bình cao thường là các nước phát triển có hệ thống chăm sóc sức khỏe tốt, đó tuổi thọ trung bình cũng sẽ cao.Mối liên hệ đồng biến nhưng không tuyến tính, thu nhập bình quân đầu người tăng nhanh hơn rất nhiều ro với tuổi thọ trung bình.Mối liên hệ đồng biến nhưng không tuyến tính, thu nhập bình quân đầu người tăng nhanh hơn rất nhiều ro với tuổi thọ trung bình.Có một vài điểm có khả năng là ngoại lai trong mối liên hệ tuyến tính này. Đây là các quốc gia có mức thu nhập bình quân khá cao (từ 10 nghìn USD - 20 nghìn USD/1 người) nhưng lại có tuổi thọ trung bình không cao. Tuy nhiên chỉ với các thông tin như trên chúng ta không thể đưa ra giải thích cho các giá trị này.Có một vài điểm có khả năng là ngoại lai trong mối liên hệ tuyến tính này. Đây là các quốc gia có mức thu nhập bình quân khá cao (từ 10 nghìn USD - 20 nghìn USD/1 người) nhưng lại có tuổi thọ trung bình không cao. Tuy nhiên chỉ với các thông tin như trên chúng ta không thể đưa ra giải thích cho các giá trị này.Hình dạng đồ họa là những hình dạng cụ thể mà bạn đọc nhìn thấy trên đồ thị, chẳng hạn như các điểm, các đường, thanh, hay các khối hình khác. Khi gọi các hình dạng đồ họa, thư viện \\(\\textbf{ggplot2}\\) luôn luôn sử dụng các hàm số bắt đầu bởi geom - viết tắt của geometries. Bạn đọc có thể thử các câu lệnh để vẽ các khối hình khác như dưới đây:Còn nhiều hàm geom_*() khác có thể được sử dụng để trực quan hóa dữ liệu từ thư viện \\(\\textbf{ggplot2}\\). Bạn đọc có thể tham khảo danh sách các geom thường sử dụng trong link dưới đây.https://www.maths.usyd.edu.au/u/UG/SM/STAT3022/r/current/Misc/data-visualization-2.1.pdfBạn đọc có thể thấy rằng trong danh sách các geom có thể sử dụng bao gồm gợi ý cho người sử dụng nên dùng hàm geom_*() nào trong từng trường hợp. Chẳng hạn như geom_point() được gợi ý khi mô tả hai biến liên tục. Hoặc geom_boxplot() được gợi ý khi mô tả một biến liên tục và một biến rời rạc. Ngoài ra, mỗi hàm geom_*() sẽ có một danh sách các thuộc tính thẩm mỹ đi kèm. Ví dụ, khi sử dụng geom_point() sẽ có các thuộc tính thẩm mỹ bao gồm x, y, alpha, color, fill, shape, size, và stroke. Chúng ta có thể tham khảo hướng dẫn sử dụng của hàm geom_point() để biết các thuộc tính thẩm mỹ này có ý nghĩa như thế nào. Trong các thuộc tính thẩm mỹ được sử dụng với geom_point(), các thuộc tính thẩm mỹ color, fill, shape, và size là các thuộc tính thẩm mỹ xuất hiện ở nhiều hàm geom_*() khác. Đây là các thuộc tính thẩm mỹ thường xuyên được sử dụng để tăng khả năng mô tả dữ liệu của các đồ thị \\(\\textbf{ggplot2}\\).Chúng ta tiếp tục với ví dụ về mô tả trực quan mối liên hệ giữa thu nhập bình quân đầu người và tuổi thọ trung bình của các quốc gia trên thế giới năm 2011. Để đồ thị giải thích tốt hơn, chúng ta cần thêm thông tin vào đồ thị trong Hình 7.4. Một phương pháp đơn giản để thêm biến khác vào một đồ thị là ánh xạ biến đó đến một trong các thuộc tính thẩm mỹ của đồ thị được vẽ bởi hàm geom_point(). Biến được thêm vào đồ thị trong Hình 7.5 là biến \\(continent\\). Chúng ta sẽ ánh xạ biến này đến thuộc tính thẩm mỹ color như sau\nFigure 7.5: Thu nhập bình quân đầu người và tuổi thọ trung bình của các quốc gia trên thế giới năm 2011. Biến continent được ánh xạ đến thuộc tính thẩm mỹ màu sắc của các điểm\nBằng cách thêm biến \\(continent\\) vào đồ thị sử dụng thuộc tính thẩm mỹ màu sắc, chúng ta đã có thể đưa ra thêm các phân tích về mối liên hệ giữa tuổi thọ trung bình và thu nhập bình quân đầu người của các quốc gia trên thế giới vào năm 2011:có sự phân bố không đồng đều về thu nhập bình quân và tuổi thọ trung bình của các quốc gia trên thế giới. Đa số các quốc gia Châu Phi có thu nhập bình quân đầu người thấp và tuổi thọ trung bình thấp trong khi các quốc gia Châu Âu có thu nhập bình quân đầu người cao và tuổi thọ trung bình cao. Có sự phân hóa rõ ràng ở Châu Đại Dương và Châu Mỹ, một vài quốc gia nằm trong nhóm các nước có thu nhập cao, tuổi thọ trung bình cao trong khi đa số các quốc gia còn lại nằm trong nhóm thu nhập thấp và tuổi thọ trung bình thấp. Sự phân hóa ở các nước Châu Á không quá rõ ràng.có sự phân bố không đồng đều về thu nhập bình quân và tuổi thọ trung bình của các quốc gia trên thế giới. Đa số các quốc gia Châu Phi có thu nhập bình quân đầu người thấp và tuổi thọ trung bình thấp trong khi các quốc gia Châu Âu có thu nhập bình quân đầu người cao và tuổi thọ trung bình cao. Có sự phân hóa rõ ràng ở Châu Đại Dương và Châu Mỹ, một vài quốc gia nằm trong nhóm các nước có thu nhập cao, tuổi thọ trung bình cao trong khi đa số các quốc gia còn lại nằm trong nhóm thu nhập thấp và tuổi thọ trung bình thấp. Sự phân hóa ở các nước Châu Á không quá rõ ràng.các nước có mối liên hệ giữa thu nhập bình quân và tuổi thọ trung bình ít giống như các nước khác là các quốc gia ở Châu Phi và Châu Mỹ.các nước có mối liên hệ giữa thu nhập bình quân và tuổi thọ trung bình ít giống như các nước khác là các quốc gia ở Châu Phi và Châu Mỹ.Có một vài nguyên tắc khi sử dụng các thuộc tính thẩm mỹ là thuộc tính thẩm mỹ color thường được sử dụng với biến rời rạc trong khi thuộc tính thẩm mỹ size thường được sử dụng với biến liên tục. Thuộc tính thẩm mỹ shape chỉ có thể được sử dụng với biến rời rạc, R sẽ báo lỗi nếu bạn ánh xạ một biến liên tục vào thuộc tính thẩm mỹ shape. Ngoài ra có 21 giá trị khác nhau cho thuộc tính thẩm mỹ shape và R sẽ đưa ra cảnh báo nếu bạn đọc ánh xạ một biến rời rạc có nhiều hơn 21 giá trị vào cấu phần thẩm mỹ này.Đồ thị dưới đây thêm biến \\(population\\) vào đồ thị bằng cách sử dụng thuộc tính thẩm mỹ size. Bạn đọc hãy luôn nhớ rằng khai báo ánh xạ thẩm mỹ từ một biến dữ liệu đến một thuộc tính thẩm mỹ luôn luôn phải thực hiện bên trong hàm aes():\nFigure 7.6: Thu nhập bình quân đầu người và tuổi thọ trung bình của các quốc gia trên thế giới năm 2011. Biến continent ánh xạ đến thuộc tính thẩm mỹ màu sắc của các điểm và biến population ánh xạ đến thuộc tính thẩm mỹ kích thước\nTham số alpha sử dụng trong hàm geom_point() trong trường hợp dữ liệu có nhiều điểm bị trùng lên nhau. alpha nhận giá trị từ 0 đến 1 cho biết độ trong suốt của các điểm tăng dần. Cho alpha nhận giá trị nhỏ hơn 1 cho phép chúng ta quan sát được tất cả các điểm trên đồ thị phân tán. alpha cũng có thể được sử dụng như một thuộc tính thẩm mỹ và chúng ta sẽ thảo luận chi tiết hơn về thuộc tính này trong phần sau của chương sách.Bạn đọc có thể thấy rằng khi thêm biến \\(population\\) bằng cách ánh xạ vào thuộc tính kích thước các điểm như Hình 7.6 giúp cho đồ thị có thêm thông tin: chúng ta có thể nhận ra vị trí của các quốc gia đông dân tiêu biểu như Trung Quốc và Ấn Độ vào năm 2011, có thể nhận thấy hai quốc gia này vẫn nằm trong nhóm các nước có thu nhập bình quân đầu người thấp; hoặc cũng có thể nhận ra Mỹ và Nhật Bản là các quốc gia nằm ở góc trên bên phải là các nước cũng có dân số tương đối lớn với các quốc gia khác. Tất nhiên, với thuộc tính thẩm mỹ màu sắc thì thuộc tính thẩm mỹ kích thước không hiệu quả bằng. Ngoài ra, bạn đọc cũng có thể nhận ra rằng khi cùng sử dụng nhiều thuộc tính thẩm mỹ trên một đồ thị, hiệu quả sẽ không được như mong muốn. Chính vì thế, để tạo các đồ thị rõ ràng hơn, đặc biệt với các dữ liệu có nhiều quan sát, bạn đọc có thể chia nhỏ dữ liệu thành các nhóm và mô tả dữ liệu trong mỗi nhóm bằng một đồ thị khác nhau. Kỹ thuật này được gọi là \\(facetting\\) và được mô tả trong Hình 7.7\nFigure 7.7: Chia dữ liệu thành năm nhóm tương ứng với năm lục địa và sử dụng năm đồ thị phân tán để mô tả phân bố của các điểm dữ liệu\nCó thể nhận thấy rằng sử dụng năm đồ thị phân tán có cùng khoảng giá trị của trục tọa độ \\(x\\) và \\(y\\) để mô tả mối quan hệ giữa thu nhập bình quân đầu người và tuổi thọ trung bình của các quốc gia thuộc năm châu lục là rõ ràng hơn rất nhiều với sử dụng một đồ thị duy nhất và phân biệt các lục địa khác nhau bằng màu sắc.Một thành phần tuy không bắt buộc nhưng bạn đọc có thể thêm vào đồ thị của \\(\\textbf{ggplot2}\\) để tăng tính thẩm mỹ và sự rành mạch là ngữ cảnh hay còn gọi là các \\(themes\\). Có một số \\(theme\\) có sẵn khi chúng ta cài đặt thư viện và cũng có các ngữ cảnh nằm trong các thư viện cài đặt bổ sung như thư viện \\(\\textbf{ggthemes}\\). Chúng ta sẽ thảo luận chi tiết về cách tùy chỉnh ngữ cảnh và tự tạo ngữ cảnh cho đồ thị ở phần sau của chương. Để thay đổi ngữ cảnh mặc định của các đồ thị ggplot2, chúng ta sử dụng các hàm theme_*(). Ví dụ, trong Hình 7.8 chúng tôi thay đổi ngữ cảnh mặc định của Hình 7.7 thành ngữ cảnh khác bằng cách sử dụng hàm theme_minimal():\nFigure 7.8: Thay đổi ngữ cảnh mặc định của ggplot sang ngữ cảnh khác giúp đồ thị rõ ràng hơn\nThành phần chưa được nhắc đến khi tạo đồ thị trực quan hóa dữ liệu là các \\(statistics\\) hay các \\(stats\\). đây là thành phần phức tạp và liên quan đến các kiến thức về xây dựng mô hình trên dữ liệu nên chúng tôi không đề cập đến trong phần này. Mục tiêu của chúng tôi trong phần giới thiệu là để bạn đọc làm quen với cách sử dụng các câu lệnh vẽ đồ thị trong \\(\\textbf{ggplot2}\\). Trong các phần tiếp theo, từng thành phần của đồ thị và các cấu phần thẩm mỹ quan trọng sẽ được thảo luận chi tiết cùng với ngữ pháp của đồ thị.","code":"\nlibrary(imputeTS) # Thư viện dùng để nội suy tuyến tính\nmydat<-gapminder%>%filter(year<=2011)\nlist_country<-unique(mydat$country)\nfor (ct in list_country){\n  ind<-(mydat$country == ct)\n  if (sum(!is.na(mydat$infant_mortality[ind]))>=2){\n    mydat$infant_mortality[ind]<-na.interpolation(mydat$infant_mortality[ind])\n  }\n  if (sum(!is.na(mydat$gdp[ind]))>=2){\n    mydat$gdp[ind] <- na.interpolation(mydat$gdp[ind])\n  }\n}\n# Biến đổi dữ liệu\ndat<-mydat%>%filter(year==2011)%>%\n  mutate(gdp_per_capita = gdp/population)\n\n# Trực quan hóa\nggplot(dat, aes(x = life_expectancy, y = gdp_per_capita)) +\n  geom_point()\n## geom_histogram() sử dụng các thanh \n## mô tả phân phối của một biến liên tục\nggplot(dat,aes(x = gdp_per_capita))+\n  geom_histogram()\n\n## geom_bar() sử dụng các thanh \n## mô tả phân phối của một biến rời rạc\nggplot(dat,aes(x = continent))+\n  geom_bar()\n\n## geom_boxplot() sử dụng các hình hộp \n## mô tả phân phối của biến liên tục\nggplot(dat,aes(x = continent, y = life_expectancy))+\n  geom_boxplot()\n\n## geom_line() sử dụng đường nối các điểm \n## mô tả các điểm theo thứ tự xuất hiện\ndat1<-filter(gapminder, year<=2011, country == \"United States\")%>%\n  select(year,gdp)\nggplot(dat1,aes(x = year, y = gdp))+\n  geom_line()\n# Vẽ đồ thị phân tán, ánh xạ biến continent đến màu sắc\nggplot(dat, aes(x=life_expectancy, y=gdp_per_capita,\n                color=continent))+\n  geom_point()\n# Vẽ đồ thị phân tán, ánh xạ biến continent đến màu sắc\n# ánh xạ population đến kích thước\nggplot(dat, aes(x = life_expectancy, y = gdp_per_capita, \n                color = continent, size = population)) + \n  geom_point(alpha = 0.5)\n# Dùng facet_wrap để chia dữ liệu ra thành các nhóm\nggplot(dat, aes(x = life_expectancy, y = gdp_per_capita,\n                size = population)) +\n  geom_point(alpha = 0.3)+\n  facet_wrap(~continent)\nggplot(dat, aes(x = life_expectancy, y = gdp_per_capita, size = population)) +\n  geom_point(shape = 21, alpha = 0.5, fill = \"blue\")+\n  facet_wrap(~continent, ncol = 2)+\n  # thêm title\n  labs( title = \"Thu nhập bình quân và tuổi thọ trung bình\")+\n  xlab(\"Tuổi thọ trung bình (năm)\")+\n  ylab(\"Gdp bình quân đầu người (USD)\")+\n  theme_minimal()# thêm ngữ cảnh"},{"path":"trực-quan-hóa-dữ-liệu.html","id":"cấu-trúc-nhiều-lớp-và-ngữ-pháp-của-đồ-thị","chapter":"Chương 7 Trực quan hóa dữ liệu","heading":"7.3 Cấu trúc nhiều lớp và ngữ pháp của đồ thị","text":"Cấu trúc theo lớp (layer) của đồ thị \\(\\textbf{ggplot2}\\) giúp cho người phân tích dữ liệu xây dựng đồ thị của mình theo một đối tượng có cấu trúc. Đồ thị được tạo thành từ \\(\\textbf{ggplot2}\\) từ đơn giản đến phức tạp đều được tạo thành từ (ít nhất) một đến nhiều lớp. Mỗi lớp trong đồ thị có mục tiêu hiển thị khác nhau:Mục tiêu hiện thị đầu tiên và cũng là mục tiêu chính, đó là để hiển thị dữ liệu. Luôn luôn có một hoặc một vài lớp chính với mục tiêu mô tả dữ liệu thô, mô tả cấu trúc tổng thể, và các giá trị ngoại lai của dữ liệu. Lớp này xuất hiện trên tất cả các đồ thị. Trong giai đoạn đầu của quá trình mô tả dữ liệu bằng trực quan hóa, lớp này thường xuất hiện duy nhất. Đơn giản như khi mô tả mỗi quan hệ giữa thu nhập bình quân đầu người và tuổi thọ trung bình của các quốc gia trên thế giới năm 2011, lớp được hiển thị bằng hàm geom_point() là lớp hiển thị dữ liệu chính.Mục tiêu hiện thị đầu tiên và cũng là mục tiêu chính, đó là để hiển thị dữ liệu. Luôn luôn có một hoặc một vài lớp chính với mục tiêu mô tả dữ liệu thô, mô tả cấu trúc tổng thể, và các giá trị ngoại lai của dữ liệu. Lớp này xuất hiện trên tất cả các đồ thị. Trong giai đoạn đầu của quá trình mô tả dữ liệu bằng trực quan hóa, lớp này thường xuất hiện duy nhất. Đơn giản như khi mô tả mỗi quan hệ giữa thu nhập bình quân đầu người và tuổi thọ trung bình của các quốc gia trên thế giới năm 2011, lớp được hiển thị bằng hàm geom_point() là lớp hiển thị dữ liệu chính.Các lớp có mục tiêu tóm tắt và mô tả ý nghĩa thống kê của dữ liệu. Bằng cách thêm vào đồ thị các mô hình hoặc bằng cách hiển thị các dự đoán dựa trên mô hình mà người tiếp nhận dữ liệu hoặc người phân tích dữ liệu sẽ nhận biết được những giá trị bên trong dữ liệu và những chi tiết mà khi xây dựng mô hình có thể bỏ sót.Các lớp có mục tiêu tóm tắt và mô tả ý nghĩa thống kê của dữ liệu. Bằng cách thêm vào đồ thị các mô hình hoặc bằng cách hiển thị các dự đoán dựa trên mô hình mà người tiếp nhận dữ liệu hoặc người phân tích dữ liệu sẽ nhận biết được những giá trị bên trong dữ liệu và những chi tiết mà khi xây dựng mô hình có thể bỏ sót.Các lớp có mục tiêu thêm vào ngữ cảnh của dữ liệu. Các lớp này hiển thị bối cảnh nền, thêm vào các chú thích giúp mang lại ý nghĩa cho dữ liệu thô hoặc các giá trị tham chiếu nhằm hỗ trợ việc sánh hoặc đánh giá. Đây thường là lớp cuối cùng được thêm vào trong đồ thị.Các lớp có mục tiêu thêm vào ngữ cảnh của dữ liệu. Các lớp này hiển thị bối cảnh nền, thêm vào các chú thích giúp mang lại ý nghĩa cho dữ liệu thô hoặc các giá trị tham chiếu nhằm hỗ trợ việc sánh hoặc đánh giá. Đây thường là lớp cuối cùng được thêm vào trong đồ thị.Lớp chính của đồ thị có thể bao gồm bảy thành phần độc lập giống như chúng ta đã giới thiệu ở phần đầu. Cấu trúc của các lớp còn lại của đồ thị \\(\\textbf{ggplot2}\\) có thể bao gồm các thành phần sau:Dữ liệu: nếu bạn không khai báo dữ liệu trong mỗi lớp, \\(\\textbf{ggplot2}\\) sẽ sử dụng dữ liệu ban đầu là dữ liệu mặc định.Dữ liệu: nếu bạn không khai báo dữ liệu trong mỗi lớp, \\(\\textbf{ggplot2}\\) sẽ sử dụng dữ liệu ban đầu là dữ liệu mặc định.Ánh xạ thẩm mỹ: được khai báo trong hàm aes() trong mỗi lớp, nếu không có khai báo riêng về ánh xạ thẩm mỹ, \\(\\textbf{ggplot2}\\) sẽ sử dụng ánh xạ thẩm mỹ được khai báo trong hàm ggplot().Ánh xạ thẩm mỹ: được khai báo trong hàm aes() trong mỗi lớp, nếu không có khai báo riêng về ánh xạ thẩm mỹ, \\(\\textbf{ggplot2}\\) sẽ sử dụng ánh xạ thẩm mỹ được khai báo trong hàm ggplot().Một hình dạng đồ họa được gọi bằng các hàm geom_*().Một hình dạng đồ họa được gọi bằng các hàm geom_*().Một biến đổi thống kê hoặc một tóm tắt của dữ liệu được gọi bằng hàm \\(stat_*()\\).Một biến đổi thống kê hoặc một tóm tắt của dữ liệu được gọi bằng hàm \\(stat_*()\\).Vị trí xuất hiện của lớp đó trong bố cục chung.Vị trí xuất hiện của lớp đó trong bố cục chung.Khi đồ thị chỉ có một lớp với mục tiêu hiển thị dữ liệu chính, bạn không cần phải hiểu về ngữ pháp của đồ thị. Bạn đọc chỉ cần khai báo chính xác ánh xạ thẩm mỹ trong hàm aes() để có được kết quả mong muốn. Tuy nhiên khi xây dựng đồ thị có nhiều lớp, bạn đọc cần phải nắm được ngữ pháp để kết hợp các lớp lại với nhau theo ý muốn của mình.","code":""},{"path":"trực-quan-hóa-dữ-liệu.html","id":"ánh-xạ-thẩm-mỹ-trong-đồ-thị-có-nhiều-lớp","chapter":"Chương 7 Trực quan hóa dữ liệu","heading":"7.3.1 Ánh xạ thẩm mỹ trong đồ thị có nhiều lớp","text":"Hãy quan sát đồ thị trong Hình 7.9, khi bạn muốn thêm vào một đường mô tả mối liên hệ giữa thu nhập bình quân đầu người và tuổi thọ trung bình của các quốc gia trên thế giới vào năm 2011. Hàm số dùng để thêm vào một đường mô tả mối liên hệ là geom_smooth()\nFigure 7.9: Đồ thị có hai lớp bao gồm một đồ thị phân tán và một đường mô tả mối liên hệ giữa các điểm. Hình bên trái: ánh xạ thẩm mỹ màu sắc được khai báo trong hàm ggplot(). Hình bên phải: ánh xạ thẩm mỹ màu sắc được khai báo trong hàm geom_point()\nBạn đọc có thể thấy sự khác nhau giữa hai đồ thị là các đường mô tả mối liên hệ giữa tuổi thọ trung bình và thu nhập bình quân đầu người được xây dựng theo từng lục địa cho hình bên trái Hình 7.9 và chỉ có duy nhất một đường được xây dựng cho tất cả các quốc gia trong hình bên phải của Hình 7.9. Sự khác biệt là trong hình bên phải chúng ta đã khai báo ánh xạ thẩm mỹ đến thuộc tính color bên trong hàm geom_point() trong khi trong hình bên trái chúng ta khai báo ánh xạ thẩm mỹ đến thuộc tính color bên trong hàm ggplot()Nếu như hàm geom_point() là lớp chính mô tả dữ liệu thô thì hàm geom_smooth() là lớp phụ được thêm vào nhằm tăng khả năng mô tả của dữ liệu. Các ánh xạ thẩm mỹ được khai báo trong hàm ggplot() cũng giống như các biến toàn cục trong một đồ thị, còn các ánh xạ thẩm mỹ được khai báo trong các hàm geom_*() giống như khai báo giá trị cho các biến cục bộ trong môi trường của hàm số đó. Các biến cục bộ nếu không được khai báo trong các hàm geom_*() sẽ được tìm trên môi trường toàn cục của hàm ggplot(). Trong trường hợp trong các hàm geom_*() và ggplot() đều không được khai báo giá trị các thuộc tính thẩm mỹ của lớp phụ sẽ nhận giá trị mặc định.Đường mô tả mối liên hệ giữa hai biến thu nhập bình quân và tuổi thọ trung bình được xây dựng bằng hàm geom_smooth() dựa trên phương pháp được gọi là hồi quy cục bộ (locally estimated scatterplot smoothing hay loess) mà chúng ta sẽ thảo luận trong phần mô hình cộng tính tổng quát. Khi thuộc tính thẩm mỹ color được sử dụng và ánh xạ đến một biến rời rạc, hàm geom_smooth() sẽ chia dữ liệu thành các nhóm, mỗi nhóm tương ứng với một giá trị của biến ánh xạ đến color, sau đó trước xây dựng mô hình hồi quy cục bộ mà thu nhập bình quân phụ thuộc vào tuổi thọ trung bình cho mỗi nhóm. Điều này giải thích tại sao trong hình bên trái có năm mô hình được xây dựng tương ứng với năm Châu lục, trong khi trong hình bên phải chỉ có một mô hình duy nhất được xây dựng cho tất cả các quốc gia trên thế giới.Vậy khi nào bạn nên khai báo ánh xạ thẩm mỹ trong hàm ggplot() và khi nào bạn nên khai báo ánh xạ thẩm mỹ bên trong hàm geom_*()? Câu trả lời là nếu đa số các lớp bạn đọc sử dụng chung một dữ liệu và chung các ánh xạ thẩm mỹ, bạn nên khai báo ánh xạ thẩm mỹ bên trong hàm ggplot(). Còn trong trường hợp các lớp sử dụng dữ liệu khác nhau, hoặc có ánh xạ thẩm mỹ khác nhau, bạn hãy khai báo ánh xạ thẩm mỹ bên trong mỗi hàm geom_*(). Trong trường hợp bạn dùng một hàm thuộc nhóm hàm geom_()* và không muốn sử dụng ánh xạ thẩm mỹ đã khai báo trong ggplot(), bạn có thể khai báo lại hoặc khai báo thuộc tính thẩm mỹ đó bằng NULL nếu không muốn sử dụng ánh xạ thẩm mỹ đó.Một lưu ý quan trọng cần được thảo luận trong ngữ pháp của đồ thị đó là sự khác nhau giữa sử dụng ánh xạ thẩm mỹ và thiết lập tham số. Trước hết, bạn đọc hãy quan sát ba đồ thị trong Hình 7.10\nFigure 7.10: Sự khác nhau giữa ánh xạ thẩm mỹ và thiết lập tham số. Hình bên trái: có hàm geom_smooth() sử dụng ánh xạ thẩm mỹ color mặc định. Hình ở giữa: hàm geom_smooth() được thiết lập tham số color (màu đen). Hình bên phải: hàm geom_smooth() sử dụng ánh xạ thẩm mỹ, thuộc tính color ánh xạ đến một biến kiểu chuỗi ký tự\nCó hai cách để chúng ta tác động đến các thuộc tính thẩm mỹ của đồ thị được vẽ bằng các hàm geom_*(), đó là dùng ánh xạ thẩm mỹ hoặc thiết lập tham số. Sự khác nhau giữa hai cách này là việc bạn khai báo giá trị của thuộc tính thẩm mỹ bên trong hay bên ngoài hàm aes(). Hình 7.10 cho thấy rằng:Hình góc trên bên trái có đường hồi quy liên tục mô tả mối liên hệ giữa hai biến có màu mặc định. Nguyên nhân là khi gọi hàm geom_smooth() chúng ta đã cho thuộc tính thẩm mỹ color nhận giá trị mặc định. Thật vậy, trong hàm aes() của geom_smooth() thuộc tính color được ánh xạ đến giá trị NULL.Hình góc trên bên trái có đường hồi quy liên tục mô tả mối liên hệ giữa hai biến có màu mặc định. Nguyên nhân là khi gọi hàm geom_smooth() chúng ta đã cho thuộc tính thẩm mỹ color nhận giá trị mặc định. Thật vậy, trong hàm aes() của geom_smooth() thuộc tính color được ánh xạ đến giá trị NULL.Hình góc trên bên phải đã thực hiện thiết lập cấu phần thẩm mỹ color bằng một giá trị cố định là “black” (màu đen). đó đường hồi quy được tạo từ geom_smooth() sẽ có màu đen đúng như yêu cầu từ thiết lập. Bạn đọc chỉ cần sử dụng giá trị màu sắc có ý nghĩa với R để thiết lập cho thuộc tính thẩm mỹ màu sắc. Nếu trong một hàm geom_*() vừa có ánh xạ thẩm mỹ được khai báo trong hàm aes() và vừa có thiết lập thuộc tính thẩm mỹ, đồ thị \\(\\textbf{ggplot2}\\) sẽ ưu tiên giá trị nằm ngoài aes(), nghĩa là ưu tiên thiết lập tham số.Hình góc trên bên phải đã thực hiện thiết lập cấu phần thẩm mỹ color bằng một giá trị cố định là “black” (màu đen). đó đường hồi quy được tạo từ geom_smooth() sẽ có màu đen đúng như yêu cầu từ thiết lập. Bạn đọc chỉ cần sử dụng giá trị màu sắc có ý nghĩa với R để thiết lập cho thuộc tính thẩm mỹ màu sắc. Nếu trong một hàm geom_*() vừa có ánh xạ thẩm mỹ được khai báo trong hàm aes() và vừa có thiết lập thuộc tính thẩm mỹ, đồ thị \\(\\textbf{ggplot2}\\) sẽ ưu tiên giá trị nằm ngoài aes(), nghĩa là ưu tiên thiết lập tham số.Hình góc dưới bên trái phức tạp hơn một chút. Khác với hình ở giữa, thuộc tính color được gán cho giá trị \\(\"black\"\\) bên trong hàm aes(). Bạn đọc có thể thấy rằng đường hồi quy được tạo từ hàm geom_smooth() không có màu đen như hình ở giữa. Nguyên nhân là khi khai báo thuộc tính thẩm mỹ trong hàm aes(), chúng ta đã ánh xạ thuộc tính color của hàm geom_smooth() đến một giá trị kiểu ký tự là \\(\"black\"\\) chứ không phải cho màu sắc của đường cong nhận giá trị màu đen tương ứng! Trong hàm geom_point() trước đó đã ánh xạ biến \\(continent\\) tới thuộc tính thẩm mỹ color, khi chúng ta tiếp tục ánh xạ một biến \\(\"black\"\\) tới thuộc tính color trong geom_smooth() thì \\(\\textbf{ggplot2}\\) sẽ hiểu rằng có thêm một giá trị mới cho thuộc tính color là \\(\"black\"\\) thêm vào cùng với các giá trị hiện có là tên của 5 châu lục. Điều này giải thích tại sao trong chú giải của hình bên tay phải có 6 loại thay vì 5 loại như hình ở giữa. Đường hồi quy tạo bởi geom_smooth() có màu xanh lá cây vì giá trị \\(\"black\"\\) có thứ hạng là 4 khi sắp xếp 6 giá trị ánh xạ tới thuộc tính color theo thứ tự tăng dần.Hình góc dưới bên trái phức tạp hơn một chút. Khác với hình ở giữa, thuộc tính color được gán cho giá trị \\(\"black\"\\) bên trong hàm aes(). Bạn đọc có thể thấy rằng đường hồi quy được tạo từ hàm geom_smooth() không có màu đen như hình ở giữa. Nguyên nhân là khi khai báo thuộc tính thẩm mỹ trong hàm aes(), chúng ta đã ánh xạ thuộc tính color của hàm geom_smooth() đến một giá trị kiểu ký tự là \\(\"black\"\\) chứ không phải cho màu sắc của đường cong nhận giá trị màu đen tương ứng! Trong hàm geom_point() trước đó đã ánh xạ biến \\(continent\\) tới thuộc tính thẩm mỹ color, khi chúng ta tiếp tục ánh xạ một biến \\(\"black\"\\) tới thuộc tính color trong geom_smooth() thì \\(\\textbf{ggplot2}\\) sẽ hiểu rằng có thêm một giá trị mới cho thuộc tính color là \\(\"black\"\\) thêm vào cùng với các giá trị hiện có là tên của 5 châu lục. Điều này giải thích tại sao trong chú giải của hình bên tay phải có 6 loại thay vì 5 loại như hình ở giữa. Đường hồi quy tạo bởi geom_smooth() có màu xanh lá cây vì giá trị \\(\"black\"\\) có thứ hạng là 4 khi sắp xếp 6 giá trị ánh xạ tới thuộc tính color theo thứ tự tăng dần.Hình góc dưới bên phải được vẽ trên dữ liệu sau khi được thêm vào cột mới có tất cả các giá trị là chuỗi ký tự \\(\"black\"\\). Đồ thị vẫn bao gồm hai lớp là geom_point() và geom_smooth(). Cả hai lớp đều sử dụng chung thuộc tính thẩm mỹ x và y. Thuộc tính thẩm mỹ color của geom_point() được ánh xạ đến biến \\(continent\\) trong khi thuộc tính thẩm color của geom_smooth() ánh xạ đến cột mới được tạo thành. Bạn đọc có thể thấy rằng đồ thị được tạo ra hoàn toàn giống như đồ thị được tạo ra trong bước trước khi chúng ta gán trực tiếp thuộc tính color của geom_smooth() với giá trị \\(\"black\"\\).Hình góc dưới bên phải được vẽ trên dữ liệu sau khi được thêm vào cột mới có tất cả các giá trị là chuỗi ký tự \\(\"black\"\\). Đồ thị vẫn bao gồm hai lớp là geom_point() và geom_smooth(). Cả hai lớp đều sử dụng chung thuộc tính thẩm mỹ x và y. Thuộc tính thẩm mỹ color của geom_point() được ánh xạ đến biến \\(continent\\) trong khi thuộc tính thẩm color của geom_smooth() ánh xạ đến cột mới được tạo thành. Bạn đọc có thể thấy rằng đồ thị được tạo ra hoàn toàn giống như đồ thị được tạo ra trong bước trước khi chúng ta gán trực tiếp thuộc tính color của geom_smooth() với giá trị \\(\"black\"\\).Khi cân nhắc sử dụng ánh xạ hay thiết lập giá trị cho các thuộc tính thẩm mỹ, bạn đọc cần cân nhắc về việc có muốn tác động lên thuộc tính thẩm mỹ nữa hay không. Nếu bạn muốn cố định giá trị cho thuộc tính thẩm mỹ, hãy sử dụng thiết lập giá trị. Còn nếu bạn muốn tác động lên thuộc tính thẩm mỹ sau đó, hãy sử dụng ánh xạ thẩm mỹ. Chúng ta sẽ thảo luận thêm về vấn đề này khi nói về các hàm scale_*().Bạn đọc hãy lưu ý về cách chú giải ghi nhận giá trị mới của một thuộc tính thẩm mỹ. Trong đồ thị góc dưới bên trái của Hình 7.10, khi chúng ta khai báo giá trị \\(\"black\"\\) cho thuộc tính color, đồ thị \\(\\textbf{ggplot2}\\) ghi nhận \\(\"black\"\\) như một giá trị mới tương đương với tên các Châu lục đã sử dụng trong khai báo trước đó. Cách ghi nhận tên biến mới trong chú giải sẽ rất hữu ích khi chúng ta muốn tạo một đồ thị nhiều lớp và đặt tên cho từng lớp trong phần chú giải của đồ thị. Ví dụ, khi chúng ta muốn sánh ba phương pháp xây dựng mô hình mô tả mối liên hệ giữa biến tuổi thọ trung bình và biến thu nhập bình quân đầu người bao gồm: phương pháp hồi quy tuyến tính thông thường (method = \"lm\"); phương pháp hồi quy loess (method = \"loess\"), và phương pháp hồi quy cộng tính tổng quát (method = \"gam\"), chúng ta có thể sử dụng ánh xạ thẩm mỹ color như sau:\nFigure 7.11: Sử dụng trực quan hóa để sánh ba phương pháp xây dựng mô hình mô tả mối liên hệ giữa tuổi thọ trung binh và thu nhập bình quân đầu người của các quốc gia trên thế giới năm 2011\nĐồ thị trong Hình 7.11 có bốn lớp. Khi chúng ta khai báo thuộc tính thẩm mỹ x và y trong hàm ggplot(), cả bốn lớp đều sử dụng chung các ánh xạ thẩm mỹ này. Hàm geom_point() không sử dụng thêm ánh xạ thẩm mỹ nào. Mỗi hàm geom_smooth() thêm một đường hồi quy vào trong đồ thị, và thêm một giá trị vào thuộc tính thẩm mỹ color. Đồ thị ggplot() sẽ sử dụng ba màu sắc mặc định để mô tả ba đường hồi quy tương ứng, với chú giải là tên của phương pháp xây dựng đường hổi quy.","code":"\n## Hình bên trái, khai báo color trong ggplot()\np1<-dat%>%ggplot(aes(x = life_expectancy, y = gdp_per_capita,\n                color = continent)) +\n  geom_point(alpha = 0.5)+\n  geom_smooth(se=FALSE)+\n  theme(legend.position = \"none\")+\n  theme_minimal()\n\n## Hình bên phải, khai báo color trong geom_point()\np2<-dat%>%ggplot(aes(x = life_expectancy, y = gdp_per_capita))+\n  geom_point(aes(color = continent), alpha = 0.5) +\n  geom_smooth(se=FALSE)+\n  theme(legend.position = \"none\")+\n  theme_minimal()\n\n## Vẽ p1 và p2 trên cùng một đồ thị\ngrid.arrange(p1,p2,nrow= 1 , ncol = 2)\n## Hình góc trên bên trái, geom_smooth có aes(color = NULL)\np1<-dat%>%ggplot(aes(x = life_expectancy, y = gdp_per_capita,\n                color = continent)) +\n  geom_point(alpha = 0.5)+\n  geom_smooth(aes(color=NULL), se = FALSE)+\n  theme(legend.position = \"right\")+\n  theme_minimal()\n\n## Hình góc trên bên phải, geom_smooth có thiết lập (color = back)\np2<-dat%>%ggplot(aes(x = life_expectancy, y = gdp_per_capita,\n                color = continent)) +\n  geom_point(alpha = 0.5)+\n  geom_smooth(color=\"black\", se = FALSE)+\n  theme(legend.position = \"right\")+\n  theme_minimal()\n\n## Hình góc dưới bên trái, geom_smooth có aes(color = \"black\")\np3<-dat%>%ggplot(aes(x = life_expectancy, y = gdp_per_capita,\n                color = continent)) +\n  geom_point(alpha = 0.5)+\n  geom_smooth(aes(color=\"black\") , se = FALSE)+\n  theme(legend.position = \"right\")+\n  theme_minimal()\n\n\n## Hình góc dưới bên phải, geom_smooth có aes(color = newcol)\np4<-dat%>%mutate(newcol = \"black\")%>%ggplot(aes(x = life_expectancy, y = gdp_per_capita)) +\n  geom_point(aes(color = continent))+\n  geom_smooth(aes(color = newcol) , se = FALSE)+\n  theme(legend.position = \"right\")+\n  theme_minimal()\n\n## Vẽ các đồ thị trên cùng một hình\ngrid.arrange(p1,p2,p3,p4, nrow= 2 , ncol = 2)\n# So sánh ba phương pháp xây dựng mô hình khác nhau của hàm geom_smooth\ndat%>%ggplot(aes(x = life_expectancy, y = gdp_per_capita)) +\n  #Layer 1: đồ thị rải điểm\n  geom_point(alpha = 0.4)+\n\n  # Layer 2: Đường hồi quy tuyến tính\n  geom_smooth(aes(color=\"Hồi quy tuyến tính\"), method = \"lm\" , se = FALSE)+\n\n  # Layer 3: Đường hồi quy loess\n  geom_smooth(aes(color=\"Hồi quy loess\"), method = \"loess\", span = 0.3 , se = FALSE)+\n\n  # Layer 4: Mô hình GAM (generalized additive model)\n  geom_smooth(aes(color=\"Mô hình cộng tính tổng quát\"), method = \"gam\" , se = FALSE)+\n  theme_minimal()"},{"path":"trực-quan-hóa-dữ-liệu.html","id":"các-hàm-geom_-cơ-bản","chapter":"Chương 7 Trực quan hóa dữ liệu","heading":"7.3.2 Các hàm geom_*() cơ bản","text":"Các hình dạng đồ họa, gọi tắt là các geoms, là một cách phổ biến để hiển thị một lớp của một đồ thị mô tả dữ liệu. Ví dụ như sử dụng geom_point() sẽ tạo ra một đồ thị phân tán hay còn gọi là đồ thị rải điểm; khi sử dụng geom_line() sẽ tạo ra các đồ thị theo đường,… Danh sách các geoms và các thuộc tính thẩm mỹ bạn đọc có thể tìm trong danh sách được liệt kê trong link ở phần @ref(sec:ggplot_intro). Trong phần này, chúng tôi phân loại và giải thích cách sử dụng các geoms chi tiết hơn.","code":""},{"path":"trực-quan-hóa-dữ-liệu.html","id":"mô-tả-một-biến","chapter":"Chương 7 Trực quan hóa dữ liệu","heading":"7.3.2.1 Mô tả một biến","text":"Khi sử dụng đồ thị của \\(\\textbf{ggplot2}\\) để mô tả một biến rời rạc, hàm thường được sử dụng là geom_bar(). Tần suất xuất hiện của các giá trị trong các biến rời rạc được mô tả dưới dạng các thanh. Các thuộc tính thẩm mỹ quan trọng của geom_bar() bao gồm:Thuộc tính thẩm mỹ x được ánh xạ đến tên biến (rời rạc).Thuộc tính thẩm mỹ x được ánh xạ đến tên biến (rời rạc).Thuộc tính thẩm mỹ color là màu sắc của đường viền xung quanh các thanh.Thuộc tính thẩm mỹ color là màu sắc của đường viền xung quanh các thanh.Thuộc tính thẩm mỹ fill là màu sắc của các thanh. fill ngoài ý nghĩa tăng tính thẩm mỹ cho đồ thị dạng thanh còn có ý nghĩa khi mô tả mối liên hệ giữa biến rời rạc được mô tả bởi thuộc tính thẩm mỹ x với một biến khác.Thuộc tính thẩm mỹ fill là màu sắc của các thanh. fill ngoài ý nghĩa tăng tính thẩm mỹ cho đồ thị dạng thanh còn có ý nghĩa khi mô tả mối liên hệ giữa biến rời rạc được mô tả bởi thuộc tính thẩm mỹ x với một biến khác.Hình 7.12 sử dụng đồ thị dạng thanh mô tả phân phối xác suất của biến \\(continent\\) trong dữ liệu \\(\\textbf{gapminder}\\) trong năm 2011.\nFigure 7.12: Phân phối xác suất của biến continent trong dữ liệu gapminder trong năm 2011. Hình bên trái: phân phối xác suất của biến continent mô tả theo thứ tự của giá trị xuất hiện. Hình bên phải: phân phối xác suất của biến continent mô tả theo thứ tự của tần suất xuất hiện của các biến tăng dần.\nKhi mô tả biến rời rạc, geom_bar() luôn mặc định sắp xếp các thanh theo thứ tự các giá trị xuất hiện trong biến rời rạc tăng dần. Tuy nhiên, để đồ thị dạng thanh mô tả hiệu quả hơn, chúng ta thường để các thanh xuất hiện theo thứ tự có chiều cao tăng dần hoặc giảm dần giống như hình bên trái của Hình 7.12. Để thực hiện việc này, chúng ta cần thay đổi thứ tự xuất hiện của các giá trị trên trục \\(x\\) theo thứ tự tần suất tăng dần hay giảm dần bằng hàm scale_x_discrete(). Chúng ta sẽ thảo luận về các hàm này trong phần sau.Hàm geom_bar() cũng thường được sử dụng để mô tả các giá trị liên tục được lưu trong một biến tương ứng với các giá trị rời rạc được lưu trong biến khác. Ví dụ, khi chúng ta muốn mô tả thu nhập bình quân của 10 nước có thu nhập bình quân đầu người lớn nhất thế giới năm 2011, chúng ta có thể sử dụng geom_bar() với đầy đủ hai thuộc tính thẩm mỹ x và y như sau:\nFigure 7.13: 10 nước có thu nhập bình quân đầu người cao nhất thế giới năm 2011. Hình bên trái: Thứ tự các nước không được sắp xếp theo thu nhập bình quân đầu người. Hình bên phải: các nước xuất hiện theo thứ tụ thu nhập bình quân đâu người giảm dần.\nKhi sử dụng geom_bar() để mô tả hai biến như Hình 7.13 chúng ta cần cho tham số stat nhận giá trị bằng \\(\"identity\"\\) để phân biệt với khi sử dụng geom_bar() khi mô tả một biến liên tục. Chúng tôi sẽ giải thích tham số stat trong phần lập trình trong ggplot2 ở phần sau. Để biễn cách cột theo thứ tự tăng dần hay giảm dần, bạn đọc sử dụng hàm reorder(). Trong đồ thị bên phải của Hình 7.13, chúng tôi ánh xạ cấu phần thẩm mỹ y của geom_bar() đến biến \\(country\\) nhưng được sắp xếp theo thứ tự \\(gdp_per_capita\\) của giảm dần.Để mô tả một biến liên tục chúng ta sử dụng geom_histogram(). Hình dạng đồ họa của geom_histogram() giống với geom_bar() là sử dụng các thanh hoặc các cột để mô tả tần suất xuất hiện của các giá trị trong biến. Nguyên tắc vẽ đồ thị dạng cột của hàm này là chia khoảng được xác định bởi giá trị nhỏ nhất và giá trị lớn nhất của biến lien tục thành \\(k\\) khoảng bằng nhau, sau đó đếm trong mỗi khoảng có bao nhiêu giá trị của biến liên tục xuất hiện. Độ rộng của các cột chính là độ rộng của các khoảng còn chiều cao của các cột là số lần (hoặc tần suất) xuất hiện của các giá của biến liên tục trong khoảng đó. Tham số bins cho biết chúng ta sử dụng bao nhiêu khoảng để chia miền giá trị của biến liên tục. Các cấu phần thẩm mỹ của geom_histogram() tương đối giống với geom_bar() nên chúng tôi sẽ không nhắc lại.Hình 7.14 mô tả biến thu nhập bình quân đầu người trong năm 2011 của tất cả các quốc gia trên thế giới.\nFigure 7.14: Phân phối của biến thu nhập bình quân đầu người của các quốc gia trên thế giới năm 2011. Hình bên trái: Sử dụng 10 bins để mô tả. Hình bên phải: Sử dụng 30 bins để mô tả.\nLựa chọn số bins để hiển thị là rất quan trọng trong mô tả phân phối xác suất của một biết liên tục. Trong trường hợp biến có ít quan sát như Hình 7.13 lựa chọn giữa 10 bins hoặc 30 bins không dẫn đến sự khác biệt nhiều. Tuy nhiên, khi biến liên tục có nhiều quan sát, lựa chọn số bins quá ít sẽ làm cho chúng ta hiểu sai về phân phối của biến. Hãy quan sát ví dụ khi chúng ta mô tả biến \\(price\\) trong dữ liệu có tên là \\(\\textbf{diamond}\\) là biến chứa giá của hơn 50 nghìn viên kim cương như Hình 7.15\nFigure 7.15: Phân phối của biến price trong dữ liệu diamonds với hơn 50 nghìn quan sát. Hình bên trái: Sử dụng 10 bins để mô tả. Hình bên phải: Sử dụng 60 bins để mô tả.\nBạn đọc có thể nhận thấy rằng nếu chúng ta sử dụng quá ít bins để mô tả một số lượng quan sát lớn có thể dẫn đến kết luận sai về phân phối của biến liên tục. Hình bên trái của Hình 7.15 là phân phối của một biến ngẫu nhiên chỉ có một giá trị mode (một đỉnh) tại giá trị 2500 và sau đó có tần suất xuất hiện giảm dần. Hình bên phải của Hình 7.15 cho biết đây là một phân phối liên tục có hai đỉnh tại các giá trị 1000 và 4500.Cũng để mô tả phân phối xác suất của biến liên tục, geom_density() có thể được sử dụng một cách độc lập hoặc bổ sung với histogram để mô tả phân phối của các biến một cách tốt hơn. Hình 7.16 mô tả cách sử dụng geom_density() cùng với geom_histogram() để mô tả phân phối của các biến liên tục.\nFigure 7.16: Kết hợp geom_density và geom_histogram để mô tả phân phối xác suất của biến liên tục. Hình bên trái: phân phối của biến thu nhập bình quân đầu người của các quốc gia trên thế giới năm 2011. Hình bên phải: phân phối của giá của các viên kim cương trong dữ liệu diamonds\nĐể hiển thị đồng thời đồ thị histogram và đồ thị density trên cùng một đồ thị, chúng ta cần phải biến đổi đồ thị histogram từ mô tả số lần xuất hiện của biến liên tục thành tần suất xuất hiện. Bạn đọc thực hiện phép biến đổi này bằng cách thêm vào sau phần khai báo ánh xạ thẩm mỹ câu lệnh after_stat(density) để tổng diện tích của các hình được tạo bởi các thanh được quy đổi về 1.Phương pháp vẽ đồ thị của geom_density() cũng giống như hàm density() có sẵn trong R xây dựng một đường liên tục là ước lượng cho hàm mật độ xác suất của một biến ngẫu nhiên liên lục. Hàm mật độ này được ước lượng bằng phương pháp kernel. Giá trị hàm mật độ tại một điểm \\(x\\) bất kỳ nằm trong miền giá trị của một biến liên tục được tính bằng trung bình giá trị hàm \\(K\\), được gọi là hàm kernel, tính trên khoảng cách từ điểm \\(x\\) tới tất cả các quan sát. Ký hiệu \\(\\hat{f}(x)\\) là giá trị hàm mật độ tính tại \\(x\\) bằng phương pháp kernel thì ta có\n\\[\\begin{align}\n\\hat{f}(x) = \\cfrac{1}{nh} \\times \\sum\\limits_{= 1}^{n} \\ K\\left( \\cfrac{x - x_i}{h} \\right)\n\\tag{7.1}\n\\end{align}\\]\ntrong đó \\(x_i\\) là giá trị quan sát thứ \\(\\) và \\(h\\) là được gọi là tham số làm mịn. \\(h\\) càng lớn thì hàm \\(\\hat{f}\\) sẽ càng mịn. Hàm \\(K\\) được sử dụng làm hàm kernel mặc định cho geom_density() là hàm mật độ của biến ngẫu nhiên phân phối chuẩn.Một hàm số khác cũng có thể được sử dụng để mô tả phân phối của một biến liên tục là geom_boxplot() nhưng hàm số này có thể được sử dụng để mô tả mối liên hệ giữa biến rời rạc và biến liên tục nên chúng tôi sẽ thảo luận ở phần sau.","code":"\np1<-dat%>%ggplot() + \n  geom_bar(aes(x = continent), color = \"orange\", fill = \"blue\", alpha = 0.5)+\n  theme_minimal()\np2<-dat%>%ggplot() + \n  geom_bar(aes(x = continent), color = \"orange\", fill = \"blue\", alpha = 0.5)+\n  scale_x_discrete(limits = names(sort(table(dat$continent))))+\n  theme_minimal()\ngrid.arrange(p1,p2, nrow= 1 , ncol = 2)\np1<-dat%>%arrange(desc(gdp_per_capita))%>%head(10)%>%\n  ggplot() + \n  geom_bar(aes(x = gdp_per_capita, y = country), \n           stat = \"identity\",\n           color = \"orange\", fill = \"blue\", alpha = 0.5)+\n  theme_minimal()\np2<-dat%>%arrange(desc(gdp_per_capita))%>%head(10)%>%\n  ggplot() + \n  geom_bar(aes(x = gdp_per_capita, y = reorder(country,gdp_per_capita)), \n           stat = \"identity\",\n           color = \"orange\", fill = \"blue\", alpha = 0.5)+\n  ylab(\"country\")+\n  theme_minimal()\ngrid.arrange(p1,p2, nrow= 1 , ncol = 2)\np1<-dat%>%ggplot() + \n  geom_histogram(aes(x = gdp_per_capita), bins = 10,\n           color = \"orange\", fill = \"blue\", alpha = 0.5)+\n  theme_minimal()\np2<-dat%>%ggplot() + \n  geom_histogram(aes(x = gdp_per_capita), bins = 40,\n           color = \"orange\", fill = \"blue\", alpha = 0.5)+\n  theme_minimal()\ngrid.arrange(p1,p2, nrow= 1 , ncol = 2)\np1<-diamonds%>%ggplot() + \n  geom_histogram(aes(x = price), bins = 10,\n           color = \"orange\", fill = \"blue\", alpha = 0.5)+\n  theme_minimal()\np2<-diamonds%>%ggplot() + \n  geom_histogram(aes(x = price), bins = 60,\n           color = \"orange\", fill = \"blue\", alpha = 0.5)+\n  theme_minimal()\ngrid.arrange(p1,p2, nrow= 1 , ncol = 2)\np1<-dat%>%ggplot() + \n  geom_histogram(aes(x = gdp_per_capita, after_stat(density)), bins = 30,\n          fill = \"blue\", alpha = 0.2)+\n  geom_density(aes(x = gdp_per_capita), color = \"orange\")+\n  theme_classic()\np2<-diamonds%>%ggplot() + \n  geom_histogram(aes(x = price, after_stat(density)), bins = 60,\n           fill = \"blue\", alpha = 0.2)+\n  geom_density(aes(x = price),color = \"orange\")+\n  theme_classic()\ngrid.arrange(p1,p2, nrow= 1 , ncol = 2)"},{"path":"trực-quan-hóa-dữ-liệu.html","id":"mô-tả-hai-biến-liên-tục","chapter":"Chương 7 Trực quan hóa dữ liệu","heading":"7.3.2.2 Mô tả hai biến liên tục","text":"Đồ thị phân tán được vẽ bằng hàm geom_point() là cách hiệu quả nhất để mô tả trực quan hai biến liên tục. Bạn đọc đã làm quen với geom_point() cùng với các thuộc tính thẩm mỹ như x, y, color, shape, size, … để mô tả mối liên hệ giữa các biến thu nhập bình quân đầu người và tuổi thọ bình quân của các quốc gia trên thế giới nên chúng tôi sẽ không nhắc lại cách sử dụng các cấu phần thẩm mỹ này của geom_point()Đồ thị phân tán thường được sử dụng cùng với geom_smooth() để mô tả mối liên hệ giữa hai biến. Phương pháp xây dựng mô hình trong hàm geom_smooth() bao gồm:Phương pháp \\(\"loess\"\\) là phương pháp mặc định để xây dựng hàm liên tục hay còn gọi là đường hồi quy mô tả mối liên hệ giữa biến được ánh xạ tới y theo biến được ánh xạ tới x. Phương pháp này được sử dụng khi số lượng quan sát nhỏ hơn 1000. Nguyên nhân là thời gian xây dựng mô hình tăng lên tỷ lệ với bình phương của số lượng quan sát, đó sẽ mất nhiều thời gian để vẽ đường hồi quy mô tả mối liên hệ giữa hai biến.Phương pháp \\(\"loess\"\\) là phương pháp mặc định để xây dựng hàm liên tục hay còn gọi là đường hồi quy mô tả mối liên hệ giữa biến được ánh xạ tới y theo biến được ánh xạ tới x. Phương pháp này được sử dụng khi số lượng quan sát nhỏ hơn 1000. Nguyên nhân là thời gian xây dựng mô hình tăng lên tỷ lệ với bình phương của số lượng quan sát, đó sẽ mất nhiều thời gian để vẽ đường hồi quy mô tả mối liên hệ giữa hai biến.Phương pháp \\(\"lm\"\\) sẽ ước lượng đường thẳng (tuyến tính) mô tả mối liên hệ giữa biến ánh xạ đến x và biến ánh xạ đến y.Phương pháp \\(\"lm\"\\) sẽ ước lượng đường thẳng (tuyến tính) mô tả mối liên hệ giữa biến ánh xạ đến x và biến ánh xạ đến y.Phương pháp \\(\"gam\"\\) sẽ ước lượng một hàm liên tục được gọi là smoothing spline mô tả mối liên hệ giữa hai biến. Đây là phương pháp mặc định để xây dựng mô hình khi dữ liệu có kích thước lớn hơn 1000. Chúng ta sẽ thảo luận về smoothing spline trong phần Mô hình cộng tính tổng quát.Phương pháp \\(\"gam\"\\) sẽ ước lượng một hàm liên tục được gọi là smoothing spline mô tả mối liên hệ giữa hai biến. Đây là phương pháp mặc định để xây dựng mô hình khi dữ liệu có kích thước lớn hơn 1000. Chúng ta sẽ thảo luận về smoothing spline trong phần Mô hình cộng tính tổng quát.Trong trường hợp mô tả trực quan hai biến liên tục nhưng có nhiều điểm bị trùng nhau lên nhau thì geom_point() có thể sẽ gây nhầm lẫn về mật độ xuất hiện của các điểm. Hãy quan sát ví dụ dưới đây, chúng tôi sử dụng geom_point() để mô tả trực quan hai biến \\(cty\\) và \\(hwy\\) của dữ liệu \\(\\textbf{mpg}\\). Đây là hai biến liên tục mô tả mức độ tiêu hao nhiên liệu của 234 loại xe ô tô khác nhau được sản xuất vào các năm 1998 và 2008. Đơn vị của hai biến này đều là miles per gallon, nghĩa là cho biết số dặm mà xe đi được trên mỗi gallon nhiên liệu.\nFigure 7.17: Sử dụng đồ thị phân tán để mô tả trực quan hai biến liên tục là hwy và cty trong dữ liệu mpg. Hình bên trái: không sử dụng tham số alpha. Hình bên phải sử dụng tham số alpha bằng 0.2 để biết mật độ xuất hiện của các điểm\nBạn đọc có thể nhận thấy rằng số lượng điểm xuất hiện trên đồ thị bên trái của Hình 7.17 không tương ứng với số quan sát của dữ liệu. Nhận xét này được khẳng định thêm từ đồ thị bên phải của Hình 7.17. Khi chúng ta cho tham số alpha nhận giá trị 0.2, độ đậm nhạt của các điểm là rất khác nhau. Điều này cho thấy sự xuất hiện trùng lặp của của các điểm tại một số giá trị nhất định. Để có hiển thị tốt hơn với dữ liệu như vậy, bạn đọc có thể sử dụng geom_jitter() thay cho geom_point(). Hàm cũng hiển thị các điểm trên hai trục tọa độ giống như geom_point(), tuy nhiên khác biệt của geom_jitter() đó là mỗi điểm sẽ được di chuyển các một cách ngẫu nhiên xung quanh điểm ban đầu để tránh việc hiển thị điểm bị trùng nhau.\nFigure 7.18: Sử dụng đồ thị phân tán kết hợp với di chuyển ngẫu nhiên bằng geom_jitter để mô tả trực quan hai biến liên tục là hwy và cty trong dữ liệu mpg. Hình bên trái: các điểm được di chuyển một cách ngẫu nhiên với trung bình bằng 1 đơn vị. Hình bên phải: các điểm được di chuyển một cách ngẫu nhiên với trung bình bằng 3 đơn vị.\nCó thể thấy rằng các điểm dữ liệu đã được hiển thị đầy đủ hơn trong Hình 7.18. Hai tham số quan trọng trong geom_jitter() là width và height cho biết các điểm được di chuyển theo chiều ngang và chiều dọc với giá trị trung bình là bao nhiêu. Nếu lựa chọn giá trị cho hai tham số này quá nhỏ, mục tiêu hiển thị đầy đủ các điểm sẽ không được đảm bảo, trong khi lựa chọn giá trị cho hai tham số này quá lớn sẽ làm cho dữ liệu bị thay đổi về bản chất.Một phương pháp khác được sử dụng để mô tả trực quan hai biến liên tục là geom_text() hoặc geom_label(). Thay vì hiển thị điểm như geom_point(), geom_text() hiển thị một biến kiểu chuỗi ký tự thay vì hiển thị điểm. geom_text() thường được sử dụng kết hợp với geom_point() hoặc cũng có thể sử dụng độc lập trên những dữ liệu nhỏ. Khi dữ liệu có kích thước trung bình hoặc lớn, geom_text() nên được sử dụng để nhấn mạnh hoặc chú thích cho một vài điểm quan trọng hơn là được sử dụng cho tất cả các điểm. Khi hiển thị biến kiểu ký tự và các điểm trên cùng một đồ thị, rất dễ dẫn đến hiện tượng ký tự và điểm bị trùng nhau hiển thị chồng lên nhau. Để điều chỉnh ký tự xuất hiện về các phía, bạn đọc cần phải sử dụng thêm các tham số như \\(hjust\\), \\(vjust\\).Theo kinh nghiệm của chúng tôi, để hiện thị biến kiểu ký tự tốt hơn, nên sử dụng các hàm geom_text_repel() và geom_label_repel() thay thế cho geom_text() và geom_label(). Để sử dụng hai hàm này cần cài đặt thêm thư viện bổ sung là thư viện \\(\\textbf{ggrepel}\\).\nFigure 7.19: Tỷ lệ sinh trung bình mỗi phụ nữ và tỷ lệ tử vong trên 1000 trẻ sơ sinh cả các quốc gia Đông Nam Á năm 2011. Hình phía trên bên phải: Sử dụng geom_text. Hình phía trên bên trái: Sử dụng geom_label. Hình phía dưới bên trái: Sử dụng geom_text_rebel. Hình phía dưới bên phải: Sử dụng geom_label_rebel\nKhi một trong hai biến liên tục là biến dạng thời gian thì chúng ta sẽ sử dụng geom_line(). geom_point() cũng có thể sử dụng cùng với geom_line() nếu số lượng điểm dữ liệu không quá lớn. Nguyên tắc vẽ hình của geom_line() là sử dụng các đường thẳng để nối các điểm xuất hiện trong dữ liệu theo thứ tự tăng dần của biến được ánh xạ tới cấu phần thẩm mỹ x. Khi vẽ đồ thị của một biến liên tục theo thời gian, biến thời gian luôn luôn được ánh xạ đến thuộc tính x của geom_line().\nFigure 7.20: Thu nhập bình quân đầu người thay đổi theo thời gian của bốn quốc gia là Pháp, Nhật Bản, Mỹ, và Vương quốc Anh từ năm 1960 đến năm 2016\nHình 7.20 mô tả sự thay đổi của thu nhập bình quân đầu người của các quốc gia phát triển trên thế giới từ năm 1960 đến 2016. Biến thu nhập bình quân đầu người được ánh xạ đến thuộc tính y trong khi biến thời gian (\\(year\\)) được ánh xạ đến thuộc tính thẩm mỹ \\(x\\). Để mô tả mỗi quốc gia bằng một đường khác nhau, bạn đọc cần\nánh xạ biến \\(country\\) đến thuộc tính thẩm mỹ như group, linetype, hoặc color.Khi số lượng quan sát cần hiển thị lớn thì hiển thị trực quan các điểm trên trục tọa độ sẽ không hiệu quả. Thay vào đó, chúng ta nên hiển thị tần xuất và mật độ xuất hiện của các điểm để đồ thị được rõ ràng hơn. geom_bin2d() và geom_density2d() là các hàm để trực quan hóa phân phối của hai biến liên tục.geom_bin2d() chia miền giá trị của từng biến thành các khoảng bằng nhau sau đó đếm trong mỗi miền tương ứng với một hình chữ nhật có kích thước bằng một đơn vị khi chia miền giá trị của từng biến có bao nhiêu điểm. Nguyên tắc trực quan hóa của geom_bin2d() là sử dụng màu sắc từ đậm nhạt để mô tả số lượng điểm trong từng hình chữ nhật từ lớn đến nhỏ.geom_bin2d() chia miền giá trị của từng biến thành các khoảng bằng nhau sau đó đếm trong mỗi miền tương ứng với một hình chữ nhật có kích thước bằng một đơn vị khi chia miền giá trị của từng biến có bao nhiêu điểm. Nguyên tắc trực quan hóa của geom_bin2d() là sử dụng màu sắc từ đậm nhạt để mô tả số lượng điểm trong từng hình chữ nhật từ lớn đến nhỏ.geom_density2d() sử dụng phương pháp kernel để tính giá trị hàm mật độ trong không gian hai chiều giống như cách tính toán của geom_density() trong công thức (7.1). Khoảng cách từ quan sát \\(x_i\\) đến điểm \\(x\\) được sử dụng là khoảng cách Euclide thay vì \\(|x_i-x|\\) trong không gian một chiều. Hàm kernel được sử dụng là hàm mật độ của biến ngẫu nhiên phân phối chuẩn hai chiều. Hàm geom_density2d() vẽ các đường nối các điểm có giá trị hàm mật độ bằng nhau lại với nhau được gọi là các đường bàng quang (hay contour). Hình vẽ dưới đây mô tả phân phối đồng của hai biến \\(price\\) và \\(carat\\) trong dữ liệu \\(diamonds\\).geom_density2d() sử dụng phương pháp kernel để tính giá trị hàm mật độ trong không gian hai chiều giống như cách tính toán của geom_density() trong công thức (7.1). Khoảng cách từ quan sát \\(x_i\\) đến điểm \\(x\\) được sử dụng là khoảng cách Euclide thay vì \\(|x_i-x|\\) trong không gian một chiều. Hàm kernel được sử dụng là hàm mật độ của biến ngẫu nhiên phân phối chuẩn hai chiều. Hàm geom_density2d() vẽ các đường nối các điểm có giá trị hàm mật độ bằng nhau lại với nhau được gọi là các đường bàng quang (hay contour). Hình vẽ dưới đây mô tả phân phối đồng của hai biến \\(price\\) và \\(carat\\) trong dữ liệu \\(diamonds\\).\nFigure 7.21: Biểu diễn phân phối của hai biến liên tục là price và carat trong dữ liệu diamonds. và geom_density2d. Hình bên trái: sử dụng geom_bin2d để chia miền giá trị của các biến ra thành các ô vuông nhỏ và sử dụng màu sắc để mô tả mật độ xuất hiện. Hình bên phải: sử dụng geom_density2d kết nối các điểm có cùng ước lượng của hàm mật độ.\nHình 7.21 cho thấy phân phối đồng thời của hai biến \\(carat\\) và \\(price\\) trong dữ liệu \\(\\textbf{diamonds}\\) là phân phối có nhiều mode. Các điểm có tập trung mật độ đặc biệt cao là các điểm có (\\(price\\), \\(carat\\)) bằng (1000, 0.3), (2200,0.7), và (4700, 1.0).","code":"\np1<-mpg%>%ggplot(aes(x = cty,y = hwy)) + \n  geom_point()+\n  geom_smooth(se = FALSE, linetype = 2, size = 0.7, color = \"orange\")+\n  theme_minimal()\np2<-mpg%>%ggplot(aes(x = cty,y = hwy)) + \n  geom_point(alpha = 0.2)+\n  geom_smooth(se = FALSE, linetype = 2, size = 0.7, color = \"orange\")+\n  theme_minimal()\ngrid.arrange(p1,p2, nrow= 1 , ncol = 2)\np1<-mpg%>%ggplot(aes(x = cty,y = hwy)) + \n  geom_jitter(width = 1, height = 1, alpha = 0.5)+\n  geom_smooth(se = FALSE, linetype = 2, size = 0.7, color = \"orange\")+\n  theme_minimal()\np2<-mpg%>%ggplot(aes(x = cty,y = hwy)) + \n  geom_jitter(width = 3, height = 3,alpha = 0.5)+\n  geom_smooth(se = FALSE, linetype = 2, size = 0.7, color = \"orange\")+\n  theme_minimal()\ngrid.arrange(p1,p2, nrow= 1 , ncol = 2)\np1<-dat%>%filter(region == \"South-Eastern Asia\")%>%\n  ggplot(aes(fertility, infant_mortality))+\n  geom_point()+geom_text(aes(label = country), vjust=  - 1.1)+\n  ggtitle(\"Sử dụng geom_text()\")+\n  theme_minimal()\np2<-dat%>%filter(region == \"South-Eastern Asia\")%>%\n  ggplot(aes(fertility, infant_mortality))+\n  geom_point()+geom_label(aes(label = country), vjust= - 1.1)+\n  ggtitle(\"Sử dụng geom_label()\")+\n  theme_minimal()\n\np3<-dat%>%filter(region == \"South-Eastern Asia\")%>%\n  ggplot(aes(fertility, infant_mortality))+\n  geom_point()+geom_text_repel(aes(label = country),vjust=  1.1)+\n  ggtitle(\"Sử dụng geom_text_repel()\")+\n  theme_minimal()\np4<-dat%>%filter(region == \"South-Eastern Asia\")%>%\n  ggplot(aes(fertility, infant_mortality))+\n  geom_point()+geom_label_repel(aes(label = country),vjust=  1.1)+\n  ggtitle(\"Sử dụng geom_label_repel()\")+\n  theme_minimal()\ngrid.arrange(p1,p2,p3,p4,nrow=2,ncol=2)\ngapminder%>%\n  filter(country %in% c(\"United States\",\"Japan\",\"France\",\"United Kingdom\"))%>%\n  ggplot(aes(x = year, y = gdp/population,color = country))+\n  geom_line()+\n  geom_point(size = 0.5, alpha = 0.5)+\n  theme_minimal()\np1<-diamonds%>%\n  ggplot(aes(price, carat))+geom_bin2d(bins = 40)+\n  scale_x_log10()+scale_y_log10()+\n  theme_minimal()+scale_fill_viridis_c()+\n  ggtitle(\"geom_bind2d\")\n\np2<-diamonds%>%\n  ggplot(aes(price, carat))+geom_density2d(color = \"blue\",alpha = 0.5)+\n  scale_x_log10()+scale_y_log10()+\n  theme_minimal()+\n  ggtitle(\"geom_density2d\")\n\ngrid.arrange(p1,p2,nrow=1, ncol = 2)"},{"path":"trực-quan-hóa-dữ-liệu.html","id":"mô-tả-một-biến-liên-tục-và-một-biến-rời-rạc","chapter":"Chương 7 Trực quan hóa dữ liệu","heading":"7.3.2.3 Mô tả một biến liên tục và một biến rời rạc","text":"Đồ thị boxplot là phương pháp thông dụng nhất để mô tả mối liên hệ giữa một biến liên tục với một biến rời rạc. Đồ thị boxplot được vẽ bằng hàm geom_boxplot() với thuộc tính thẩm mỹ x được ánh xạ đến biến rời rạc và thuộc tính thẩm mỹ y được ánh xạ đên biến liên tục. Các thuộc tính thẩm mỹ như color và fill được sử dụng giống như geom_histogram(). Hình 7.22 mô tả mối liên hệ giữa biến thu nhập bình quân đầu người và biến Châu lục của tất cả các quốc gia trên thế giới vào năm 2011.\nFigure 7.22: Logarit của thu nhập bình quân đầu người của các quốc gia trên thế giới theo Châu lục năm 2011. Hình bên trái: Các châu lục được sắp xếp theo thứ tự tên Châu lục tăng dần. Hình bên phải: các châu lục được sắp xếp theo thứ tự giá trị trung bình của biến thu nhập bình quân đầu người tăng dần\nKhi mô tả mối liên hệ giữa một biến liên tục với một biến rời rạc, các đồ thị hộp nên được sắp xếp theo thứ tự mà giá trị trung bình của biến liên tục tương ứng với mỗi giá trị tăng dần giống như đồ thị bên phải của Hình 7.22. Chúng ta có thể thấy rằng thu nhập bình quân đầu người của Châu Mỹ thấp hơn Châu Á mặc dù các giá trị trung vị và tứ phân vị thứ nhất cao hơn. Nguyên nhân là phân phối của biến thu nhập bình quân đầu người của châu Á lệch phải mạnh hơn với phân phối của biến này tại các nước châu Mỹ.geom_boxplot() chỉ thể hiện các giá trị phân vị của phân phối xác suất, nên đôi khi sẽ không cung cấp đầy đủ thông tin về phân phối của biến liên tục. đó, người phân tích dữ liệu thường sử dụng geom_violin() kết hợp cùng với geom_boxplot() để cho mô tả tốt hơn về phân phối xác suất của biến liên tục trong từng nhóm. geom_violin() đơn giản là vẽ hàm mật độ của biến liên tục trong từng nhóm được định nghĩa bởi biến rời rạc.\nFigure 7.23: Logarit thu nhập bình quân đầu người của các quốc gia trên thế giới theo Châu lục năm 2011 kết hợp geom_boxplot và geom_violin\nTham số draw_quantiles cho biết các giá ngưỡng phân vị mà chúng ta muốn vẽ cùng với các hàm mật độ. Trong Hình 7.23 sử dụng các ngưỡng phân vị 25%, 50% và 75% tương tự như geom_boxplot().","code":"\np1<-dat%>%ggplot(aes(x = continent, y = log(gdp_per_capita)))+\n  geom_boxplot()+\n  theme_minimal()\np2<-dat%>%ggplot(aes(x = reorder(continent, gdp_per_capita), \n                     y = log(gdp_per_capita)))+\n  geom_boxplot()+\n  theme_minimal()\ngrid.arrange(p1,p2,nrow=1, ncol = 2)\np1<-dat%>%ggplot(aes(x = reorder(continent, gdp_per_capita), \n                     y = log(gdp_per_capita)))+\n  geom_boxplot()+\n  theme_minimal()\np2<-dat%>%ggplot(aes(x = reorder(continent, gdp_per_capita), \n                     y = log(gdp_per_capita)))+\n  geom_violin(draw_quantiles = c(0.25,0.5,0.75), \n              alpha = 0.3)+\n  theme_minimal()\ngrid.arrange(p1,p2,nrow=1, ncol = 2)"},{"path":"trực-quan-hóa-dữ-liệu.html","id":"mô-tả-hai-biến-rời-rạc","chapter":"Chương 7 Trực quan hóa dữ liệu","heading":"7.3.2.4 Mô tả hai biến rời rạc","text":"Đồ thị thường được sử dụng để mô tả trực quan phân phối của hai biến rời rạc là đồ thị kiểu bong bóng. Hàm số được sử dụng để vẽ đồ thị này là geom_count(). Tương ứng với mỗi cặp giá trị của hai biến rời rạc, hàm geom_count() tính toán số lượng điểm dữ liệu tương ứng với hai giá trị này và phản ánh số lượng điểm dữ liệu lên trên đồ thị thông qua kích thước của mỗi bong bóng. Chúng ta mô tả phân phối của hai biến rời rạc là \\(cut\\) và \\(color\\) trong dữ liệu \\(\\textbf{diamonds}\\) như sau:\nFigure 7.24: Đồ thị bong bóng mô tả phân phối của hai biến rời rạc là cut và color trong dữ liệu diamonds. Hình bên trái sử dụng hình dạng tròn (shape = 21) để mô tả số điểm dữ liệu. Hình bên phải sử dụng hình vuông (shape = 22) để dễ phân biệt kích thước hơn\nCác thuộc tính thẩm mỹ của geom_count() hoàn toàn giống với geom_point() nên chúng ta có thể lựa chọn các hình dạng cho phép phân biệt kích thước tốt hơn thay vì sử dụng hình tròn như mặc định. Đồ thị trong Hình 7.24 còn sử dụng hàm scale_size() để giúp cho hiển thị được tốt hơn. Chúng ta sẽ thảo luận về các hàm scale_*() trong các phần tiếp theo. Từ Hình 7.24 để nhận ra tỷ lệ lớn các viên kim cương có biến \\(cut\\) nhận giá trị \\('ideal'\\) và tỷ lệ lớn các viên kim cương có màu sắc nhận giá trị \\('G'\\). Các viên kim cương có biến \\(cut\\) nhận giá trị \\('ideal'\\) và biến \\(color\\) nhận giá trị \\('G'\\) cũng xuất hiện nhiều nhất trong dữ liệu với số lượng hơn 4000 viên.Một phương pháp khác để trực quan hai biến rời rạc là trực quan một biến bằng geom_bar() sau đó ánh xạ thuộc tính fill đến biến rời rạc còn lại.","code":"\np1<-diamonds%>%ggplot(aes(cut,color))+\n  geom_count(fill=\"blue\",alpha = 0.5,shape = 21)+\n  theme_minimal()+scale_size(range = c(1,12))+\n  theme(legend.position = \"top\")\n\np2<-diamonds%>%ggplot(aes(cut,color))+\n  geom_count(fill=\"blue\",alpha = 0.5,shape = 22)+\n  theme_minimal()+scale_size(range = c(1,12))+\n  theme(legend.position = \"top\")\ngrid.arrange(p1,p2,nrow=1, ncol = 2)"},{"path":"trực-quan-hóa-dữ-liệu.html","id":"mô-tả-ba-biến","chapter":"Chương 7 Trực quan hóa dữ liệu","heading":"7.3.2.5 Mô tả ba biến","text":"Thư viện \\(\\textbf{ggplot2}\\) có hỗ trợ vẽ các hình ảnh trong không gian ba chiều, nhưng sử dụng các hình ảnh kiểu 3 chiều không phải là một phương pháp tốt để hiển thị ba biến trên cùng một đồ thị. Khi mô tả mối liên hệ giữa ba biến, phương pháp đơn giản và hiệu quả nhất là sử dụng ba đồ thị để mô tả mối liên hệ giữa ba cặp biến với nhau. Chỉ khi nào gặp trường hợp bắt buộc, chúng ta mới sử dụng cả ba biến trên cùng một đồ thị. Để mô tả ba biến, nguyên tắc là lựa chọn đồ thị cho hai biến trước sau đó ánh xạ biến còn lại vào một thuộc tính thẩm mỹ.Khi hai trong ba biến là biến liên tục và biến còn lại là biến rời rạc, hãy luôn luôn sử dụng đồ thị phân tán để mô tả hai biến liên tục và sử dụng màu sắc để ánh xạ tới biến rời rạc còn lại.Khi hai trong ba biến là biến liên tục và biến còn lại là biến rời rạc, hãy luôn luôn sử dụng đồ thị phân tán để mô tả hai biến liên tục và sử dụng màu sắc để ánh xạ tới biến rời rạc còn lại.Khi hai trong ba biến là các biến rời rạc trong khi biến còn lại là biến liên tục, chúng ta có thể sử dụng geom_tile(). Hàm geom_tile() đặc biệt hiệu quả khi mô tả mối liên hệ giữa hai biến rời rạc với một biến liên tục bằng cách ánh xạ hai biến rời rạc vào các thuộc tính thẩm mỹ x và y trong khi giá trị của biến liên tục được mô tả thông qua màu sắc. tương ứng với màu sắc chỉ có một giá trị duy nhất của biến liên tục nên nếu tương ứng với một giá trị của biến rời rạc ánh xạ đến trục x và một giá trị của biến liên tục ánh xạ tới trục y có nhiều giá trị của biến liên tục thì màu sắc sẽ tương ứng với giá trị trung bình của biến liên tục. Ví dụ, khi chúng ta muốn mô tả ba biến \\(region\\), \\(year\\), và \\(life\\_expectancy\\) của dữ liệu \\(\\textbf{gapminder}\\) trên cùng một đồ thị, chúng ta sẽ cần tính \\(life\\_expectancy\\) trung bình của mỗi vùng trong mỗi năm trước khi thực hiện trực quan hóa.Khi hai trong ba biến là các biến rời rạc trong khi biến còn lại là biến liên tục, chúng ta có thể sử dụng geom_tile(). Hàm geom_tile() đặc biệt hiệu quả khi mô tả mối liên hệ giữa hai biến rời rạc với một biến liên tục bằng cách ánh xạ hai biến rời rạc vào các thuộc tính thẩm mỹ x và y trong khi giá trị của biến liên tục được mô tả thông qua màu sắc. tương ứng với màu sắc chỉ có một giá trị duy nhất của biến liên tục nên nếu tương ứng với một giá trị của biến rời rạc ánh xạ đến trục x và một giá trị của biến liên tục ánh xạ tới trục y có nhiều giá trị của biến liên tục thì màu sắc sẽ tương ứng với giá trị trung bình của biến liên tục. Ví dụ, khi chúng ta muốn mô tả ba biến \\(region\\), \\(year\\), và \\(life\\_expectancy\\) của dữ liệu \\(\\textbf{gapminder}\\) trên cùng một đồ thị, chúng ta sẽ cần tính \\(life\\_expectancy\\) trung bình của mỗi vùng trong mỗi năm trước khi thực hiện trực quan hóa.\nFigure 7.25: Tuổi thọ trung bình của các vùng trên thế giới thay đổi qua các năm từ năm 1960 đến năm 2015\nTrong đồ thị của Hình 7.25 chúng tôi đã sử dụng thêm các hàm scale_*() để kiểm soát ánh xạ thẩm mỹ. Chẳng hạn giá trị năm (trục x) sẽ là cách đều 5 năm, các vùng trên trục \\(y\\) được sắp xếp theo thứ tự có tuổi thọ trung bình trên toàn bộ dữ liệu tăng dần. Dải màu sắc cũng được gán giá trị cho dải màu liên tục từ cam sang xanh. Chúng ta sẽ thảo luận về scale_*() trong phần sau của chương sách.","code":"\n# Tạo danh sách các vùng có tuổi thọ trung bình tăng dần\ndat1<-gapminder%>%group_by(region)%>%\n     summarise(life_expectancy = mean(life_expectancy,na.rm = TRUE))%>%\n    arrange(desc(life_expectancy))\nregion_list<-dat1$region\n\n# Tạo dải màu rời rạc từ cam tới xanh\nmycol<-colorRampPalette(c(\"orange\", \"blue\"),space = \"Lab\")(10)\n\n# Trực quan hóa 3 biến region, year, và life_expectancy\ngapminder%>%group_by(year,region)%>%\n     summarise(life_expectancy = mean(life_expectancy,na.rm = TRUE))%>%\n     ggplot()+\n     geom_tile(aes(x = year, y = region , fill = life_expectancy), color= \"grey\")+\n     scale_fill_gradientn(colours = mycol)+\n     scale_x_continuous(breaks = seq(1960,2015,5))+\n     scale_y_discrete(limits = region_list)+\n     theme_minimal()+\n    xlab(\"\")+ylab(\"\")+\n    theme(legend.position = \"top\")"},{"path":"trực-quan-hóa-dữ-liệu.html","id":"các-hàm-stat_","chapter":"Chương 7 Trực quan hóa dữ liệu","heading":"7.3.3 Các hàm stat_*()+–","text":"Bạn đọc cũng có thể xây dựng các lớp cho đồ thị \\(\\textbf{ggplot2}\\) bằng các hàm stat_*(). Các hàm số này không hiển thị dữ liệu ở trạng thái ban đầu mà thường hiển thị dữ liệu dưới một phép biến đổi thống kê hoặc một phương pháp tóm tắt dữ liệu. Có sự tương đương giữa các hàm stat_*() với các hàm geom_*(), nghĩa là chúng ta có thể gọi hàm geom_*() bằng một hàm stat_*() và ngược lại. Ví dụ stat_bin() tương đương với geom_histogram() và geom_bar(); stat_smooth() tương đương với geom_smooth(), … Về bản chất, các geom và các stat đều có nguồn gốc từ chung một hàm tạo một lớp mới cho đồ thị là hàm layer(). Chúng ta sẽ thảo luận về các hàm này trong phần Kiến thức nâng cao về \\(\\textbf{ggplot2}\\).Thay vì sử dụng geom_*(), chúng ta có thể sử dụng stat_*() để mô tả phân phối của các biến liên tục:\nFigure 7.26: Phân phối của biến tỷ lệ sinh trung bình một phụ nữ (fertility) năm 2011. Hình bên trái: histogram và mật độ của biến fertility. Hình bên phải: phân phối của biến fertility theo các lục địa\nBạn đọc có thể thấy rằng các đồ thị trong Hình 7.26 được tạo bằng các hàm stat_() và cho kết quả hoàn toàn giống với các hàm geom_() tương ứng.","code":"\np1<-dat%>%ggplot(aes(fertility, after_stat(density)))+\n  stat_bin(color = \"blue\",alpha = 0.5)+\n  stat_density(color = \"orange\",alpha = 0.1)+\n  theme_minimal()\np2<-dat%>%ggplot(aes(x = reorder(continent,fertility), y = fertility))+\n  stat_boxplot(color = \"blue\", alpha = 0.5, fill = \"grey40\")+\n  theme_minimal()\ngrid.arrange(p1,p2,nrow=1,ncol=2)"},{"path":"trực-quan-hóa-dữ-liệu.html","id":"hàm-scale_","chapter":"Chương 7 Trực quan hóa dữ liệu","heading":"7.4 Hàm scale_*()","text":"Các hàm scale_*() trong thư viện \\(\\textbf{ggplot2}\\) kiểm soát ánh xạ từ các biến của dữ liệu đến thuộc tính thẩm mỹ của đồ thị. Các hàm này lấy dữ liệu ban đầu và biến đổi giá trị trong dữ liệu thành các đối tượng trực quan mà chúng ta có thể nhìn thấy, như kích thước, màu sắc, vị trí, hoặc hình dạng. Bạn đọc có thể tạo đồ thị bằng thư viện \\(\\textbf{ggplot2}\\) mà không cần biết chính xác các ánh xạ thẩm mỹ hoạt động như thế nào. Tuy nhiên, hiểu về scales và cách thao tác các hàm scale_*() sẽ giúp bạn kiểm soát hoàn toàn những đối tượng trực quan trên đồ thị.","code":""},{"path":"trực-quan-hóa-dữ-liệu.html","id":"vị-trí-xuất-hiện-trên-trục-tọa-độ","chapter":"Chương 7 Trực quan hóa dữ liệu","heading":"7.4.1 Vị trí xuất hiện trên trục tọa độ","text":"Đa số các đồ thị trong thư viện \\(\\textbf{ggplot2}\\) hiển thị dữ liệu trên trục tọa độ Descartes nên chúng tôi sẽ tập trung vào cách dữ liệu hiển thị trên hai trục tọa độ \\(x\\) và \\(y\\). Khi ánh xạ các biến của dữ liệu tới các trục tọa độ \\(x\\) và \\(y\\), nếu chúng ta không sử dụng các hàm scale_*(), các điểm sẽ được hiển thị đúng như giá trị của điểm đó trên các trục tọa độ. Trong nhiều trường hợp, hiển thị tại vị trí đúng như dữ liệu ban đầu sẽ không mang lại hiệu quả. Ví dụ như khi mô tả hai biến \\(total\\) và \\(population\\) của dữ liệu \\(\\textbf{murders}\\), bạn đọc có thể sánh cách hiển thị giữa việc không kiểm soát và có kiểm soát ánh xạ thẩm mỹ như Hình 7.27\nFigure 7.27: Hiển thị biến số vụ sát nhân bằng súng (total) và biến dân số của các bang trong dữ liệu murder. Hình bên trái: vị trí các điểm trên trục tọa độ là giá trị dữ liệu thô. Hình bên phải: vị trí trên các trục tọa độ đã được biến đổi bằng cách lấy logarit cơ số 10\nCó thể thấy rằng đồ thị bên phải của Hình 7.27 hiển thị rõ ràng hơn hình bên trái sau khi chúng ta có tác động đến ánh xạ từ biến \\(population\\) đến thuộc tính x và ánh xạ từ biến \\(total\\) đến thuộc tính y bằng các hàm scale_x_continuous() và scale_y_continuous(). Đây là hai hàm số được dùng để kiểm soát vị trí xuất hiện của các điểm trên trục tọa độ khi các biến trong ánh xạ là các biến kiểu số liên tục. Các tham số có thể được sử dụng trong các hàm này bao gồm có:Tham số trans, là viết tắt của transformation, nhận giá trị mặc định là \\('identity'\\) nghĩa là lấy chính xác giá trị của biến ánh xạ vào thuộc tính x hoặc y tương ứng. Để biết các giá trị mà tham số này có thể nhận được bạn đọc có thể tham khảo trong tài liệu đi kèm với hàm scale_x_continuous() và scale_y_continuous(). Khi sử dụng scale trên các biến, chẳng hạn như \\(x_1\\) và \\(x_2\\), là các biến ánh xạ tới x và y trong ánh xạ thẩm mỹ bằng một hàm \\(f\\) được khai báo bằng tham số trans, giá trị xuất hiện trên trục tọa độ \\(x\\) và \\(y\\) sẽ tương ứng là \\(f(x_1)\\) và \\(f(x_2)\\). Chẳng hạn như trong đồ thị phía bên phải ở trên, khi chúng ta thực hiện scale, sử dụng hàm \\(log10\\), tọa độ của các điểm (các quốc gia) sẽ là \\(log10(population)\\) và \\(log10(total)\\). Việc chuyển đổi này sẽ hữu ích bởi rất đa số các quốc gia có dân số nhỏ, trong khi có một vài quốc gia có dân số rất lớn. Thực hiện chuyển đổi dữ liệu bằng các hàm \\(log\\) sẽ giúp cho khoảng cách của các điểm cách đều nhau hơn và dễ dàng phân biệt hơn.Tham số trans, là viết tắt của transformation, nhận giá trị mặc định là \\('identity'\\) nghĩa là lấy chính xác giá trị của biến ánh xạ vào thuộc tính x hoặc y tương ứng. Để biết các giá trị mà tham số này có thể nhận được bạn đọc có thể tham khảo trong tài liệu đi kèm với hàm scale_x_continuous() và scale_y_continuous(). Khi sử dụng scale trên các biến, chẳng hạn như \\(x_1\\) và \\(x_2\\), là các biến ánh xạ tới x và y trong ánh xạ thẩm mỹ bằng một hàm \\(f\\) được khai báo bằng tham số trans, giá trị xuất hiện trên trục tọa độ \\(x\\) và \\(y\\) sẽ tương ứng là \\(f(x_1)\\) và \\(f(x_2)\\). Chẳng hạn như trong đồ thị phía bên phải ở trên, khi chúng ta thực hiện scale, sử dụng hàm \\(log10\\), tọa độ của các điểm (các quốc gia) sẽ là \\(log10(population)\\) và \\(log10(total)\\). Việc chuyển đổi này sẽ hữu ích bởi rất đa số các quốc gia có dân số nhỏ, trong khi có một vài quốc gia có dân số rất lớn. Thực hiện chuyển đổi dữ liệu bằng các hàm \\(log\\) sẽ giúp cho khoảng cách của các điểm cách đều nhau hơn và dễ dàng phân biệt hơn.Tham số limits giới hạn giá trị trên các thuộc tính thẩm mỹ x và y của đồ thị. Mỗi khi chúng ta vẽ đồ thị sử dụng thư viện \\(\\textbf{ggplot2}\\), tham số limits mặc định sẽ đảm bảo việc hiển thị được đầy đủ nhất. Tuy nhiên nhiều khi chúng ta cần phải thay đổi các giá trị giới hạn của các trục tọa độ, chẳng hạn như khi muốn sánh hai dữ liệu trên cùng một miền giá trị của \\(x\\) và \\(y\\). Các đồ thị trong Hình 7.28 mô tả hai biến \\(fertility\\) và \\(life\\_expectancy\\) trong các năm 1960 và 2011 và không sử dụng scale.Tham số limits giới hạn giá trị trên các thuộc tính thẩm mỹ x và y của đồ thị. Mỗi khi chúng ta vẽ đồ thị sử dụng thư viện \\(\\textbf{ggplot2}\\), tham số limits mặc định sẽ đảm bảo việc hiển thị được đầy đủ nhất. Tuy nhiên nhiều khi chúng ta cần phải thay đổi các giá trị giới hạn của các trục tọa độ, chẳng hạn như khi muốn sánh hai dữ liệu trên cùng một miền giá trị của \\(x\\) và \\(y\\). Các đồ thị trong Hình 7.28 mô tả hai biến \\(fertility\\) và \\(life\\_expectancy\\) trong các năm 1960 và 2011 và không sử dụng scale.\nFigure 7.28: Mối liên hệ giữa hai biến tỷ lệ sinh trung bình của mỗi phụ nữ và tuổi thọ trung bình trong hai năm 1960 và 2010 sử dụng miền giá trị trên trục tọa độ khác nhau. Hình bên trái: dữ liệu của năm 1960. Hình bên phải: dữ liệu của năm 2011\nKhông thể nhận biết được sự khác biệt trong mối liên hệ giữa hai biến tỷ lệ sinh trung bình của mỗi phụ nữ và tuổi thọ trung bình trong các năm 1960 và 2010 chúng ta không biểu diễn các biến trên cùng một miền giá trị của \\(x\\) và \\(y\\). Để khắc phục vấn đề này, Hình 7.29 sử dụng tham số \\(limits\\) trong các hàm scale_x_continuous() và scale_y_continuous(). Để khai báo tham số cho tham số này, chúng ta sử dụng một véc-tơ hai chiều chứa giá trị nhỏ nhất và giá trị lớn nhất trên trục tọa độ mà bạn muốn hiển thị.\nFigure 7.29: Mối liên hệ giữa hai biến tỷ lệ sinh trung bình của mỗi phụ nữ và tuổi thọ trung bình trên cùng một miền giá trị của trục tọa độ. Hình bên trái: dữ liệu của năm 1960. Hình bên phải: dữ liệu của năm 2011\nTham số breaks kiểm soát vị trí các điểm được đánh dấu xuất hiện trên các trục tọa độ \\(x\\) và trục tọa độ \\(y\\). Chúng tôi thường kết hợp breaks với tham số labels để kiểm soát đồng thời vị trí và cách hiển thị trên các trục số. Ví dụ như trong hình sánh đồ thị mô tả hai biến \\(fertility\\) và \\(life\\_expectancy\\) của các năm 1960 và 2010, chúng ta muốn giá trị xuất hiện trên trục \\(x\\) là các số 2, 4, 6, 8 và các số trên trục \\(y\\) xuất hiện tại các vị trí 10, 30, 50, 70, và 90, chúng ta chỉ cần khai báo giá trị cho tham số breaks bằng véc-tơ chứa các giá trị mà chúng ta muốn hiển thị. Lưu ý rằng breaks có chữ \\(s\\) ở cuối để phân biệt với từ khóa break.\nFigure 7.30: Mối liên hệ giữa hai biến tỷ lệ sinh trung bình của mỗi phụ nữ và tuổi thọ trung bình trên cùng một miền giá trị của trục tọa độ. Giá trị trên trục tọa độ được định nghĩa lại bằng các tham số breaks và labels. Hình bên trái: dữ liệu của năm 1960. Hình bên phải: dữ liệu của năm 2011\nKhi một trong hai biến liên tục là biến kiểu thời gian thì hàm số sử dụng để kiểm soát giá trị hiển thị là scale_x_date() với hai tham số thường được sử dụng là \\(date\\_break\\) và \\(date\\_labels\\). Cách sử dụng của các tham số này tương tựng như các tham số breaks và labels. Chúng ta sẽ mô tả biến số lượng hành khách trung bình từng tháng được lưu trong dữ liệu \\(\\textbf{AirPassengers}\\) theo một biến thời gian bắt đầu từ tháng 01 năm 1949.\nFigure 7.31: Số lượng hành khách trung bình theo tháng dữ liệu AirPassenger. Hình bên trái: không sử dụng scale_x_date. Hình bên phải: sử dụng scale_x_date để hiển thị tốt hơn giá trị ngày tháng trên trục x\nKhi giá trị trên trục \\(x\\) hoặc trục \\(y\\) là các giá trị rời rạc, các hàm số sử dụng để kiểm soát ánh xạ thẩm mỹ từ các biến đến các trục tọa độ là các hàm scale_x_discrete() và scale_y_discrete(). Các tham số thường sử dụng bao gồm limits và labels. Tham số limits được sử dụng để cho biết các giá trị nào của biến rời rạc xuất hiện trên đồ thị, trong khi tham số labels cho biết từng giá trị của biến rời rạc xuất hiện như thế nào\nFigure 7.32: Phân phối của biến tỷ lệ số vụ xả súng trên một triệu người dân theo vùng vào năm 2010 tại Mỹ. Hình bên trái: không sử dụng scale. Hình bên phải: sử dụng limits trên trục y và labels trên trục x cho hiển thị tốt hơn\n","code":"\np1<-murders%>%ggplot(aes(x = population,y = total))+\n  geom_point(size = 2, shape = 21, fill = \"grey40\", alpha = 0.5,color = \"blue\")+\n  theme_minimal()+ggtitle(\"Không sử dụng scale\")\np2<-murders%>%ggplot(aes(x = population,y = total))+\n  geom_point(size = 2, shape = 21, fill = \"grey40\", alpha = 0.5,color = \"blue\")+\n  theme_minimal()+\n  scale_x_continuous(trans = \"log10\")+\n  scale_y_continuous(trans = \"log10\")+\n  ggtitle(\"Có sử dụng scale (log10)\")\ngrid.arrange(p1,p2,nrow=1,ncol=2)\np1<-gapminder%>%filter(year==1960)%>%\n  ggplot(aes(fertility,life_expectancy))+\n  geom_point(size = 2, shape = 21, fill = \"grey40\", alpha = 0.5,color = \"blue\")+\n  ggtitle(\"Năm 1960\")+\n  theme_minimal()\np2<-gapminder%>%filter(year==2011)%>%\n  ggplot(aes(fertility,life_expectancy))+\n  geom_point(size = 2, shape = 21, fill = \"grey40\", alpha = 0.5,color = \"blue\")+\n  ggtitle(\"Năm 2011\")+\n  theme_minimal()\ngrid.arrange(p1,p2,nrow=1,ncol=2)\np1<-gapminder%>%filter(year==1960)%>%\n  ggplot(aes(fertility,life_expectancy))+\n  geom_point(size = 2, shape = 21, fill = \"grey40\", alpha = 0.5,color = \"blue\")+\n  ggtitle(\"Năm 1960\")+\n  scale_x_continuous(limits = c(1,9))+\n  scale_y_continuous(limits = c(25,85))+\n  theme_minimal()\np2<-gapminder%>%filter(year==2010)%>%\n  ggplot(aes(fertility,life_expectancy))+\n  geom_point(size = 2, shape = 21, fill = \"grey40\", alpha = 0.5,color = \"blue\")+\n  ggtitle(\"Năm 2010\")+\n   scale_x_continuous(limits = c(1,9))+\n  scale_y_continuous(limits = c(25,85))+\n  theme_minimal()\ngrid.arrange(p1,p2,nrow=1,ncol=2)\np1<-gapminder%>%filter(year==1960)%>%\n  ggplot(aes(fertility,life_expectancy))+\n  geom_point(size = 2, shape = 21, fill = \"grey40\", alpha = 0.5,color = \"blue\")+\n  ggtitle(\"Năm 1960\")+\n  scale_x_continuous(limits = c(1,9),\n                     breaks = c(2,4,6,8),\n                     labels = paste(c(2,4,6,8),\"trẻ em\"))+\n  scale_y_continuous(limits = c(25,85),\n                     breaks = c(10,30,50,70,90),\n                     labels = paste(c(10,30,50,70,90),\"tuổi\"))+\n  theme_minimal()\np2<-gapminder%>%filter(year==2010)%>%\n  ggplot(aes(fertility,life_expectancy))+\n  geom_point(size = 2, shape = 21, fill = \"grey40\", alpha = 0.5,color = \"blue\")+\n  ggtitle(\"Năm 2010\")+\n  scale_x_continuous(limits = c(1,9),\n                     breaks = c(2,4,6,8),\n                     labels = paste(c(2,4,6,8),\"trẻ em\"))+\n  scale_y_continuous(limits = c(25,85),\n                     breaks = c(10,30,50,70,90),\n                     labels = paste(c(10,30,50,70,90),\"tuổi\"))+\n  theme_minimal()\ngrid.arrange(p1,p2,nrow=1,ncol=2)\ndat0<-data.frame(Number_Passengers = AirPassengers,\n                Month = seq(as.Date(\"1949-01-01\"), by = \"month\", length.out = 144))\np1<-dat0%>%ggplot(aes(x = Month, y = Number_Passengers))+\n  geom_line() + ggtitle(\"Không sử dụng scale\")+\n  theme_minimal()\np2<-dat0%>%ggplot(aes(x = Month, y = Number_Passengers))+\n  geom_line()+ ggtitle(\"Sử dụng scale_x_date()\")+\n  scale_x_date(date_break = \"2 years\", date_labels = \"%b\\n%Y\" )+\n  scale_y_continuous(breaks = seq(100,600,length=6))+\n  theme_minimal()\n\ngrid.arrange(p1,p2,nrow=1,ncol=2)\n# Không sử dụng scale\np1<-murders%>%mutate(rate = total/population*10^6)%>%\n  ggplot(aes(reorder(region,rate),rate))+\n  geom_boxplot(fill = \"grey40\", color = \"blue\", alpha = 0.5)+\n  ggtitle(\"Không sử dụng scale\")+\n  theme_minimal()+xlab(\"\")\n\n# Sử dụng tham số labels cho trục x\n# và sử dụng limits cho trục y\np2<-murders%>%mutate(rate = total/population*10^6)%>%\n  ggplot(aes(reorder(region,rate),rate))+\n  geom_boxplot(fill = \"grey40\", color = \"blue\",alpha = 0.5)+\n  scale_y_continuous(limits = c(0,50))+\n  # Thay thế giá trị hiển thị trên trục số bằng labels\n  scale_x_discrete(labels = c(\"Northeast\" = \"Đông Bắc\",\n                              \"West\" = \"Miền Tây\",\n                              \"South\" = \"Miền Nam\",\n                              \"North Central\" = \"Miền Bắc\"))+\n  ggtitle(\"Sử dụng limits và labels\")+\n  theme_minimal()+xlab(\"\")\ngrid.arrange(p1,p2,nrow=1,ncol=2)"},{"path":"trực-quan-hóa-dữ-liệu.html","id":"màu-sắc-hiển-thị","chapter":"Chương 7 Trực quan hóa dữ liệu","heading":"7.4.2 Màu sắc hiển thị","text":"Thuộc tính thẩm mỹ được sử dụng phổ biến nhất là màu sắc. Có nhiều cách để ánh xạ giá trị của biến tới màu sắc trong thư viện \\(\\textbf{ggplot2}\\). Vì màu sắc là một chủ đề phức tạp nên chúng tôi sẽ bắt đầu bằng thảo luận sơ lược về lý thuyết màu sắc. Sau đó chúng tôi sẽ giới thiệu đến bạn đọc về thang màu liên tục, thang màu rời rạc và thang màu tổng hợp được sử dụng để ánh xạ đến các biến rời rạc, biến liên tục trong trực quan hóa dữ liệu. Chúng tôi cũng sẽ đề cập đến cả các thang màu dành cho biến kiểu thời gian, kiểu ngày tháng, độ trong của các màu sắc hiển thị và nguyên tắc chú giải cho màu sắc được thiết lập trong các đồ thị \\(\\textbf{ggplot2}\\).","code":""},{"path":"trực-quan-hóa-dữ-liệu.html","id":"cảm-nhận-của-con-người-về-màu-sắc","chapter":"Chương 7 Trực quan hóa dữ liệu","heading":"7.4.2.1 Cảm nhận của con người về màu sắc","text":"Trong vật lý, màu sắc được tạo ra bởi hỗn hợp các bước sóng ánh sáng. Để mô tả đầy đủ về một màu sắc, chúng ta cần biết sự kết hợp chính xác của các bước sóng. Sự thật thì mắt con người chỉ có ba cơ quan cảm nhận màu sắc khác nhau, và vì vậy chúng ta có thể tóm tắt khả năng cảm nhận về bất kỳ màu nào chỉ bằng ba con số. Một không gian màu quen thuộc với bạn đọc là không gian màu RGB, không gian mà mọi màu sắc được xác định theo cường độ ánh sáng đỏ, xanh da trời và xanh lá cây để tạo ra màu đó. Ưu điểm của không gian màu này là sự đơn giản mỗi màu sắc đều được mô tả bằng ba con số từ 0 đến 255 hoàn toàn độc lập với nhau. Một vấn đề với không gian này là các dải màu liên tục nhận được bằng cách tăng giảm các cường độ màu đỏ, xanh lá cây, xanh dương lại không giống như cách nhận thức về màu sắc của còn người. Khi nhìn vào một màu cụ thể, chúng ta không thể ước tính được cường độ mỗi màu là bao nhiêu, điều này có thể gây khó khăn cho việc tạo ánh xạ từ một biến liên tục sang một dải màu.Mỗi khi hiển thị một giá trị màu sắc trong không gian RGB, R thường sử dụng ký tự có 6 chữ số trong hệ 16 (từ 0 đến F) và bắt đầu bằng một dấu ‘#’ thay vì một véc-tơ ba chiều đại diện cho 3 sắc đỏ, xanh lá cây, và xanh dương. Hai chữ số đầu đại diện cho sắc đỏ, 2 chữ số tiếp theo đại diện cho màu xanh lá cây và 2 chữ số cuối đại diện cho màu xanh dương. Chẳng hạn như “#FF0000” sẽ là màu đỏ, “#00FF00” là màu xanh lá cây và “#0000FF” là màu xanh dương.Một không gian màu được chuyển đổi từ không gian RGB là không gian Lab trong đó L đại diện cho độ tương phản sáng-tối của màu sắc, trục tọa độ và b cho biết các vị trí của màu trên hai trục: trục từ đỏ đến xanh dương và trục từ vàng đến xanh lá. Cải tiến từ không gian RGB sang không gian Lab giúp cho các dải màu sắc tương ứng hơn với khả năng nhận biết màu sắc của con người, tuy nhiên vẫn còn khoảng cách giữa không gian Lab với nhận thức màu sắc. Không gian Lab cũng có các ưu điểm riêng, đó \\(\\textbf{ggplot2}\\) mặc định sử dụng không gian Lab khi nội suy tuyến tính các màu sắc nằm giữa hai màu bất kỳ khi chúng ta ánh xạ một biến liên tục lên thuộc tính thẩm mỹ màu sắc.Một không gian màu khác có thể hạn chế vấn đề của không gian RGB là không gian màu HCL với ba thành phần màu: màu sắc (Hue), sắc độ (Chroma) và độ chói (Luminance):Màu sắc (Hue) nằm trong khoảng từ 0 đến 360 (một góc) và cho biết màu muốn hiển thị.Màu sắc (Hue) nằm trong khoảng từ 0 đến 360 (một góc) và cho biết màu muốn hiển thị.Sắc độ là “độ tinh khiết” của một màu, nằm trong khoảng từ 0 (xám) đến mức tối đa thay đổi theo độ sáng.Sắc độ là “độ tinh khiết” của một màu, nằm trong khoảng từ 0 (xám) đến mức tối đa thay đổi theo độ sáng.Độ sáng là độ sáng của màu, dao động từ 0 (đen) đến 1 (trắng).Độ sáng là độ sáng của màu, dao động từ 0 (đen) đến 1 (trắng).Ba chiều có những đặc tính khác nhau. Tương tự như không gian màu Lab, màu sắc trong HCL được sắp xếp xung quanh một hình tròn và không được coi là có trật tự; ví dụ: màu xanh lá dường như không lớn hơn hay nhỏ hơn màu đỏ và màu xanh dương dường như không lớn hơn hay nhỏ hơn màu xanh lá. Ngược lại, cả sắc độ và độ sáng đều được coi là có trật tự: màu hồng được coi là nằm giữa màu đỏ và trắng, và màu xám được coi là nằm giữa màu đen và trắng. Tạo các thang màu sắc từ không gian HCL thường được dựa trên nguyên tắc cố định 2 tham số và thay đổi tham số còn lại. không gian màu HCL gần với nhận thức màu sắc của con người hơn nên các dải màu được tạo ra sẽ “cách đều” nhau hơn theo cách mà chúng ta nhận thức.Xin được nhắc lại rằng màu sắc là một chủ để phức tạp mà phạm vi của nó vượt rất xa những gì mà chúng tôi đề cập ở trên. Bạn đọc nên tham khảo thêm các tài liệu chuyên ngành khoa học máy tính để có thể sử dụng màu sắc một cách hiệu quả nhất.","code":""},{"path":"trực-quan-hóa-dữ-liệu.html","id":"dải-màu-liên-tục","chapter":"Chương 7 Trực quan hóa dữ liệu","heading":"7.4.2.2 Dải màu liên tục","text":"Dải màu liên tục được sử dụng để hiển thị giá trị của một biến liên tục trên bề mặt phẳng. Để kiểm soát màu sắc trong thư viện \\(\\textbf{ggplot2}\\), chúng ta sử dụng các hàm scale_color_*(). Lưu ý rằng các thuộc tính thẩm mỹ color và fill có thể được sử dụng song song với đa số các hình dạng đồ họa, đó bất kỳ hàm scale_color_*() cũng có hàm scale_fill_*() tương ứng.Dải màu liên tục thường được sử dụng cùng với các geom có hình dạng đồ họa cần màu sắc để phân biệt trên trên mặt phẳng như geom_polygon(), geom_tile() (hoặc geom_raster()), và geom_bin2d(). Mỗi khi chúng ta cho một biến liên tục ánh xạ đến thuộc tính thẩm mỹ màu sắc, thư viện \\(\\textbf{ggplot2}\\) sẽ tự động hiểu rằng chúng ta sử dụng dải màu liên tục để mô tả biến đó.Chúng ta sẽ làm quen với các dải màu liên tục thông qua trực quan hóa hàm mật độ của biến ngẫu nhiên phân phối chuẩn hai chiều với trung bình 0, phương sai 1 và hệ số tương quan \\(\\rho = 0.8\\). Lưu ý rằng hàm mật độ của hai biến ngẫu nhiên phân phối chuẩn \\(\\mathcal{N}(0,1)\\) với hệ số tương quan \\(\\rho = 0.8\\) được tính như sau\n\\[\\begin{align}\nf(x,y) = \\cfrac{1}{2 \\pi \\sqrt{1-\\rho^2}} \\ \\exp \\left(- \\cfrac{x^2 + y^2 - 2\\rho x y}{1-\\rho^2}  \\right)\n\\end{align}\\]Đồ thị hàm mật độ của biến ngẫu nhiên phân phối chuẩn hai chiều được lưu trong đối tượng có tên p như dưới đây:Phương pháp đơn giản nhất để kiểm soát ánh xạ thẩm mỹ từ một biến liên tục đến màu sắc là lựa chọn các dải màu liên tục có sẵn trong của thư viện \\(\\textbf{ggplot2}\\) hoặc có trong các thư viện bổ sung. Các dải màu có sẵn này đều được xây dựng để ngay cả những người gặp khó khăn trong phân biệt màu sắc cũng có thể cảm nhận được. Trong Hình 7.32 vẽ chúng tôi lựa chọn các dải màu sau: 1. Dải màu liên tục mặc định của thư viện \\(\\textbf{ggplot2}\\), 2. Dải màu liên tục viridis, 3. Dải màu liên tục distiller; và 4. Dải màu liên tục fermenter. Mỗi dải màu đều có tham số palette để lựa chọn dải các dải màu sắc phù hơpj\nFigure 7.33: Hàm mật độ của biến ngẫu nhiên phân phối chuẩn hai chiều với hệ số tương quan bằng 0.8. Hình góc trên bên trái: dải màu liên tục mặc định của ggplot2. Hình góc trên bên phải: dải màu viridis với option = ‘’. Hình góc dưới bên trái: dải màu liên tục distiller với dải màu số 3. Hình góc dưới bên phải: dải màu liên tục fermenter với dải màu liên tục số 2\nĐể dải màu sắc liên tục có tính cá nhân hóa cao hơn, bạn đọc cần chỉ định thang màu sắc thay vì sử dụng các thang màu có sẵn như Hình 7.33. Các hàm scale_*_gradient() là một công cụ mạnh mẽ giúp bạn đọc thực hiện việc này. Bạn cần cung cấp các giá trị màu sắc tương ứng với giá trị bắt đầu của dải màu, giá trị kết thúc của dải màu, và có thể thêm một vài giá trị trung gian, thư viện \\(\\textbf{ggplot2}\\) sẽ nội suy tuyến tính ra các màu sắc thành một dải màu tương ứng với các giá trị mà bạn khai báo. Các hàm số có thể sử dụng để tạo dải màu liên tục bao gồm:scale_fill_gradient() tạo một thang màu liên tục giữa hai màu sắc mà bạn khai báo. Hai tham số được sử dụng để khai báo là giá trị bắt đầu và giá trị kết thúc của dải màu là tham số low và tham số high. Đây cũng chính là cách tạo dải màu liên tục mặc định của thư viện \\(\\textbf{ggplot2}\\): mỗi khi chúng ta sử dụng ánh xạ thẩm mỹ từ một biến liên tục đến thuộc tính màu sắc, thư viện \\(\\textbf{ggplot2}\\) sử dụng dải màu liên tục theo hàm số scale_fill_gradient() với giá trị tham số low là \\(\"#132B43\"\\) và giá trị tham số high là \\(\"#56B1F7\"\\). Không gian nội suy tuyến tính dải màu là không gian màu Lab.scale_fill_gradient() tạo một thang màu liên tục giữa hai màu sắc mà bạn khai báo. Hai tham số được sử dụng để khai báo là giá trị bắt đầu và giá trị kết thúc của dải màu là tham số low và tham số high. Đây cũng chính là cách tạo dải màu liên tục mặc định của thư viện \\(\\textbf{ggplot2}\\): mỗi khi chúng ta sử dụng ánh xạ thẩm mỹ từ một biến liên tục đến thuộc tính màu sắc, thư viện \\(\\textbf{ggplot2}\\) sử dụng dải màu liên tục theo hàm số scale_fill_gradient() với giá trị tham số low là \\(\"#132B43\"\\) và giá trị tham số high là \\(\"#56B1F7\"\\). Không gian nội suy tuyến tính dải màu là không gian màu Lab.Một hàm số khác cũng được dùng để tạo một dải màu liên tục là scale_fill_gradient2(). Ngoài hai tham số low và high tương ứng với là hai điểm bắt đầu và kết thúc của thang màu, chúng ta cần khai báo thêm một màu ở giữa bằng tham số mid. Ngoài ra, chúng ta cần khai báo tham số midpoint để cho biết giá trị nào của biến liên tục tương ứng với màu được khai báo với tham số \\(mid\\). Nếu không khai báo, tham số midpoint sẽ nhận giá trị mặc định là 0.Một hàm số khác cũng được dùng để tạo một dải màu liên tục là scale_fill_gradient2(). Ngoài hai tham số low và high tương ứng với là hai điểm bắt đầu và kết thúc của thang màu, chúng ta cần khai báo thêm một màu ở giữa bằng tham số mid. Ngoài ra, chúng ta cần khai báo tham số midpoint để cho biết giá trị nào của biến liên tục tương ứng với màu được khai báo với tham số \\(mid\\). Nếu không khai báo, tham số midpoint sẽ nhận giá trị mặc định là 0.Sau cùng, hàm scale_fill_gradientn() tạo một thang màu liên tục từ một véc-tơ chứa các màu sắc mà bạn đọc khai báo. Dải màu này bắt đầu từ màu sắc có vị trí đầu tiên trong véc-tơ, đi qua lần lượt các màu sắc được khai báo, và kết thúc ở màu sắc tương ứng với giá trị cuối cùng của véc-tơ.Sau cùng, hàm scale_fill_gradientn() tạo một thang màu liên tục từ một véc-tơ chứa các màu sắc mà bạn đọc khai báo. Dải màu này bắt đầu từ màu sắc có vị trí đầu tiên trong véc-tơ, đi qua lần lượt các màu sắc được khai báo, và kết thúc ở màu sắc tương ứng với giá trị cuối cùng của véc-tơ.\nFigure 7.34: Hàm mật độ của biến ngẫu nhiên phân phối chuẩn hai chiều với hệ số tương quan bằng 0.8. Hình góc trên bên trái: dải màu liên tục mặc định của ggplot2. Hình góc trên bên phải: dải màu liên tục bắt đầu từ xanh dương kết thúc ở đỏ. Hình góc dưới bên trái: dải màu liên tục bắt đầu tư xanh dương đi qua màu trắng và kết thúc ở màu đỏ. Hình góc dưới bên phải: dải màu liên tục bắt đầu từ xanh lá cây đi qua trắng, xanh dương và kết thúc ở màu vàng\nCả ba hàm số kể trên đều nội suy tuyến tính trong không gian màu Lab để tạo ra các giải màu liên tục. Khi nói đến nội suy tuyến tính giữa hai màu sắc, sẽ dễ hiểu nếu chúng ta sử dụng không gian RGB mà tất cả các màu đều nằm trong một hình lập phương với điểm (0,0,0) là màu đen, (1,1,1) là màu trắng… Bạn đọc có thể hiểu như sau: trong mỗi không gian mỗi màu sắc hiển thị có ba thành phần là cường độ màu đỏ (r), cường độ màu xanh lá (g) cường độ màu xanh lam (b) … Một dải màu bao gồm \\(n\\) màu, bắt đầu từ màu \\(m_1\\) bao gồm các thành phần \\((r_1, g_1, b_1)\\), đến màu \\(m_n\\) với thành phần \\((r_n, g_n, b_n)\\) sẽ là các màu \\(m_i\\) có các thành phần tương ứng\n\\[\\begin{align}\nr_i = \\left[r_1 + (-1) * \\cfrac{r_n - r_1}{(n-1)} \\right] \\\\\ng_i = \\left[g_1 + (-1) * \\cfrac{g_n - g_1}{(n-1)} \\right] \\\\\nb_i = \\left[b_1 + (-1) * \\cfrac{b_n - b_1}{(n-1)} \\right]\n\\end{align}\\]Đáng tiếc là trong không gian Lab việc nội suy màu sắc không đơn giản như vậy. Việc nội suy dựa trên các tính toán phức tạp và kết quả cuối cùng là các công thức gần đúng. Ưu điểm của nội suy màu sắc trong không gian Lab với không gian RGB sự chuyển đổi màu sắc giữa các điểm mượt mà hơn rất nhiều trong cách nhận biết màu sắc của con người.Hàm số để nội suy một véc-tơ màu rời rạc từ hai màu sắc bất kỳ trên không gian RGB hoặc không gian Lab là hàm colorRampPalette() của thư viện \\(\\textbf{grDevices}\\). Ví dụ, chúng ta nội suy một véc-tơ màu bắt đầu từ màu xanh dương và kết thúc ở màu cam như sauĐể sánh cách nội suy dải màu trên không gian RGB và không gian Lab, chúng ta sẽ sử dụng hai dải màu kể trên mô tả hàm mật độ xác suất của biến phân phối chuẩn hai chiều. các hàm scale_fill_gradient() luôn nội suy trên không gian Lab nên để hiển thị dải màu RGB, chúng tôi rời rạc hóa giá trị mật độ hàm mật độ trước khi ánh xạ đến dải màu rời rạc được tạo ra từ hàm colorRampPalette()\nFigure 7.35: Hàm mật độ của biến ngẫu nhiên phân phối chuẩn hai chiều với hệ số tương quan bằng 0.8. Hình bên trái: dải màu nội suy trên không gian RGB. Hình bên phải: dải màu nội suy trên không gian Lab\nCả hai hình đều sử dụng dải màu liên tục từ xanh dương đến cam để mô tả mật độ của phân phối chuẩn hai chiều có hệ số tương quan \\(\\rho=0.8\\). Bạn đọc có thể thấy rằng việc chuyển hóa màu sắc trên không gian màu Lab tự nhiên với mắt quan sát hơn với không gian RGB. Đây là lý tại sao các dải màu liên tục của thư viện \\(\\textbf{ggplot2}\\) luôn sử dụng không gian Lab để nội suy màu sắc.Các tham số limits, breaks, và label cũng có thể được sử dụng trong các hàm scale_fill_*() và scale_color_*() để kiểm soát các thang màu liên tục. Tham số limits nhận giá trị là một véc-tơ hai phần tử, phần tử thứ nhất cho biết màu sắc bắt đầu trong thang màu tương ứng với giá trị nào trong biến liên tục và phần tử thứ hai cho biết màu sắc kết thúc của thang màu ứng với giá trị nào của biến liên tục. Trong khi các tham số breaks và labels được sử dụng để thay đổi giá trị trên thang màu của chú giải.\nFigure 7.36: Hàm mật độ của biến ngẫu nhiên phân phối chuẩn hai chiều với hệ số tương quan bằng 0.8. Hình bên trái: sử dụng tham số limits trong scale_fill_gradient. Hình bên phải: sử dụng limits, breaks và labels trong scale_fill_gradient\nĐồ thị bên trái của Hình 7.36 sử dụng giá trị của tham số limits là từ 0 đến 0.8 trong khi giá trị lớn nhất của các hàm mật độ chỉ khoảng 0.3. Màu bắt đầu của dải màu là màu xanh dương tương ứng với giá trị thứ nhất của tham số limits là 0 và màu kết thúc của dải màu là màu cam tương ứng với giá trị thứ hai của tham số limit là 0.8. Điều này giải thích tại sao cả các giá trị nằm trong tâm của hình ellipse màu sắc cũng chưa chuyển thành màu cam. Đồ thị bên phải của Hình 7.36 sử dụng tham số limits từ 0 đến 0.3 nên các giá trị càng nằm gần tâm đường ellipse càng chuyển sang màu cam. Tham số breaks thay đổi các vị trí giải thích thang màu trên chú giải của thuộc tính màu sắc, trong khi tham số labels kiểm soát cách hiển thị tại các vị trí trên thang màu.","code":"\n# tạo lưới điểm trên hình vuông [-2,2] * [2-,2]\nn<-100\nx<-rep(1:n,n)/n*4-2\ny<-sort(x, decreasing = FALSE)\nrho<-0.8\ndat0<-data.frame(x=x,y=y,\n                dens = 1/(2*pi*sqrt(1-rho^2)) * \n                  exp(-(x^2+y^2-2*rho*x*y)/(1-rho^2)))\np<-dat0%>%ggplot(aes(x,y,fill=dens))+geom_raster()+\n  theme_minimal()\n# tạo lưới điểm trên hình vuông [-2,2] * [2-,2]\np1<-p + scale_fill_continuous()+\n  ggtitle(\"Màu mặc định\") # sử dụng dải màu mặc định\np2<-p + scale_fill_viridis_c(option = \"A\")+ # Dải màu viridis liên tục\n  ggtitle(\"Dải màu viridis\")\np3<-p + scale_fill_distiller(palette = 3)+ # Dải màu distiller\n  ggtitle(\"Dải màu distiller\")\np4<-p + scale_fill_fermenter(palette = 2)+ # Dải màu fermenter\n  ggtitle(\"Dải màu fermenter\")\ngrid.arrange(p1,p2,p3,p4,nrow=2,ncol=2)\np1<-p + ggtitle(\"Màu mặc định\") # sử dụng dải màu mặc định\np2<-p + scale_fill_gradient(low = \"blue\", high = \"red\")+\n  ggtitle(\"Dải màu từ xanh lam đến đỏ\") # sử dụng dải màu từ xanh lam đến đỏ\np3<-p + scale_fill_gradient2(low = \"blue\", high = \"red\", mid = \"white\", midpoint = 0.12)+\n  ggtitle(\"Dải màu từ xanh lam đến đỏ điểm giữa là trắng\")\np4<-p +  scale_fill_gradientn(colours = c(\"#00FF00\",\"#FFFFFF\",\"#0000FF\", \"#FFFF00\"))+\n  ggtitle(\"Dải màu đi qua nhiều điểm màu\")\ngrid.arrange(p1,p2,p3,p4,nrow=2,ncol=2)\nn<-200\nmy_rgb_palette<-colorRampPalette(c(\"blue\", \"orange\"),space = \"rgb\")(n+1)\nmy_lab_palette<-colorRampPalette(c(\"blue\", \"orange\"),space = \"Lab\")(n+1)\nx<-rep(1:n,n)/n*4-2\ny<-sort(x, decreasing = FALSE)\nrho<-0.8\ndat0<-data.frame(x=x,y=y,dens = 1/(2*pi*sqrt(1-rho^2)) * exp(-(x^2+y^2-2*rho*x*y)/(1-rho^2)))\nh<-(max(dat0$dens)-min(dat0$dens))/n\ndat0<-mutate(dat0,dens.d = round((dens - min(dat0$dens))/h))\ndat0$dens.d<-as.factor(dat0$dens.d)\n\nmycol<-colorRampPalette(c(\"blue\", \"orange\"),space = \"rgb\")(n+1)\np1<-dat0%>%ggplot(aes(x,y,fill=dens.d))+geom_raster()+theme_minimal()+\n  scale_fill_manual(values= mycol)+\n  theme(legend.position = \"none\")+\n  ggtitle(\"Nội suy trên RGB\")\n\nmycol<-colorRampPalette(c(\"blue\", \"orange\"),space = \"Lab\")(n+1)\np2<-dat0%>%ggplot(aes(x,y,fill=dens.d))+geom_raster()+theme_minimal()+\n  scale_fill_manual(values= mycol)+\n  theme(legend.position = \"none\")+\n  ggtitle(\"Nội suy trên Lab\")\ngrid.arrange(p1,p2,ncol=2,nrow=1)\n# limits cho biết hai giá trị tương ứng với điểm đầu và cuối của dải màu\np1<-p + scale_fill_gradient(low = \"blue\", high = \"orange\",\n                            limits = c(0,0.8))+\n  ggtitle(\"Tham số limits\")\n# breaks cho biết các giá trị nào xuất hiện trên chú giải\n# labels cho biết giá trị hiển thị trong chú giải\np2<-p + scale_fill_gradient(low = \"blue\", high = \"orange\",\n                            limits = c(0,0.3),\n                            breaks = c(0.1,0.15,0.25),\n                            labels = paste(\"Density at\", c(0.1,0.15,0.25)))+\n  ggtitle(\"Tham số breaks và labels\")\ngrid.arrange(p1,p2,nrow=1,ncol=2)"},{"path":"trực-quan-hóa-dữ-liệu.html","id":"dải-màu-rời-rạc","chapter":"Chương 7 Trực quan hóa dữ liệu","heading":"7.4.2.3 Dải màu rời rạc","text":"Dải màu rời rạc dùng để mô tả thuộc tính thẩm mỹ màu sắc khi ánh xạ đến các biến rời rạc. Hàm số dùng để kiểm soát màu sắc rời rạc của thư viện \\(\\textbf{ggplot2}\\) là các hàm scale_fill_discrete() và scale_color_discrete(). Mỗi khi sử dụng các hàm số kể trên, thư viện \\(\\textbf{ggplot2}\\) sẽ mặc định sử dụng dải màu rời rạc cách đều nhau trong không gian màu HCL. Dải màu mặc định này có cùng sắc độ (Chromes hay còn gọi là tham số \\(c\\)), độ sáng (Luminance hay còn gọi là tham số \\(l\\)) và giá trị \\(h\\) cách đều nhau từ góc 15 độ (\\(h\\) nhận giá trị từ 0 đến 360 độ). Bạn đọc muốn sử dụng các dải màu rời rạc trong không gian HCL để ánh xạ tới biến rời rạc thì có thể sử dụng các hàm scale_fill_hue() và scale_color_hue() thay thể cho scale_fill_discrete() và scale_color_discrete().\nFigure 7.37: Mô tả phân phối của biến continent trong dữ liệu Gapminder năm 2011. Hình bên trái: Sử dụng dải màu rời rạc mặc định. Hình ở giữa: sử dụng dải màu rời rạc trong không gian HCL với tham số h thay đổi. Hình bên phải: sử dụng dải màu rời rạc trong không gian HCL với tham số c thay đổi\nDải màu mặc định đối với biến rời rạc sử dụng tham số \\(c\\) bằng 100 và tham số \\(l\\) bằng 65 trong khi tham số \\(h\\) nhận các giá trị cách đều nhau, bắt đầu từ \\(h = 15\\) (độ). Lưu ý rằng \\(h\\) nhận giá trị từ 0 độ đến 360 độ nên trong trường hợp biến rời rạc có năm giá trị, các màu sắc sẽ lần lượt nhận các giá trị \\(h = 15\\), \\(15 + 360/5\\), \\(15 + 2 \\times 360/5\\), \\(15 + 3 \\times 360/5\\) và \\(15 + 4 \\times 360/5\\). Đó là màu sắc của các thanh trong đồ thị bên trái của Hình 7.37 theo thứ tự từ trái qua phải. Trong đồ thị ở giữa, khi chúng ta sử dụng tịnh tiến giá trị \\(h\\) thêm 360/5 (độ), chúng ta có thể thấy các màu sắc bắt đầu từ \\(h = 15 + 360/5\\) và kết thúc ở \\(h = 15\\). Nghĩa là màu sắc thứ nhất trong đồ thị bên trái đã trở thành màu thứ năm trong đồ thi ở giữa. Trong đồ thị bên phải, chúng tôi giảm độ chói (tham số \\(c\\)) xuống còn 40. Chúng ta có thể thấy dải màu vẫn tương tự như hai đồ thị còn lại nhưng không đạt được độ sáng như vậy.Bạn đọc cũng có thể sử dụng các dải màu rời rạc được thiết kế sẵn cho mục đích trực quan hóa các biến rời rạc. Dải màu rời rạc mà chúng tôi thường sử dụng là dải màu brewer. Những dải màu này được thiết kế để hoạt động tốt trong nhiều tình huống khác nhau kể cả đối với những người khó khăn khi nhận biết màu sắc hay khi sử dụng để hiển thị trên những bề mặt lớn. Hàm số để kiểm soát ánh xạ thẩm mỹ màu sắc sử dụng dải màu brewer là scale_color_brewer() và scale_fill_brewer(). Bạn đọc cần cài đặt thư viện \\(\\textbf{RColorBrewer}\\) để sử dụng được các hàm này. Để xem các dải màu có sẵn trong thư viện này, bạn đọc sử dụng câu lệnh sauTham số palette trong hàm scale_color_brewer() được sử dụng để lựa chọn dải màu:\nFigure 7.38: Mô tả phân phối của biến continent trong dữ liệu Gapminder năm 2011 sử dụng dải màu brewer. Hình bên trái: sử dụng dải màu Dark2. Hình ở giữa: sử dụng dải màu Set1. Hình bên phải: sử dụng dải màu Spectral\nĐể tạo ra dải màu rời rạc theo ý muốn bạn đọc sử dụng các hàm scale_fill_manual() và scale_color_manual(). Tham số values được sử dụng để nhận giá trị là véc-tơ chứa màu sắc mà bạn đọc tự tạo. Số lượng phần tử trong véc-tơ phải tương ứng với số lượng phần tử trong biến rời rạc.Như chúng tôi đã giới thiệu trong phần dải màu sắc liên tục, hàm số colorRampPalette() của thư viện \\(\\textbf{grDevices}\\) có thể được sử dụng để nội suy ra một véc-tơ màu rời rạc giữa hai giá trị màu cho trước. Ví dụ, để tạo ra một véc-tơ có độ dài 5, mỗi giá trị là một màu sắc được nội suy tuyến tính từ màu xanh dương đến màu cam chúng ta sử dụng colorRampPalette() như sau:Các đồ thị trong Hình sử dụng các véc-tơ màu sắc rời rạc tự định nghĩa bằng cách liệt kê các màu sắc trong scale_fill_manual() và bằng nội suy tuyến tính trong không gian RGB và không gian Lab.\nFigure 7.39: Mô tả phân phối của biến continent trong dữ liệu Gapminder năm 2011 sử dụng dải màu tự định nghĩa. Hình bên trái: dải màu tự định nghĩa bằng cách liệt kê tên các màu sắc. Hình ở giữa: nội suy trong không gian RGB giữa xanh dương và cam. Hình bên phải: nội suy trong không gian Lab giữa xanh dương và cam\nCách sử dụng tham số limits, breaks, và label trong các hàm scale_fill_manual() và scale_color_manual() cũng tương tự như đối với dải màu liên tục.Tham số limits cho biết các giá trị nào trong biến rời rạc được ánh xạ tới dải màu sắc.Tham số limits cho biết các giá trị nào trong biến rời rạc được ánh xạ tới dải màu sắc.Tham số breaks cho biết các giá trị nào không được sử dụng trong ánh xạ thẩm mỹ.Tham số breaks cho biết các giá trị nào không được sử dụng trong ánh xạ thẩm mỹ.Tham số label cho biết cách các màu sắc hiển thị trong phần chú giải.Tham số label cho biết cách các màu sắc hiển thị trong phần chú giải.Theo kinh nghiệm của chúng tôi thì tham số breaks không có nhiều ý nghĩa khi sử dụng đối với dải màu sắc rời rạc, trong khi tham số limits có ý nghĩa quan trọng khi bạn đọc cần cố định ánh xạ màu sắc lên biến rời rạc khi vẽ nhiều biểu đồ khác nhau và để kiểm soát thứ tự xuất hiện của biến liên tục trên đồ thị.\nFigure 7.40: Dân số của ba nước Philippines, Việt Nam, và Indonesia trong top 10 nước đông dân nhất châu Á. Hình bên trái: Dữ liệu năm 1960. Hình bên phải: Dữ liệu năm 2011\nChúng ta sử dụng tham số limits để cố định màu sắc tương ứng với các giá trị của biến rời rạc như trong Hình 7.40. Bạn đọc có thể dễ dàng nhận ra sự thay đổi về thứ hạng về quy mô dân số của ba 3 quốc gia Philippines, Việt Nam, và Indonesia trong nhóm 10 nước có dân số lớn nhất châu Á trong các năm 1960 và 2010.","code":"\np1<-gapminder%>%filter(year==2011)%>%\n  ggplot(aes(continent,fill=continent))+\n  geom_bar()+ggtitle(\"Màu rời rạc mặc định\")+\n  theme_minimal()\np2<-gapminder%>%filter(year==2011)%>%\n  ggplot(aes(continent,fill=continent))+geom_bar()+\n  scale_fill_hue(h=c(0,360)+15+360/5)+\n  ggtitle(\"Thay đổi tham số h\")+theme_minimal()\np3<-gapminder%>%filter(year==2011)%>%\n  ggplot(aes(continent,fill=continent))+geom_bar()+\n  scale_fill_hue(c=30)+ggtitle(\"Thay đổi tham số c\")+theme_minimal()\ngrid.arrange(p1,p2,p3,nrow=1,ncol=3)\ndisplay.brewer.all()\np1<-gapminder%>%filter(year==2011)%>%\n  ggplot(aes(continent,fill=continent))+geom_bar()+\n  scale_fill_brewer(palette = \"Dark2\")+\n  ggtitle(\"Sử dụng dải màu Dark2\")+\n  theme_minimal()\np2<-gapminder%>%filter(year==2011)%>%\n  ggplot(aes(continent,fill=continent))+geom_bar()+\n  scale_fill_brewer(palette = \"Set1\")+\n  ggtitle(\"Sử dụng dải màu Set1\")+\n  theme_minimal()\np3<-gapminder%>%filter(year==2011)%>%\n  ggplot(aes(continent,fill=continent))+geom_bar()+\n  scale_fill_brewer(palette = \"Spectral\")+\n  ggtitle(\"Sử dụng dải màu Spectral\")+\n  theme_minimal()\ngrid.arrange(p1,p2,p3,nrow=1,ncol=3)\n# nội suy trong RGB\nmypalette1<-colorRampPalette(c(\"blue\",\"orange\"), space = \"rgb\")(5)\n# nội suy trong Lab\nmypalette2<-colorRampPalette(c(\"blue\",\"orange\"), space = \"Lab\")(5)\np1<-gapminder%>%filter(year==2011)%>%\n  ggplot(aes(continent,fill=continent))+geom_bar()+\n  scale_fill_manual(values = c(\"blue\",\"green\",\"grey\",\"yellow\",\"orange\"))+\n  ggtitle(\"Màu tự định nghĩa\")+\n  theme_minimal()\np2<-gapminder%>%filter(year==2011)%>%\n  ggplot(aes(continent,fill=continent))+geom_bar()+\n  scale_fill_manual(values = mypalette1)+\n  ggtitle(\"Màu nội suy trong RGB\")+\n  theme_minimal()\np3<-gapminder%>%filter(year==2011)%>%\n  ggplot(aes(continent,fill=continent))+geom_bar()+\n  scale_fill_manual(values = mypalette2)+\n  ggtitle(\"Màu nội suy trong Lab\")+\n  theme_minimal()\ngrid.arrange(p1,p2,p3,nrow=1,ncol=3)\np1<-gapminder%>%filter(year==1960, continent == \"Asia\")%>%\n  arrange(-population)%>%head(10)%>%\n  ggplot(aes(fill=country))+\n  geom_bar(aes(x = population, y = reorder(country,population)),\n           stat=\"identity\",col=\"blue\", alpha = 0.7)+\n  ylab(\"\")+ggtitle(\"Năm 1960\")+theme_minimal()+\n  theme(legend.position = \"top\")+\n  scale_x_continuous(labels = scales::comma)+\n  scale_fill_manual(values = c(\"blue\",\"red\",\"yellow\"), limits = c(\"Philippines\",\"Vietnam\", \"Indonesia\"))\np2<-gapminder%>%filter(year==2011, continent == \"Asia\")%>%\n  arrange(-population)%>%head(10)%>%\n  ggplot(aes(fill=country))+\n  geom_bar(aes(x = population, y = reorder(country,population)),\n           stat=\"identity\",col=\"blue\", alpha = 0.7)+\n  ylab(\"\")+ggtitle(\"Năm 2010\")+theme_minimal()+theme(legend.position = \"top\")+\n  scale_x_continuous(labels = scales::comma)+\n  scale_fill_manual(values = c(\"blue\",\"red\",\"yellow\"), limits = c(\"Philippines\",\"Vietnam\", \"Indonesia\"))\n\ngrid.arrange(p1,p2,nrow=1,ncol=2)"},{"path":"trực-quan-hóa-dữ-liệu.html","id":"các-thuộc-tính-thẩm-mỹ-khác","chapter":"Chương 7 Trực quan hóa dữ liệu","heading":"7.4.3 Các thuộc tính thẩm mỹ khác","text":"Ngoài vị trí trên các trục tọa độ và màu sắc, còn có một số thuộc tính thẩm mỹ khác mà thư viện \\(\\textbf{ggplot2}\\) có thể sử dụng để mô tả dữ liệu. Trong phần này, chúng ta sẽ xem xét thuộc tính kích thước (size), hình dạng (shape), chiều rộng của line và kiểu line, sử dụng cùng với các thuộc tính vị trí và màu sắc để thể hiện tốt nhất các biến trong dữ liệu. Ngoài đề cập đến các giá trị mặc định, chúng tôi cũng sẽ thảo luận về các hàm số để bạn đọc có thể sử dụng để kiểm soát tốt các thuộc tính này.","code":""},{"path":"trực-quan-hóa-dữ-liệu.html","id":"thuộc-tính-thẩm-mỹ-kích-thước","chapter":"Chương 7 Trực quan hóa dữ liệu","heading":"7.4.3.1 Thuộc tính thẩm mỹ kích thước","text":"Thuộc tính thẩm mỹ kích thước thường được sử dụng để mô tả hình dạng đồ họa kiểu điểm hoặc ký tự. Như chúng tôi đã đề cập trong phần giới thiệu, thuộc tính kích thước thường được sử dụng với biến liên tục. Nếu không có hàm kiểm soát ánh xạ thẩm mỹ, bán kính của điểm tương ứng với giá trị nhỏ nhất luôn là 1 và bán kính của điểm có giá trị lớn nhất luôn là 6, nghĩa là có bán kính gấp 6 lần bán kính của điểm nhỏ nhất. Khi nội suy ra kích thước của các điểm khác, thư viện \\(\\textbf{ggplot2}\\) mặc định cho kích thước của điểm là diện tích của hình tròn mô tả điểm đó chứ không phải đường kính của hình tròn. Kích thước của điểm sẽ phụ thuộc vào thứ hạng (rank) của giá trị đó trong biến liên tục chứ không được tính bằng giá trị thực của điểm đó. Nếu \\(area_m\\) là diện tích của hình tròn tương ứng với giá trị nhỏ nhất và \\(area_M\\) tương ứng với diện tích của hình tròn tương ứng với giá trị lớn nhất thì diện tích của hình tròn tương ứng với giá trị có thứ hạng \\(k\\) trong tổng số \\(n\\) giá trị của biến liên tục là\n\\[\\begin{align}\narea = area_m + (k-1) \\times \\cfrac{area_M - area_m}{n - 1}\n\\end{align}\\]Để hiểu về cách thư viện \\(\\textbf{ggplot2}\\) ánh xạ kích thước đến giá trị các biến, bạn đọc có thể quan sát kích thước của các hình tròn trong Hình 7.41\nFigure 7.41: Ánh xạ các số tự nhiên đến thuộc tính thẩm mỹ kích thước. Hình bên trái: ánh xạ các số 1, 2, 3 đến kích thước ba hình tròn. Hình ở giữa: ánh xạ bình phương của 1, 2, 3 đến kích thước các hình tròn. Hình bên phải: ánh xạ hai số 2 và 3 đến kích thước của hai hình tròn.\nTừ Hình 7.41 có thể thấy rằng:Đồ thị bên trái: kích thước của các hình tròn ở các tọa độ (1,1), (2,2), và (3,3) được ánh xạ đến các giá trị số 1, 2, và 3. Tham số mặc định của hàm scale_size_*() là range = c(1,6) nên hình tròn tại vị trí (1,1) có bán kính là 1 trong khi hình tròn ở vị trí (3,3) có bán kính là 6. Diện tích của hình tròn nằm ở tọa độ (2,2) bằng trung bình cộng diện tích của hình tròn nằm ở vị trí (1,1) và (3,3). diện tích của hình tròn nằm ở vị trí (3,3) bằng \\(6^2 = 36\\) lần diện tích của hình tròn tại vị trí \\((1,1)\\) nên diện tích của hình tròn tại (2,2) bằng \\(\\cfrac{36+1}{2} = 18,5 \\textit{(lần)}\\) diện tích hình tròn tại tọa độ (1,1), hay nói cách khác đường kính của hình tròn tại vị trí (2,2) bằng \\(\\sqrt{18,5} \\sim 4,3 \\text{ (lần)}\\) đường kính của hình tròn tại vị trí (1,1).Đồ thị bên trái: kích thước của các hình tròn ở các tọa độ (1,1), (2,2), và (3,3) được ánh xạ đến các giá trị số 1, 2, và 3. Tham số mặc định của hàm scale_size_*() là range = c(1,6) nên hình tròn tại vị trí (1,1) có bán kính là 1 trong khi hình tròn ở vị trí (3,3) có bán kính là 6. Diện tích của hình tròn nằm ở tọa độ (2,2) bằng trung bình cộng diện tích của hình tròn nằm ở vị trí (1,1) và (3,3). diện tích của hình tròn nằm ở vị trí (3,3) bằng \\(6^2 = 36\\) lần diện tích của hình tròn tại vị trí \\((1,1)\\) nên diện tích của hình tròn tại (2,2) bằng \\(\\cfrac{36+1}{2} = 18,5 \\textit{(lần)}\\) diện tích hình tròn tại tọa độ (1,1), hay nói cách khác đường kính của hình tròn tại vị trí (2,2) bằng \\(\\sqrt{18,5} \\sim 4,3 \\text{ (lần)}\\) đường kính của hình tròn tại vị trí (1,1).Đồ thị ở giữa: chúng ta ánh xạ thuộc tính thẩm mỹ kích thước với \\(z^2\\), nghĩa là \\(1^2\\), \\(2^2\\), và \\(3^2\\) thì kích thước các hình tròn xuất hiện vẫn không hề thay đổi với hình bên trái. Nguyên nhân là thư viện \\(\\textbf{ggplot2}\\) sử dụng thứ hạng của các giá trị trong véc-tơ số chứ không sử dụng giá trị thực. Thứ hạng của \\(1^2\\), \\(2^2\\), và \\(3^2\\) vẫn là 1, 2, và 3 nên kích thước của các hình xuất hiện vẫn không thay đổiĐồ thị ở giữa: chúng ta ánh xạ thuộc tính thẩm mỹ kích thước với \\(z^2\\), nghĩa là \\(1^2\\), \\(2^2\\), và \\(3^2\\) thì kích thước các hình tròn xuất hiện vẫn không hề thay đổi với hình bên trái. Nguyên nhân là thư viện \\(\\textbf{ggplot2}\\) sử dụng thứ hạng của các giá trị trong véc-tơ số chứ không sử dụng giá trị thực. Thứ hạng của \\(1^2\\), \\(2^2\\), và \\(3^2\\) vẫn là 1, 2, và 3 nên kích thước của các hình xuất hiện vẫn không thay đổiĐồ thị bên phải: khi chúng ta chỉ vẽ hai điểm tại vị trí (2,2) và (3,3) với kích thước được ánh xạ vào hai số là 2 và 3 thì diện tích hình tròn nhỏ nhất và hình tròn lớn nhất vẫn không thay đổi.Đồ thị bên phải: khi chúng ta chỉ vẽ hai điểm tại vị trí (2,2) và (3,3) với kích thước được ánh xạ vào hai số là 2 và 3 thì diện tích hình tròn nhỏ nhất và hình tròn lớn nhất vẫn không thay đổi.Hàm số dùng để kiểm soát giá trị của ánh xạ thẩm mỹ kích thước là hàm scale_size(). Để thay đổi miền giá trị của thuộc tính kích thước, chúng ta sử dụng tham số range.\nFigure 7.42: Ánh xạ các số tự nhiên đến thuộc tính thẩm mỹ kích thước. Hình bên trái: ánh xạ các số 1, 2, 3 đến kích thước ba hình tròn. Hình bên phải: sử dụng scale_size để hình nhỏ nhất có bán kính bằng 6 và hình lớn nhất có bán kính bằng 24.\nHình 7.42 mô tả cách sử dụng tham số range trong hàm scale_size().Đồ thị bên trái: bán kính của hình nhỏ nhất là 1, của hình lớn nhất là 6.Đồ thị bên trái: bán kính của hình nhỏ nhất là 1, của hình lớn nhất là 6.Đồ thị bên phải: bán kính của hình nhỏ nhất là 6 bằng với kích thước của hình lớn nhất của đồ thị bên trái. Và bán kính của hình lớn nhất là 24. Mặc dù tham số khai báo với range là bán kính của các hình, nhưng khi nội suy kích thước các hình có kích thước nằm giữa hình nhỏ nhất và hình lớn nhất, scale_size() lại luôn nội suy theo diện tích của các hình.Đồ thị bên phải: bán kính của hình nhỏ nhất là 6 bằng với kích thước của hình lớn nhất của đồ thị bên trái. Và bán kính của hình lớn nhất là 24. Mặc dù tham số khai báo với range là bán kính của các hình, nhưng khi nội suy kích thước các hình có kích thước nằm giữa hình nhỏ nhất và hình lớn nhất, scale_size() lại luôn nội suy theo diện tích của các hình.Trong trường hợp bạn đọc muốn sử dụng nội suy theo bán kính của điểm thay vì diện tích, hãy sử dụng hàm scale_radius() thay thế cho scale_size(). Hình 7.43 mô tả sự khác nhau khi sử dụng scale_size() và scale_radius():\nFigure 7.43: Sự khác nhau giữa ánh xạ kích thước bằng scale_size và scale_radius. Hình bên trái: sử dụng scale_size với range = c(1,7). Hình ở giữa: sử dụng scale_radius với range = c(1,7). Hình bên phải: sử dụng scale_size với range = c(4,10)\nĐồ thị bên trái sử dụng scale theo diện tích và bán kính hình tròn lớn nhất bằng 7 lần đường tròn nhỏ; hình tròn ở giữa có bán kính bằng \\(\\sqrt{\\cfrac{7^2+1^2}{2}} = 5 \\text{ (lần)}\\) bán kính hình tròn nhỏ nhất.Đồ thị bên trái sử dụng scale theo diện tích và bán kính hình tròn lớn nhất bằng 7 lần đường tròn nhỏ; hình tròn ở giữa có bán kính bằng \\(\\sqrt{\\cfrac{7^2+1^2}{2}} = 5 \\text{ (lần)}\\) bán kính hình tròn nhỏ nhất.Đồ thị ở giữa, scale theo bán kính hình tròn nên hình ở giữa có bán kính bằng \\(\\cfrac{7+1}{2} = 4 \\textit{ (lần)}\\) bán kính hình tròn nhỏ. Bạn đọc có thể thấy rằng kích thước của hình tròn ở vị trí (2,2) trong đồ thị ở giữa nhỏ hơn hình tròn ở vị trí tương ứng trong hình bên trái.Đồ thị ở giữa, scale theo bán kính hình tròn nên hình ở giữa có bán kính bằng \\(\\cfrac{7+1}{2} = 4 \\textit{ (lần)}\\) bán kính hình tròn nhỏ. Bạn đọc có thể thấy rằng kích thước của hình tròn ở vị trí (2,2) trong đồ thị ở giữa nhỏ hơn hình tròn ở vị trí tương ứng trong hình bên trái.Trong đồ thị bên phải, bán kính của hình nhỏ nhất là 4, của hình tròn lớn nhất là 10, nên bán kính hình ở giữa là \\(\\cfrac{4+10}{2} = 7\\) nội suy bằng hàm scale_radius(). Bạn đọc có thể thấy rằng kích thước của hình ở vị trí (2,2) của đồ thị này bằng với kích thước của hình tròn ở vị trí (3,3) của hình ở giữa.Trong đồ thị bên phải, bán kính của hình nhỏ nhất là 4, của hình tròn lớn nhất là 10, nên bán kính hình ở giữa là \\(\\cfrac{4+10}{2} = 7\\) nội suy bằng hàm scale_radius(). Bạn đọc có thể thấy rằng kích thước của hình ở vị trí (2,2) của đồ thị này bằng với kích thước của hình tròn ở vị trí (3,3) của hình ở giữa.Các tham số limits, breaks, và label được sử dụng tương tự như thuộc tính thẩm mỹ màu sắc:Tham số limits cho biết miền giá trị nào của biến được ánh xạ đến thuộc tính thẩm mỹ size.Tham số limits cho biết miền giá trị nào của biến được ánh xạ đến thuộc tính thẩm mỹ size.Tham số breaks cho biết các giá trị nào của kích thước nào xuất hiện trên chú giải.Tham số breaks cho biết các giá trị nào của kích thước nào xuất hiện trên chú giải.Tham số labels mô tả thuộc tính thẩm mỹ kích thước trên chú giải của đồ thị.Tham số labels mô tả thuộc tính thẩm mỹ kích thước trên chú giải của đồ thị.\nFigure 7.44: Tuổi thọ trung bình và tỷ lệ trẻ sơ sinh tử vong của các quốc gia trên thế giới năm 2011\nHình 7.44 mô tả ba biến liên tục là tuổi thọ trung bình, tỷ lệ trẻ sơ sinh tử vong và dân số của các quốc gia trên thế giới vào năm 2011. Biến dân số được ánh xạ đến thuộc tính thẩm mỹ kích thước của các điểm. Ánh xạ này được điều chỉnh bằng hàm scale_size() như sauTham số range = c(1,12) cho biết bán kính của hình tròn tương ứng với nước có dân số nhỏ nhất bằng 1 và bán kính của hình tròn tương ứng với nước có dân số lớn nhất là 12.Tham số range = c(1,12) cho biết bán kính của hình tròn tương ứng với nước có dân số nhỏ nhất bằng 1 và bán kính của hình tròn tương ứng với nước có dân số lớn nhất là 12.Tham số limits cho biết chỉ các nước có dân số 10 triệu trở lên được đưa vào trong đồ thị.Tham số limits cho biết chỉ các nước có dân số 10 triệu trở lên được đưa vào trong đồ thị.Tham số breaks cho biết các giá trị xuất hiện trên chú giải là các giá trị 100 triệu, 200 triệu, 500 triệu và 1 tỷ.Tham số breaks cho biết các giá trị xuất hiện trên chú giải là các giá trị 100 triệu, 200 triệu, 500 triệu và 1 tỷ.Tham số labels cho biết các số viết trên chú giải sử dụng cách viết giá trị lên chú giải là theo đơn vị triệu.Tham số labels cho biết các số viết trên chú giải sử dụng cách viết giá trị lên chú giải là theo đơn vị triệu.","code":"\ndat0<-data.frame(x=1:3,y=1:3,z=1:3)\n# Hình bên trái\np1<-dat0%>%ggplot(aes(x,y,size=z))+geom_point(shape=21,fill= \"blue\",alpha = 0.5)+\n  theme(legend.position = \"none\")\n# Hình ở giữa\np2<-dat0%>%ggplot(aes(x,y,size=z^2))+geom_point(shape=21,fill= \"blue\",alpha = 0.5)+\n  theme(legend.position = \"none\")\n# Hình bên phải\np3<-dat0%>%filter(z>1)%>%ggplot(aes(x,y,size=z))+geom_point(shape=21,fill= \"blue\",alpha = 0.5)+\n  theme(legend.position = \"none\")\ngrid.arrange(p1,p2,p3,ncol=3,nrow=1)\ndat0<-data.frame(x=1:3,y=1:3,z=1:3)\n# Hình bên trái\np1<-dat0%>%ggplot(aes(x,y,size=z))+\n  geom_point(shape=21,fill= \"blue\",alpha = 0.5)+\n  theme(legend.position = \"none\")+\n  theme_minimal()+ggtitle(\"Không sử dụng scale_size\")\n# Hình bên phải\np2<-dat0%>%ggplot(aes(x,y,size=z))+\n  geom_point(shape=21,fill= \"blue\",alpha = 0.5)+\n  scale_size(range=c(6,24))+\n  theme(legend.position = \"none\")+\n  theme_minimal()+ggtitle(\"Sử dụng scale_size với range = (6,24)\")\ngrid.arrange(p1,p2,ncol=2,nrow=1)\ndat0<-data.frame(x=1:3,y=1:3,z=1:3)\n# Hình bên trái\np1<-dat0%>%ggplot(aes(x,y,size=z))+\n  geom_point(shape=21,fill= \"blue\",alpha = 0.5)+\n  theme(legend.position = \"none\")+\n  scale_size(range = c(1,7))\n\np2<-dat0%>%ggplot(aes(x,y,size=z))+\n  geom_point(shape=21,fill= \"blue\",alpha = 0.5)+\n  scale_radius(range=c(1,7))+\n  theme(legend.position = \"none\")\n# Hình bên phải\np3<-dat0%>%ggplot(aes(x,y,size=z))+\n  geom_point(shape=21,fill= \"blue\",alpha = 0.5)+\n  scale_radius(range=c(4,10))+\n  theme(legend.position = \"none\")\ngrid.arrange(p1,p2,p3,ncol=3,nrow=1)\ndat%>%\n  ggplot(aes(infant_mortality,life_expectancy, size = population))+\n  geom_point(shape = 21, fill = \"blue\",alpha = 0.5)+\n  scale_size(range = c(1,12),\n             limits = c(10^7,max(gapminder$population)),\n             breaks = c(10^8,2*10^8,5*10^8,10^9),\n             labels = c(paste(c(10^8,2*10^8,5*10^8)/10^6,\"triệu\"), \"Một tỷ\"))"},{"path":"trực-quan-hóa-dữ-liệu.html","id":"thuộc-tính-thẩm-mỹ-hình-dạng","chapter":"Chương 7 Trực quan hóa dữ liệu","heading":"7.4.3.2 Thuộc tính thẩm mỹ hình dạng","text":"Hình dạng thường được sử dụng để mô tả một biến rời rạc có không quá nhiều giá trị riêng biệt. Theo kinh nghiệm của chúng tôi thì hình dạng chỉ nên sử dụng với các biến có nhỏ hơn năm giá trị riêng biệt. Mặc dù thư viện \\(\\textbf{ggplot2}\\) cho phép sử dụng lên đến hơn 20 hình dạng khác nhau, nhưng sử dụng nhiều hơn hoặc bằng năm hình dạng trong một đồ thị sẽ làm cho đồ thị trở nên rắc rối và khó khăn khi nhận diện. Tại phiên bản \\(\\textbf{ggplot2}\\) mà chúng tôi đang sử dụng, có 25 hình dạng khác nhau có thể dùng để mô tả biến rời rạc. Các hình dạng này được hiển thị bằng cách thiết lập tham số tương ứng với 25 số tự nhiên từ 1 đến 25 được mô tả trong Hình 7.45\nFigure 7.45: Các hình dạng có thể được sử dụng trong trực quan hóa dữ liệu của thư viện ggplot2\nBạn đọc cần lưu ý rằng có một số hình dạng trông giống nhau nhưng lại có thuộc tính thẩm mỹ khác nhau. Chẳng hạn như hình dạng tương ứng với số 1 là một điểm hình tròn với thuộc tính thẩm mỹ color là màu sắc của toàn bộ hình tròn đó, trong khi hình dạng tương ứng với số 21 có thuộc tính thẩm mỹ color là màu viền bên ngoài của hình tròn và thuộc tính thẩm mỹ fill mới là màu sắc bên trong hình tròn.Để kiểm soát ánh xạ thẩm mỹ đến thuộc tính hình dạng, bạn đọc sử dụng hàm scale_shape_manual(). Cách sử dụng hàm này như sau:\nFigure 7.46: Thu nhập bình quân đầu người và tuổi thọ trung bình của các quốc gia châu Á vào năm 2011\nTham số được sử dụng để gán giá trị cho biến rời rạc đến hình dạng cụ thể là tham số values trong hàm scale_shape_manual(). Nhìn chung, ánh xạ biến rời rạc đến thuộc tính thẩm mỹ hình dạng chỉ cho hiệu quả tốt khi dữ liệu không có quá nhiều quan sát và số lượng nhóm nhỏ. Trong trường hợp dữ liệu có nhiều quan sát và biến rời rạc nhận nhiều hơn năm giá trị khác nhau, bạn đọc nên thận trọng khi sử dụng thuộc tính thẩm mỹ này.","code":"\ndat0<-data.frame(x=c(rep(1:10,2),1:5),y = c(rep(3,10),rep(2,10),rep(1,5)), z = 1:25)\ndat0%>%ggplot(aes(x,y,shape=as.factor(z)))+geom_point(size=3)+\n  scale_shape_manual(values = 1:25)+theme_classic()+geom_text(aes(label=z),vjust=-1)+\n  theme(legend.position = \"none\")+\n  theme_void()\ngapminder%>%filter(year==2011, continent == \"Asia\")%>%\n  ggplot(aes(gdp/population,life_expectancy, shape = region))+\n  geom_point()+\n  scale_x_continuous(trans=\"log10\")+\n  scale_shape_manual(values=c(21:24,8) )+\n  theme_minimal()"},{"path":"trực-quan-hóa-dữ-liệu.html","id":"kích-thước-và-hình-dạng-của-các-đường","chapter":"Chương 7 Trực quan hóa dữ liệu","heading":"7.4.3.3 Kích thước và hình dạng của các đường","text":"Đối với hình dạng đồ họa là các đường được vẽ bằng các hàm như geom_line(), geom_path() hay geom_segment(), chúng ta có thể ánh xạ các biến rời rạc vào độ rộng hoặc hình dạng của đường. Hình dạng và kích thước của các đường được sử dụng tương đương như hình dạng và kích thước của các điểm nên không có nhiều kiến thức để thảo luận trong phần này.Hình 7.47 mô tả sự thay đổi của biến tổng thu nhập quốc dân (\\(gdp\\)) của ba quốc gia bao gồm Mỹ, Trung Quốc và Nhật Bản theo thời gian từ năm 1960 đến năm 2011 từ dữ liệu \\(\\textbf{gapminder}\\) bằng cách sử dụng ba kiểu vẽ đường khác nhau:\nFigure 7.47: Tổng thu nhập quốc dân của Mỹ, Trung Quốc, và Nhật Bản từ năm 1960 đến 2011\nHàm số dùng để kiểm soát ánh xạ thẩm mỹ vào hình dạng của các đường là scale_linetype_manual(). Thư viện \\(\\textbf{ggplot2}\\) có 13 hình dạng cho các đường được đánh số từ 1 đến 13 như Hình 7.48 dưới đây\nFigure 7.48: Hình dạng của các đường có thể sử dụng để trực quan hóa dữ liệu trong thư viện ggplot2\nĐể các đường có hình dạng như mong muốn, chúng ta gán giá trị tham số values trong hàm scale_linetype_manual() cho một véc-tơ chứa các số nhận giá trị từ 1 đến 13 tương ứng với hình dạng mà bạn lựa chọn.\nFigure 7.49: Tổng thu nhập quốc dân của Mỹ, Trung Quốc, và Nhật Bản từ năm 1960 đến 2011\n","code":"\ngapminder%>%filter(country %in% c(\"United States\", \"China\", \"Japan\"), year <= 2011)%>%\n  ggplot(aes(x = year,y = gdp/10^9))+\n  geom_line(aes(linetype = country))+\n  theme_minimal()+\n  ylab(\"GDP in Billion USD\")+\n  scale_x_continuous(breaks = seq(1960,2010,10))+\n  scale_y_continuous(labels = scales::label_comma())\ngapminder%>%filter(country %in% c(\"United States\", \"China\", \"Japan\"), year <= 2011)%>%\n  ggplot(aes(x = year,y = gdp/10^9))+\n  geom_line(aes(linetype = country))+\n  theme_minimal()+\n  ylab(\"GDP in $B\")+\n  scale_x_continuous(breaks = seq(1960,2010,10))+\n  scale_y_continuous(labels = scales::label_comma())+\n  scale_linetype_manual(values = c(4,7,12))"},{"path":"trực-quan-hóa-dữ-liệu.html","id":"tùy-chỉnh-chú-giải-của-ánh-xạ-thẩm-mỹ","chapter":"Chương 7 Trực quan hóa dữ liệu","heading":"7.5 Tùy chỉnh chú giải của ánh xạ thẩm mỹ","text":"Về mặt hình thức, nếu coi các hàm scale_*() như các ánh xạ từ tập hợp các giá trị của biến đến tập hợp các giá trị của thuộc tính thẩm mỹ thì chú giải là ánh xạ ngược từ thuộc tính thẩm mỹ đến miền giá trị của biến. Chú giải cho phép bạn chuyển đổi các thuộc tính trực quan trở lại giá trị của dữ liệu. Giá trị xuất hiện trên các trục tọa độ và các chú giải có cách hiển thị khác nhau nhưng về bản chất lại có cùng một mục đích là cho phép người tiếp nhận quan sát các hình ảnh đồ họa trực quan và ánh xạ chúng trở lại giá trị của dữ liệu. Các cấu phần khác nhau của chú giải được mô tả trong Hình 7.50\nFigure 7.50: Các thành phần và tên gọi của chú giải\nChú giải có khả năng giải thích tốt hơn giá trị xuất hiện trên các trục tọa độ bởi các nguyên nhân sauChú giải có thể giải thích nhiều biến cùng lúc trong khi giá trị trên trục tọa độ chỉ cho phép một biến.Chú giải có thể giải thích nhiều biến cùng lúc trong khi giá trị trên trục tọa độ chỉ cho phép một biến.Chú giải có thể tùy biến dễ hơn: có thể xuất hiện ở các vị trí theo ý muốn của người xây dựng đồ thị, có thể xuất hiện theo bất kỳ hướng nào.Chú giải có thể tùy biến dễ hơn: có thể xuất hiện ở các vị trí theo ý muốn của người xây dựng đồ thị, có thể xuất hiện theo bất kỳ hướng nào.Bạn đọc hãy lưu ý rằng dù chúng ta không gọi bất kỳ hàm scale_*() nào trong các câu lệnh thì mỗi khi vẽ đồ thị, thư viện \\(\\textbf{ggplot2}\\) vẫn luôn luôn sử dụng một nhóm hàm scale_*() mặc định để ánh xạ từ giá trị của biến đến giá trị của thuộc tính thẩm mỹ. Mỗi khi bạn gọi hàm scale_*() để kiểm soát ánh xạ thẩm mỹ, các giá trị mà bạn khai báo sẽ thay thế cho các giá trị mặc định. Trong trường hợp bạn gọi nhiều hàm scale_*() tác động đến một thuộc tính thẩm mỹ thì chỉ có hàm scale_*() bạn gọi ra sau cùng được sử dụng.\nFigure 7.51: Thu nhập bình quân đầu người của các quốc gia Đông Nam Á năm 2011\nĐồ thị trong Hình 7.51 được vẽ bằng các câu lệnh có các hàm scale_x_continuous() được lặp lại 2 lần và scale_y_continuous() được lặp lại 3 lần. Quan sát kết quả, bạn đọc có thể thấy rằng chỉ có câu lệnh sau cùng được chấp nhận. Ngoài ra, khi thực thi đoạn lệnh ở trên, thư viện \\(\\textbf{ggplot2}\\) đưa ra các cảnh báo về các hàm scale_*() đã xuất hiện và sẽ bị thay thế bằng các hàm cùng tên.Để kiểm soát chú giải của các ánh xạ thẩm mỹ, bạn đọc sử dụng tham số guide trong các hàm scale_*() tương ứng. Giá trị gán cho tham số guide là một trong các hàm số sau đây:Hàm guide_axis() là hàm số dùng để gán cho tham số guide khi chúng ta sử dụng các hàm scale_*() nhằm kiểm soát ánh xạ thẩm mỹ đến các trục tọa độ.\nFigure 7.52: Thu nhập bình quân đầu người của các quốc gia Tây Âu năm 2011. Hàm guide_axis được sử dụng để kiểm soát chú giải cho các trục tọa độ x và y\nBạn đọc có thể thấy rằng tham số title trong hàm guide_axis() đã thay thế cho tham số name trong các hàm scale_(). Tham số angle cho biết hướng các giá trị xuất hiện trên trục tọa độ. Trong Hình 7.52, tên các quốc gia trên trục tọa độ x đã được xoay một góc 90 độ. Bạn đọc tham khảo hướng dẫn sử dụng hàm guide_axis() để hiểu về các tham số khác như n.dodge, order, hay position.Hàm guide_legend() là hàm số dùng để gán cho tham số guide khi gọi các hàm scale_*() kiểm soát ánh xạ từ các biến rời rạc đến màu sắc. Có rất nhiều tham số có thể sử dụng trong hàm số này. Bạn đọc tham khảo hướng dẫn sử dụng hàm để biết đầy đủ các tham số.\nFigure 7.53: Thu nhập bình quân đầu người của các quốc gia Nam Mỹ năm 2011. Hàm guide_legend được sử dụng để kiểm soát chú giải cho ánh xạ thẩm mỹ màu sắc\nguide_colorbar() được dùng khi chú giải cho các ánh xạ từ biến liên tục đến dải màu liên tụcguide_colorbar() được dùng khi chú giải cho các ánh xạ từ biến liên tục đến dải màu liên tụcguide_bin() được dùng khi chú giải cho các ánh xạ từ biến liên tục đến thuộc tính thẩm mỹ kích thước (size).guide_bin() được dùng khi chú giải cho các ánh xạ từ biến liên tục đến thuộc tính thẩm mỹ kích thước (size).","code":"\np<-gapminder%>%filter(year==2011,region==\"South-Eastern Asia\")%>%\n  mutate(gdp_per_capita = gdp/population)%>%\n  ggplot(aes(reorder(country,gdp_per_capita),gdp_per_capita,fill = country))+\n  geom_bar(stat=\"identity\")+\n  theme_minimal()\np+scale_y_continuous(name = \"Thu nhập bình quân đầu người\", labels = scales::label_comma())+\n  scale_x_discrete(name = \"Country\")+\n  scale_y_continuous(trans = \"sqrt\")+\n  scale_x_discrete(name = \"Quốc gia\", labels = c(\n    \"Vietnam\" = \"VN\",\n    \"Thailand\" = \"TL\",\n    \"Timor-Leste\" = \"Đông Timor\"))+\n  scale_y_continuous(name = \"Thu nhập bình quân đầu người\", labels = scales::label_dollar())\ngapminder%>%filter(year==2011,region==\"Western Europe\")%>%\n  mutate(gdp_per_capita = gdp/population)%>%\n  filter(!is.na(gdp_per_capita))%>%\n  ggplot(aes(reorder(country,gdp_per_capita),gdp_per_capita,fill = country))+\n  geom_bar(stat=\"identity\")+\n  theme_minimal()+\n  scale_x_discrete(name = \"Country\",\n                   guide = guide_axis(title = \"Quốc gia\",\n                                      angle = 90))+\n  scale_y_continuous(name = \"GDP per capita\", labels = scales::label_dollar(),\n                     guide = guide_axis(title = \"Thu nhập bình quân đầu người\"))\ngapminder%>%filter(year==2011,region==\"South America\")%>%\n  mutate(gdp_per_capita = gdp/population)%>%\n  filter(!is.na(gdp_per_capita))%>%\n  ggplot(aes(reorder(country,gdp_per_capita),gdp_per_capita,fill = country))+\n  geom_bar(stat=\"identity\")+\n  theme_minimal()+\n  scale_x_discrete(guide = guide_axis(title = \"Quốc gia\",angle = 90))+\n  scale_y_continuous(labels = scales::label_dollar(),\n                     guide = guide_axis(title = \"Thu nhập bình quân đầu người\"))+\n  scale_fill_brewer(palette = \"Paired\",\n                    guide = guide_legend(title = \"Quốc gia\",title.position = \"top\",ncol = 2))"},{"path":"trực-quan-hóa-dữ-liệu.html","id":"chủ-đề-và-ngữ-cảnh-của-đồ-thị","chapter":"Chương 7 Trực quan hóa dữ liệu","heading":"7.6 Chủ đề và ngữ cảnh của đồ thị","text":"Ngữ cảnh cho phép bạn đọc kiểm soát tốt các cấu phần không ánh xạ đến dữ liệu trong đồ thị như phông chữ, hình nền, vị trí chú giải, … Sự phân tách giữa các thành các phần có ánh xạ đến dữ liệu và thành phần không ánh xạ đến dữ liệu trong thư viện \\(\\textbf{ggplot2}\\) là điểm khác biệt với đồ họa cơ sở. Trong đồ họa cơ sở hầu hết các hàm đều có một số lượng lớn các tham số số chỉ định cả hình thức dữ liệu và phần không liên quan đến dữ liệu, điều này làm cho các hàm trong đồ thị cơ sở trở nên phức tạp. Thư viện \\(\\textbf{ggplot2}\\) tiếp cận theo cách khác: khi tạo đồ thị, bạn xác định cách hiển thị dữ liệu trước, sau đó bạn có thể chỉnh sửa mọi chi tiết không liên quan đến dữ liệu bằng các hàm kiểm soát chủ đề và ngữ cảnh. Để kiểm soát chủ đề và ngữ cảnh của đồ thị, bạn đọc cần nắm vững các nội dung sau:Các chủ đề và ngữ cảnh đã được hoàn chỉnh và sẵn có trong thư viện \\(\\textbf{ggplot2}\\) và trong các thư viện cài đặt bổ sung, chẳng hạn như \\(\\textbf{ggthemes}\\).Các chủ đề và ngữ cảnh đã được hoàn chỉnh và sẵn có trong thư viện \\(\\textbf{ggplot2}\\) và trong các thư viện cài đặt bổ sung, chẳng hạn như \\(\\textbf{ggthemes}\\).Cách kiểm soát các thành phần của chủ đề và ngữ cảnh như: tiêu đề của đồ thị (kiểu chữ, kích thước, vị trí), cách hiển thị các số trên các trục, cách hiển thị các hình dạng đồ họa trên chú giải, kiểu chữ, kích thước hay vị trí của chú giải…Cách kiểm soát các thành phần của chủ đề và ngữ cảnh như: tiêu đề của đồ thị (kiểu chữ, kích thước, vị trí), cách hiển thị các số trên các trục, cách hiển thị các hình dạng đồ họa trên chú giải, kiểu chữ, kích thước hay vị trí của chú giải…Kiểm soát các tùy biến của các hàm dùng để gán giá trị cho các thành phần của chủ đề. Ví dụ như hàm element_text() có thể dùng để chỉnh kích thước phông chữ, màu sắc và giao diện của các thành phần văn bản.Kiểm soát các tùy biến của các hàm dùng để gán giá trị cho các thành phần của chủ đề. Ví dụ như hàm element_text() có thể dùng để chỉnh kích thước phông chữ, màu sắc và giao diện của các thành phần văn bản.Cách sử dụng hàm theme() với một danh sách dài các tùy biến cho phép bạn sao chép lên các thành phần của chủ đề và ngữ cảnh mặc định.Cách sử dụng hàm theme() với một danh sách dài các tùy biến cho phép bạn sao chép lên các thành phần của chủ đề và ngữ cảnh mặc định.","code":""},{"path":"trực-quan-hóa-dữ-liệu.html","id":"tạo-đồ-thị-tương-tác","chapter":"Chương 7 Trực quan hóa dữ liệu","heading":"7.7 Tạo đồ thị tương tác","text":"Các đồ thị được vẽ bằng thư viện \\(\\textbf{ggplot2}\\) đều là các đồ thị tĩnh. Các đồ thị động hay đồ thị tương tác có lợi thế hơn đồ thị tĩnh ở việc thu hút thị giác của người tiếp nhận và có khả năng mô tả dữ liệu một cách đầy đủ thông tin hơn:Các đồ thị dạng động đặc biệt hiệu quả trong việc mô tả sự thay đổi dữ liệu theo thời gian.Các đồ thị dạng động đặc biệt hiệu quả trong việc mô tả sự thay đổi dữ liệu theo thời gian.Các đồ thị tương tác cho phép hiển thị thông tin bằng con trỏ, hoặc phóng , thu nhỏ từng phần của đồ thị. Bạn đọc tránh phải hiển thị quá nhiều thông tin lên đồ thị cùng lúc.Các đồ thị tương tác cho phép hiển thị thông tin bằng con trỏ, hoặc phóng , thu nhỏ từng phần của đồ thị. Bạn đọc tránh phải hiển thị quá nhiều thông tin lên đồ thị cùng lúc.Khuyết điểm lớn nhất của các đồ thị tương tác và các đồ thị động đó là không thể biểu diễn trên các bản cứng!Trong phần này của chương sách, chúng tôi sẽ thảo luận về hai thư viện dùng để tạo đồ thị tương tác và đồ thị dạng động là \\(\\textbf{ggiraph}\\) và \\(\\textbf{plotly}\\). Nếu như \\(\\textbf{ggiraph}\\) là thư viện bổ sung cho \\(\\textbf{ggplot2}\\) và được xây dựng dựa trên cấu trúc ngữ pháp đồ thị thì \\(plotly\\) là một thư viện độc lập với \\(\\textbf{ggplot2}\\) và chuyên được sử dụng để tạo đồ thị dạng động và tương tác.","code":""},{"path":"trực-quan-hóa-dữ-liệu.html","id":"tạo-đồ-thị-tương-tác-với-textbfggiraph","chapter":"Chương 7 Trực quan hóa dữ liệu","heading":"7.7.1 Tạo đồ thị tương tác với \\(\\textbf{ggiraph}\\)","text":"Ưu điểm lớn nhất của thư viện \\(\\textbf{ggiraph}\\) đó là các câu lệnh tạo đồ thị cũng được dựa trên ngữ pháp của đồ thị, nghĩa là hoàn toàn tương đồng với các câu lệnh trong thư viện \\(\\textbf{ggplot2}\\). Để tạo một đồ thị trong \\(\\textbf{ggiraph}\\), bạn đọc chỉ cần thêm các thuộc tính thẩm mỹ của đồ thị tương tác và đồ thị động cùng với các thuộc tính của đồ thị tĩnh mà chúng ta đã làm quen trong thư viện \\(\\textbf{ggplot2}\\). Tại thời điểm chúng tôi viết chương sách này, thư viện \\(\\textbf{ggiraph}\\) đang ở phiên bản 0.8.7 và hướng dẫn sử dụng ở trong link dưới đâyhttps://cloud.r-project.org/web/packages/ggiraph/ggiraph.pdfTrong danh sách các hàm số trong thư viện \\(\\textbf{ggiraph}\\), bạn đọc có thể thấy rằng đa số các hàm geom_*() của thư viện \\(\\textbf{ggplot2}\\) đều có một hàm tương ứng để tạo đồ thị tương tác là geom_*_interactive(). Chẳng hạn như hàm geom_point() của thư viện \\(\\textbf{ggplot2}\\) sẽ có hàm tương ứng trong thư viện \\(\\textbf{ggiraph}\\) là geom_point_interactive(). Hai cấu phần thẩm mỹ để tạo đồ thị tương tác là \\(tooltip\\) và \\(data_id\\). Tuy nhiên, hàm geom_point_interactive() không trực tiếp tạo ra đồ thị tương tác, mà bạn đọc cần lưu đối tượng được tạo bằng hàm số này sau đó thực hiện câu lệnh tạo đồ thị tương tác bằng hàm girafe().Hình 7.54 là đồ thị tương tác được vẽ bằng thư viện \\(\\textbf{ggiraph}\\) mô tả số vụ sát nhân bằng súng tại Mỹ năm 2010 từ dữ liệu \\(\\textbf{murders}\\).\nFigure 7.54: Đồ thị tương tác mô tả số vụ sát nhân bằng súng tại 51 bang của Mỹ vào năm 2010\nThuộc tính thẩm mỹ tooltip cho biết thông tin hiển thị của các điểm trên đồ thị khi sử dụng con trỏ. Trong Hình 7.54 chúng tôi ánh xạ thuộc tính này đến các biến \\(state\\), \\(region\\) và \\(population\\). Các biến này chỉ hiển thị khi bạn đọc sử dụng con trỏ di chuyển đến một điểm trên đồ thị.Thuộc tính thẩm mỹ tooltip cho biết thông tin hiển thị của các điểm trên đồ thị khi sử dụng con trỏ. Trong Hình 7.54 chúng tôi ánh xạ thuộc tính này đến các biến \\(state\\), \\(region\\) và \\(population\\). Các biến này chỉ hiển thị khi bạn đọc sử dụng con trỏ di chuyển đến một điểm trên đồ thị.Thuộc tính thẩm mỹ data_id khi được ánh xạ đến một biến sẽ cho biết các quan sát có cùng giá trị trên biến đó. Bạn đọc có thể sử dụng con trỏ di chuyển đến từng các điểm trên đồ thị trong Hình 7.54 để xem kết quả của ánh xạ đến thuộc tính tooltip và data_id.Thuộc tính thẩm mỹ data_id khi được ánh xạ đến một biến sẽ cho biết các quan sát có cùng giá trị trên biến đó. Bạn đọc có thể sử dụng con trỏ di chuyển đến từng các điểm trên đồ thị trong Hình 7.54 để xem kết quả của ánh xạ đến thuộc tính tooltip và data_id.Cách sử dụng các thuộc tính thẩm mỹ tooltip và data_id hoàn toàn tương tự trong các đồ thị cơ bản khác.Hình 7.55 mô tả đồ thị bong bóng dạng tương tác. mắt quan sát không dễ dàng đánh giá được kích thước của các hình tròn, kể cả khi sử dụng chú giải cho kích thước, việc sử dụng đồ thị tương tác để hiển thị số lượng điểm tại mỗi hình tròn giúp cho dữ liệu càng trở nên sinh động và trực quan hơn.\nFigure 7.55: Đồ thị tương tác dạng bong bóng mô tả số lượng viên kim cương theo màu sắc (color) và giác cắt (cut)\nHình 7.56 sử dụng đồ thị dạng đường và đồ thị dạng hình hộp chữ nhật có tương tác\nFigure 7.56: Đồ thị tương tác mô tả tỷ lệ thất nghiệp của nước Mỹ qua các thời kỳ Tổng thống và các Đảng cầm quyền\nĐỒ thị trong Hình 7.56 được tạo từ hai dữ liệu, trong đó dữ liệu chính là dữ liệu \\(\\textbf{economics}\\) với biến \\(unemploy\\) cho biết số lượng người thất nghiệp tại Mỹ quan sát theo tháng từ năm 1967 đến năm 2015, dữ liệu thứ hai là dữ liệu về các nhiệm kỳ của các tổng thống Mỹ trong các khoảng thời gian tương ứng. Bạn đọc có thể nhận thấy sự khác biệt về sự biến động của tỷ lệ thất nghiệp qua các thời kỳ cầm quyền của các đảng cầm quyền: tỷ lệ thất nghiệp luôn có xu thế giảm trong giai đoạn đảng Dân chủ nắm quyền, trong khi lại có xu thế tăng trong giai đoạn đảng Cộng hòa nắm chính quyền.Hình 7.57 sử dụng các đồ thị dạng thanh tương tác để mô tả thu nhập bình quân đầu người của các quốc gia vùng Đông Á\nFigure 7.57: Thu nhập bình quân đầu người của các quốc gia vùng Đông Á năm 2011\nHình 7.58 sử dụng bản đồ tương tác để mô tả biến tỷ lệ trẻ sơ sinh tử vong của dữ liệu \\(\\textbf{gapminder}\\)\nFigure 7.58: Bản đồ mô tả tỷ lệ trẻ sơ sinh tử vong tại tất cả các quốc gia trên thế giới năm 2011\nBạn đọc có thể thấy rằng bản đồ tương tác đặc biệt hiệu quả trong hiển thị thông tin thay thế cho chú giải. Màu sắc từ xanh lá cây đến đỏ cho biết vùng quốc gia - vùng lãnh thổ nào có tỷ lệ trẻ sơ sinh tử vong cao và quốc gia - vùng lãnh thổ nào có tỷ lệ trẻ sơ sinh tử vong thấp. Bạn đọc muốn biết thông tin chi tiết về quốc gia đó có thể sử dụng con trỏ để hiển thị thông tin, bao gồm có thông tin về tên nước, dân số, và tỷ lệ trẻ sơ sinh tử vong.","code":"\np<-murders %>% ggplot(aes(y = total, x = population)) +\n  geom_point_interactive(aes(fill=region,\n                             tooltip = paste0(\"Bang: \", state, \"\\n Vùng: \", region, \"\\n Dân số: \", round(population/1000,0)*1000 ),\n                             data_id = region),\n                         size = 4, shape=21, alpha = 0.8, color = \"black\") +\n  geom_smooth(method = \"lm\", se = FALSE, linetype = 2, color=\"grey\")+\n  scale_x_continuous(trans = \"log10\", labels = scales::label_comma()) +\n  scale_y_log10() +\n  scale_fill_brewer(palette = \"Dark2\",\n                    guide = guide_legend(title = \"Vùng\"))+\n  theme_minimal()+\n  ggtitle(\"Số vụ sát nhân bằng súng tại các bang năm 2010\")+\n  xlab(\"Dân số\")+ylab(\"Số vụ sát nhân bằng súng\")\ngirafe(ggobj = p)\np<-diamonds%>%group_by(cut,color)%>%mutate(ave_price = mean(price))%>%ungroup()%>%\n  as.data.frame()%>%\n  ggplot(aes(cut,color))+\n  geom_count_interactive(aes(tooltip = paste0(\"Số lượng: \", after_stat(n))), \n                         alpha = 0.5, color = \"blue\",shape = 21, fill = \"grey40\")+\n  scale_size(range=c(1,12))+\n  theme_minimal()+\n  theme(legend.position = \"none\")+xlab(\"Giác cắt\")+ylab(\"Màu sắc\")\ngirafe(ggobj = p)\ndat1<-presidential[3:11,]\np<-economics%>%mutate(unemploy_rate = unemploy/pop)%>%\n  ggplot()+\n  geom_rect_interactive(data=dat1,\n            aes(xmin = start, xmax = end,\n                ymin = 0.01, ymax = 0.052,\n                tooltip = paste(\"Tổng thống: \", name),\n                data_id = name,\n                fill = party),color = \"grey40\",size=0.1,alpha=0.5)+\n  scale_fill_manual(values=c(\"blue\",\"red\"),labels = c(\"Democratic\" = \"Dân chủ\",\"Republican\" = \"Cộng hòa\"),\n                    guide = guide_legend(title = \"Đảng cầm quyền\"))+\n  geom_line(aes(x = date, y = unemploy_rate),size = 0.5)+\n  geom_point_interactive(aes(x = date, y = unemploy_rate, \n                             tooltip = paste(round(unemploy_rate*100,2),\"%\")), size = 0.5, alpha = 0.5)+\n  scale_y_continuous(limits = c(0.01,0.052),labels = scales::label_percent())+\n  theme_minimal()+\n  xlab(\"Năm\") + ylab(\"Tỷ lệ thất nghiệp\")\ngirafe(ggobj = p)\np<-gapminder%>%filter(year==2011,region==\"Eastern Asia\")%>%\n  mutate(gdp_per_capita = gdp/population)%>%\n  filter(!is.na(gdp_per_capita))%>%\n  ggplot(aes(reorder(country,gdp_per_capita),gdp_per_capita,fill = country))+\n  geom_bar_interactive(aes(tooltip = paste0(\"GDP: \", round(gdp/10^9,2), \" tỷ USD \\n Dân số: \", round(population/10^6,2), \n                                            \" triệu \\n Tuổi thọ bình quân: \", life_expectancy)), \n                       stat=\"identity\")+\n  theme_minimal()+\n  scale_x_discrete(guide = guide_axis(title = \"\", angle = 90))+\n  scale_y_continuous(labels = scales::label_dollar(),\n                     guide = guide_axis(title = \"Thu nhập bình quân đầu người\"))+\n  scale_fill_brewer(palette = \"Paired\",\n                    guide = guide_legend(title = \"Quốc gia\",title.position = \"top\"))+\n  ggtitle(\"Các nước Đông Á năm 2011\")\ngirafe(ggobj = p)\ndat_map<-map_data(\"world\")\ndat<-filter(gapminder,year == 2011)\ndat$country<-as.character(dat$country)\ndat$country[dat$country == \"Congo, Dem. Rep.\"]<-\"Democratic Republic of the Congo\"\ndat$country[dat$country == \"Congo, Rep.\"]<-\"Republic of Congo\"\ndat$country[dat$country == \"Dominican Republic\"]<-\"Dominica\"\ndat$country[dat$country == \"Kyrgyz Republic\"]<-\"Kyrgyzstan\"\ndat$country[dat$country == \"Lao\"]<-\"Laos\"\ndat$country[dat$country == \"St. Lucia\"]<-\"Saint Lucia\"\ndat$country[dat$country == \"United States\"]<-\"USA\"\ndat$country[dat$country == \"United Kingdom\"]<-\"UK\"\ndat$country[dat$country == \"Trinidad and Tobago\"]<-\"Trinidad\"\ndat$country<-as.factor(dat$country)\n\nind<-match(dat_map$region,dat$country)\ndat_map<-dat_map%>%mutate(gdp = dat$gdp[ind],\n                 population = dat$population[ind],\n                 infant_mortality = dat$infant_mortality[ind])\nind<-is.na(dat_map$infant_mortality)\ndat_map$infant_mortality[ind]<-round(mean(dat_map$infant_mortality,na.rm=TRUE),2)\n\np<-dat_map%>%\n  ggplot(aes(x=long,y=lat,group=group,label = region, fill = infant_mortality))+\n  geom_polygon_interactive(aes(tooltip = paste0(region, \"\\n Dân số: \", round(population/10^6,2),\n                                                \" triệu \\n Tỷ lệ tử vong trẻ sơ sinh: \", infant_mortality,\"/1000\")), \n                           color=\"black\",size = 0.1)+\n  scale_x_continuous(expand=c(0,0))+\n  scale_fill_gradientn(colors = colorRampPalette(c(\"lightgreen\",\"red\"), space = \"Lab\")(10))+\n  theme_minimal()+xlab(\"\")+ylab(\"\")+ggtitle(\"Tỷ lệ trẻ sơ sinh tử vong năm 2011\")+\n  theme(legend.position = \"none\")\ngirafe(ggobj = p)"},{"path":"trực-quan-hóa-dữ-liệu.html","id":"tạo-đồ-thị-tương-tác-bằng-thư-viện-textbfplotly","chapter":"Chương 7 Trực quan hóa dữ liệu","heading":"7.7.2 Tạo đồ thị tương tác bằng thư viện \\(\\textbf{plotly}\\)","text":"Để tạo một đồ thị tương tác bằng thư viện \\(\\textbf{plotly}\\) dễ dàng hơn với sử dụng thư viện \\(\\textbf{ggiraph}\\) vì chúng ta không cần viết các câu lệnh có ngữ pháp. Việc duy nhất bạn đọc cần làm là sử dụng hàm ggplotly() trên một đối tượng được tạo bằng hàm ggplot(). Đoạn câu lệnh dưới đây mô tả dữ liệu \\(murders\\) dưới dạng đồ thị phân tán tương tác.\nFigure 7.59: Đồ thị tương tác mô tả số vụ sát nhân bằng súng tại 51 bang của Mỹ vào năm 2010\nBạn đọc có thể tương tác với đồ thị tạo bằng ggplotly() bằng các thao tác như sau:Sử dụng con trỏ chỉ vào các điểm để xem thông tin chính xác về dân số, số vụ sát nhân, tên bang, và tên vùng của mỗi điểm.Sử dụng con trỏ chỉ vào các điểm để xem thông tin chính xác về dân số, số vụ sát nhân, tên bang, và tên vùng của mỗi điểm.Sử dụng con trỏ, hoặc các nút phóng , thu nhỏ để xem từng phần của đồ thị.Sử dụng con trỏ, hoặc các nút phóng , thu nhỏ để xem từng phần của đồ thị.Sử dụng con trỏ trên chú giải để lựa chọn các vùng nào hiển thị, hoặc không hiển thị trên đồ thị.Sử dụng con trỏ trên chú giải để lựa chọn các vùng nào hiển thị, hoặc không hiển thị trên đồ thị.Sử dụng con trỏ trượt theo đường thẳng tạo bởi geom_smooth() để biết giá trị trên trục total và population của mỗi điểm trên đường thẳng. Lưu ý rằng giá trị xuất hiện là giá trị sau khi đã chuyển đổi bằng hàm \\(log10()\\).Sử dụng con trỏ trượt theo đường thẳng tạo bởi geom_smooth() để biết giá trị trên trục total và population của mỗi điểm trên đường thẳng. Lưu ý rằng giá trị xuất hiện là giá trị sau khi đã chuyển đổi bằng hàm \\(log10()\\).Các thông tin bạn đọc muốn hiển thị bằng con trỏ là tất cả các biến dữ liệu đã được ánh xạ vào trong các thuộc tính thẩm mỹ của đồ thị. Đồ thị trong Hình 7.59 hiển thị thông tin trên các điểm bao gồm giá trị các biến \\(population\\), \\(total\\), \\(state\\), và \\(region\\) là tất cả các biến được sử dụng trong ánh xạ thẩm mỹ của hàm geom_point(). Dọc theo đường hồi quy tuyến tính, chúng ta sẽ có thông tin về giá trị của các biến \\(population\\) và \\(total\\) là các biến được sử dụng trong ánh xạ thẩm mỹ của hàm geom_smooth().Tham số tooltip trong hàm ggplotly() được sử dụng để kiểm soát các thuộc tính thẩm mỹ xuất hiện trên đồ thị tương tác. Ví dụ như trong đồ thị phân tán trong Hình 7.59, nếu chúng ta chỉ muốn hiển thị thông tin về tên của bang và thông tin về vùng. tên của bang được ánh xạ tới thuộc tính thẩm mỹ group và vùng được ánh xạ tới thuộc tính thẩm mỹ fill, nên chúng ta có thể sử dụng tham số tooltip như sauHàm số ggplotly() có thể được sử dụng để tạo đồ thị tương tác với đa số các đồ thị được tạo bởi \\(ggplot2\\), dưới đây là một số ví dụHình 7.60 vẽ đồ thị kiểu bong bóng tương tác mô tả số lượng và giá trung bình của kim cương khi phân loại theo màu sắc và giác cắt.\nFigure 7.60: Số lượng và giá trung bình của kim cương phân loại theo màu sắc và giác cắt\nHình 7.61 sử dụng đồ thị tương tác dạng đường để mô tả tổng thu nhập quốc dân của năm quốc gia phát triển trên thế giới là Mỹ, Đức, Pháp, Nhật và Trung Quốc từ năm 1970 đến năm 2011.\nFigure 7.61: Tổng thu nhập quốc dân của các quốc gia Mỹ, Đức, Pháp, Nhật và Trung Quốc từ năm 1970 đến năm 2011\nHình 7.62 sử dụng đồ thị tương tác dạng thanh để trực quan hóa hai biến rời rạc là thu nhập bình quân đầu người và châu lục vào năm 2011. Thu nhập bình quân đầu người được phân loại theo ba mức độ: thấp tương ứng với thu nhập bình quân dưới 3000 USD, trung bình tương ứng với thu nhập bình quân từ 3000 USD đến 8000 USD, và cao tương ứng với thu nhập bình quân trên 8000 USD.\nFigure 7.62: Tỷ lệ các nước thu nhập thấp, trung bình, và cao tại các châu lục năm 2011\nHình 7.63 sử dụng đồ thị tương tác dạng bản đồ để mô tả tỷ lệ số vụ xả súng tại các bang tại Mỹ năm 2010.\nFigure 7.63: Tỷ lệ số vụ xả súng trên 1 triệu dân tại các bang của nước Mỹ năm 2010\n","code":"\np<-murders %>% \n  ggplot(aes(x = population/10^6, y = total)) +\n  geom_point(aes(group = state, fill = region ), size = 3, shape=21, alpha = 0.8) +\n  geom_smooth(method = \"lm\", se = FALSE, linetype = 2, color=\"grey80\")+\n  scale_x_log10() +\n  scale_y_log10() +\n  scale_fill_brewer(palette = \"Dark2\",\n                    guide = guide_legend(title = \"Vùng\"))+\n  xlab(\"Dân số của bang (triệu dân)\") +\n  ylab(\"Tổng số vụ sát nhân bằng súng\") +\n  ggtitle(\"Số vụ sát nhân bằng súng trong năm 2010 tại Mỹ\")+\n  theme_minimal()\nggplotly(p)\n# Thông tin chỉ bao gồm tên bang và vùng\nggplotly(p, tooltip = c(\"group\",\"fill\"))\np<-diamonds%>%group_by(cut,color)%>%mutate(ave_price = round(mean(price)))%>%ungroup()%>%\n  as.data.frame()%>%\n  ggplot(aes(cut,color,color = ave_price))+\n  geom_count(alpha = 0.9)+\n  scale_color_gradientn(colors = colorRampPalette(c(\"darkblue\",\"orange\"), space = \"Lab\")(10))+\n  scale_size(range=c(1,12))+\n  theme_minimal()\nggplotly(p, tooltip = c(\"n\", \"color\"))\np<-gapminder%>%filter(country %in% c(\"United States\",\"Japan\",\"Germany\",\"France\", \"China\"),\n                   year <= 2011, year >= 1970)%>%mutate(gdp_bil_usd = gdp/10^9)%>%\n  ggplot(aes(x = year, y = gdp_bil_usd, color = country))+\n  geom_line()+\n  scale_y_continuous(labels = scales::label_comma())+\n  theme_minimal()\nggplotly(p, tooltip = c(\"x\",\"y\", \"color\"))\np<-gapminder%>%filter(year == 2011)%>%drop_na()%>%\n  mutate(gdp_per_capita = gdp/population,\n         gdp_levels = ifelse(gdp_per_capita<3000,\"Thấp\",\n                            ifelse(gdp_per_capita<8000,\"Trung bình\",\"Cao\")),\n         gdp_range = factor(gdp_levels, levels = c(\"Cao\",\"Trung bình\",\"Thấp\")))%>%\n  ggplot(aes(x = continent,fill = gdp_range))+\n  geom_bar(color=\"grey40\",alpha=0.8)+\n  scale_fill_manual(values = colorRampPalette(c(\"orange\",\"grey\"))(3))+\n  theme_minimal()\nggplotly(p, tooltip = \"count\")\ndat<-map_data(\"state\")\ndat1<-murders%>%mutate(murder_rate=total/population*10^6,\n                       state = tolower(state))\n\np<-dat%>%mutate(state = region)%>%\n  mutate(murder_rate=dat1$murder_rate[match(state,dat1$state)])%>%\n  ggplot(aes(x=long,y=lat,group=group,label = state, fill=murder_rate))+\n  geom_polygon(color=\"black\",size = 0.1)+\n  scale_x_continuous(expand=c(0,0))+\n  scale_fill_gradientn(colors = c(rgb(0.95,0.95,0.95),rgb(0.95,0.3,0.3),\n                                  rgb(0.95,0.1,0.1)))+\n  theme_minimal()+\n  theme(legend.position = \"bottom\")\nggplotly(p)"},{"path":"trực-quan-hóa-dữ-liệu.html","id":"đồ-thị-động","chapter":"Chương 7 Trực quan hóa dữ liệu","heading":"7.8 Đồ thị động","text":"Đồ thị động (dynamic graph) là một phương pháp thường được sử dụng để mô tả dữ liệu biến đổi theo thời gian. Đồ thị dạng động ngoài yếu tố bắt mắt còn giúp cho người tiếp nhận dữ liệu cảm nhận được sự thay đổi của các biến liên tục và rời rạc theo thời gian một cách trực quan nhất. Chúng tôi sẽ giới thiệu đến bạn đọc cách tạo các đồ thị động với thư viện \\(\\textbf{plotly}\\) trước và sau đó là thư viện \\(\\textbf{gganimate}\\). Để bạn đọc hiểu cách đồ thị động được tạo thành, hãy bắt đầu với một dữ liệu đơn giản bao gồm hai biến liên tục là \\(x\\), \\(y\\) và một biến thời gian:Để trực quan hóa ba biến kiểu số bao gồm \\(x\\), \\(y\\) và \\(time\\), phương pháp thường được sử dụng là trực quan hóa hai biến \\(x\\) và \\(y\\) bằng một đồ thị phân tán, sau đó ánh xạ biến \\(time\\) vào một thuộc tính thẩm mỹ phù hợp, chẳng hạn như kích thước của các điểm. Đồ thị kiểu như vậy được mô tả trong Hình 7.64\nFigure 7.64: Mô tả ba biến liên tục sử dụng thuộc tính thẩm mỹ kích thước\nMột phương pháp là sử dụng đồ thị dạng động, là tập hợp của các nhiều đồ thị tĩnh xuất hiện liên tục mà mỗi đồ thị tương ứng với một giá trị của biến \\(time\\). Bạn đọc có thể thực hiện việc này bằng thư viện \\(\\textbf{plotly}\\). Thuộc tính thẩm mỹ để tạo đồ thị dạng động là frame. Chúng ta chỉ cần khai báo thêm ánh xạ thẩm mỹ từ tham số frame đến biến time để tạo một đồ thị động trực quan như Hình 7.65\nFigure 7.65: Đồ thị động mô tả sự chuyển động của một điểm theo biến time của dữ liệu\nĐồ thị dạng động sẽ được kích hoạt mỗi khi chúng ta bấm nút play. Bạn đọc có thể thấy rằng cách mô tả sự thay đổi của điểm theo thời gian của đồ thị động trong Hình 7.65 trực quan và hiệu quả hơn với Hình 7.64.Hàm ggplotly() rất hiệu quả khi mô tả các biến liên tục theo thời gian. Quay trở lại với dữ liệu \\(\\textbf{gapminder}\\), đồ thị dạng động cho phép chúng ta xây dựng các đồ thị trực quan sinh động. Ví dụ, chúng ta muốn mô tả hai biến tuổi thọ trung bình và tỷ lệ sinh trung bình của một phụ nữ qua các năm, chúng ta có thể ánh xạ hai biến lên hai trục tọa độ, sau đó sử dụng màu sắc để mô tả biến châu lục, sử dụng kích thước để mô tả biến dân số, và sau cùng là sử dụng đồ thị dạng động để mô tả biến thời gian (year).\nFigure 7.66: Đồ thị động mô tả sự thay đổi của tuổi thọ trung bình và tỷ lệ sinh trung bình của một phụ nữ từ năm 1960 đến 2011 của tất cả các quốc gia trên thế giới\n","code":"\ndat<-data.frame(x=1:30,y=(1:30)^2,time=1:30)\ndat%>%ggplot(aes(x,y,size = time))+\n  geom_point(alpha=0.3,shape = 21, color = \"blue\", fill = \"grey40\")+\n  scale_size(range=c(1,15))+\n  theme_minimal()\np<-dat%>%ggplot(aes(x=x,y=y,size=time,frame = time))+\n  geom_point(alpha=0.5,shape = 21, color = \"blue\", fill = \"grey40\")+\n  scale_size(range=c(1,15))+\n  theme_minimal()\nggplotly(p, tooltip = \"size\")\np<-gapminder%>%filter(year %in% 1960:2011)%>%\n  ggplot(aes(x = fertility, y = life_expectancy, size = population,\n             fill = continent, frame = year))+\n  geom_point(alpha = 0.5,shape=21)+\n  scale_fill_brewer(palette = \"Set1\")+\n  scale_size(range=c(1,15))+\n  theme_minimal()+\n  ggtitle(\"Tuổi thọ và tỷ lệ sinh trung bình 1960 đến 2011\")\nggplotly(p, tooltip = c())"},{"path":"trực-quan-hóa-dữ-liệu.html","id":"bài-tập-3","chapter":"Chương 7 Trực quan hóa dữ liệu","heading":"7.9 Bài tập","text":"","code":""},{"path":"trực-quan-hóa-dữ-liệu.html","id":"phụ-lục-3","chapter":"Chương 7 Trực quan hóa dữ liệu","heading":"7.10 Phụ lục","text":"","code":""},{"path":"trực-quan-hóa-dữ-liệu.html","id":"lập-trình-trong-textbfggplot2","chapter":"Chương 7 Trực quan hóa dữ liệu","heading":"7.10.1 Lập trình trong \\(\\textbf{ggplot2}\\)","text":"","code":""},{"path":"trực-quan-hóa-dữ-liệu.html","id":"tạo-dashboard-bằng-thư-viện-textbfshiny","chapter":"Chương 7 Trực quan hóa dữ liệu","heading":"7.10.2 Tạo dashboard bằng thư viện \\(\\textbf{shiny}\\)","text":"","code":"\nlibrary(readxl)\nlibrary(dplyr)## \n## Attaching package: 'dplyr'## The following objects are masked from 'package:stats':\n## \n##     filter, lag## The following objects are masked from 'package:base':\n## \n##     intersect, setdiff, setequal, union\nlibrary(knitr)\nlibrary(kableExtra)## \n## Attaching package: 'kableExtra'## The following object is masked from 'package:dplyr':\n## \n##     group_rows\nlibrary(ggplot2)\nlibrary(forcats)\nlibrary(ggpubr)\nlibrary(grid)\nlibrary(gridExtra)## \n## Attaching package: 'gridExtra'## The following object is masked from 'package:dplyr':\n## \n##     combine\nlibrary(forcats)\nlibrary(pryr)## \n## Attaching package: 'pryr'## The following object is masked from 'package:dplyr':\n## \n##     where\nlibrary(RColorBrewer)\nlibrary(mvtnorm)\nlibrary(latex2exp)\nlibrary(caret)## Loading required package: lattice\nselect <- function(...) dplyr::select(...)"},{"path":"mô-hình-hồi-quy-tuyến-tính.html","id":"mô-hình-hồi-quy-tuyến-tính","chapter":"Chương 8 Mô hình hồi quy tuyến tính","heading":"Chương 8 Mô hình hồi quy tuyến tính","text":"Chương sách này thảo luận về mô hình hồi quy tuyến tính - phương pháp cơ bản nhất trong các phương pháp xây dựng mô hình học máy có giám sát. Mặc dù đơn giản nhưng mô hình hồi quy tuyến tính lại là một công cụ hữu ích để đưa ra các dự đoán hoặc mô tả sự tác động của các một biến giải thích lên các biến khác. Hồi quy tuyến tính là một chủ đề đã được nghiên cứu từ rất lâu, từ trước khi có máy tính điện tử, đồng thời cũng là chủ đề của vô số sách tham khảo. Trong thời đại ngày nay, mặc dù mô hình này có vẻ hơi nhàm chán hơn với một số phương pháp học thống kê/học máy hiện đại, nhưng hồi quy tuyến tính vẫn là một phương pháp học thống kê hữu ích và được sử dụng rộng rãi. Hơn nữa, đây còn là điểm khởi đầu tốt cho các phương pháp tiếp cận mới hơn như chúng ta sẽ thấy trong các chương sau. Nhiều phương pháp học máy tiên tiến nhất hiện nay có thể được coi là sự khái quát hóa hoặc mở rộng của hồi quy tuyến tính. đó, tầm quan trọng của việc hiểu rõ về hồi quy tuyến tính trước khi nghiên cứu các phương pháp phức tạp hơn là không thể phủ nhận.Trong phần đầu của chương này, chúng ta xem xét một số ý tưởng chính làm cơ sở cho mô hình hồi quy tuyến tính, cũng như phương pháp bình phương nhỏ nhất được sử dụng phổ biến nhất để ước lượng tham số cho mô hình này. Trong phần sau của chương, chúng ta sẽ thảo luận về các phương pháp lựa chọn mô hình và các phương pháp rút gọn tham số (shrinkage).","code":""},{"path":"mô-hình-hồi-quy-tuyến-tính.html","id":"những-nội-dung-cơ-bản-của-mô-hình-quy-tuyến-tính","chapter":"Chương 8 Mô hình hồi quy tuyến tính","heading":"8.1 Những nội dung cơ bản của mô hình quy tuyến tính","text":"Trước khi đi vào các nội dung cơ bản, hãy lấy một ví dụ đơn giản về một yêu cầu trên dữ liệu mà có thể tìm được lời giải được bằng mô hình hồi quy tuyến tính. Giả sử một công ty thực hiện một chiến dịch quảng cáo sản phẩm cho 55 cửa hàng trên cả nước trong một năm thông qua ba phương thức quảng cáo là 1. qua truyền hình (\\(TV\\)), 2. qua mạng xã hội (\\(Social\\_Media\\)) và 3. qua tờ rơi (\\(Flyers\\)). Hình vẽ dưới đây mô tả mối liên hệ giữa doanh số bán hàng (\\(Sales\\)-đơn vị tỷ đồng) với chi phí thực hiện các phương thức quảng cáo ở các cửa hàng (đơn vị triệu đồng)Giả sử với vai trò là một chuyên gia tư vấn, chúng ta được yêu cầu đưa ra đề xuất trên cơ sở dữ liệu quan sát được, một kế hoạch quảng cáo cho năm tới nhằm mang lại doanh số bán sản phẩm cao. Thông tin nào từ dữ liệu sẽ hữu ích để đưa ra khuyến nghị cho chiến dịch quảng cáo? Dưới đây là một số câu hỏi quan trọng mà chúng ta cần tìm cách giải quyết nhằm đưa ra khuyến nghịCó mối quan hệ giữa ngân sách chi cho từng hình thức quảng cáo và doanh số bán hàng không? Để trả lời câu hỏi này chúng ta cần xác định xem dữ liệu có cung cấp minh chứng về mối liên hệ tuyến tính giữa chi tiêu cho từng hình thức quảng cáo với doanh số bán hàng. Nếu mối liên hệ là yếu, hoặc thậm chí là mối liên hệ âm thì chúng ta có thể lập luận rằng không nên chi tiền cho quảng cáo!Có mối quan hệ giữa ngân sách chi cho từng hình thức quảng cáo và doanh số bán hàng không? Để trả lời câu hỏi này chúng ta cần xác định xem dữ liệu có cung cấp minh chứng về mối liên hệ tuyến tính giữa chi tiêu cho từng hình thức quảng cáo với doanh số bán hàng. Nếu mối liên hệ là yếu, hoặc thậm chí là mối liên hệ âm thì chúng ta có thể lập luận rằng không nên chi tiền cho quảng cáo!Mối liên hệ giữa ngân sách chi cho quảng cáo và doanh thu nếu tồn tại thì mạnh đến mức nào? Nếu mối liên hệ là mạnh, thì với một ngân sách quảng cáo nhất định, liệu chúng ta có thể dự đoán doanh số bán hàng với độ chính xác cao không, hay dự đoán về doanh số bán hàng dựa trên chi tiêu quảng cáo chỉ tốt hơn một chút với dự đoán ngẫu nhiên?Mối liên hệ giữa ngân sách chi cho quảng cáo và doanh thu nếu tồn tại thì mạnh đến mức nào? Nếu mối liên hệ là mạnh, thì với một ngân sách quảng cáo nhất định, liệu chúng ta có thể dự đoán doanh số bán hàng với độ chính xác cao không, hay dự đoán về doanh số bán hàng dựa trên chi tiêu quảng cáo chỉ tốt hơn một chút với dự đoán ngẫu nhiên?Phương tiện truyền thông nào góp phần tăng doanh số bán hàng? Cả ba phương tiện truyền thông TV, mạng xã hội và phát tờ rơi có đóng góp vào doanh số bán hàng hay chỉ một hoặc hai phương tiện quảng cáo có đóng góp? Để trả lời câu hỏi này, chúng ta phải tìm cách tách biệt những tác động riêng lẻ của từng phương tiện khi chúng ta đã chi tiền cho cả ba phương tiện.Phương tiện truyền thông nào góp phần tăng doanh số bán hàng? Cả ba phương tiện truyền thông TV, mạng xã hội và phát tờ rơi có đóng góp vào doanh số bán hàng hay chỉ một hoặc hai phương tiện quảng cáo có đóng góp? Để trả lời câu hỏi này, chúng ta phải tìm cách tách biệt những tác động riêng lẻ của từng phương tiện khi chúng ta đã chi tiền cho cả ba phương tiện.Chúng ta có thể ước tính chính xác tác động của từng phương tiện đến doanh số bán hàng như thế nào? Với đồng chi cho quảng cáo trên một phương tiện cụ thể, doanh số bán hàng sẽ tăng bao nhiêu? Chúng ta có thể dự đoán mức tăng này chính xác đến mức nào?Chúng ta có thể ước tính chính xác tác động của từng phương tiện đến doanh số bán hàng như thế nào? Với đồng chi cho quảng cáo trên một phương tiện cụ thể, doanh số bán hàng sẽ tăng bao nhiêu? Chúng ta có thể dự đoán mức tăng này chính xác đến mức nào?Mối liên hệ/tác động của chi cho từng hình thức quảng cáo đến doanh số bán hàng có tuyến tính không? Nếu không, liệu có phương pháp biến đổi biến như thế nào để mối liên hệ vẫn là tuyến tính.Mối liên hệ/tác động của chi cho từng hình thức quảng cáo đến doanh số bán hàng có tuyến tính không? Nếu không, liệu có phương pháp biến đổi biến như thế nào để mối liên hệ vẫn là tuyến tính.Có sự tác động qua lại giữa các phương tiện quảng cáo không? Chẳng hạn như nên chi 100 triệu cho quảng cáo trên mạng xã hội và 100 triệu cho quảng cáo tờ rơi liệu có mang lại doanh thu cao hơn việc phân bổ 200 triệu cho riêng từng kênh? Trong tiếp thị quảng cáo, đây được gọi là hiệu ứng tổng hợp.Có sự tác động qua lại giữa các phương tiện quảng cáo không? Chẳng hạn như nên chi 100 triệu cho quảng cáo trên mạng xã hội và 100 triệu cho quảng cáo tờ rơi liệu có mang lại doanh thu cao hơn việc phân bổ 200 triệu cho riêng từng kênh? Trong tiếp thị quảng cáo, đây được gọi là hiệu ứng tổng hợp.Những cơ sở của mô hình hồi quy tuyến tính được thảo luân trong chương này sẽ giúp bạn đọc lần lượt trả lời các câu hỏi của bài toán ở trên.","code":""},{"path":"mô-hình-hồi-quy-tuyến-tính.html","id":"mô-hình-hồi-quy-tuyến-tính-đơn-biến","chapter":"Chương 8 Mô hình hồi quy tuyến tính","heading":"8.1.1 Mô hình hồi quy tuyến tính đơn biến","text":"Hồi quy tuyến tính đơn biến, đúng như tên gọi, đó là một cách tiếp cận tuyến tính đơn giản để dự đoán phản ứng định lượng của một biến mục tiêu \\(Y\\) trên cơ sở biến độc lập \\(X\\). Mô hình giả định rằng có mối quan hệ tuyến tính giữa \\(X\\) và \\(Y\\). Về mặt toán học, chúng ta có thể viết mối quan hệ tuyến tính này như sau\n\\[\\begin{align}\nY \\sim \\beta_0 + \\beta_1 \\cdot X\n\\tag{8.1}\n\\end{align}\\]\nBạn có thể hiểu \\(\\sim\\) theo nghĩa xấp xỉ hoặc gần đúng. Đôi khi chúng ta sẽ mô tả (8.1) bằng cách nói rằng chúng ta đang hồi quy \\(Y\\) theo \\(X\\). Trong ví dụ trình bày ở trên \\(X\\) có thể đại diện cho chi phí quảng cáo trên truyền hình (\\(TV\\)) và \\(Y\\) có thể đại diện cho doanh số bán hàng (\\(Sales\\)) tại các cửa hàng. Sau đó, chúng ta có thể hồi quy doanh số bán hàng theo chi phí quảng cáo trên truyền hình theo một mô hình hồi quy tuyến tính đơn biến như sau\n\\[\\begin{align}\nSales \\sim \\beta_0 + \\beta_1 \\cdot TV\n\\tag{8.2}\n\\end{align}\\]Trong phương trình (8.2), \\(\\beta_0\\) và \\(\\beta_1\\) là hai hằng số chưa biết biểu thị hệ số chặn và hệ số góc của đường thẳng trong mô hình tuyến tính. Cùng với nhau, \\((\\beta_0, \\beta_1)\\) được gọi là các hệ số tuyến tính hoặc tham số của mô hình. Các hệ số này sẽ được ước lượng dựa trên dữ liệu thu thập được dựa trên các phương pháp người xây dựng mô hình lựa chọn. Các ước lượng cho hệ số tuyến tính thường được thêm dấu mũ ở trên để phân biệt với tham số của mô hình tuyến tính, nói cách khác chúng ta có \\(\\hat{\\beta}_0\\) và \\(\\hat{\\beta}_1\\) là các ước lượng của \\(\\beta_0\\) và \\(\\beta_1\\). Với \\(Y\\) là biến doanh thu bán hàng và \\(X\\) là biến chi phí quảng cáo trên truyền hình chúng ta sẽ có một dự đoán cho doanh thu bán hàng \\(\\hat{y}\\) dựa trên một quan sát của chi phí quảng cáo \\(X = x\\)\n\\[\\begin{align}\n\\hat{y} = \\hat{\\beta}_0 + \\hat{\\beta}_1 \\cdot x\n\\tag{8.3}\n\\end{align}\\]Lưu ý rằng chúng tôi luôn sử dụng dấu mũ để mô tả một ước lượng cho một tham số, hoặc là giá trị dự đoán cho giá trị không biết.","code":""},{"path":"mô-hình-hồi-quy-tuyến-tính.html","id":"ước-lượng-hệ-số-trong-mô-hình-đơn-biến","chapter":"Chương 8 Mô hình hồi quy tuyến tính","heading":"8.1.1.1 Ước lượng hệ số trong mô hình đơn biến","text":"Trong mô hình hồi quy tuyến tính đơn biến, \\(\\beta_0\\) và \\(\\beta_1\\) là các tham số không biết và cần được ước lượng. Giá sử dữ liệu chúng ta quan sát được bao gồm \\(n\\) cặp \\((x_i, y_i)\\) như sau\n\\[\\begin{align}\n(x_1, y_1), (x_2, y_2), \\cdots, (x_n, y_n)\n\\end{align}\\]\ntrong đó \\(x_i\\) là các giá trị quan sát được của biến \\(X\\) và \\(y_i\\) là các giá trị quan sát được tương ứng của biến ngẫu nhiên \\(Y\\). Trong ví dụ về chi phí cho quảng cáo, tập dữ liệu này bao gồm ngân sách quảng cáo qua truyền hình và doanh số bán sản phẩm ở \\(n = 55\\) cửa hàng khác nhau. Mục tiêu của chúng ta là thu được các ước lượng cho hệ số \\(\\beta_0\\) và \\(\\beta_1\\) sao cho mô hình tuyến tính (8.2) phù hợp tốt với dữ liệu có sẵn. Nói cách khác, chúng ta muốn tìm hệ số chặn \\(\\beta_0\\) và hệ số góc \\(\\beta_1\\) sao cho đường thẳng kết quả càng gần \\(n = 55\\) điểm dữ liệu càng tốt. Có các phương pháp khác nhau để định nghĩa thế nào là một đường thẳng gần với một tập hợp điểm. Tuy nhiên, cho đến nay cách tiếp cận phổ biến nhất là liên quan đến tối thiểu tổng bình phương khoảng cách từ các điểm đến đường thẳng đó, hay còn gọi là phương pháp \\(bình\\) \\(phương\\) \\(nhỏ\\) \\(nhất\\) và chúng ta áp dụng cách tiếp cận đó trong chương này. Chi tiết về phương pháp bình phương nhỏ nhất sẽ được trình bày trong các phần sau của chương.\nFigure 8.1: Hồi quy đơn biến doanh thu bán hàng theo chi phí quảng cáo trên truyền hình\nVới \\(\\hat{y}_i = \\hat{\\beta}_0 + \\hat{\\beta}_1 \\cdot x_i\\) là dự đoán cho \\(Y\\) dựa trên giá trị thứ \\(\\) của \\(X\\). Khi đó ta ký hiệu \\(e_i = y_i − \\hat{y}_i\\) đại diện cho phần dư đối với quan sát thứ \\(\\). Như vậy phần dư là sự khác biệt giữa giá trị quan sát được của biến mục tiêu và giá trị ước lượng được cho biến mục tiêu được tính toán bởi mô hình tuyến tính. Chúng ta xác định tổng bình phương của phần dư, ký hiệu là RSS (Residual Sum Squares)\n\\[\\begin{align}\nRSS & = e_1^2 + e_2^2 + \\cdots + e_n^2 \\\\\n& = (y_1 - \\hat{\\beta}_0 - \\hat{\\beta}_1 \\cdot x_1) + (y_2 - \\hat{\\beta}_0 - \\hat{\\beta}_1 \\cdot x_2) + \\cdots + (y_n - \\hat{\\beta}_0 - \\hat{\\beta}_1 \\cdot x_n)\n\\end{align}\\]Phương pháp bình phương nhỏ nhất lựa chọn \\(\\hat{\\beta}_0\\) và \\(\\hat{\\beta}_1\\) sao cho giá trị của RSS là nhỏ nhất. Bạn đọc có thể giải bài toán tối ưu bằng cách cho đạo hàm của RSS theo \\(\\hat{\\beta}_0\\) và \\(\\hat{\\beta}_1\\) bằng 0 và cho kết quả như sau\n\\[\\begin{align}\n\\hat{\\beta}_1 & = \\cfrac{\\sum\\limits_{=1}^n (x_i - \\bar{x}) (y_i - \\bar{y})}{\\sum\\limits_{=1}^n (x_i - \\bar{x})^2} \\\\\n\\hat{\\beta}_0 & = \\bar{y} - \\hat{\\beta}_1 \\bar{x}\n\\tag{8.4}\n\\end{align}\\]\ntrong đó \\(\\bar{y} = \\sum\\limits_{=1}^n y_i\\) và \\(\\bar{x} = \\sum\\limits_{=1}^n x_i\\) là các giá trị trung bình của các quan sát.Hàm số để thực hiện ước lượng mô hình tuyến tính trong R là hàm lm(). Bạn đọc thực hiện câu lệnh ước lương như sauHình 8.1 mô tả đường hồi quy tuyến tính đơn được xây dựng trên dữ liệu về quảng cáo với biến phụ thuộc là doanh số bán hàng và biến độc lập là chi phí quảng cáo trên truyền hình, với \\(\\hat{\\beta}_0\\) = 5.1030285 và \\(\\hat{\\beta}_1\\) = 0.0240875. Nói cách khác, theo các ước lượng này, thêm 1 triệu đồng chi cho quảng cáo truyền hình có liên quan đến việc tăng thêm khoảng 24.1 triệu từ doanh thu bán hàng.","code":"\n# Lấy dữ liệu advertising vào R\nAdvertising<-read.csv(\".../advertise.csv\")\n# Thực hiện ước lượng mô hình\n# Doanh thu bán hàng (Sales) hồi quy theo TV\nlm(Sales~TV, data = Advertising)"},{"path":"mô-hình-hồi-quy-tuyến-tính.html","id":"đánh-giá-sự-chính-xác-của-các-ước-lượng","chapter":"Chương 8 Mô hình hồi quy tuyến tính","heading":"8.1.1.2 Đánh giá sự chính xác của các ước lượng","text":"Nhắc lại rằng trong các mô hình học máy có giám sát, mối liên hệ thực tế giữa biến mục tiêu \\(Y\\) và biến phụ thuộc \\(X\\) được mô tả thông qua một hàm \\(f\\)\n\\[\\begin{align}\nY = f(X) + \\epsilon\n\\end{align}\\]\ntrong đó \\(\\epsilon\\) là thành phần hoàn toàn độc lập với biến \\(X\\), hay nói một cách khác là không thể đưa ra được thêm bất kỳ thông tin nào về \\(\\epsilon\\) dựa trên dữ liệu \\(X\\). Trong trường hợp hàm \\(f\\) thực sự là một hàm tuyến tính, mối liên hệ giữa \\(X\\) và \\(Y\\) sẽ được mô tả thông qua hệ số chặn \\(\\beta_0\\) và hệ số góc \\(\\beta_1\\)\n\\[\\begin{align}\nY = \\beta_0 + \\beta_1 X + \\epsilon\n\\tag{8.5}\n\\end{align}\\]\nMô hình tuyến tính trong phương trình (8.5) được gọi là đường hồi quy tuyến tính thật. Trong thực tế, không thể biết được đường hồi quy tuyến tính thật, mà chúng ta chỉ có thể dựa trên các giá trị quan sát được của biến mục tiêu và biến phụ thuộc để ước lượng ra các tham số \\(\\hat{\\beta}_0\\) và \\(\\hat{\\beta}_1\\). Đường thẳng có hệ số chặn \\(\\hat{\\beta}_0\\) và hệ số góc \\(\\hat{\\beta}_1\\) được gọi là đường hồi quy tuyến tính ước lượng được.\nFigure 8.2: Hình bên trái: Đường hồi quy tuyến tính thật màu đỏ và đường hồi quy tuyến tính ước lượng màu đen. Hình bên phải: Đường hồi quy tuyến tính thật màu đỏ và các đường hồi quy tuyến tính ước lượng màu xanh\nĐường màu đỏ ở phần bên trái của Hình 8.2 hiển thị đường hồi quy tuyến tính thực, \\(f(x) = 1 + 2 \\cdot x\\), trong khi đường màu đen là ước lượng bình phương nhỏ nhất dựa trên dữ liệu quan sát được. Mối quan hệ thực sự thường không thể biết được dữ liệu thực, nhưng đường bình phương nhỏ nhất luôn có thể được ước lượng bằng cách sử dụng phương trình (8.4). Nói cách khác, trong các ứng dụng thực tế, mỗi khi chúng ta có một tập hợp các quan sát mà từ đó chúng ta có thể tính toán đường bình phương nhỏ nhất; tuy nhiên, đường hồi quy tuyến tính thực là thể quan sát được. Trong phần bên phải của Hình 8.2, chúng tôi đã tạo ra mười bộ dữ liệu khác nhau từ mô hình hồi quy thật và vẽ mười đường bình phương nhỏ nhất tương ứng. Lưu ý rằng các tập dữ liệu khác nhau được tạo ra từ cùng một mô hình thực sẽ dẫn đến các đường bình phương nhỏ nhất hơi khác nhau một chút, nhưng đường hồi quy tổng thể không quan sát được không thay đổi.Quan sát trên hình vẽ, sự khác biệt giữa đường hồi quy tổng thể và đường bình phương nhỏ nhất có vẻ khó nhận thấy và khó hiểu. Về cơ bản, khái niệm hai đường này là sự mở rộng của phương pháp thống kê tiêu chuẩn về việc sử dụng thông tin từ một mẫu để ước tính các đặc điểm của một tổng thể. Ví dụ, giả sử chúng ta muốn biết trung bình tổng thể \\(\\mu\\) của một biến ngẫu nhiên \\(Y\\) nào đó. \\(\\mu\\) là một giá trị không biết, nhưng chúng ta có \\(n\\) quan sát từ của \\(Y\\), mà chúng ta có thể viết là \\(y_1, y_2, \\cdots , y_n\\) và chúng ta có thể sử dụng để ước lượng \\(\\mu\\). Một ước lượng hợp lý cho \\(\\mu\\) là \\(\\hat{\\mu} = \\bar{y}\\), trong đó \\(\\bar{y} = \\sum\\limits_{=1}^n y_i\\) là giá trị trung bình mẫu. Trung bình mẫu và trung bình tổng thể là khác nhau, nhưng nói chung trung bình mẫu sẽ cung cấp ước tính tốt về trung bình tổng thể. Theo cách tương tự, các hệ số chưa biết \\(\\beta_0\\) và \\(\\beta_1\\) trong hồi quy tuyến tính xác định đường hồi quy tổng thể. Chúng ta ước lượng các hệ số chưa biết này bằng cách sử dụng \\(\\hat{\\beta_0}\\) và \\(\\hat{\\beta_1}\\).Sự tương tự giữa ước lượng các hệ số của hồi quy tuyến tính và ước lượng giá trị trung bình của một biến ngẫu nhiên còn được thể hiện qua các tính chất của các ước lượng. Chẳng hạn như chúng ta luôn mong muốn các ước lượng là các ước lượng không chệch. Đối với trung bình tổng thể, chúng ta luôn tìm các ước lượng \\(\\hat{\\mu}\\) sao cho \\(\\mathbb{E}(\\hat{\\mu}) = \\mu\\). Tính chất không chệch của ước lượng đảm bảo rằng nếu chúng ta có thể có một số lượng đủ lớn các quan sát thì giá trị trung bình mẫu sẽ xấp xỉ giá trị trung bình tổng thể. Các hệ số của mô hình hồi quy tuyến tính bằng phương pháp bình phương nhỏ nhất cũng là các ước lượng không chệch, nghĩa là nếu chúng ta ước lượng \\(\\beta_0\\) và \\(\\beta_1\\) trên cơ sở một tập dữ liệu cụ thể thì các ước lượn sẽ không chính xác bằng \\(\\beta_0\\) và \\(\\beta_1\\). Nhưng nếu chúng ta có thể tính trung bình các ước lượng thu được từ một số lượng lớn tập dữ liệu thì giá trị trung bình của các ước lượng này sẽ xấp xỉ \\(\\beta_0\\) và \\(\\beta_1\\)! Trên thực tế, chúng ta có thể thấy từ bảng bên phải của Hình 8.2 rằng giá trị trung bình của nhiều đường bình phương tối thiểu, mỗi đường được ước tính từ một tập dữ liệu riêng biệt, khá gần với đường hồi quy tổng thể thực.Một câu hỏi khác cần được đặt ra với ước lượng trung bình tổng thể \\(\\mu\\) của biến ngẫu nhiên \\(Y\\) là: giá trị trung bình mẫu \\(\\hat{\\mu}\\) ước tính của \\(\\mu\\) chính xác như thế nào? Chúng ta đã biết rằng giá trị trung bình của các \\(\\hat{\\mu}\\) trên nhiều tập dữ liệu sẽ rất gần với \\(\\mu\\), nhưng một ước tính duy nhất của \\(\\hat{\\mu}\\) trên một dự liệu cụ thể sẽ chênh lệch với \\(\\mu\\) như thế nào? Nhìn chung, để trả lời câu hỏi này chúng ta cần tính độ lệch chuẩn của \\(\\hat{\\mu}\\), được ký hiệu là \\(SE(\\hat{\\mu})\\). Chúng ta đã biết rằng\n\\[\\begin{align}\nSE(\\hat{\\mu}) = \\cfrac{\\sigma}{\\sqrt{n}}\n\\end{align}\\]\nvới \\(\\sigma\\) là độ lệch chuẩn của biến \\(Y\\). \\(SE(\\hat{\\mu})\\) cho chúng ta biết một ước lượng cụ thể \\(\\hat{\\mu}\\) sẽ chênh lệch với \\(\\mu\\) như thế nào. Có thể dễ dàng thấy rằng khi số lượng quan sát \\(n\\) đủ lớn, \\(SE(\\hat{\\mu})\\) sẽ càng gần đến 0 và chênh lệch giữa \\(\\hat{\\mu}\\) với \\(\\mu\\) sẽ càng nhỏ. Lập luận hoàn toàn tương tự, để biết các ước lượng cho các hệ số chặn và hệ số góc trong mô hình tuyến tính đơn chênh lệch với các giá trị thật \\(\\beta_0\\) và \\(\\beta_1\\) như thế nào, chúng ta cần tính toán độ lệch chuẩn của các ước lượng đó. Tính toán độ lệch chuẩn của các ước lượng cho hệ số sẽ được trình bày chi tiết trong các phần sau. Bạn đọc cần biết là độ lệch chuẩn của \\(\\hat{\\beta_0}\\) và \\(\\hat{\\beta_1}\\) có thể tính toán được như sau\n\\[\\begin{align}\nSE(\\hat{\\beta}_0) = \\sigma \\cdot \\sqrt{\\cfrac{1}{n} + \\cfrac{\\bar{x}^2}{\\sum\\limits_{=1}^n (x_i - \\bar{x})^2} } ; SE(\\hat{\\beta}_1) = \\cfrac{\\sigma}{\\sqrt{\\sum\\limits_{=1}^n (x_i - \\bar{x})^2} }\n\\tag{8.7}\n\\end{align}\\]\ntrong đó \\(\\sigma = \\sqrt{Var(\\epsilon)}\\). Các công thức cho độ lệch tiêu chuẩn của \\(\\hat{\\beta_0}\\) và \\(\\hat{\\beta_1}\\) ở trên đi kèm với giả định là các sai số \\(\\epsilon_i\\) là độc lập với nhau và có cùng phương sai là \\(\\sigma^2\\). Giả thiết này thường không đạt được trong thực tế tuy nhiên công thức (8.7) vẫn là một xấp xỉ tốt cho phương sai của các ước lượng. Lưu ý trong công thức ở trên rằng \\(SE(\\hat{\\beta}_1)\\) nhỏ hơn khi \\(x_i\\) trải rộng hơn quanh giá trị trung bình của nó. Chúng ta cũng thấy rằng \\(SE(\\hat{\\beta}_0)\\) sẽ giống như độ lệch chuẩn của trung bình mẫu nếu \\(\\bar{x}\\) bằng 0. Điểm đáng lưu ý là \\(\\sigma^2\\) cũng là một đại lượng chưa biết chưa nhưng có thể ước lượng được từ dữ liệu. Ước lượng cho \\(\\sigma\\) được gọi là độ lệch chuẩn của phần dư, ký hiệu RSE (Residual Standard Error), và được tính theo công thức RSE = .RSE có thể được sử dụng để tính toán các khoảng tin cậy. Khoảng tin cậy ở một mức xác suất, chẳng hạn như mức \\(\\alpha\\), được định nghĩa là một khoảng giá trị sao cho với xác suất \\(\\alpha\\), khoảng giá trị đó sẽ chứa giá trị thực chưa biết của tham số. Với giả thiết phần dư \\(\\epsilon\\) có phân phối chuẩn, có thể chứng minh được rằng \\(\\hat{\\beta_0}\\) và \\(\\hat{\\beta_1}\\) cũng có phân phối chuẩn. Khoảng tin cậy ở mức xác suất \\(\\alpha\\) được sử dụng là làm các khoảng tin cây cho tham số \\(\\beta_i\\) có dạng\n\\[\\begin{align}\n\\left[\\hat{\\beta_i} - z_{1 + \\alpha/2} \\cdot SE(\\hat{\\beta_i}); \\hat{\\beta_i} + z_{1 + \\alpha/2} \\cdot SE(\\hat{\\beta_i}) \\right]\n\\end{align}\\]\ntrong đó \\(z_{1+\\alpha/2}\\) là giá trị tại mức xác suất \\((1 + \\alpha/2)\\) của biến ngẫu nhiên phân phối chuẩn \\(\\mathcal{N}(0,1)\\).Trong ví dụ về quảng cáo, với mức xác suất \\(\\alpha = 95\\%\\), giá trị tại mức xác suất \\((1 + \\alpha/2)\\) của phân phối chuẩn \\(\\mathcal{N}(0,1)\\) xấp xỉ bằng 1.96; ta có khoảng tin cậy cho hệ số chặn là [3.372; 6.834] và khoảng tin cậy cho hệ số góc là [0.0124; 0.0357]. Điều này có nghĩa là, không tính đến quảng cáo trên truyền hình, doanh thu trung bình của các cửa hàng rơi vào khoảng 3.372 tỷ đồng đến 6.834 tỷ đồng. Đồng thời, mỗi triệu đồng tăng thêm cho quảng cáo trên truyền hình, sẽ làm cho doanh thu trung bình tăng thêm khoảng 12.4 triệu đồng đến 35.7 triệu đồng.Độ lệch tiêu chuẩn còn được sử dụng để trả lời câu hỏi là liệu mối liên hệ giữa biến \\(X\\) và \\(Y\\) có thực sự có ý nghĩa. Theo thống kê toán, chúng ta cần phải kiểm đinh cặp giả thuyết:\n\\[\\begin{align}\nH_0: \\beta_1 = 0 \\\\\nH_1: \\beta_1 \\neq 0\n\\end{align}\\]\nvì nếu \\(\\beta_1 = 0\\) thì mô hình hồi quy tuyến tính đơn trở thành \\(Y = \\beta_0 + \\epsilon\\) và \\(X\\) không có liên hệ với \\(Y\\) . Để kiểm định giả thuyết \\(H_0\\), chúng ta cần xác định xem liệu ước lượng của \\(\\beta_1\\), là \\(\\hat{\\beta}_1\\), có đủ xa giá trị 0 để chúng ta có thể tin tưởng rằng \\(\\beta_1\\) khác 0 hay không. Nhưng như thế nào là đủ xa thì lại phụ thuộc vào độ chính xác của \\(\\hat{\\beta}_1\\), nghĩa là cũng phụ thuộc vào \\(SE(\\hat{\\beta}_1)\\). Nếu \\(\\hat{\\beta}_1\\) tương đối nhỏ, nhưng \\(SE(\\hat{\\beta}_1)\\) lại rất nhỏ, thì chúng ta vẫn có thể khá chắc chắn rằng \\(\\beta \\neq 0\\), và đó có mối liên hệ giữa \\(X\\) và \\(Y\\). Ngược lại, nếu \\(\\hat{\\beta}_1\\) tương đối xa giá trị 0, nhưng \\(SE(\\hat{\\beta}_1)\\) lại rất lớn, thì cũng rất khó để khẳng định rằng \\(\\beta \\neq 0\\). Trong thực tế, chúng ta tính toán một thống kê \\(t\\)\n\\[\\begin{align}\nt = \\cfrac{\\hat{\\beta}_1 - 0}{SE(\\hat{\\beta}_1)}\n\\tag{8.8}\n\\end{align}\\]\ndùng để đo độ lệch tương đối giữa \\(\\hat{\\beta}\\) với giá trị 0. Với giả thiết \\(\\epsilon\\) có phân phối chuẩn, và dưới giả thuyết \\(H_0\\), có thể chứng minh được rằng thống kê \\(t\\) sẽ có phân phối \\(student\\) với bậc tự là \\(n-2\\). Phân phối \\(student\\) cũng có hình dạng quả chuông giống như phân phối chuẩn và sẽ tiệm cận phân phối chuẩn nếu tham số bậc tự đủ lớn. đó, chúng ta có thể tính toán được xác suất mà một biến ngẫu nhiên phân phối \\(student\\) bất kỳ có giá trị tuyệt đối lớn hơn hoặc bằng giá trị thống kê \\(t\\) tính toán được trong phương trình (8.8). Xác suất này còn thường được gọi là \\(p-value\\). Nhìn chung, chúng ta có thể suy diễn \\(p-value\\) như sau: nếu \\(p-value\\) nhận giá trị nhỏ thì rất khó có thể có được giá trị thống kê \\(t\\) có giá trị tuyệt đối lớn như vậy dưới giả thuyết \\(H_0\\), nghĩa là có cơ sở để bác bỏ giả thuyết \\(H_0\\). Hay nói một cách khác, có mối liên hệ giữa biến độc lập và biến mục tiêu. Giá trị \\(p-value\\) thường được coi là nhỏ nếu nằm dưới các ngưỡng như 5% hoặc thậm chí 1%.\nTable 8.1: Các hệ số ước lượng trong mô hình hồi quy đơn cho dữ liệu Quảng cáo.\nBảng 8.1 cung cấp thông tin chi tiết về tham số ước lượng được trong mô hình hồi quy tuyến tính đơn bằng phương pháp bình phương nhỏ nhất để hồi quy doanh thu bán hàng đơn theo ngân sách quảng cáo trên truyền hình trong dữ liệu về quảng cáo. Lưu ý rằng các hệ số \\(\\hat{\\beta_0}\\) và \\(\\hat{\\beta_1}\\) rất lớn với độ lệch chuẩn của các ước lượng này, đó giá trị thống kê t cũng lớn. Xác suất để một biến ngẫu nhiên phân phối \\(student\\) có tham số bậc tự bằng 55 - 2 = 53 có giá trị lớn hơn các giá trị tuyệt đối của thống kê \\(t\\) dưới giả thuyết \\(H_0\\) đúng là gần như bằng 0. đó chúng ta có thể kết luận rằng \\(\\beta_0 \\neq 0\\) và \\(\\beta_1 \\neq 0\\).","code":""},{"path":"mô-hình-hồi-quy-tuyến-tính.html","id":"đánh-giá-mô-hình-hồi-quy-tuyến-tính-đơn","chapter":"Chương 8 Mô hình hồi quy tuyến tính","heading":"8.1.1.3 Đánh giá mô hình hồi quy tuyến tính đơn","text":"Sau khi chúng ta đã bác bỏ giả thuyết về các hệ số bằng 0, công việc tiếp theo sẽ là đưa ra các đánh giá định lượng về mức độ phù hợp của mô hình với dữ liệu. Chất lượng của mô hình hồi quy tuyến tính thường được đánh giá bằng cách sử dụng hai đại lượng: thứ nhất: độ lệch chuẩn của phần dư (RSE) và hệ số \\(R^2\\).Độ lệch chuẩn của phần dư, được viết tắt là RSE (Residual Standard Error). Phần dư trong mô hình tuyến tính là phần độc lập với biến giải thích \\(X\\), nghĩa là trong các mô hình có sự hiện diện của phần dư, ngay cả khi chúng ta biết được đường hồi quy thực cũng không thể đưa ra dự đoán chính xác được về \\(Y\\) từ \\(X\\). Phần dư luôn có giá trị trung bình bằng 0 vì nếu không phần giá trị trung bình sẽ được giải thích bằng hệ số chặn. đó, người xây dựng mô hình thường quan tâm đến độ lệch chuẩn của phần dư, hay còn gọi là RSE\n\\[\\begin{align}\nRSE = \\sqrt{\\cfrac{RSS}{n-2}} = \\sqrt{\\cfrac{\\sum\\limits_{=1}^n (y_i - \\hat{y}_i)^2 }{n-2}}\n\\tag{8.9}\n\\end{align}\\]Trong mô hình hồi quy tuyến tính trên dữ liệu Quảng cáo, RSE bằng 0.235. Điều này có nghĩa là sai số giữa doanh thu bán hàng thực tế và doanh thu bán hàng được ước lượng từ mô hình hồi quy tuyến tính sẽ có độ lệch chuẩn khoảng 0.235 tỷ đồng. Tuy nhiên, độ lệch chuẩn là 0.235 có phải là con số có thể chấp nhận được hay không còn tùy thuộc vào bối cảnh. Trong dữ liệu quảng cáo, giá trị trung bình của doanh số bán hàng trên tất cả các cửa hàng là khoảng 8.7 tỷ đồng và đó sai số phần trăm là khoảng 0.235/8.7 \\(\\approx\\) 2.7 %. RSE có thể coi là thước đo mức độ phù hợp của các mô hình tuyến tính trên dữ liệu. Nếu các dự đoán thu được bằng cách sử dụng mô hình rất gần với giá trị kết quả thực, tức là, nếu các \\(\\hat{y}_i\\) rất gần với các \\(y_i\\), với \\(= 1, 2, \\cdots, n\\), khi đó RSE sẽ nhỏ và chúng ta có thể kết luận rằng mô hình rất phù hợp với dữ liệu. Mặt khác, \\(\\hat{y}_i\\) rất xa các \\(y_i\\) đối với một hoặc nhiều quan sát thì RSE có thể khá lớn, cho thấy mô hình không phù hợp với dữ liệu.RSE là một thước đo tuyệt đối về sự phù hợp hay không của mô hình hồi quy tuyến tính trên dữ liệu. Nhưng vì RSE được đo bằng độ lệch chuẩn nên khi tính toán RSE tương đối trên giá trị trung bình của \\(Y\\) sẽ không cho chúng ta một cái nhìn chính xác thế nào là một RSE tốt. Thay vào đó, hệ số \\(R^2\\) cung cấp một thước đo tương đối về mức độ phù hợp của mô hình. \\(R^2\\) có dạng tỷ lệ giữa phương sai được giải thích trên tổng phương sai nên hệ số này luôn nhận giá trị từ 0 đến 1 và không phụ thuộc vào đơn vị của biến \\(Y\\). Hệ số \\(R^2\\), hay còn được gọi là \\(R-squared\\), được tính bằng công thức sau\n\\[\\begin{align}\nR^2 = \\cfrac{TSS - RSS}{TSS} = 1 - \\cfrac{RSS}{TSS}\n\\tag{8.9}\n\\end{align}\\]\nvới \\(TSS = \\sum (y_i - \\bar{y})^2\\).\\(TSS\\) (Total Sum Squares) là tổng phương sai của biến mục tiêu \\(Y\\) và có thể được coi là mức độ biến thiên của biến mục tiêu xung quanh giá trị trung bình của nó. Giá trị này không phụ thuộc vào mô hình hồi quy tuyến tính. \\(RSS\\) đo lường mức độ biến thiên mà không giải thích được bởi mô hình hồi quy tuyến tính. đó, \\(TSS − RSS\\) đo lường mức độ biến thiên được giải thích bằng cách thực hiện hồi quy và hệ số \\(R^2\\) đo lường tỷ lệ biến thiên của biến mục tiêu \\(Y\\) có thể được giải thích bằng biến giải thích \\(X\\) trong mô hình tuyến tính. Hệ số \\(R^2\\) gần bằng 1 cho biết rằng phần lớn sự biến thiên trong biến mục tiêu đã được giải thích bằng mô hình hồi quy. Giá trị \\(R^2\\) càng gần 0 cho thấy mô hình hồi quy không giải thích được nhiều về sự biến thiên của biến mục tiêu. Mô hình có \\(R^2\\) nhỏ thông thường là dạng của mô hình tuyến tính là sai, mô hình tuyến tính bị thiếu biến giải thích, hoặc phần dư \\(\\epsilon\\) có phương sai lớn.Các đại lượng \\(RSS\\), \\(RSE\\), \\(TSS\\), và \\(R^2\\) trong mô hình tuyến tính đơn mà biến mục tiêu doanh thu được hồi quy theo chi phí quảng cáo trên truyền hình được cho trong bảng 8.2\nTable 8.2: Đo lường mức độ phù hợp của mô hình tuyến tính đơn trên dữ liệu Quảng cáo\nTrong Bảng 8.2, \\(R^2\\) bằng 0.237 có nghĩa là chỉ 23.7% sự biến thiên trong doanh số được giải thích bằng hồi quy tuyến tính theo chi phí quảng cáo trên truyền hình. Rõ ràng, hệ số \\(R^2\\) dễ dàng diễn giải hơn với \\(RSE\\), vì đại lượng này luôn nằm trong khoảng từ 0 đến 1. Tuy nhiên, vẫn có thể gặp khó khăn khi xác định thế nào là giá trị \\(R^2\\) tốt. Điều này lại tùy thuộc vào từng ngữ cảnh thực tế. Ví dụ, trong một số dữ liệu thu thập được từ vật lý hay khoa học máy tính, chúng ta có thể đã biết rằng dữ liệu thực sự đến từ một mô hình tuyến tính có phần dư nhỏ. Hệ số \\(R^2\\) thu được sẽ xấp xỉ bằng 1 và giá trị \\(R^2\\) nhỏ hơn 0.9 có thể cho thấy có vấn đề với thử nghiệm mà dữ liệu được tạo ra. Mặt khác, trong các ứng dụng điển hình về kinh tế hay xã hội học, các mô hình tuyến tính thường có phần dư có phương sai rất lớn có rất nhiều các yếu tố khác không được đo lường được từ dữ liệu. Trong trường hợp này, chúng ta chỉ cần một tỷ lệ phương sai được giải thích rất nhỏ. Hệ số \\(R^2\\) bằng 0.05 hoặc 0.1 trong các dữ liệu như vậy lại có thể là bằng chứng cho một mô hình tốt!Trong mô hình hồi quy tuyến tính đơn biến, giá trị \\(R^2\\) chính là bình phương của hệ số tương quan giữa biến mục tiêu \\(Y\\) và biến độc lập \\(X\\). Hệ số tương quan giữa \\(Y\\) và \\(X\\), ký hiệu \\(\\rho{X,Y}\\), và được ước lượng bằng công thức như sau\n\\[\\begin{align}\n\\hat{\\rho}(X,Y) = \\cfrac{ \\sum\\limits_{=1}^n (x_i - \\bar{x})(y_i - \\bar{y}) }{ \\sqrt{\\sum\\limits_{=1}^n (x_i - \\bar{x})^2}\\sqrt{\\sum\\limits_{=1}^n (y_i - \\bar{y})^2} }\n\\end{align}\\]Hệ số tương quan \\(\\rho(X,Y)\\) đo lường mối liên hệ tuyến tính giữa hai biến \\(X\\) và \\(Y\\). Hệ số \\(\\rho(X,Y)\\) nằm trong khoảng \\([-1,1]\\), vàKhi \\(\\rho(X,Y) = 0\\), chúng ta nói rằng giữa \\(X\\) và \\(Y\\) không có mối liên hệ tuyến tính.Khi \\(\\rho(X,Y) = 0\\), chúng ta nói rằng giữa \\(X\\) và \\(Y\\) không có mối liên hệ tuyến tính.Khi \\(\\rho(X,Y) > 0\\), chúng ta nói rằng giữa \\(X\\) và \\(Y\\) có mối liên hệ tuyến tính cùng chiều, nghĩa là khi \\(X\\) tăng thì nhiều khả năng \\(Y\\) cũng sẽ tăng theo một tỷ lệ cố định.Khi \\(\\rho(X,Y) > 0\\), chúng ta nói rằng giữa \\(X\\) và \\(Y\\) có mối liên hệ tuyến tính cùng chiều, nghĩa là khi \\(X\\) tăng thì nhiều khả năng \\(Y\\) cũng sẽ tăng theo một tỷ lệ cố định.Khi \\(\\rho(X,Y) < 0\\), chúng ta nói rằng giữa \\(X\\) và \\(Y\\) có mối liên hệ tuyến tính ngược chiều, nghĩa là khi \\(X\\) tăng thì nhiều khả năng \\(Y\\) sẽ giảm theo một tỷ lệ cố định.Khi \\(\\rho(X,Y) < 0\\), chúng ta nói rằng giữa \\(X\\) và \\(Y\\) có mối liên hệ tuyến tính ngược chiều, nghĩa là khi \\(X\\) tăng thì nhiều khả năng \\(Y\\) sẽ giảm theo một tỷ lệ cố định.Bạn đọc hãy lưu ý rằng đẳng thức \\(R^2 = \\left(\\hat{\\rho}(X,Y)\\right)^2\\) chỉ đúng trong mô hình hồi quy đơn biến. Trong phần tiếp theo, chúng ta sẽ thảo luận về mô hình hồi quy tuyến tính đa biến, trong đó chúng ta sẽ sử dụng đồng thời nhiều biến độc lập để giải thích một biến mục tiêu. Chúng ta sẽ thấy nhiều hơn ý nghĩa của hệ số \\(R^2\\) trong các mô hình như vậy.","code":""},{"path":"mô-hình-hồi-quy-tuyến-tính.html","id":"mô-hình-hồi-quy-tuyến-tính-đa-biến","chapter":"Chương 8 Mô hình hồi quy tuyến tính","heading":"8.1.2 Mô hình hồi quy tuyến tính đa biến","text":"Hồi quy tuyến tính đơn là một phương pháp hữu ích để dự đoán biến mục tiêu dựa trên một biến giải thích duy nhất. Tuy nhiên, trong thực tế chúng ta thường có nhiều hơn một yếu tố để giải thích biến mục tiêu. Ví dụ: trong dữ liệu Quảng cáo, chúng ta đã kiểm tra mối quan hệ giữa doanh thu bán hàng và ngân sách quảng cáo trên truyền hình. Chúng ta cũng có dữ liệu về số tiền chi cho quảng cáo trên các mạng xã hội và quảng cáo thông qua tờ rơi, đó chúng ta sẽ muốn biết rằng liệu các phương thức quảng cáo này có liên quan đến doanh thu bán hàng hay không; nghĩa là làm cách nào chúng ta có thể mở rộng phân tích dữ liệu Quảng cáo để phù hợp khi bổ sung thêm hai biến giải thích.Bạn đọc có thể sử dụng ba mô hình hồi quy tuyến tính đơn riêng biệt, mỗi mô hình sử dụng một biến giải thích tương ứng với một phương thức quảng cáo khác nhau làm biến giải thích. Kết quả ước lượng ba mô hình tuyến tính đơn được cho trong Bảng 8.3\nTable 8.3: Ước lượng ba mô hình đơn biến dữ liệu quảng cáo\nBạn đọc có thể nhận thấy rằng nếu sử dụng ba mô hình hồi quy đơn, các biến giải thích đều có tác động lên biến mục tiêu một cách có ý nghĩa các giá trị p-value đều rất nhỏ. Chúng ta sẽ thảo luận chi tiết về các hệ số tuyến tính trong Bảng 8.3 trong phần sau của cuốn sách. Tuy nhiên cách tiếp cận như trên sẽ gặp phải hai vấn đề. Thứ nhất: chúng ta sẽ không biết làm thế nào để đưa ra một dự đoán duy nhất về doanh thu bán hàng tương ứng với một phân bổ ngân sách quảng cáo cho ba hình thức quảng cáo, vì khi phân bổ ngân sách đến từng phương tiện quảng cáo sẽ có ba giá trị dự đoán riêng biệt. Thứ hai, mỗi phương trình hồi quy đơn đều bỏ qua hai phương tiện còn lại trong việc hình thành ước tính cho các hệ số hồi quy. Chúng ta sẽ sớm thấy rằng nếu ngân sách truyền thông có tương quan với nhau tại các cửa hàng, điều mà rất có thể xảy ra, thì điều này có thể dẫn đến những ước lượng có sai lệch rất lớn về tác động của từng phương tiện quảng cáo lên doanh thu bán hàng.Thay vì điều chỉnh một mô hình hồi quy tuyến tính đơn giản riêng biệt cho từng yếu tố dự đoán, cách tiếp cận tốt hơn là mở rộng mô hình hồi quy tuyến tính đơn bằng cách cho tương ứng với mỗi biến giải thích một hệ số góc riêng. Nói chung, giả sử rằng chúng ta có \\(p\\) biến giải thích riêng biệt. Khi đó mô hình hồi quy tuyến tính đa biến có dạng\n\\[\\begin{align}\nY = \\beta_0 + \\beta_1 \\cdot X_1 + \\beta_2 \\cdot X_2 + \\cdots + \\beta_p \\cdot X_p + \\epsilon\n\\tag{8.10}\n\\end{align}\\]\ntrong đó \\(X_j\\) đại diện cho biến giải thích thứ \\(j\\) và hệ số \\(\\beta_j\\) định lượng mối liên hệ tuyến tính giữa biến giải thích đó và biến mục tiêu. Có thể coi \\(\\beta_j\\) là đại lượng phản ánh sự thay đổi của biến mục tiêu \\(Y\\) khi biến giải thích \\(X_j\\) tăng thêm một đơn vị trong khi tất cả các biến giải thích khác không thay đổi. Trong ví dụ về dữ liệu quảng cáo, ta có mô hình hồi quy tuyến tính như sau\n\\[\\begin{align}\nSales = \\beta_0 + \\beta_1 \\times TV + \\beta_2 \\times Social\\_Media + \\beta_3 \\times Flyer + \\epsilon\n\\tag{8.11}\n\\end{align}\\]\ntrong đó \\(Sales\\) là doanh thu từ bán hàng của 55 cửa hàng, \\(TV\\) là chi phí quảng cáo trên truyền hình, \\(Social\\_Media\\) là chi phí quảng cáo qua mạng xã hội, và \\(Flyer\\) là chi phí quảng cáo qua tờ rơi.","code":""},{"path":"mô-hình-hồi-quy-tuyến-tính.html","id":"ước-lượng-tham-số-cho-mô-hình-đa-biến","chapter":"Chương 8 Mô hình hồi quy tuyến tính","heading":"8.1.2.1 Ước lượng tham số cho mô hình đa biến","text":"Tương tự như trong mô hình hồi quy tuyến tính đơn, các hệ số hồi quy \\(\\beta_0\\), \\(\\beta_1\\), \\(\\cdots\\), \\(\\beta_p\\) trong phương trình (8.10) là chưa biết và cần được ước lượng. Các tham số này cũng được ước lượng bằng cách sử dụng phương pháp bình phương nhỏ nhất. Tuy nhiên, không giống như các ước lượng hồi quy tuyến tính đơn, các ước lượng hệ số hồi quy đa biến khá phức tạp cần được biểu diễn dưới dạng véc-tơ và ma trận. Chính vì lý này, chúng tôi không đi sâu vào vấn đề này ở đây. Chi tiết của phương pháp bình phương nhỏ nhất trong hồi quy đa biến bạn đọc có thể tham khảo trong phần 8.2. Với các ước lượng \\(\\hat{\\beta}_0\\), \\(\\hat{\\beta}_1\\), \\(\\cdots\\), \\(\\hat{\\beta}_p\\) chúng ta có thể đưa ra dự đoán cho biến mục tiêu \\(y\\) như sau\n\\[\\begin{align}\n\\hat{y} = \\hat{\\beta}_0 + \\hat{\\beta}_1 \\cdot x_1 + \\hat{\\beta}_2 \\cdot x_2 + \\cdots + \\hat{\\beta}_p \\cdot x_p\n\\tag{8.12}\n\\end{align}\\]Bạn đọc có thể sử dụng hàm lm() để thực hiện ước lượng các hệ số tuyến tính. Chúng ta thực hiện ước lượng mô hình đa biến cho dữ liệu Quảng cáo như sau\nTable 8.4: Các hệ số ước lượng trong mô hình hồi quy đa biến cho dữ liệu Quảng cáo.\nBảng 8.4 hiển thị ước tính hệ số hồi quy bội khi ngân sách quảng cáo trên truyền hình, mạng xã hội, và tờ rơi được sử dụng để dự đoán doanh thu bán sản phẩm trên dữ liệu Quảng cáo. Chúng ta có thể giải thích những kết quả này như sau:Đối với một ngân sách cố định cho quảng cáo trên truyền hình và một ngân sách cố định cho quảng cáo qua mạng xã hội, việc chi thêm 1 triệu đồng cho quảng cáo bằng hình thức tờ rơi sẽ dẫn đến doanh thu bán hàng trung bình tăng khoảng 11.7 triệu đồng. Con số tương tự với quảng cáo trên truyền hình và qua mạng xã hội lần lượt là 63 triệu đồng và 16.3 triệu đồng. sánh các con số này với Bảng 8.3, chúng ta có thể nhận thấy rằng các hệ số ước lượng đã thay đổi đáng kể với việc sử dụng ba mô hình hồi quy đơn biến.Đối với một ngân sách cố định cho quảng cáo trên truyền hình và một ngân sách cố định cho quảng cáo qua mạng xã hội, việc chi thêm 1 triệu đồng cho quảng cáo bằng hình thức tờ rơi sẽ dẫn đến doanh thu bán hàng trung bình tăng khoảng 11.7 triệu đồng. Con số tương tự với quảng cáo trên truyền hình và qua mạng xã hội lần lượt là 63 triệu đồng và 16.3 triệu đồng. sánh các con số này với Bảng 8.3, chúng ta có thể nhận thấy rằng các hệ số ước lượng đã thay đổi đáng kể với việc sử dụng ba mô hình hồi quy đơn biến.Trong mô hình hồi quy đơn, hệ số tuyến tính của biến chi phí quảng cáo qua tờ rơi (\\(Flyer\\)) là có ý nghĩa, trong khi trong mô hình hồi quy đa biến, hệ số của biến này lại không khác 0 một cách có ý nghĩa. Điều này thể hiện qua giá trị của thống kê \\(t\\) khá nhỏ và \\(p-value\\) khá lớn (khoảng 0.343)Trong mô hình hồi quy đơn, hệ số tuyến tính của biến chi phí quảng cáo qua tờ rơi (\\(Flyer\\)) là có ý nghĩa, trong khi trong mô hình hồi quy đa biến, hệ số của biến này lại không khác 0 một cách có ý nghĩa. Điều này thể hiện qua giá trị của thống kê \\(t\\) khá nhỏ và \\(p-value\\) khá lớn (khoảng 0.343)Sự khác biệt này xuất phát từ thực tế là trong trường hợp hồi quy đơn, hệ số góc thể hiện tác động trung bình của việc tăng 1 triệu đồng trong quảng cáo qua tờ rơi và bỏ qua các yếu tố dự đoán khác là quảng cáo qua truyền hình và qua mạng xã hội. Ngược lại, trong mô hình hồi quy đa biến, hệ số tuyến tính của biến \\(Flyer\\) thể hiện tác động trung bình của việc tăng chi phí quảng cáo qua tờ rơi thêm 1 triệu đồng trong khi giữ nguyên chi phí quảng cáo trên truyền hình và qua mạng xã hội. Vậy liệu có hợp lý không khi mô hình hồi quy đa biến cho thấy không có mối quan hệ giữa doanh thu bán hàng và chi phí quảng cáo qua tờ rơi trong khi hồi quy tuyến tính đơn lại hàm ý ngược lại? Câu trả lời là có! Hãy quan sát ma trận hệ số tương quan của ba biến giải thích và biến mục tiêu trong Bảng 8.5.\nTable 8.5: Ma trận hệ số tương quan của các biến trong dữ liệu Quảng cáo\nBạn đọc có thể thấy rằng hệ số tương quan giữa chi phí quảng cáo qua tờ rơi với hai biến giải thích còn lại là khá cao, lần lượt là 0.54 và 0.51. Điều này cho thấy xu hướng chi tiêu nhiều hơn cho quảng cáo qua hình thức tờ rơi ở các cửa hàng nơi chi tiêu nhiều hơn cho quảng cáo trên qua truyền hình hoặc qua mạng xã hội. Các mô hình hồi quy đơn và mô hình hồi quy đa biến đều cho kết luận là tăng chi tiêu quảng cáo qua truyền hình và quảng cáo qua mạng xã hội thực sự có ý nghĩa làm tăng doanh thu. Giả sử rằng kết luận này là đúng, khi đó việc sử dụng mô hình hồi quy tuyến tính đơn để kiểm tra mối liên hệ giữa doanh thu bán hàng theo quảng cáo trên tờ rơi cho hệ số ước lượng có ý nghĩa là cả hai biến này đều có tương quan cao với chi phí quảng cáo qua truyền hình và mạng xã hội, chứ thực sự thì chi tiêu cho quảng cáo qua tờ rơi không có tác động đến doanh thu bán hàng.Đây là kết quả rất thường gặp khi xây dựng mô hình trên dữ liệu thực tế. Một ví dụ thường được nhắc đến để mô tả tình huống này trong nhiều sách tham khảo là khi hồi quy số các cuộc tấn công của cá mập theo doanh số bán kem trên các bãi biển trong một khoảng thời gian nhất định. Đây là hai biến về bản chất không có mối liên hệ nhưng sẽ cho hệ số góc là một số dương có ý nghĩa thống kê. Cũng giống như chi phí quảng cáo qua hình thức tờ rơi và doanh thu bán hàng, việc không tính đến các biến giải thích có tác động thực sự lên biến mục tiêu sẽ khiến cho chúng ta lầm tưởng rằng doanh số bán kem có tác động đến số cuộc tấn công của cá mập! Trên thực tế, nhiệt độ cao hơn khiến nhiều người đến bãi biển hơn, từ đó dẫn đến doanh số bán kem nhiều hơn và nhiều vụ cá mập tấn công hơn. Nếu chúng ta xây dựng mô hình hồi quy bội mà số các cuộc tấn công của cá mập phụ thuộc vào doanh số bán kem và nhiệt độ của vùng đó sẽ cho kết quả là doanh số bán kem không còn có ý nghĩa giải thích số các cuộc tấn công!","code":"\n# Doanh thu bán hàng (Sales) hồi quy theo 3 biến\nlm(Sales ~ TV + Social_Media + Flyer, data = Advertising)"},{"path":"mô-hình-hồi-quy-tuyến-tính.html","id":"kiểm-định-mô-hình-tuyến-tính-đa-biến","chapter":"Chương 8 Mô hình hồi quy tuyến tính","heading":"8.1.2.2 Kiểm định mô hình tuyến tính đa biến","text":"Mục đích của kiểm định mô hình đa biến là để trả lời hai câu hỏiThứ nhất: mô hình hồi quy có ý nghĩa giải thích biến mục tiêu \\(Y\\) hay không? Hay nói một cách khác là có ít nhất một biến trong số các biến giải thích \\(X_1\\), \\(X_2\\), \\(\\cdots\\), \\(X_p\\) có tác động lên biến mục tiêu?Thứ nhất: mô hình hồi quy có ý nghĩa giải thích biến mục tiêu \\(Y\\) hay không? Hay nói một cách khác là có ít nhất một biến trong số các biến giải thích \\(X_1\\), \\(X_2\\), \\(\\cdots\\), \\(X_p\\) có tác động lên biến mục tiêu?Thứ hai: nếu mô hình hồi quy đa biến có ý nghĩa, thì tất cả các biến đều có ý nghĩa tác động lên biến mục tiêu, hay chỉ một tập hợp con các biến có tác động?Thứ hai: nếu mô hình hồi quy đa biến có ý nghĩa, thì tất cả các biến đều có ý nghĩa tác động lên biến mục tiêu, hay chỉ một tập hợp con các biến có tác động?Để trả lời cho câu hỏi thứ nhất, nhắc lại với bạn đọc rằng trong kiểm định mô hình hồi quy tuyến tính đơn, để xác định liệu có mối quan hệ giữa biến mục tiêu và biến giải thích hay không, chúng ta chỉ cần kiểm định giả thuyết \\(H_0: \\beta_1 = 0\\). Trong mô hình hồi quy đa biến với \\(p\\) biến giải thích dự đoán, chúng ta cần kiểm định giả thuyết liệu hệ số hồi quy của tất cả các biến giải thích đều bằng 0, tức là liệu có xảy ra trường hợp \\(\\beta_1 = \\beta_2 = \\cdots = \\beta_p = 0\\). Cặp giả thuyết \\(H_0\\) - \\(H_1\\) trong mô hình hồi quy đa biến được viết như sau\n\\[\\begin{align}\n& H_0: \\beta_1 = \\beta_2 = \\cdots = \\beta_p = 0 \\\\\n& H_1: \\text{ít nhất có một } \\beta_j \\text{ khác 0}\n\\tag{8.13}\n\\end{align}\\]Để kiểm định cặp giả thuyết trong (8.13), chúng ta sử dụng thống kê \\(F\\)\n\\[\\begin{align}\nF = \\cfrac{(TSS - RSS)/p}{RSS/(n-p-1)}\n\\tag{8.14}\n\\end{align}\\]Nếu giả thuyết \\(H_0\\) là đúng thì thống kê \\(F\\) sẽ có phân phối \\(\\mathcal{F}(p, n - p - 1)\\). giá trị trung bình của biến ngẫu nhiên phân phối \\(\\mathcal{F}(p, n - p - 1)\\) là 1 nên khi giá trị thống kê \\(F\\) lớn thì khả năng bác bỏ giả thuyết \\(H_0\\) là lớn. Để hiểu được tại sao lại sử dụng phân phối \\(\\mathcal{F}\\) để kiểm định giả thuyết, bạn đọc tham khảo phần 8.2. Chúng tôi không giải thích chi tiết vấn đề này tại đây để tránh sự phức tạp không cần thiết.Thống kê \\(F\\) cho mô hình hồi quy tuyến tính đa biến thu được bằng cách hồi quy doanh thu bán hàng theo chi phí quảng cáo qua truyền hình, mạng xã hội, và tờ rơi được trình bày là 18.7457433. Vì giá trị này lớn hơn 1 rất nhiều nên đây có cơ sở để bác bỏ giả thuyết H0. Nói cách khác, giá trị thống kê \\(F\\) lớn cho thấy rằng ít nhất một trong các phương tiện quảng cáo phải liên quan đến doanh thu bán hàng. Tuy nhiên, thống kê \\(F\\) cần phải lớn đến mức nào để chúng ta có thể bác bỏ \\(H_0\\) và kết luận rằng có mối quan hệ và điều gì sẽ xảy ra nếu thống kê \\(F\\) gần với 1 hơn? Để trả lời câu hỏi này còn phụ thuộc vào giá trị của \\(n\\) và \\(p\\). Khi \\(n\\) lớn, ngay cả khi thống kê \\(F\\) chỉ lớn hơn 1 một chút chúng ta vẫn có thể có cơ sở để bác bỏ \\(H_0\\). Ngược lại, thống kê \\(F\\) cần lớn hơn để có cơ sở bác bỏ \\(H_0\\) nếu \\(n\\) nhỏ.Bạn đọc có thể quan sát hàm mật độ của biến ngẫu nhiên phân phối \\(\\mathcal{F}\\) với các tham số \\((3,51)\\) trong Hình 8.3. Khả năng một biến ngẫu nhiên có phân phối \\(\\mathcal{F}\\) lớn hơn giá trị thống kê \\(F\\) tính toán từ dữ liệu là khoảng \\(2.5 \\times 10^{-8}\\). Nói cách khác, với \\(p-value\\) rất nhỏ, chúng ta có cơ sở để bác bỏ giả thuyết \\(H_0\\). Điều này đồng nghĩa với việc có ít nhất một ngân sách chi cho quảng cáo có tác động đến doanh thu bán hàng.\nFigure 8.3: Hàm mật độ của phân phối F(3,51) cho dữ liệu quảng cáo. Giá trị thống kê F (F - value) đủ lớn để bác bỏ giả thuyết \\(H_0\\)\nĐể trả lời cho câu hỏi thứ hai, chúng ta cần thực hiện các kiểm định liệu một nhóm biến giải thích có tác động lên biến mục tiêu hay không. Giả sử các biến giải thích có số thứ tự lần lượt là \\(1 \\leq i_1 < i_2 < \\cdots < i_h \\leq p\\). Khi đó, giả thuyết \\(H_0\\) - \\(H_1\\) để thực hiện kiểm định giả thuyết được viết như sau\n\\[\\begin{align}\n& H_0: \\beta_{i_j} = 0 \\ \\ \\forall j = 1,2, \\cdots, h \\\\\n& H_1: \\text{Tồn tại ít nhất $j$ sao cho} \\ \\beta_{i_j} > 0\n\\tag{8.15}\n\\end{align}\\]Tương tự như trong trường hợp kiểm định giả thuyết trong phương trình (8.13), chúng ta có thể sử dụng phân phối \\(\\mathcal{F}\\) để thực hiện kiểm định giả thuyết. Thống kê \\(F\\) được tính toán như sau\n\\[\\begin{align}\nF = \\cfrac{(RSS_1 - RSS)/h}{RSS/(n-p-1)}\n\\tag{8.16}\n\\end{align}\\]\ntrong đó \\(RSS_1\\) là tổng bình phương sai số của mô hình hồi quy tuyến tính không bao gồm các biến độc lập có hệ số tuyến tính được liệt kê trong phương trình (8.15).\nNếu giả thuyết \\(H_0\\) trong (8.15) là đúng, thì có thể chứng minh được rằng (tham khảo phần 8.2) thống kê \\(F\\) sẽ có phân phối \\(\\mathcal{F}(h,n-p-1)\\). Giá trị của thống kê \\(F\\) lớn cho thấy rằng ít nhất một trong các biến độc lập có hệ số tuyến tính được liệt kê trong phương trình (8.15) có tác động tuyến tính lên biến phụ thuộc. Trong trường hợp đặc biệt khi \\(h=1\\), nghĩa là khi chúng ta cần kiểm định từng biến có ý nghĩa ở trong mô hình hồi quy tuyến tính, giá trị thống kê \\(F\\) trong phương trình (8.16) chính là bình phương của thống kê \\(t\\) khi kiểm định từng biến độc lập riêng lẻ. Giá trị của thống kê \\(t\\) và giá trị \\(p-value\\) tương ứng khi kiểm định từng chi phí quảng cáo có tác động đến doanh thu bán hàng hay không được cho trong Bảng 8.4. Các giá trị \\(p-value\\) này chỉ ra rằng truyền hình và mạng xã hội có liên quan đến doanh thu bán hàng, nhưng không có bằng chứng nào cho thấy quảng cáo qua hình thức tờ rơi có liên quan đến doanh thu bán hàng khi tính đến cả quảng cáo trên truyền hình và quảng cáo qua mạng xã hội.Khi đã có các \\(p-value\\) riêng lẻ cho từng biến, tại sao chúng ta cần xem xét thống kê \\(F\\) trong kiểm định đồng thời? Liệu có phải rằng \\(p-value\\) của một biến riêng lẻ là nhỏ thì ít nhất một trong các yếu tố dự đoán có liên quan đến phản hồi? Điều này không phải lúc nào cũng đúng, đặc biệt khi số lượng biến giải thích \\(p\\) khá lớn. Chẳng hạn như khi số lượng biến giải thích \\(p = 20\\) và chúng ta kiểm định giả thuyết \\(H_0: \\beta_1 = \\beta_2 = \\cdots = \\beta_{20} = 0\\). Ngay cả khi \\(H_0\\) là thực sự đúng, thì với mức ý nghĩa 5%, vẫn sẽ có trung bình \\(5\\% \\times 20 = 1\\) (biến) không lớn hơn 0.05! Điều này cũng giống như khi chúng ta tạo ra 20 biến ngẫu nhiên phân phối \\(student\\) thì sẽ có trung bình một biến rơi vào miền giá trị có xác suất 0.05. đó, nếu chúng ta sử dụng thống kê \\(t\\) riêng lẻ và các \\(p-value\\) liên quan để quyết định xem có bất kỳ mối liên hệ nào giữa các biến giải thích và biến mục tiêu hay không, thì có khả năng cao là chúng ta sẽ kết luận sai rằng có một mối quan hệ. Sử dụng thống kê \\(F\\) không gặp phải vấn đề này vì thống kê \\(F\\) có tính toán đến số lượng biến giải thích đưa vào trong kiểm định. Nếu giả thuyết \\(H_0\\) thực sự đúng thì chỉ có 5% khả năng thống kê \\(F\\) có \\(p-value\\) nhỏ hơn 0.05, bất kể số lượng biến giải thích là bao nhiêu.Bước đầu tiên trong xây dựng một mô hình tuyến tính thường là ước lượng mô hình và tính toán giá trị thống kê \\(F\\). Nếu chúng ta kết luận dựa trên p-value của thống kê \\(F\\) rằng ít nhất một trong các biến giải thích có liên quan đến biến mục tiêu, thì câu hỏi tiếp theo cần trả lời sẽ là các biến nào sẽ thực sự có ý nghĩa trong mô hình. Chúng ta có thể xem xét các \\(p-value\\) riêng lẻ cho từng biến như trong bảng 8.4, nhưng như đã thảo luận, nếu \\(p\\) khá lớn thì chúng ta có thể thực hiện một số nhận định sai. Quá trình xác định những biến giải thích có liên quan đến biến mục tiêu để tìm ra một mô hình duy nhất chỉ bao gồm các biến có liên quan được gọi là quá trình lựa chọn biến. Vấn đề lựa chọn biến được thảo luận kỹ hơn trong phần 8.3.","code":""},{"path":"mô-hình-hồi-quy-tuyến-tính.html","id":"kiểm-tra-sự-phù-hợp-của-mô-hình","chapter":"Chương 8 Mô hình hồi quy tuyến tính","heading":"8.1.2.3 Kiểm tra sự phù hợp của mô hình","text":"Như đã đề cập trong mô hình tuyến tính đơn, hai thước đo định lượng phổ biến nhất về mức độ phù hợp của mô hình hồi quy tuyến tính là sai số của phần dư \\(RSE\\) và hệ số \\(R^2\\). Hãy nhớ lại rằng trong hồi quy đơn, \\(R^2\\) là bình phương hệ số tương quan giữa biến mục tiêu và biến giải thích. Trong hồi quy tuyến tính đa biến, có thể chứng minh được rằng hệ số \\(R^2\\) chính là bình phương hệ số tương quan giữa \\(Y\\) và \\(\\hat{Y}\\). Giá trị \\(R^2\\) gần bằng 1 cho thấy mô hình giải thích được phần lớn phương sai của biến mục tiêu. Hệ số \\(R^2\\) trong hồi quy đa biến được tính toán dựa trên phương trình \\tag{8.9} tương tự như hồi quy đơn. Hệ số \\(R^2\\) được tính toán bằng hàm lm() như sauĐối với Dữ liệu quảng cáo, mô hình sử dụng cả ba phương tiện quảng cáo để dự đoán doanh thu bán hàng có hệ số \\(R^2\\) là 0.5244189. Mặt khác, mô hình chỉ sử dụng TV và \\(Social\\_Media\\) để dự đoán doanh thu bán hàng có giá trị \\(R^2\\) là 0.5158851. Nói cách khác, có một sự gia tăng nhỏ trong \\(R^2\\) nếu chúng ta đưa quảng cáo bằng tờ rơi vào mô hình đã có sẵn quảng cáo trên truyền hình và mạng xã hội, mặc dù trước đó chúng ta đã thấy rằng giá trị \\(p-value\\) cho quảng cáo trên tờ rơi trong Bảng 8.4 là không đáng kể. Thực ra thì hệ số \\(R^2\\) sẽ luôn tăng khi có nhiều biến hơn được thêm vào mô hình, ngay cả khi những biến đó không có liên quan hoặc liên quan yếu đến biến mục tiêu. Bạn đọc cần lưu ý vấn đề này khi lựa chọn mô hình. Hệ số \\(R^2\\) mà chúng ta thảo luận ở đây chỉ là hệ số \\(R^2\\) tính trên dữ liệu huấn luyện mô hình chứ không phải là trên dữ liệu kiểm tra mô hình. Theo kinh nghiệm thực tế thì khi thêm các biến như quảng cáo tờ rơi vào mô hình chỉ làm cho \\(R^2\\) tăng thêm một chút là bằng chứng cho thấy biến \\(Flyer\\) nên bị loại khỏi mô hình. Ngược lại, khi chúng ta sử dụng mô hình chỉ chứa biến \\(TV\\) là biến giải thích có hệ số \\(R^2\\) là 0.2368573. Việc thêm biến \\(Social\\_Media\\) vào mô hình sẽ dẫn đến sự cải thiện đáng kể về \\(R^2\\). Điều này ngụ ý rằng mô hình sử dụng hai biến chi phí quảng cáo qua truyền hình và qua mạng xã hội để dự đoán doanh thu bán hàng sẽ tốt hơn đáng kể với mô hình chỉ sử dụng quảng cáo trên truyền hình.\\(RSE\\) cũng có thể là một thước đo định lượng để đánh giá sự phù hợp của mô hình. Bạn đọc có thể quan sát \\(RSE\\) của các mô hình với tổ hợp các biến giải thích khác nhau trong bảng 8.6\nTable 8.6: Sai số phần dư của các mô hình tuyến tính trên dữ liệu Quảng cáo\nTrong các mô hình đơn biến, có thể thấy rằng mô hình sử dụng biến \\(Flyer\\) làm biến giải thích có \\(RSE\\) thậm chí còn nhỏ hơn với mô hình chỉ sử dụng biến \\(TV\\). Khi sử dụng hai biến để giải thích biến doanh thu, bạn đọc có thể nhận thấy rằng mô hình sử dụng \\(TV\\) và \\(Social\\_Media\\) có \\(RSE\\) nhỏ hơn hẳn với các mô hình còn lại. Khi thêm biến \\(Flyer\\) vào mô hình đã bao gồm \\(TV\\) và \\(Social\\_Media\\), \\(RSE\\) gần như không thay đổi. Không giống như hệ số \\(R^2\\) luôn tăng khi thêm biến vào mô hình, các mô hình có nhiều biến hơn có thể có \\(RSE\\) cao hơn nếu mức giảm \\(RSS\\) nhỏ hơn với sự gia tăng số lượng biến.Sau khi chúng ta đã tìm ra mô hình phù, có thể sử dụng các hệ số tuyến tính để đưa ra dự đoán cho biến mục tiêu \\(Y\\) trên giá trị các biến giải thích \\(X_1, X_2, \\cdots , X_p\\). Tuy nhiên, trước khi đưa ra dự đoán cho biến mục tiêu dựa trên mô hình tuyến tính, có những vấn đề mà bạn đọc cần lưu ý:Thứ nhất: kể cả khi mối quan hệ giữa biến mục tiêu với các biến giải thích là mối quan hệ tuyến tính, thì chúng ta cũng không biết được giá trị thực của các hệ số tuyến tính. Các hệ số \\(\\hat{\\beta}_0, \\hat{\\beta}_1, \\cdots, \\hat{\\beta}_p\\) chỉ là các ước lượng cho các hệ số tuyến tính thực \\(\\beta_0, \\beta_1, \\cdots, \\beta_p\\) dựa trên dữ liệu quan sát được. Sai số giữa ước lượng và các giá trị thực có thể giảm bớt được dựa trên độ lớn của dữ liệu và kỹ năng của người xây dựng mô hình.Thứ nhất: kể cả khi mối quan hệ giữa biến mục tiêu với các biến giải thích là mối quan hệ tuyến tính, thì chúng ta cũng không biết được giá trị thực của các hệ số tuyến tính. Các hệ số \\(\\hat{\\beta}_0, \\hat{\\beta}_1, \\cdots, \\hat{\\beta}_p\\) chỉ là các ước lượng cho các hệ số tuyến tính thực \\(\\beta_0, \\beta_1, \\cdots, \\beta_p\\) dựa trên dữ liệu quan sát được. Sai số giữa ước lượng và các giá trị thực có thể giảm bớt được dựa trên độ lớn của dữ liệu và kỹ năng của người xây dựng mô hình.Lưu ý thứ hai đó là sai số về mặt mô hình, nghĩa là mối quan hệ giữa các biến giải thích và các biến mục tiêu không phải là mối liên hệ tuyến tính nhưng chúng ta sử dụng mô hình tuyến tính để đưa ra dự đoán. Sai số này có thể được giảm bớt tùy theo kỹ năng của người xây dựng mô hình, chẳng hạn như sử dụng các phép biến đổi dữ liệu, hoặc thay đổi kiểu mô hình. Các mô hình phi tuyến sẽ được trình bày trong các phần sau của cuốn sách.Lưu ý thứ hai đó là sai số về mặt mô hình, nghĩa là mối quan hệ giữa các biến giải thích và các biến mục tiêu không phải là mối liên hệ tuyến tính nhưng chúng ta sử dụng mô hình tuyến tính để đưa ra dự đoán. Sai số này có thể được giảm bớt tùy theo kỹ năng của người xây dựng mô hình, chẳng hạn như sử dụng các phép biến đổi dữ liệu, hoặc thay đổi kiểu mô hình. Các mô hình phi tuyến sẽ được trình bày trong các phần sau của cuốn sách.Lưu ý thứ ba đó là ngay cả khi chúng ta biết được mối quan hệ thực giữa biến mục tiêu và các biến giải thích, vẫn có những sai số mà hoàn toàn không thể được giải thích dựa trên dữ liệu. Các sai số này là không thể giảm bớt được.Lưu ý thứ ba đó là ngay cả khi chúng ta biết được mối quan hệ thực giữa biến mục tiêu và các biến giải thích, vẫn có những sai số mà hoàn toàn không thể được giải thích dựa trên dữ liệu. Các sai số này là không thể giảm bớt được.Với các ước lượng cho hệ số tuyến tính và sai số của phần dư, chúng ta có thể xây dựng được khoảng tin cậy với mức xác suất \\(\\alpha\\) cho giá trị trung bình của biến mục tiêu\n\\[\\begin{align}\n\\left(\\hat{\\beta}_0 + \\hat{\\beta}_1 \\cdot x_1 + \\cdots + \\hat{\\beta}_p \\cdot x_p - z_{1 - \\alpha/2} \\hat{\\sigma} ; \\hat{\\beta}_0 + \\hat{\\beta}_1 \\cdot x_1 + \\cdots + \\hat{\\beta}_p \\cdot x_p + z_{1 - \\alpha/2} \\hat{\\sigma}\\right)\n\\tag{8.17}\n\\end{align}\\]Đối với dữ liệu Quảng cáo, giả sử mô hình được lựa chọn là mô hình với hai biến giải thích là \\(TV\\) và \\(Social\\_Media\\). Với ngân sách cho quảng cáo trên trền hình là 150 triệu đồng và ngân sách cho quảng cáo trên mạng xã hội 30 triệu, chúng ta có khoảng tin cậy 95% cho doanh thu bán sản phẩm là \\(\\left(8.64 ; 9.38 \\right)\\) tỷ đồng. Khoảng tin cậy này được tính bởi các tham số được ước lượng từ dữ liệu quan sát như sau:\n\\[\\begin{align}\n\\left(4.06 + 0.019 \\times 150 + 0.07 \\times 30 - 1.96 \\times 0.189 ;4.06 + 0.019 \\times 150 + 0.07 \\times 30 + 1.96 \\times 0.189 \\right)\n\\tag{8.18}\n\\end{align}\\]","code":"\n# Hệ số R-squared trong mô hình hồi quy 3 biến\nsummary(lm(Sales ~ TV + Social_Media + Flyer,data = Advertising))$r.squared\n\n# Hệ số R-squared trong mô hình hồi quy 2 biến: TV, Social_Media\nsummary(lm(Sales ~ TV + Social_Media,data = Advertising))$r.squared"},{"path":"mô-hình-hồi-quy-tuyến-tính.html","id":"những-cân-nhắc-khi-xây-dựng-mô-hình-hồi-quy-tuyến-tính","chapter":"Chương 8 Mô hình hồi quy tuyến tính","heading":"8.1.3 Những cân nhắc khi xây dựng mô hình hồi quy tuyến tính","text":"Trong phần này chúng ta sẽ thảo luận thêm về các vấn đề thường gặp phải khi xây dựng mô hình tuyến tính trên dữ liệu thực tế bao gồm có vấn đề biến giải thích có kiểu định tính và vấn đề về tồn tại mối liên hệ phi tuyến tính giữa biến mục tiêu và biến giải thích","code":""},{"path":"mô-hình-hồi-quy-tuyến-tính.html","id":"biến-giải-thích-là-biến-định-tính","chapter":"Chương 8 Mô hình hồi quy tuyến tính","heading":"8.1.3.1 Biến giải thích là biến định tính","text":"Khi ước lượng hệ số tuyến tính bằng phương pháp bình phương nhỏ nhất, chúng ta luôn giả định rằng tất cả các biến trong mô hình đều là biến định lượng. Trong thực tế chúng ta lại rất hay thường gặp các biến giải thích là kiểu biến định tính. Ví dụ: bộ dữ liệu có tên \\(exposure.csv\\) là dữ liệu về số lần yêu cầu bồi thường viện phí và tổng số tiền bồi thường của khác hàng mua bảo hiểm sức khỏe tại một công ty bảo hiểm. Chúng tôi để mô hình ở dạng đơn giản nhất khi chỉ có hai biến phụ thuộc là giới tính (\\(Gender\\)) và tuổi (\\(Age\\)) của người được thanh toán bảo hiểm sức khỏe. Biến mục tiêu trong mô hình sẽ là số tiền bồi thường trung bình của những khách hàng (\\(Ave\\_Claim\\)) và được tính bằng tổng số tiền của tất cả các lần bồi thường chia cho tổng số lần khách hàng gửi yêu cầu.\nFigure 8.4: Hình bên trái: Mối liên hệ giữa số tiền bồi thường trung bình với độ tuổi của người được bảo hiểm; Hình bên phải: Mối liên hệ giữa số tiền bồi thường trung bình với giới tính của người được bảo hiểm\nHình 8.4 mô tả mối liên hệ giữa số tiền yêu cầu bồi thường trung bình với độ tuổi và giới tính của người được bảo hiểm. Bạn đọc có thể thấy rằng có mối liên hệ giữa các biến giải thích đến các biến mục tiêu, số tiền yêu cầu bồi thường trung bình có xu hướng tăng khi tuổi của người được bảo hiểm tăng, và số tiền yêu cầu bồi thường trung bình của nữ cao hơn với nam giới. Như vậy tuổi và giới tính có nhiều khả năng là các biến có liên hệ đến biến mục tiêu. Ước lượng hệ số tuyến tính của biến \\(Age\\) có thể được thực hiện giống như các biến định lượng thông thường. Để sử dụng biến giới tính như một biến giải thích, chúng ta tạo một biến mới có dạng như sau\n\\[\\begin{align}\nGender_i = \\begin{cases}\n1 \\text{ nếu giới tính là nam} \\\\\n0 \\text{ nếu giới tính là nữ}\n\\end{cases}\n\\tag{8.19}\n\\end{align}\\]Mô hình tuyến tính với biến mục tiêu là số tiền yêu cầu bồi thường trung bình (\\(Y_i\\)) và hai biến giải thích là độ tuổi và giới tính được viết như sau\n\\[\\begin{align}\nY_i = & \\beta_0 + \\beta_1 \\cdot Gender_i + \\beta_2 \\cdot Age_i + \\epsilon_i \\\\\n= &\n\\begin{cases}\n(\\beta_0 + \\beta_1) + \\beta_2 \\cdot Age_i + \\epsilon_i  \\ \\ \\text{ nếu giới tính là nam} \\\\\n\\beta_0 + \\beta_2 \\cdot Age_i + \\epsilon_i \\ \\ \\text{ nếu giới tính là nữ}\n\\end{cases}\n\\tag{8.18}\n\\end{align}\\]Hệ số \\(\\beta_0\\) trong phương trình (8.17) là hệ số chặn trong mô hình hồi quy tuyến tính đơn mà số tiền yêu cầu bồi thường trung bình phụ thuộc vào độ tuổi nếu người được bảo hiểm là nữ giới, trong khi \\((\\beta_0 + \\beta_1)\\) là hệ số chặn trong mô hình hồi quy tuyến tính đơn mà số tiền yêu cầu bồi thường trung bình phụ thuộc vào độ tuổi nếu người được bảo hiểm là nam giới. Để ước lượng mô hình hồi quy tuyến tính với biến định tính \\(Gender\\) bằng hàm lm(), bạn đọc hãy đảm bảo biến định tính có kiểu factor trước khi đưa vào trong hàm ước lượng.\nTable 8.7: Các hệ số ước lượng trong mô hình hồi quy tuyến tính với biến định tính Gender nhận một trong hai giá trị là Male hoặc Female\nHệ số ước lượng của mô hình tuyến tính với biến phụ thuộc định tính được trình bày trong bảng 8.7. Bạn đọc thấy rằng các hệ số ước lượng được đều có ý nghĩa thống kê vì giá trị \\(p-value\\) đều rất nhỏ. Hệ số tuyến tính của biến độ tuổi bằng 1.95 cho thấy rằng nếu tuổi của người được bảo hiểm tăng thêm 1 tuổi, thì số tiền bồi thường trung bình sẽ tăng khoảng 1.95 triệu đồng. Hệ số tuyến tính của biến \\(GenderMale\\) là số âm cho biết cùng một độ tuổi, trung bình mỗi lần yêu cầu bồi thường nam giới sẽ có số tiền yêu cầu ít hơn nữ giới khoảng 14.32 triệu đồng. Việc lựa chọn mã hóa giới tính trong phương trình (8.19) là hoàn toàn tự và không ảnh hưởng đến kết quả của mô hình hồi quy. Nếu bạn đọc sử dụng cách mã hóa nữ giới là 1 và nam giới là 0, kết quả thu được sẽ có hệ số của biến \\(GenderFemale\\) là số dương, có giá trị bằng với giá trị tuyệt đối của hệ số của biến \\(GenderMale\\) trong bảng 8.7.Bạn đọc có thể đặt ra câu hỏi về việc biến định tính nhận nhiều hơn hai giá trị. Cách ước lượng của mô hình tuyến tính là hoàn toàn tương tự như trường hợp hai biến. Giả sử mô hình hồi quy tuyến tính có biến \\(Y\\) là biến mục tiêu và hai biến giải thích: \\(X_1\\) là biến định lượng và \\(X_2\\) là biến định tính. \\(X_2\\) có thể nhận \\(J\\) giá trị khác nhau lần lượt là \\(1, 2, cdots, J\\). Khi đó, ước lượng mô hình tuyến tính có \\(J + 1\\) hệ số tuyến tính cần được ước lượng như sau\n\\[\\begin{align}\nY = \\begin{cases}\n\\beta_0 + \\beta_1 \\cdot X_1 + \\epsilon \\ \\ \\text{ nếu } X_2 = 1 \\\\\n(\\beta_0 + \\beta_2) + \\beta_1 \\cdot X_1 + \\epsilon \\ \\ \\text{ nếu } X_2 = 2 \\\\\n(\\beta_0 + \\beta_3) + \\beta_1 \\cdot X_1 + \\epsilon \\ \\ \\text{ nếu } X_2 = 3 \\\\\n\\cdots \\\\\n(\\beta_0 + \\beta_J) + \\beta_1 \\cdot X_1 + \\epsilon \\ \\ \\text{ nếu } X_2 = J\n\\end{cases}\n\\tag{8.20}\n\\end{align}\\]\nCó thể thấy rằng, nếu biến định tính nhận quá nhiều giá trị, số lượng tham số của mô hình tuyến tính tăng lên tương ứng. Khi mô hình sử dụng quá nhiều hệ số sẽ dễ dẫn đến hiện tượng overfitting. Giải pháp khi gặp biến định tính nhận nhiều giá trị là nhóm các giá trị có hệ số \\(\\beta\\) không khác nhau vào cùng một nhóm để giảm số lượng biến. Chúng ta sẽ thảo luận kỹ hơn về giải pháp này trong phần thực hành trên mô hình tuyến tính.","code":"\n# Load dữ liệu exposure\ndat<-read.csv(\"../KHDL_KTKD Final/Dataset/exposure.csv\")\n\n# Chỉ giữ lại những khác hàng có yêu cầu bồi thường\ndat<-filter(dat,Claim_Count>0)\n# Đổi dữ liệu cột Gender thành factor\ndat$Gender<-as.factor(dat$Gender)"},{"path":"mô-hình-hồi-quy-tuyến-tính.html","id":"basiclmconsideration2","chapter":"Chương 8 Mô hình hồi quy tuyến tính","heading":"8.1.3.2 Mối quan hệ phi tuyến giữa biến mục tiêu và biến giải thích","text":"Mối liên hệ giữa số tiền yêu cầu bồi thường trung bình và biến độ tuổi trong Hình 8.4 không phải là một mối liên hệ tuyến tính. Bạn đọc có thể thấy rằng khi độ tuổi tăng thì số tiền yêu cầu bồi thường tăng lên nhanh hơn, điều này giải thích tại sao đường thẳng mô tả mối liên hệ giữa hai biến có độ dốc tăng dần khi độ tuổi tăng. Điều này gợi ý cho người xây dựng mô hình rằng mối liên hệ giữa số tiền bồi thường trung bình và độ tuổi là mối liên hệ phi tuyến hơn là tuyến tính. Đa số các phương pháp xây dựng mô hình hiện đại đều được xây dựng để mô tả mối quan hệ phi tuyến giữa biến giải thích và biến mục tiêu. Trong khuôn khổ mô hình hồi quy tuyến tính, chúng tôi giới thiệu một phương pháp tiếp cận đơn giản nhất, đó là hồi quy theo đa thức. Mối liên hệ giữa biến mục tiêu và biến giải thích trong Hình 8.4 có dạng parabol, đó chúng ta có thể thêm vào mô hình biến giải thích là bình phương của độ tuổi với hi vọng là sẽ có một mô hình giải thích tốt hơn biến mục tiêu. Mô hình có dạng như sau\n\\[\\begin{align}\nY = \\begin{cases}\n(\\beta_0 + \\beta_1) + \\beta_2 \\cdot Age + \\beta_3 \\cdot Age^2 + \\epsilon  \\ \\ \\text{ nếu giới tính là nam} \\\\\n\\beta_0 + \\beta_2 \\cdot Age_i + \\beta_3 \\cdot Age^2 + \\epsilon \\ \\ \\text{ nếu giới tính là nữ}\n\\end{cases}\n\\tag{8.21}\n\\end{align}\\]\nTable 8.8: Các hệ số ước lượng trong mô hình hồi quy quy đa thức với bình phương của biến Age là biến giải thích\nKết quả ước lượng trong Bảng 8.8 cho thấy tất cả các hệ số tuyến tính đều có ý nghĩa, điều này cho thấy mối liên hệ giữa số tiền bồi thường trung bình và độ tuổi là mối liên hệ phi tuyến hơn là tuyến tính. Mô hình có biến giải thích độ tuổi bình phương có hệ số \\(R^2\\) lớn hơn nhiều với mô hình không có biến độ tuổi bình phương, điều này cho thấy mô hình có biến độ tuổi bình phương phù hợp hơn để giải thích biến mục tiêu. Tuy nhiên, bạn đọc cũng có thể nhận thấy rằng, mô hình đã trở nên khó giải thích hơn một chút. Chúng ta không thể đưa ra đánh giá ngay lập tức cho biến mục tiêu khi tuổi của người yêu cầu bồi thường tăng 1 hay giảm 1 tuổi. Đó là sự đánh đổi giữa khả năng giải thích và khả năng dự đoán mà bạn đọc sẽ thường xuyên gặp phải khi xây dựng mô hình. Chúng ta sẽ thảo luận về các kỹ thuật mô tả mối liên hệ phi tuyến giữa biến mục tiêu và biến giải thích trong phần xxxxxxx","code":""},{"path":"mô-hình-hồi-quy-tuyến-tính.html","id":"sự-kết-hợp-giữa-các-biến-giải-thích-không-chỉ-là-cộng-tính","chapter":"Chương 8 Mô hình hồi quy tuyến tính","heading":"8.1.3.3 Sự kết hợp giữa các biến giải thích không chỉ là cộng tính","text":"Trong nhiều trường hợp, biến mục tiêu không chỉ phụ thuộc vào từng biến giải thích một cách riêng lẻ, mà còn phụ thuộc vào sự tương tác giữa các biến giải thích. Một ví dụ điển hình cho trường hợp này là khi sử dụng mô hình tuyến tính để mô tả mối liên hệ giữa số lượng thành phẩm của một nhà máy sản xuất (\\(products\\)) với số lượng công nhân (\\(workers\\)) và số lượng máy chế tạo (\\(machine\\)). Nhìn chung khi tăng số lượng công nhân hoặc tăng số lượng máy thì số lượng thành phẩm sẽ tăng lên. Tuy nhiên mô hình hồi quy tuyến tính chỉ bao gồm hai biến giải thích \\(workers\\) và \\(machines\\) sẽ không mô tả được thực tế là khi tăng số lượng công nhân lên quá nhiều sẽ dẫn đến việc công nhân không có máy để sản xuất nên số lượng thành phẩm cũng sẽ không tăng theo tương ứng. Chính vì thế, để mô tả được thực tế đó, mô hình tuyến tính cần có biến giải thích mô tả sự tương tác giữa \\(workers\\) và \\(machines\\):\n\\[\\begin{align}\nproducts = \\beta_0 + \\beta_1 \\cdot workers  + \\beta_2 \\cdot machines + \\beta_3 \\cdot workers \\times machines + \\epsilon\n\\tag{8.22}\n\\end{align}\\]\nhoặc chúng ta cũng có thể viết mô hình (8.22) dưới dạng mô hình tuyến tính mà hệ số tuyến tính của biến \\(machines\\) phụ thuộc vào biến \\(workers\\)\n\\[\\begin{align}\nproducts = & \\beta_0 + \\beta_1 \\cdot workers  + (\\beta_2 + \\beta_3 \\cdot workers) \\cdot machines + \\epsilon \\\\\n= & \\beta_0 + (\\beta_1 + \\beta_3 \\cdot machines)  \\cdot workers  + \\beta_2 \\cdot machines + \\epsilon\n\\tag{8.23}\n\\end{align}\\]Có thể giải thích mô hình (8.23) rằng mỗi khi tăng thêm 1 máy sản xuất, số lượng thành phẩm sẽ tăng lên tương ứng là bằng \\((\\beta_2 + \\beta_3 \\cdot workers)\\), hoặc tăng thêm 1 công nhân, số lượng thành phẩm sẽ tăng lên là \\((\\beta_1 + \\beta_3 \\cdot machines)\\). Hay nói một cách khác, số lượng thành phẩm tăng khi số lượng máy móc tăng nhưng tốc độ tăng còn phụ thuộc vào số lượng công nhân hiện tại; hoặc số lượng thành phẩm tăng khi tăng số lượng công nhân nhưng tốc độ tăng còn phụ thuôc vào số máy móc hiện có. Mô hình (8.23) sẽ phù hợp hơn mô hình tuyến tính chỉ bao gồm hai biến giải thích \\(workers\\) và \\(machines\\) khi giải thích biến mục tiêu \\(products\\).","code":""},{"path":"mô-hình-hồi-quy-tuyến-tính.html","id":"những-khiếm-khuyết-của-mô-hình-hồi-quy-tuyến-tính","chapter":"Chương 8 Mô hình hồi quy tuyến tính","heading":"8.1.4 Những khiếm khuyết của mô hình hồi quy tuyến tính","text":"Khi chúng ta ước lượng mô hình hồi quy tuyến tính cho dữ liệu cụ thể những vấn đề dưới đây có thể xảy ra làm cho kết quả ước lượng của mô hình trở nên kém hiệu quả:Tồn tại mối liên hệ phi tuyến giữa biến mục tiêu và biến giải thích;Tồn tại mối liên hệ phi tuyến giữa biến mục tiêu và biến giải thích;Các sai số \\(\\epsilon_i\\) có tương quan với nhau.Các sai số \\(\\epsilon_i\\) có tương quan với nhau.Phương sai của các \\(\\epsilon_i\\) không phải là hằng số.Phương sai của các \\(\\epsilon_i\\) không phải là hằng số.Trong dữ liệu có điểm ngoại lai.Trong dữ liệu có điểm ngoại lai.Các biến giải thích có tương quan cao với nhau, hay còn gọi là đa cộng tuyến.Các biến giải thích có tương quan cao với nhau, hay còn gọi là đa cộng tuyến.Trong thực tế, việc xác định và khắc phục những vấn đề này là những chủ đề khoa học được nghiên cứu xuyên suốt cho đến thời điểm hiện tại. Có nhiều cuốn sách có chủ đề tập trung vào mô hình hồi quy tuyến tính có thể giải quyết một hoặc một vài vấn đề được nêu ở trên. Vì mô hình hồi quy tuyến tính không phải là trọng tâm của cuốn sách này nên chúng tôi sẽ chỉ tóm tắt ngắn gọn về các vấn đề và một số hướng giải quyết ngắn gọn.","code":""},{"path":"mô-hình-hồi-quy-tuyến-tính.html","id":"dữ-liệu-quan-sát-được-có-dạng-phi-tuyến-tính","chapter":"Chương 8 Mô hình hồi quy tuyến tính","heading":"8.1.4.1 Dữ liệu quan sát được có dạng phi tuyến tính","text":"Như chúng tôi đã trình bày ở phần ??, khi tồn tại mối liên hệ phi tuyến giữa biến mục tiêu và biến giải thích, sử dụng mô hình tuyến tính thông thường sẽ không phù hợp và làm cho kết quả dự đoán không được chính xác. Mối liên hệ phi tuyến có thể được phát hiện khi vẽ đồ thị biến mục tiêu theo biến giải thích giống như Hình 8.4 hoặc chúng ta vẽ đồ thị phần dư của mô hình theo biến mục tiêu\nFigure 8.5: Đồ thị mô tả phần dư của mô hình tuyến tính theo biến mục tiêu trên dữ liệu về số tiền yêu cầu bồi thường bảo hiểm y tế. Hình bên trái: Phần dư của mô hình tuyến tính thông thường theo biến mục tiêu. Hình bên phải: Phần dư của mô hình hồi quy đa thức bậc hai theo biến mục tiêu\nHình 8.5 mô tả mối liên hệ giữa phần dư của mô hình tuyến tính thông thường và mô hình hồi quy đa thức với biến mục tiêu là số tiền yêu cầu bồi thường trung bình. Đường mô tả mối liên hệ trong mô hình hồi quy đa thức gần với đường trung bình của phần dư hơn cho thấy mối liên hệ phi tuyến giữa biến mục tiêu và phần dư tuy đã giảm bớt với hồi quy tuyến tính thông thường nhưng vẫn còn tồn tại trong mô hình hồi quy đa thức. Như vậy, có thể thấy rằng thêm các biến giải thích là các hàm phi tuyến của các biến giải thích ban đầu vào trong mô hình hồi quy tuyến tính là một phương pháp để mô tả mối quan hệ phi tuyến trong dữ liệu. Các biến đổi phi tuyến thường được dùng có dạng hàm mũ, hàm \\(\\log\\) của biến giải thích. Nghĩa là từ biến giải thích \\(X\\) ban đầu, nếu đồ thị mô tả mối liên hệ giữa \\(X\\) và \\(Y\\) cho thấy có mối liên hệ phi tuyến, tùy theo hình dạng của đồ thị mà chúng ta có thể thêm vào mô hình các biến giải thích như \\(\\sqrt{X}\\), \\(X^2\\), \\(X^3\\), \\(\\log(X)\\), \\(\\cdots\\), …, để có được mô hình phù hợp hơn. Trong phần sau của cuốn sách chúng ta sẽ thảo luận về các kỹ thuật hiện đại hơn để mô tả tốt hơn mối liên hệ phi tuyến như vậy.","code":""},{"path":"mô-hình-hồi-quy-tuyến-tính.html","id":"tồn-tại-tương-quan-giữa-các-phần-dư","chapter":"Chương 8 Mô hình hồi quy tuyến tính","heading":"8.1.4.2 Tồn tại tương quan giữa các phần dư","text":"Một giả thiết quan trọng của mô hình hồi quy tuyến tính là các sai số \\(\\epsilon_1\\), \\(\\epsilon_2\\), \\(\\cdots\\), \\(\\epsilon_n\\) không tương quan với nhau. Điều này có nghĩa là kể cả khi đã biết \\(\\epsilon_i\\), chúng ta cũng không có thông tin gì về các \\(\\epsilon_j\\) khi \\(j \\neq \\). Giả thiết các sai số có phân phối chuẩn và không tương quan có ý nghĩa quan trọng trong xây dựng các khoảng tin cậy cho hệ số tuyến tính. Trên thực tế, nếu có mối tương quan giữa các sai số thì phương sai của các hệ số tuyến tính ước lượng bằng phương pháp bình phương nhỏ nhất sẽ nhỏ hơn nhiều cho với phương sai thực. Kết quả là khoảng tin cậy ước lượng được sẽ nhỏ hơn các khoảng tin cậy thực sự.Bạn đọc có thể hình dung việc phần dư có tương quan với nhau cũng giống như việc chúng ta trong quá trình thu thập dữ liệu có sai sót dẫn đến dữ liệu bị trùng lặp. Chẳng hạn như mỗi dòng dữ liệu bị lặp lại một lần, nghĩa là dữ liệu đúng để ước lượng mô hình chỉ có \\(n\\) dòng nhưng chúng ta đã nhân đôi dữ liệu lên trước khi thực hiện ước lượng. Khi tính toán độ lệch chuẩn của sai số, chúng ta sử dụng \\(2 n\\) quan sát để tính toán thay vì \\(n\\) làm cho độ lệch chuẩn bị giảm xuống một tỷ lệ là \\(\\sqrt{2}\\). Các khoảng tin cậy khi tính toán với dữ liệu bị trùng lặp sẽ bị thu hẹp lại với khoảng tin cậy được tính toán với dữ liệu chính xác.Khi nào thì chúng ta sẽ gặp phải hiện tượng phần dư có tương quan với nhau? Ngoài việc sai sót trong quá trình thu thập dữ liệu làm cho dữ liệu vị trùng lặp, chúng ta cũng thường gặp phải hiện tượng phần dư có tương quan khi sử dụng mô hình hồi quy tuyến tính trong dữ liệu dạng chuỗi thời gian. Trong trường hợp mà mỗi dòng dữ liệu là một quan sát thu được tại các thời điểm liền kề nhau thì rất có nhiều khả năng các biến mục tiêu sẽ có tương quan với nhau, dẫn đến tương quan giữa các phần dư trong mô hình tuyến tính.Để xác định xem phần dư từ một mô hình hồi quy tuyến tính có tương quan hay không, chúng ta có thể quan sát đồ thị phần dư. Nếu phần dư không có tương quan thì sẽ không có mối liên hệ rõ ràng nào. Mặt khác, nếu có tồn tại tương quan dương thì chúng ta có thể thấy có sự liên kết giữa các giá trị phần dư.\nFigure 8.6: Đồ thị mô tả phần dư có tương quan và không có tương quan. Hình trên: các phần dư có tương quan bằng 0. Hình ở giữa: hai giá trị phần dư cạnh nhau có tương quan 0.5. Hình dưới: hai giá trị phần dư cạnh nhau có tương quan 0.9\nHình ?? minh họa đồ thị phần dư của ba mô hình khác nhau. Trong hình trên cùng, chúng ta thấy phần dư từ mô hình hồi quy tuyến tính tương ứng với dữ liệu mà biến mục tiêu không có tương quan với nhau. Bạn đọc có thể thấy rằng không có mối liên hệ nào rõ ràng về xu hướng của các giá trị phần dư. Ngược lại, phần dư ở hình dưới cùng là từ tập dữ liệu trong đó các sai số liền kề có hệ số tương quan là 0.9. Bạn đọc có thể nhận thấy có một sự liên kết rõ ràng trong phần dư mà trong đó các giá trị liền kề nhau có xu hướng nhận các giá trị tương tự hay cùng dấu. Cuối cùng, hình ở giữa minh họa một trường hợp ít rõ ràng hơn mà trong đó phần dư có hệ số tương quan là 0,5. Vẫn có bằng chứng về sự liên hệ nhưng không rõ ràng như trường hợp có hệ số tương quan 0.9.Nhìn chung giả định về phần dư không tương quan là vô cùng quan trọng đối với mô hình hồi quy tuyến tính nói riêng cũng như đối với các mô hình học máy hiện đại. Ngoài nguyên nhân từ xây dựng mô hình hay cách lựa chọn biến, sự tương quan giữa các phần dư cũng có thể tồn tại ngay trong chính cách dữ liệu được thu thập, đặc biệt là những dữ liệu mà biến mục tiêu và biến giải thích cùng chịu sự tác động từ các yếu tố bên ngoài. Có nhiều phương pháp đã được phát triển để xác định các mối tương quan của phần dư trong mô hình hồi quy tuyến tính, đặc biệt là đối với mô hình tuyến tính có các biến mục tiêu và biến giải thích có dạng dữ liệu chuỗi thời gian. Nội dung của các phương pháp này bạn đọc có thể tham khảo trong các sách tham khảo dành riêng cho mô hình hồi quy tuyến tính.","code":""},{"path":"mô-hình-hồi-quy-tuyến-tính.html","id":"phương-sai-của-phần-dư-thay-đổi","chapter":"Chương 8 Mô hình hồi quy tuyến tính","heading":"8.1.4.3 Phương sai của phần dư thay đổi","text":"Một giả thiết quan trọng khác của mô hình hồi quy tuyến tính là các phần dư có phương sai không thay đổi: \\(\\mathbb{V}(\\epsilon_i) = \\sigma^2\\) \\(\\forall = 1, 2, \\cdots, n\\). Ước lượng các hệ số tuyến tính, xây dựng các khoảng tin cậy cho hệ số, hay kiểm định các giả thuyết của mô hình tuyến tính đều dựa trên giả định này. Thực tế là chúng ta rất hay gặp phải trường hợp phương sai của phần dư là không cố định. Một nguyên nhân hiện tượng phần dư có phương sai thay đổi đến từ việc mối liên hệ giữa biến mục tiêu và biến giải thích là phi tuyến. Nếu xuất phát từ nguyên nhân này, chúng ta có thể biến đổi biến mục tiêu trước khi thực hiện ước lượng.Một ví dụ điển hình thường gặp phải là khi phương sai của phần dư tăng theo giá trị của biến mục tiêu. Chúng ta có thể xác định được hiện tượng phương sai của phần dư thay đổi bằng cách sử dụng đồ thị của phần dư theo giá trị ước lượng được của biến mục tiêu.\nFigure 8.7: Đồ thị mô tả phần dư có phương sai thay đổi. Hình ở trên: cho thấy phần dư có phương sai thay đổi. Hình ở dưới: Phần dư có phương sai ổn định\nMột ví dụ cho phần dư có phương sai thay đổi được thể hiện trong hình 8.7. Khi chúng ta sử dụng biến mục tiêu \\(Y\\), độ lớn của phần dư có xu hướng tăng theo các giá trị của biến mục tiêu. Khi gặp vấn đề này, một giải pháp đơn giản là biến đổi biến mục tiêu \\(Y\\) bằng cách sử dụng các hàm \\(\\log\\) Y hoặc hàm \\(\\sqrt\\). Các phép biến đổi này có thể làm giảm hiện tượng phương sai thay đổi. Hình phía dưới của Hình 8.7 mô tả phần dư theo giá trị của biến mục tiêu sau khi sử dụng phép biến đổi \\(\\log\\). Phần dư đã trở nên ổn định hơn mặc dù có một số dấu hiệu về mối quan hệ phi tuyến trong giữa biến mục tiêu và biến giải thích.Hiện tượng phương sai của sai số thay đổi có thể là kết quả của quá trình dữ liệu được thu thập, khi mà biến mục tiêu thứ \\(\\) là giá trị trung bình của \\(n_i\\) quan sát độc lập. Ví dụ, dữ liệu về yêu cầu bồi thường của các khách hàng của một công ty bảo hiểm, mỗi khách hàng có thể yêu cầu bồi thường nhiều lần trong khoảng thời gian một năm nhưng dữ liệu chỉ được lưu trữ dưới dạng tổng số tiền khách hàng yêu cầu bồi thường và tổng số lần khách hàng yêu cầu bồi thường. Khi xây dựng mô hình với biến mục tiêu là số tiền yêu cầu bồi thường trung bình thì độ lệch chuẩn của biến mục tiêu sẽ tỷ lệ nghịch với số lần khách hàng yêu cầu bồi thường. Trong trường hợp như vậy, một phương pháp khắc phục đơn giản là ước lượng mô hình sử dụng phương pháp bình phương nhỏ nhất có trọng số. Trọng số được sử dụng tỷ lệ nghịch với phương sai của biến mục tiêu. Chẳng hạn như trong ví dụ về yêu cầu bồi thường, trọng số được sử dụng đối với quan sát thứ \\(\\) chính là số lần khách hàng yêu cầu bồi thường.","code":""},{"path":"mô-hình-hồi-quy-tuyến-tính.html","id":"dữ-liệu-có-giá-trị-ngoại-lai","chapter":"Chương 8 Mô hình hồi quy tuyến tính","heading":"8.1.4.4 Dữ liệu có giá trị ngoại lai","text":"Điểm ngoại lai là điểm dữ liệu mà giá trị biến mục tiêu \\(y_i\\) khác xa giá trị được dự đoán bởi mô hình \\(\\hat{y}_i\\). Các ngoại lai có thể phát sinh vì nhiều lý , chẳng hạn như sự không chính xác trong quá trình thu thập dữ liệu. Bạn đọc có thể tham khảo thêm về giá trị ngoại lai trong phần @ref(#ourlier)\nFigure 8.8: Dữ liệu chứa giá trị ngoại lai là điểm màu đỏ. Hình bên trái: đồ thị mô tả biến mục tiêu theo biến giải thích và các đường hồi quy tuyến tính được xây dụng cho hai trường hợp là có chứa điểm ngoại lai (nét liền màu xanh) và không chứa điểm ngoại lai (nét đứt màu đen). Hình bên phải: Đồ thị phần dư được điều chỉnh theo biến giải thích, điểm ngoại lai có phần dư có giá trị tuyệt đối lớn hơn hẳn các phần dư khác\nĐiểm màu đỏ ở hình bên trái của Hình 8.8 minh họa một ngoại giá trị ngoại lai điển hình. Đường liền màu xanh dương là đường hồi quy tuyến tính sử dụng đầy đủ dữ liệu, trong khi đường nét đứt màu đen là đường hồi quy tuyến tính sau khi loại bỏ đi điểm ngoại lai. Trong trường hợp này, việc loại bỏ giá trị ngoại lai ít ảnh hưởng đến đường hồi quy tuyến tính vì bạn đọc có thể thấy hai đường hồi quy khá gần nhau. Thông thường, một giá trị ngoại lai duy nhất sẽ ít ảnh hưởng đến sự hình dạng của đường hồi quy tuyến tính, tuy nhiên, điểm ngoại lai này lại có thể gây ra các vấn đề khác. Trong ví dụ ở trên, \\(RSE\\) là 1.8 khi giá trị ngoại lai được đưa vào hồi quy, và \\(RSE\\) chỉ bằng 0.9 khi giá trị ngoại lai bị loại bỏ. Vì chúng ta sẽ sử dụng \\(RSE\\) để tính toán các khoảng tin cậy và các \\(p-value\\), nên sự thay đổi đáng kể của \\(RSE\\) như vậy sẽ có tác động đến việc giải thích sự phù hợp của mô hình. Tương tự, việc đưa giá trị ngoại lai vào làm cho \\(R^2\\) giảm từ 0.973 xuống 0.89.Chúng ta có thể xác định một quan sát là ngoại lai hay không bằng cách vẽ đồ thị phần dư. Trong ví dụ ở trên, giá trị ngoại lai có thể được xác định rõ ràng trong Hình 8.8. Nhưng trong thực tế, có thể khó đưa ra được quyết định là phần dư cần phải lớn đến mức nào để chúng ta coi điểm đó là điểm bất thường. Để giải quyết vấn đề này, thay vì vẽ đồ thị phần dư, chúng ta có thể vẽ đồ thị phần dư sau khi chia phần dư cho \\(RSE\\). Nếu các giả thiết của mô hình hồi quy tuyến tính là đúng, phần dư được điều chỉnh (sau khi chia cho \\(RSE\\)) sẽ có phân phối Student. Nếu một giá trị quan sát của phần dư vượt quá các ngưỡng xác suất của phân phối Student, nhiều khả năng đó là giá trị ngoại lai. Trong Hình 8.8, tất cả các quan sát có phần dư nằm trong khoảng -2 đến 2 trong khi giá trị ngoại lai có giá trị là gần 6. Nói một cách khác khả năng điểm màu đỏ với phần dư được điểu chỉnh gần bằng 6 có khả năng rất cao là giá trị ngoại lai.","code":""},{"path":"mô-hình-hồi-quy-tuyến-tính.html","id":"đa-cộng-tuyến","chapter":"Chương 8 Mô hình hồi quy tuyến tính","heading":"8.1.4.5 Đa cộng tuyến","text":"Đa cộng tuyến là sự kiện mà trong đó hai hoặc nhiều biến giải thích có tương quan cao với nhau với nhau. Để giải thích rõ ràng khái niệm này, chúng ta hãy lấy một ví dụ khi xây dựng mô hình hồi quy tuyến tính trên dữ liệu có tên là \\(Credit\\_Card\\) . Đây là dữ liệu về thông tin thẻ tín dụng của các khách hàng tại một ngân hàng với biến mục tiêu là số dư tài khoản (\\(Balance\\)) và 10 biến giải thích. Để giải thích về đa cộng tuyến, chúng tôi chỉ sử dụng ba biến giải thích là 1. Độ tuổi của chủ thẻ tín dụng (\\(Age\\)), 2. Hạn mức thẻ tín dụng (\\(Limit\\)), và 3. Điểm tín dụng của khách hàng (\\(Rating\\)). Hiện tượng đa cộng tuyến được minh họa trên Hình 8.9\nFigure 8.9: Mối liên hệ giữa các biến giải thích trong dữ liệu Credit. Hình bên trái: Không cho thấy có mối tương quan giữa hạn mức tín dụng với tuổi của khách hàng. Hình ở giữa: Tương quan giữa hạn mức tín dụng và điểm tính dụng là rất cao. Hình bên phải: Không có tương quan giữa tuổi của khách hàng với xếp hạng tín dụng\nTrong hình bên trái của Hình 8.9, hai biến giải thích là hạn mức tín dụng và độ tuổi không có mối tương quan rõ ràng. Tương tự, trong hình bên phải, cũng không có mối tương quan rõ ràng giữa độ tuổi với xếp hạng tín dụng. Ngược lại, trong hình ở giữa của Hình 8.9, hạn mức tín dụng và điểm tín dụng có mối tương quan rất cao với nhau bởi các điểm gần như nằm trên một đường thẳng.Hiện tượng đa cộng tuyến gây ra các vấn đề khi ước lượng và giải thích mô hình hồi quy bởi khó có thể tách biệt các tác động riêng lẻ của các biến có tương quan cao lên biến mục tiêu. Trong ví dụ ở trên. hạn mức tín dụng và điểm tín dụng có xu hướng tăng hoặc giảm cùng nhau nên khó có thể xác định xem từng biến riêng biệt có liên quan như thế nào đến biến mục tiêu là số dư tài khoản. Một vấn đề đáng kể khác khi gặp hiện tượng đa cộng tuyến đó là phương sai của các hệ số ước lượng sẽ rất lớn dẫn đến các ước lượng trở nên ít tin cậy hơn và chúng ta sẽ rất khó bác bỏ được giả thuyết hệ số tuyến tính bằng 0.\nTable 8.9: Các hệ số ước lượng trong mô hình hồi quy tuyến tính số dư tài khoản phụ thuộc vào độ tuổi và hạn mức tín dụng\n\nTable 8.10: Các hệ số ước lượng trong mô hình hồi quy tuyến tính số dư tài khoản phụ thuộc vào hạn mức tín dụng và điểm tín dụng\nBảng 8.9 và 8.10 sánh các hệ số tuyến tính ước lượng được được từ hai mô hình hồi quy riêng biệt. Trước tiên là hồi quy số dư tài khoản thẻ tín dụng theo độ tuổi và hạn mức tín dụng, sau đó là hồi quy số dư tài khoản theo hạn mức tín dụng và điểm tín dụng. Trong mô hình hồi quy đầu tiên, độ tuổi và hạn mức tín dụng đều có ý nghĩa giá trị \\(p-value\\) rất nhỏ. Trong mô hình thứ hai, hiện tượng đa cộng tuyến giữa hạn mức tín dụng và điểm tín dụng đã khiến độ lệch chuẩn của ước tính hệ số giới hạn tăng lên gấp 13 lần và \\(p-value\\) tăng lên thành 0.701. Nói cách khác, sự quan trọng của biến hạn mức tín dụng đã bị che khuất hiện tượng đa cộng tuyến. Để tránh rơi vào tình trạng như vậy, cần xác định và giải quyết vấn đề đa cộng tuyến trước khi ước lượng mô hình.Một cách đơn giản để phát hiện hiện tượng đa cộng tuyến là xem xét ma trận tương quan của các biến giải thích. Một phần tử của ma trận này có giá trị tuyệt đối lớn là dấu hiệu cho thấy một cặp biến có tương quan cao và đó có hiện tượng đa cộng tuyến trong dữ liệu. Tuy nhiên, bạn đọc cần lưu ý là vấn đề về đa cộng tuyến luôn có thể được phát hiện bằng cách kiểm tra ma trận tương quan bởi vì có thể tồn tại sự cộng tuyến giữa ba hoặc nhiều biến ngay cả khi không có cặp biến nào có tương quan cao. Thay vì kiểm tra ma trận tương quan, cách tốt hơn để đánh giá hiện tượng đa cộng tuyến là tính hệ số lạm phát phương sai (Variance Inflation Factor hay \\(VIF\\)). \\(VIF\\) là tỷ lệ giữa phương sai của hệ số \\(\\hat{\\beta}_j\\) khi ước lượng mô hình đầy đủ biến và hệ số \\(\\hat{\\beta}_j\\) khi ước lượng mô hình với riêng biến đó. Một cách đơn giản hơn để tính \\(VIF\\) là sử dụng hệ số \\(R^2\\) trong mô hình hồi quy tuyến tính biến \\(X_j\\) theo các biến giải thích còn lại:\n\\[\\begin{align}\nVIF_j = \\cfrac{1}{1 - R^2_{X_j|X_{-j}}}\n\\tag{8.22}\n\\end{align}\\]\ntrong đó \\(R^2_{X_j|X_{-j}}\\) là hệ số \\(R^2\\) trong mô hình hồi quy biến \\(X_j\\) theo các biến giải thích còn lại. Nếu không tồn tại đa cộng tuyến, hệ số \\(R^2_{X_j|X_{-j}}\\) sẽ gần bằng 0 và \\(VIF_j\\) sẽ lớn hơn 1 một chút. Ngược lại, nếu biến \\(X_j\\) có thể được xấp xỉ bằng tổ hợp tuyến tính của các biến giải thích còn lại, hệ số \\(R^2_{X_j|X_{-j}}\\) sẽ xấp xỉ 1 và dẫn đến \\(VIF_j\\) có giá trị rất lớn.\nTable 8.11: Hệ số lạm phát phương sai (VIF) của các biến giải thích trong dữ liệu về thẻ tín dụng\nTừ Bảng 8.11 được tính toán từ dữ liệu về thẻ tín dụng, có thể thấy rằng các biến giải thích độ tuổi, hạn mức tín dụng, và điểm tín dụng có giá trị \\(VIF\\) lần lượt là \\(1.01\\), \\(160.67\\) và \\(160.59\\). Có thể kết luận là có hiện tượng đa cộng tuyến trong dữ liệu thẻ tín dụng! Khi gặp hiện tượng đa cộng tuyến như vậy, có hai giải pháp đơn giản thường được sử dụng. Giải pháp trước tiên là loại bỏ một trong các biến có hệ số VIF cao ra khỏi mô hình hồi quy. Giải pháp này thường được thực hiện mà không ảnh hưởng nhiều đến sự phù hợp của mô hình hồi quy. Trong ví dụ về dữ liệu thẻ tín dụng, chúng ta có thể hồi quy số dư tài khoản theo độ tuổi và hạn mức tín dụng và bỏ qua biến điểm tín dụng mà không làm cho hệ số \\(R^2\\) giảm một cách đáng kể. Giải pháp thứ hai là kết hợp các biến có đa cộng tuyến lại với nhau thành một biến giải thích duy nhất. Chẳng hạn như chúng ta có thể lấy giá trị trung bình biến hạn mức tín dụng và biến điểm tín dụng để tạo ra một biến giải thích mới trong mô hình hồi quy tuyến tính.Trong phần tiếp theo, chúng tôi sẽ đi sâu vào giải thích phương pháp bình phương nhỏ nhất được sử dụng để ước lượng mô hình hồi quy tuyến tính và tính chất của các ước lượng. Mục tiêu là để bạn đọc hiểu rõ hơn những kết quả đã được sử dụng hoặc công nhận ở phần trên. Những bạn đọc cảm thấy không cần thiết có thể bỏ qua và chuyển sang các phần tiếp theo mà không gặp bất kỳ khó khăn nào khi sử dụng các kết quả của mô hình hồi quy tuyến tính.","code":""},{"path":"mô-hình-hồi-quy-tuyến-tính.html","id":"leastsquared","chapter":"Chương 8 Mô hình hồi quy tuyến tính","heading":"8.2 Mô hình hồi quy tuyến tính và phương pháp bình phương nhỏ nhất","text":"Mô hình hồi quy tuyến tính, đúng như tên gọi của nó, cho rằng hàm \\(f\\) được sử dụng để mô tả tác động của các biến giải thích \\(X_1, X_2, \\cdot ,X_p\\) lên biến mục tiêu \\(Y\\) là có dạng hàm tuyến tính\n\\[\\begin{align}\nf(\\textbf{X}) = \\beta_0 + \\beta_1 \\cdot X_1 + \\beta_2 \\cdot X_2 + \\cdots + \\beta_p \\cdot X_p\n\\tag{8.24}\n\\end{align}\\]\nCác hệ số \\(\\beta_i\\) trong phương trình (8.24) được gọi là các tham số của mô hình hồi quy tuyến tính hoặc còn được gọi là các hệ số hồi quy. Các biến \\(X_i\\) có thể được đưa vào mô hình từ những cách như sau:Biến \\(X_i\\) là một biến kiểu số.Biến \\(X_i\\) là một biến kiểu số.Biến \\(X_i\\) là một biến đổi của một biến kiểu số để mối liên hệ giữa \\(Y\\) và \\(X_i\\) trở nên tuyến tính. Các phép biến đổi thường gặp có thể là phép biến đổi \\(\\log\\), phép lấy căn, lấy bình phương, …Biến \\(X_i\\) là một biến đổi của một biến kiểu số để mối liên hệ giữa \\(Y\\) và \\(X_i\\) trở nên tuyến tính. Các phép biến đổi thường gặp có thể là phép biến đổi \\(\\log\\), phép lấy căn, lấy bình phương, …Biến \\(X_i\\) có thể là một đa thức của một biến trong dữ liệu ban đầu. Trong trường hợp này chúng ta thường gọi là mô hình hồi quy đa thức.Biến \\(X_i\\) có thể là một đa thức của một biến trong dữ liệu ban đầu. Trong trường hợp này chúng ta thường gọi là mô hình hồi quy đa thức.Trong trường hợp \\(X_i\\) là một biến kiểu \\(factor\\) và nhận \\(J\\) giá trị khác nhau. Giả sử \\(J\\) giá trị được mã hóa thành \\(1, 2, \\cdots, J\\) thì để mô tả tác động của biến \\(X_i\\) lên biến mục tiêu trong mô hình tuyến tính, chúng ta cần \\(J\\) giá trị hệ số tuyến tính: \\(\\beta_{,1}\\), \\(\\beta_{,2}\\), \\(\\cdots\\), \\(\\beta_{,J}\\); trong đó hệ số tuyến tính \\(\\beta_{,j}\\) mô tả tác động của giá trị \\(X_i = j\\) lên biến mục tiêu.Trong trường hợp \\(X_i\\) là một biến kiểu \\(factor\\) và nhận \\(J\\) giá trị khác nhau. Giả sử \\(J\\) giá trị được mã hóa thành \\(1, 2, \\cdots, J\\) thì để mô tả tác động của biến \\(X_i\\) lên biến mục tiêu trong mô hình tuyến tính, chúng ta cần \\(J\\) giá trị hệ số tuyến tính: \\(\\beta_{,1}\\), \\(\\beta_{,2}\\), \\(\\cdots\\), \\(\\beta_{,J}\\); trong đó hệ số tuyến tính \\(\\beta_{,j}\\) mô tả tác động của giá trị \\(X_i = j\\) lên biến mục tiêu.Biến giải thích được tính toán từ tác động qua lại giữa các biến giải thích khác, chẳng hạn như \\(X_i \\cdot X_j\\).Biến giải thích được tính toán từ tác động qua lại giữa các biến giải thích khác, chẳng hạn như \\(X_i \\cdot X_j\\).","code":""},{"path":"mô-hình-hồi-quy-tuyến-tính.html","id":"ước-lượng-các-hệ-số-tuyến-tính","chapter":"Chương 8 Mô hình hồi quy tuyến tính","heading":"8.2.1 Ước lượng các hệ số tuyến tính","text":"Dù biến giải thích được tính toán như thế nào, biến mục tiêu \\(Y\\) vẫn là một hàm số tuyến tính của các hệ số \\((\\beta_0, \\beta_1, \\beta_2,\\cdots, \\beta_p)\\). Để dễ dàng triển khai các công thức, chúng tôi sẽ sử dụng ký hiệu \\(\\boldsymbol{\\beta}\\) tương đương như véc-tơ hệ số \\((\\beta_0, \\beta_1, \\beta_2,\\cdots, \\beta_p)\\). Phương pháp thông dụng nhất để ước lượng các hệ số của mô hình hồi quy tuyến tính là phương pháp bình phương nhỏ nhất, nghĩa là tham số \\(\\boldsymbol{\\beta}\\) được tính toán từ bài toán tối ưu\n\\[\\begin{align}\n\\hat{\\boldsymbol{\\beta}} = \\underset{\\boldsymbol{\\beta}}{\\operatorname{argmin}} \\sum\\limits_{=1}^n \\left( y_i - \\beta_0 - \\sum\\limits_{j = 1}^p \\beta_j \\cdot x_{,j} \\right)^2\n\\tag{8.25}\n\\end{align}\\]Sai số giữa \\(y_i\\) và \\(\\beta_0 + \\sum\\limits_{j = 1}^p \\beta_j \\cdot x_{,j}\\) được gọi là phần dư (hay \\(residuals\\)) trong mô hình hồi quy tuyến tính. Vế bên phải của công thức (8.25) là tổng bình phương của các phần dư và được viết tắt là \\(RSS\\). Nếu coi tổng bình phương sai số là hàm số của các hệ số hồi quy \\(\\boldsymbol{\\beta}\\) và viết công thức tổng bình phương sai số dưới dạng ma trận ta sẽ có\n\\[\\begin{align}\nRSS(\\boldsymbol{\\beta}) = (\\textbf{y} - \\textbf{x} \\boldsymbol{\\beta})^T \\ (\\textbf{y} - \\textbf{x} \\boldsymbol{\\beta})\n\\tag{8.26}\n\\end{align}\\]\nvới \\(\\text{x}\\) là dữ liệu huấn luyện mô hình có kích thước \\(n \\times (p+1)\\); \\(\\textbf{y}\\) là véc-tơ biến mục tiêu có kích thước \\(n \\times 1\\); và véc-tơ tham số \\(\\boldsymbol{\\beta}\\) có kích thước \\((p+1) \\times 1\\). Véc-tơ gradient của \\(RSS(\\boldsymbol{\\beta})\\) là véc-tơ có độ dài \\((p+1)\\) mà phần tử thứ \\((j+1)\\) là giá trị đạo hàm của RSS theo \\(\\beta_j; j = 0, 1, \\cdots, p\\); và được xác định như sau\n\\[\\begin{align}\n\\cfrac{\\nabla RSS(\\boldsymbol{\\beta})}{\\nabla \\boldsymbol{\\beta}} &= \\cfrac{\\nabla (\\textbf{y} - \\textbf{x} \\boldsymbol{\\beta})^T (\\textbf{y} - \\textbf{x} \\boldsymbol{\\beta}) }  {\\nabla \\boldsymbol{\\beta}} \\\\\n& = - 2 \\textbf{x}^T \\  (\\textbf{y} - \\textbf{x} \\boldsymbol{\\beta})\n\\tag{8.27}\n\\end{align}\\]Ma trận Hessian là ma trận kích thước \\(((p+1) \\times (p+1))\\) mà phần tử hàng \\(+1\\) cột \\(j+1\\) là đạo hàm cấp hai của \\(RSS\\) lần lượt theo \\(\\beta_i\\) rồi theo \\(\\beta_j\\)\n\\[\\begin{align}\n\\cfrac{\\nabla RSS(\\boldsymbol{\\beta})}{\\nabla \\boldsymbol{\\beta} \\ \\nabla \\boldsymbol{\\beta}^T} = 2 \\textbf{x}^T \\ \\textbf{x}\n\\tag{8.28}\n\\end{align}\\]Giả sử rằng ma trận biến giải thích không có cột nào là tổ hợp tuyến tính của các cột còn lại, hay nói cách khác, hạng của ma trận \\(\\textbf{x}\\) là \\((p+1)\\). Khi đó ta có \\((\\textbf{x}^T \\ \\textbf{x})\\) là ma trận xác định dương. Giá trị \\(\\hat{\\boldsymbol{\\beta}}\\) làm tối thiểu hóa \\(RSS(\\boldsymbol{\\beta})\\) là nghiệm của\n\\[\\begin{align}\n\\cfrac{\\nabla RSS(\\boldsymbol{\\beta})}{\\nabla \\boldsymbol{\\beta}} = \\textbf{0}\n\\tag{8.29}\n\\end{align}\\]\nnghĩa là \\(\\hat{\\beta}\\) được tính toán như sau\n\\[\\begin{align}\n\\textbf{x}^T \\  (\\textbf{y} - \\textbf{x} \\hat{\\boldsymbol{\\beta}}) = \\textbf{0} \\\\\n\\rightarrow \\textbf{x}^T \\ \\textbf{y} = \\textbf{x}^T \\ \\textbf{x} \\hat{\\boldsymbol{\\beta}} \\\\\n\\rightarrow \\hat{\\boldsymbol{\\beta}} = (\\textbf{x}^T \\ \\textbf{x} )^{-1} \\textbf{x}^T \\ \\textbf{y}\n\\tag{8.30}\n\\end{align}\\]Với ma trận dữ liệu \\(\\textbf{x}\\), giá trị dự báo \\(\\hat{\\textbf{y}}\\) được tính toán từ công thức dưới đây\n\\[\\begin{align}\n\\hat{\\textbf{y}} = \\textbf{x} \\ (\\textbf{x}^T \\ \\textbf{x} )^{-1} \\textbf{x}^T \\ \\textbf{y}\n\\tag{8.31}\n\\end{align}\\]Điều gì xảy ra nếu hạng của ma trận \\(\\textbf{x}\\) nhỏ hơn \\((p+1)\\), nghĩa là một (hoặc một vài cột dữ liệu) là tổ hợp tuyến tính của các cột dữ liệu khác, hoặc trong trường hợp ma trận \\(\\textbf{x}\\) có số hàng ít hơn số cột. Khi đó ma trận \\(\\textbf{x}^T \\ \\textbf{x}\\) sẽ không khả nghịch và phương trình tuyến tính (8.29) sẽ có vô số nghiệm \\(\\boldsymbol{\\beta}\\). Đa số các hàm có sẵn khi xây dựng và ước lượng mô hình tuyến tính đều tính toán đến vấn đề này, mỗi khi thêm một biến vào trong mô hình, luôn có bước kiểm tra nếu biến được thêm vào có phải là tổ hợp tuyển tính (hoặc xấp xỉ bằng tổ hợp tuyến tính) của các biến sẵn có để loại bỏ biến đó khỏi mô hình.Để có thể đưa ra các suy diễn về véc-tơ hệ số, chúng ta cần có giả thiết về phân phối của biến phụ thuộc \\(Y\\). Mô hình hồi quy tuyến tính có giả thiết quan trọng là biến mục tiêu \\(Y\\) có phân phối chuẩn. Nói một cách khác, biến ngẫu nhiên \\(Y|X = x_i\\), được viết tắt là \\(Y_i\\), là các biến ngẫu nhiên phân phối chuẩn độc lập có giá trị trung bình \\(\\textbf{x}_i^T \\boldsymbol{\\beta}\\) và phương sai cố định là \\(\\sigma^2\\) (không phụ thuộc vào \\(\\)). Mô hình hồi quy tuyến tính được viết như sau\n\\[\\begin{align}\n& Y_i = \\beta_0 + \\beta_1 \\cdot x_{,1} + \\beta_2 \\cdot x_{,2} + \\cdots + \\beta_p \\cdot x_{,p} + \\epsilon_i \\\\\n& \\epsilon_i \\sim \\mathcal{N}(0, \\sigma^2) \\\\\n& Cov(\\epsilon_i, \\epsilon_j) = 0 \\ \\ \\forall \\neq j\n\\tag{8.32}\n\\end{align}\\]Hoặc chúng ta có thể viết mô hình hồi quy tuyến tính dưới dạng ma trận\n\\[\\begin{align}\nY \\sim \\mathcal{N}(\\textbf{x} \\boldsymbol{\\beta}, \\sigma^2 \\ \\textbf{}_n)\n\\tag{8.33}\n\\end{align}\\]\ntrong đó \\(\\textbf{}_n\\) là ma trận đơn vị kích thước \\(n \\times n\\). Với giả thiết phân phối chuẩn của \\(Y\\) trong phương trình (8.33) và kết hợp với (8.30) chúng ta thấy rằng \\(\\hat{\\boldsymbol{\\beta}}\\) là một phép biến đổi tuyến tính của một véc-tơ phân phối chuẩn nên cũng là một véc-tơ phân phối chuẩn. Véc-tơ trung bình của \\(\\hat{\\boldsymbol{\\beta}}\\) được xác định như sau\n\\[\\begin{align}\n\\mathbb{E}(\\hat{\\boldsymbol{\\beta}}) &= \\mathbb{E}\\left((\\textbf{x}^T \\ \\textbf{x} )^{-1} \\textbf{x}^T \\ \\textbf{y}\\right) \\\\\n& = (\\textbf{x}^T \\ \\textbf{x} )^{-1} (\\textbf{x}^T \\textbf{x}) \\ \\boldsymbol{\\beta} \\\\\n& = \\boldsymbol{\\beta}\n\\tag{8.34}\n\\end{align}\\]\nMa trận hiệp phương sai của véc-tơ \\(\\hat{\\boldsymbol{\\beta}}\\)\n\\[\\begin{align}\n\\mathbb{V}ar(\\hat{\\boldsymbol{\\beta}}) &= \\mathbb{V}ar\\left((\\textbf{x}^T \\ \\textbf{x} )^{-1} \\textbf{x}^T \\ \\textbf{y}\\right) \\\\\n&= \\sigma^2 \\ (\\textbf{x}^T \\ \\textbf{x} )^{-1}\n\\tag{8.35}\n\\end{align}\\]Có thể thấy rằng véc-tơ trung bình của \\(\\hat{\\boldsymbol{\\beta}}\\) hoàn toàn phụ thuộc vào dữ liệu trong khi ma trận hiệu phương sai lại phụ thuộc vào một tham số không biết là \\(\\sigma\\). Để xây dựng được các khoảng tin cậy hoặc kiểm định được các hệ số có khác không hay không, chúng ta cần ước lượng tham số \\(\\sigma^2\\).","code":""},{"path":"mô-hình-hồi-quy-tuyến-tính.html","id":"ước-lượng-phương-sai-của-biến-phụ-thuộc.","chapter":"Chương 8 Mô hình hồi quy tuyến tính","heading":"8.2.2 Ước lượng phương sai của biến phụ thuộc.","text":"Tham số thứ hai của mô hình hồi quy tuyến tính là phương sai của biến phụ thuộc, ký hiệu \\(\\sigma^2\\), được ước lượng như sau\n\\[\\begin{align}\n\\hat{\\sigma}^2 = RSS(\\hat{\\boldsymbol{\\beta}}) = \\cfrac{\\sum \\hat{\\epsilon}_i^2}{n - (p+1)} = \\cfrac{\\hat{\\boldsymbol{\\epsilon}}^{T} \\ \\hat{\\boldsymbol{\\epsilon}}}{n - (p+1)}\n\\tag{8.35}\n\\end{align}\\]\nvới \\(\\hat{\\epsilon}_i = y_i - \\hat{y}_i\\).Với giả thiết phân phối chuẩn của các \\(\\epsilon_i\\) như (8.32), \\(\\hat{\\boldsymbol{\\beta}}\\) được ước lượng từ phương trình (8.30), và \\(\\hat{y}\\) được tính toán từ (8.31), ta có thể chứng minh được rằng \\(\\cfrac{\\sum \\hat{\\epsilon}_i^2}{\\sigma^2}\\) là một biến ngẫu nhiên phân phối \\(\\chi^2\\) với bậc tự là \\(n-(p+1)\\). Thật vậy, ta có\n\\[\\begin{align}\n\\hat{\\boldsymbol{\\epsilon}} &= \\textbf{y} - \\hat{\\textbf{y}} = y - \\textbf{x} \\hat{\\beta} \\\\\n& = \\textbf{y} - \\textbf{x} (\\textbf{x}^T \\ \\textbf{x} )^{-1} \\textbf{x}^T \\ \\textbf{y} \\\\\n& = Q \\textbf{y}\n\\end{align}\\]\nvới ma trận \\(Q = I_n - \\textbf{x} (\\textbf{x}^T \\ \\textbf{x} )^{-1} \\textbf{x}^T\\). Ma trận \\(Q\\) có các tính chất sau:Thứ nhất là Q có vết (\\(Trace\\)) bằng \\(n - (p+1)\\), thật vậy\n\\[\\begin{align}\nTrace(Q) = Trace(\\textbf{}_n) - Trace(\\textbf{x} (\\textbf{x}^T \\ \\textbf{x} )^{-1} \\textbf{x}^T)\n\\end{align}\\]\nđồng thời\n\\[\\begin{align}\nTrace(\\textbf{x} (\\textbf{x}^T \\ \\textbf{x} )^{-1} \\textbf{x}^T) &= Trace( \\textbf{x}^T \\textbf{x} (\\textbf{x}^T \\ \\textbf{x} )^{-1} ) \\\\\n& = Trace(I_{p+1}) = (p+1)\n\\end{align}\\]\nđó \\(Trace(Q) = n - (p+1)\\)Thứ nhất là Q có vết (\\(Trace\\)) bằng \\(n - (p+1)\\), thật vậy\n\\[\\begin{align}\nTrace(Q) = Trace(\\textbf{}_n) - Trace(\\textbf{x} (\\textbf{x}^T \\ \\textbf{x} )^{-1} \\textbf{x}^T)\n\\end{align}\\]\nđồng thời\n\\[\\begin{align}\nTrace(\\textbf{x} (\\textbf{x}^T \\ \\textbf{x} )^{-1} \\textbf{x}^T) &= Trace( \\textbf{x}^T \\textbf{x} (\\textbf{x}^T \\ \\textbf{x} )^{-1} ) \\\\\n& = Trace(I_{p+1}) = (p+1)\n\\end{align}\\]\nđó \\(Trace(Q) = n - (p+1)\\)Thứ hai, có thể thấy rằng \\(Q = Q^{'}\\) và \\(Q = Q^2\\) đó các giá trị riêng của Q chỉ có thể nhận giá trị là 0 hoặc 1. Như vậy, nếu gọi \\(V\\) là ma trận có các cột là các véc-tơ riêng của Q thì chúng ta có’\n\\[\\begin{align}\nV Q V^{'} = \\Delta = diag(1,1,\\cdots,1,0,0,\\cdots,0)\n\\tag{8.36}\n\\end{align}\\]\ntrong đó \\(\\Delta\\) là ma trận đường chéo chỉ chứa 0 và 1 trên đường chéo chính. Kết hợp với kết quả \\(Trace(Q)\\) \\(=\\) \\(n - (p+1)\\) có thể kết luận rằng \\(Q\\) có \\(n - (p+1)\\) giá trị riêng bằng 1 và \\((p+1)\\) giá trị riêng bằng 0.Thứ hai, có thể thấy rằng \\(Q = Q^{'}\\) và \\(Q = Q^2\\) đó các giá trị riêng của Q chỉ có thể nhận giá trị là 0 hoặc 1. Như vậy, nếu gọi \\(V\\) là ma trận có các cột là các véc-tơ riêng của Q thì chúng ta có’\n\\[\\begin{align}\nV Q V^{'} = \\Delta = diag(1,1,\\cdots,1,0,0,\\cdots,0)\n\\tag{8.36}\n\\end{align}\\]\ntrong đó \\(\\Delta\\) là ma trận đường chéo chỉ chứa 0 và 1 trên đường chéo chính. Kết hợp với kết quả \\(Trace(Q)\\) \\(=\\) \\(n - (p+1)\\) có thể kết luận rằng \\(Q\\) có \\(n - (p+1)\\) giá trị riêng bằng 1 và \\((p+1)\\) giá trị riêng bằng 0.Ta có \\(\\boldsymbol{\\epsilon} \\sim \\mathcal{N}(\\textbf{0}, \\sigma^2 )\\) và \\(\\hat{\\boldsymbol{\\epsilon}} \\sim \\mathcal{N}(\\textbf{0}, \\sigma^2 Q)\\). Từ (8.36) ta có \\(V \\hat{\\boldsymbol{\\epsilon}} ~ \\sim \\mathcal{N}(\\textbf{0}, \\sigma^2 \\Delta)\\). Nói cách khác, nếu cho \\(\\textbf{z} = V \\hat{\\epsilon}\\) thì \\(\\textbf{z}\\) là véc-tơ phân phối chuẩn với trung bình là \\(\\textbf{0}\\) và ma trận hiệp phương sai \\(\\sigma^2 \\Delta\\). Ma trận đường chéo \\(\\Delta\\) có \\(n-(p+1)\\) phần tử nằm trên đường chéo chính bằng 1 và (p+1) phần tử nằm trên đường chéo chính bằng 0. Nói một cách khác, các phần tử từ vị trí thứ \\(1\\) đến \\(n-(p+1)\\) trong \\(\\textbf{z}\\) có phương sai bằng \\(\\sigma^2\\) và \\(p+1\\) phần tử còn lại trong \\(\\textbf{z}\\) có phương sai bằng 0.\\(V\\) là ma trận các giá trị riêng thỏa mãn \\(V V' = I_n\\) nên\n\\[\\begin{align}\n\\hat{\\boldsymbol{\\epsilon}}^{T} \\ \\hat{\\boldsymbol{\\epsilon}} = (V \\hat{\\boldsymbol{\\epsilon}})^T V \\hat{\\boldsymbol{\\epsilon}} = \\textbf{z}^T \\textbf{z} = z_1^2 + z_2^2 + \\cdots + z_{n-(p+1)}^2 \\sim \\sigma^2 \\cdot \\chi^2_{n-(p+1)}\n\\tag{8.37}\n\\end{align}\\]Như vậy, từ phương trình (8.37) chúng ta có\n\\[\\begin{align}\n\\left(n-(p+1)\\right) \\times \\hat{\\sigma}^2 = \\hat{\\boldsymbol{\\epsilon}}^{T} \\ \\hat{\\boldsymbol{\\epsilon}} \\sim \\sigma^2 \\times \\chi^2_{n-(p+1)}\n\\tag{8.38}\n\\end{align}\\]\\(\\hat{\\sigma}^2\\) là ước lượng không chệch của \\(\\sigma^2\\) vì\n\\[\\begin{align}\n\\mathbb{E}\\left(\\hat{\\sigma}^2\\right) & = \\cfrac{1}{(n-(p+1))} \\mathbb{E}\\left(\\sigma^2 \\cdot \\chi^2_{n-(p+1)} \\right) \\\\\n& = \\sigma^2 \\times \\cfrac{\\mathbb{E}\\left(\\chi^2_{n-(p+1)} \\right)}{(n-(p+1))} \\\\\n& = \\sigma^2\n\\end{align}\\]\nvì giá trị trung bình của biến ngẫu nhiên \\(\\chi^2_{n-(p+1)}\\) là \\(n-(p+1)\\).Có thể tóm tắt các ước lượng tham số của mô hình hồi quy tuyến tính, bao gồm các hệ số tuyến tính va phương sai của biến mục tiêu, sử dụng phương pháp bình phương nhỏ nhất như sau\n\\[\\begin{align}\n\\hat{\\boldsymbol{\\beta}} & = P \\cdot \\textbf{y} \\\\\n\\hat{\\sigma}^2 & = \\cfrac{1}{n-(p+1)} \\ (Q \\cdot \\textbf{y})^T (Q \\cdot \\textbf{y}) \\\\\nP & = (\\textbf{x}^T \\ \\textbf{x} )^{-1} \\textbf{x}^T \\\\\nQ & = \\textbf{}_n - \\textbf{x} \\ (\\textbf{x}^T \\ \\textbf{x} )^{-1} \\textbf{x}^T\n\\tag{8.39}\n\\end{align}\\]Lưu ý rằng \\(P \\cdot Q^T\\) là một ma trận kích thước \\((p+1) \\times n\\) có tất cả các phần tử bằng 0, đó \\(\\hat{\\boldsymbol{\\beta}}\\) và \\(\\hat{\\sigma}^2\\) là các biến ngẫu nhiên độc lập.","code":""},{"path":"mô-hình-hồi-quy-tuyến-tính.html","id":"kiểm-định-các-hệ-số-ước-lượng.","chapter":"Chương 8 Mô hình hồi quy tuyến tính","heading":"8.2.3 Kiểm định các hệ số ước lượng.","text":"Từ các phương trình (8.34), (8.35), và (8.38), chúng ta có phân phối xác suất của các tham số của mô hình hồi quy tuyến tính:\n\\[\\begin{align}\n& \\hat{\\beta}_j \\sim \\mathcal{N}(\\beta_j, \\sigma^2 a_{jj}) \\ \\ \\forall 1 \\leq j \\leq (p+1) \\\\\n& \\hat{\\sigma}^2 \\sim \\cfrac{\\sigma^2}{n-(p+1)} \\ \\chi^2_{n-(p+1)}\n\\tag{8.40}\n\\end{align}\\]\nvới \\(a_{jj}\\) là phần tử nằm ở hàng \\(j\\) cột \\(j\\) của ma trận \\((\\textbf{x}^T \\ \\textbf{x} )^{-1}\\).Sau khi ước lượng hệ số tuyến tính từ phương trình (8.39), chúng ta thường quan tâm đến sự kiện \\(\\beta_j \\neq 0\\) ở một mức độ tin cậy nào đó, nghĩa là biến độc lập \\(X_j\\) có tác động tuyến tính lên biến mục tiêu \\(Y\\) một cách có ý nghĩa. Để trả lời câu hỏi này, chúng ta cần kiểm định giả thuyết \\(H_0: \\beta_j = 0\\). Dưới giả thuyết \\(H_0\\), \\(\\hat{\\beta}_j\\) là biến ngẫu nhiên phân phối chuẩn có giá trị trung bình bằng 0, tuy nhiên phương sai của \\(\\hat{\\beta}_j\\) phụ thuộc vào giá trị không biết là \\(\\sigma^2\\). Cho biến ngẫu nhiên \\(T_j = \\hat{\\beta}_j/\\left(\\sqrt{a_{jj}} \\cdot \\hat{\\sigma}\\right)\\) thì cùng chia cả tử và mẫu của \\(T_j\\) cho \\(\\sqrt{a_{jj}} \\cdot \\sigma\\) ta có\n\\[\\begin{align}\nT_j = \\hat{\\beta}_j/(\\hat{\\sigma} \\cdot  \\sqrt{a_{jj}})  = \\cfrac{\\hat{\\beta}_j/(\\sigma \\cdot  \\sqrt{a_{jj}})}{\\hat{\\sigma}/\\sigma}\n\\tag{8.41}\n\\end{align}\\]Dưới giả thuyết \\(H_0\\) ta có\n\\[\\begin{align}\n& \\hat{\\beta}_j/(\\sigma \\cdot  \\sqrt{a_{jj}}) \\sim \\mathcal{N}(0,1) \\\\\n& \\hat{\\sigma}/\\sigma \\sim \\sqrt{\\cfrac{\\chi^2_{n-(p+1)}}{n-(p+1)}}\n\\tag{8.42}\n\\end{align}\\]\nđó \\(T_j\\) là biến ngẫu nhiên phân phối Student với bậc tự \\(n-(p+1)\\). Lưu ý rằng khi bậc tự đủ lớn, biến ngẫu nhiên phân phối Student sẽ hội tụ đến phân phối chuẩn với trung bình bằng 0 và phương sai bằng 1. đó trong nhiều trường hợp, người xây dựng mô hình sử dụng phân phối \\(\\mathcal{N}(0,1)\\) để kiểm định giả thuyết \\(H_0: \\beta_j = 0\\).Giá trị của \\(T_j\\) tính từ công thức (8.41) là cơ sở để bác bỏ hoặc không bác bỏ giả thuyết \\(H_0\\). Một cách tự nhiên, nếu giá trị tuyệt đối của \\(T_j\\) lớn, nghĩa là \\(T_j\\) nằm xa giá trị 0, xác suất để bác bỏ giả thuyết \\(H_0\\) là lớn hơn với khi \\(T_j\\) gần 0. \\(p-value\\) được tính bởi công thức dưới đây\n\\[\\begin{align}\np-value = 2 \\mathbb{P}(T_{n-(p+1)} > |T_j|)\n\\end{align}\\]\nKhi \\(p-value\\) nhỏ hơn một mức ý nghĩa \\(\\alpha\\) thì có thể kết luận rằng với độ tin cậy \\((1-\\alpha)\\), hệ số \\(\\beta_j\\) là khác 0 một cách có ý nghĩa thống kê.Trong nhiều trường hợp chúng ta cần phải thực hiện kiểm định giả thuyết mà nhiều hệ số tuyến tính đồng thời bằng 0. Chẳng hạn như các hệ số tuyến tính của một nhóm các biến liên tục, hoặc hệ số tuyến tính của 1 biến rời rạc nhận từ ba giá trị trở lên. Giả sử các hệ số cần được kiểm định đồng thời là \\(\\beta_{j_1}, \\beta_{j_2}, \\cdots, \\beta_{j_h}\\), khi đó cặp giả thuyết \\(H_0\\) - \\(H_1\\) sẽ là\n\\[\\begin{align}\n& H_0: \\beta_{j_i} = 0 \\ \\ \\forall = 1,2, \\cdots, h \\\\\n& H_1: \\text{Tồn tại ít nhất $$ sao cho} \\ \\beta_{j_i} \\neq 0\n\\end{align}\\]Dưới giả thuyết \\(H_0\\) ta có\n\\[\\begin{align}\n\\cfrac{1}{h} \\sum\\limits_{= 1}^h T^2_{j_i} & =  \\cfrac{ \\cfrac{1}{h}  \\sum\\limits_{= 1}^h \\left( \\hat{\\beta}_{j_i}/(\\sigma \\cdot  \\sqrt{a_{jj}}) \\right)^2}{{\\hat{\\sigma}^2/\\sigma^2}}\n= \\cfrac{\\chi^2_{h}/h}{\\chi^2_{n-(p+1)}/(n-(p+1))} \\sim \\mathcal{F}(h,n-(p+1))\n\\tag{8.43}\n\\end{align}\\]Trong đó \\(\\mathcal{F}(h,n-(p+1))\\) là phân phối \\(\\mathcal{F}\\) với các tham số \\(h\\) và \\(n - (p+1)\\). Một cách tự nhiên, nếu giá trị của \\(\\sum\\limits_{= 1}^h T^2_{j_i}/h\\) đủ lớn, chúng ta sẽ bác bỏ giả thuyết \\(H_0\\), nghĩa là tồn tại ít nhất một \\(\\) sao cho \\(\\beta_{j_i} \\neq 0\\). Nhắc lại rằng chúng tôi đã đề cập đến phân phối \\(\\mathcal{F}\\) khi kiểm định mô hình tuyến tính đa biến. Chúng tôi đã sử dụng thống kê \\(F\\) được tính toán bằng công thức (8.14). Trong trường hợp tổng quát, nếu \\(RSS_1\\) là tổng bình phương sai số của mô hình tuyến tính bao gồm đầy đủ \\(p+1\\) biến giải thích trong khi \\(RSS_1\\) là tổng bình phương sai số của mô hình tuyến tính không bao gồm các biến \\(X_{j_1}, X_{j_2}, \\cdots, X_{j_h}\\), thống kê \\(F\\) được tính toán bằng công thức sau\n\\[\\begin{align}\nF = \\cfrac{(RSS_1 - RSS_0)/h}{RSS_0/\\left(n-(p+1)\\right)}\n\\tag{8.44}\n\\end{align}\\]Nếu giả thuyết \\(H_0\\) là đúng, nghĩa là các hệ số \\(\\beta_{j_i}\\) đều nhận giá trị bằng 0, có thể chứng minh được rằng thống kê \\(F\\) sẽ có phân phối \\(\\mathcal{F}\\) với tham số \\(h\\) và \\(n - (p+1)\\).\nxxxxxxxxxxxxxxx","code":""},{"path":"mô-hình-hồi-quy-tuyến-tính.html","id":"linearmodelselection","chapter":"Chương 8 Mô hình hồi quy tuyến tính","heading":"8.3 Các phương pháp chọn biến trong mô hình hồi quy tuyến tính.","text":"Có hai lý khiến những người xây dựng mô hình thường không hài lòng với kết quả từ phương pháp bình phương nhỏ nhấtThứ nhất là khả năng dự đoán của mô hình: ước lượng tham số bằng phương pháp bình phương nhỏ nhất thường có bais thấp nhưng variance lớn. Độ chính xác của dự đoán đôi khi có thể được cải thiện bằng các phương pháp như thu nhỏ số lượng biến giải thích hoặc các phương pháp shinkage. Bằng cách tiếp cận như vậy, người xây dựng mô hình chấp nhận tăng bais để giảm variance của các giá trị dự đoán để có thể cải thiện độ chính xác trong dự đoán.Thứ nhất là khả năng dự đoán của mô hình: ước lượng tham số bằng phương pháp bình phương nhỏ nhất thường có bais thấp nhưng variance lớn. Độ chính xác của dự đoán đôi khi có thể được cải thiện bằng các phương pháp như thu nhỏ số lượng biến giải thích hoặc các phương pháp shinkage. Bằng cách tiếp cận như vậy, người xây dựng mô hình chấp nhận tăng bais để giảm variance của các giá trị dự đoán để có thể cải thiện độ chính xác trong dự đoán.Lý thứ hai là khả năng diễn giải của mô hình. Khi số lượng biến giải thích là quá nhiều, chúng ta thường muốn xác định một tập hợp nhỏ hơn những biến có tác động mạnh đáng kể nhất nhằm diễn giải mô hình một cách tốt nhất.Lý thứ hai là khả năng diễn giải của mô hình. Khi số lượng biến giải thích là quá nhiều, chúng ta thường muốn xác định một tập hợp nhỏ hơn những biến có tác động mạnh đáng kể nhất nhằm diễn giải mô hình một cách tốt nhất.Trong phần này, trước tiên chúng ta sẽ thảo luận một số cách tiếp cận để lựa chọn biến giải thích để đưa vào trong mô hình hồi quy tuyến tính bao gồm các phương pháp như best subset selection, forward stepwise selection, backward stepwise selection. Trong phần tiếp theo, chúng ta thảo luận về các phương pháp rút gọn (shrinkage) với mục tiêu kiểm soát phương sai của dự đoán.Nhìn chung, khi lựa chọn biến giải thích đưa vào mô hình, chúng ta chỉ giữ lại một tập hợp con của các biến và loại bỏ phần còn lại khỏi mô hình. Phương pháp bình phương nhỏ nhất được sử dụng để ước tính các hệ số tuyến tính. Các tiêu chí đánh giá mô hình sẽ được đưa ra nhằm sánh các tập hợp biến khác nhau.","code":""},{"path":"mô-hình-hồi-quy-tuyến-tính.html","id":"phương-pháp-lựa-chọn-tập-hợp-con-tốt-nhất","chapter":"Chương 8 Mô hình hồi quy tuyến tính","heading":"8.3.1 Phương pháp lựa chọn tập hợp con tốt nhất","text":"Đúng như tên gọi, lựa chọn biến bằng phương pháp lựa chọn tập hợp con tốt nhất có nghĩa là với mỗi giá trị của \\(k \\\\{0,1,2,... ,p\\}\\) người xây dựng mô hình cố gắng tìm tập hợp con có bao gồm đúng \\(k\\) biến giải thích sao cho tổng bình phương phần dư đạt giá trị nhỏ nhất. Điểm bất lợi của phương pháp này là khối lượng tính toán quá lớn bởi vì số lượng mô hình cần ước lượng là \\(2^p\\). Phương pháp tiếp cận của Furnival và Wilson (1974) giúp cho thuật toán này có thể thực hiện được với \\(p\\) lên đến 40. Tuy nhiên, thời gian tính toán chậm vẫn là điểm bất lợi nhất của phương pháp này.\nFigure 8.10: Phương pháp lựa chọn tập hợp con tốt nhất được thực hiện trên dữ liệu Boston. Đường ranh giới dưới là các mô hình có tổng bình phương phần dư nhỏ nhất\nHình 8.10 hiển thị tất cả các mô hình có tập hợp con \\(k\\) biến, \\(k = 0, 1, \\cdots, 13\\) cho ví dụ về giá nhà ở Boston. Đường ranh giới dưới biểu thị các mô hình đủ điều kiện để lựa chọn theo cách tiếp cận tập hợp con tốt nhất. Lưu ý rằng tập hợp con tốt nhất có kích thước bằng 2 không nhất thiết bao gồm biến nằm trong tập con tốt nhất có kích thước 1. Đường cong tập hợp con tốt nhất (ranh dưới màu đỏ) luôn giảm theo \\(k\\), đó không thể được sử dụng để chọn kích thước tập hợp con tối ưu. Câu hỏi đặt ra là làm thế nào để chọn \\(k\\) để cân bằng giữa sai lệch và phương sai của mô hình tuyến tính, đồng thời không quá tốn nhiều tài nguyên tính toán. Có một số tiêu chí mà chúng ta có thể cân nhắc; trong thực tế chúng tôi thường sử dụng xác thực chéo để ước tính sai số dự đoán và lựa chọn \\(k\\), hoặc khi khối lượng tính toán cho xác thực chéo quá lớn thì cũng có thể sử dụng tiêu chí AIC là một lựa chọn thay thế.","code":""},{"path":"mô-hình-hồi-quy-tuyến-tính.html","id":"forward--and-backward-stepwise-selection","chapter":"Chương 8 Mô hình hồi quy tuyến tính","heading":"8.3.2 Forward- and Backward-Stepwise Selection","text":"Thay vì tìm kiếm qua tất cả các tập hợp con, điều mà này trở nên không khả thi đối với \\(p\\) lớn, chúng ta có thể tìm kiếm tập hợp con một cách tuần tự theo từng bước, mà ở đó, kết quả của bước tiếp theo phụ thuộc vào bước trước đó.Forward-stepwise selection bắt đầu bằng cách lựa chọn mô hình có 1 biến tốt nhất. Sau đó, phương pháp này tìm kiểm mô hình có \\(k\\) biến bằng cách lấy \\((k-1)\\) biến đã được lựa chọn ở bước trước đó để thêm vào một biến. Như vậy, tại bước thứ \\(k\\), sẽ có \\((p-k+1)\\) mô hình tuyến tính cần được ước lượng. Số mô hình cần được ước lượng trong phương pháp forward-stepwise selection là\n\\[\\begin{align}\np + (p-1) + \\cdots + 1 = \\cfrac{p(p+1)}{2}\n\\end{align}\\]Forward-stepwise selection bắt đầu bằng cách lựa chọn mô hình có 1 biến tốt nhất. Sau đó, phương pháp này tìm kiểm mô hình có \\(k\\) biến bằng cách lấy \\((k-1)\\) biến đã được lựa chọn ở bước trước đó để thêm vào một biến. Như vậy, tại bước thứ \\(k\\), sẽ có \\((p-k+1)\\) mô hình tuyến tính cần được ước lượng. Số mô hình cần được ước lượng trong phương pháp forward-stepwise selection là\n\\[\\begin{align}\np + (p-1) + \\cdots + 1 = \\cfrac{p(p+1)}{2}\n\\end{align}\\]Bước thứ nhất của phương pháp Backward-stepwise selection bắt đầu bằng ước lượng mô hình có đầy đủ \\(p\\) biến. Sau đó, tại bước thứ \\(k\\), mô hình tìm cách loại bỏ đi 1 biến trong \\((p+2-k)\\) biến trong mô hình được ước lượng tại bước thứ \\((k-1)\\). Như vậy, tại bước thứ \\(k\\) với \\(k >1\\) sẽ có \\((p+2-k)\\) mô hình cần được ước lượng. Tổng số mô hình cần được ước lượng trong phương pháp là\n\\[\\begin{align}\n1 + p + (p-1) + \\cdots + 2 = \\cfrac{p(p+1)}{2}\n\\end{align}\\]Bước thứ nhất của phương pháp Backward-stepwise selection bắt đầu bằng ước lượng mô hình có đầy đủ \\(p\\) biến. Sau đó, tại bước thứ \\(k\\), mô hình tìm cách loại bỏ đi 1 biến trong \\((p+2-k)\\) biến trong mô hình được ước lượng tại bước thứ \\((k-1)\\). Như vậy, tại bước thứ \\(k\\) với \\(k >1\\) sẽ có \\((p+2-k)\\) mô hình cần được ước lượng. Tổng số mô hình cần được ước lượng trong phương pháp là\n\\[\\begin{align}\n1 + p + (p-1) + \\cdots + 2 = \\cfrac{p(p+1)}{2}\n\\end{align}\\]Hai phương pháp mô tả ở trên có bất lợi với phương pháp lựa chọn tập hợp con tốt nhất là không chắc chắn tìm thấy được mô hình có \\(k\\) biến tốt nhất kết quả của các bước phụ thuộc vào các bước trước đó. Tuy nhiên, lợi thế của các phương pháp này có thể kể đến làNguồn lực tính toán: với \\(p\\) lớn thì thời gian tính toán của hai phương pháp kể trên nhanh hơn nhiều với best subset selection. Các phương pháp có thể được áp dụng cả trong trường hợp \\(p \\geq N\\).Nguồn lực tính toán: với \\(p\\) lớn thì thời gian tính toán của hai phương pháp kể trên nhanh hơn nhiều với best subset selection. Các phương pháp có thể được áp dụng cả trong trường hợp \\(p \\geq N\\).Về mặt ý nghĩa thống kê: phương pháp lựa chọn tập hợp con tốt nhất tìm kiếm mô hình có \\(k\\) biến tốt nhất trong tất cả các lựa chọn có thể, đó kết quả thường thu được mô hình có phương sai cao hơn. Ngược lại, các phương pháp forward và backward stepwise selection chỉ tìm kiếm mô hình \\(k\\) biến trong một không gian nhỏ hơn (phụ thuộc vào các biến đã được lựa chọn trong các bước trước đó) nên thường có phương sai nhỏ hơn.Về mặt ý nghĩa thống kê: phương pháp lựa chọn tập hợp con tốt nhất tìm kiếm mô hình có \\(k\\) biến tốt nhất trong tất cả các lựa chọn có thể, đó kết quả thường thu được mô hình có phương sai cao hơn. Ngược lại, các phương pháp forward và backward stepwise selection chỉ tìm kiếm mô hình \\(k\\) biến trong một không gian nhỏ hơn (phụ thuộc vào các biến đã được lựa chọn trong các bước trước đó) nên thường có phương sai nhỏ hơn.Hình vẽ 8.11 mô tả các mô hình có \\(k\\) biến tốt nhất với \\(k = 0, 1, \\cdots, 13\\) sử dụng phương pháp forward stepwise selection. Trong trường hợp dữ liệu \\(Boston\\), ba phương pháp lựa chọn biến chúng tôi mô tả ở trên cho cùng một kết quả.\nFigure 8.11: Phương pháp forward stepwise selection được thực hiện trên dữ liệu Boston. Đường ranh giới dưới là các mô hình có sai số trung bình nhỏ nhất.\n","code":""},{"path":"mô-hình-hồi-quy-tuyến-tính.html","id":"hồi-quy-tuyến-tính-có-ràng-buộc-tham-số","chapter":"Chương 8 Mô hình hồi quy tuyến tính","heading":"8.4 Hồi quy tuyến tính có ràng buộc tham số","text":"Bằng cách tìm kiếm một tập hợp con các biến giải thích, các phương pháp lựa chọn biến trình bày ở phần trên có thể giúp bạn đọc tìm ra các mô hình có khả năng giải thích tốt và có khả năng có sai lệch thấp hơn với mô hình gồm đầy đủ tất cả các biến. Hạn chế của các mô hình này ở chỗ, mỗi biến giải thích chỉ có một trong hai khả năng là có xuất hiện hoặc không xuất hiện, nên các mô hình kết quả vẫn sẽ có phương sai lớn. Các phương pháp ràng buộc tham số được trình bày trong phần này là các phương pháp thường được sử dụng để cải thiện các mô hình có phương sai lớn với mục đích giảm phương sai của các mô hình và chấp nhận (đánh đổi) khả năng sai lệch có thể tăng.","code":""},{"path":"mô-hình-hồi-quy-tuyến-tính.html","id":"hồi-quy-ridge","chapter":"Chương 8 Mô hình hồi quy tuyến tính","heading":"8.4.1 Hồi quy ridge","text":"Hồi quy ridge hạn chế ảnh hưởng của biến giải thích lên biến mục tiêu bằng cách thêm vào một hàm “phạt” nếu giá trị tuyệt đối của hệ số tuyến tính tăng lên. Thay vì tìm các hệ số \\(\\hat{\\boldsymbol{\\beta}}\\) để tối thiểu hóa \\(RSS(\\beta)\\), hồi quy ridge tìm các hệ số \\(\\hat{\\boldsymbol{\\beta}}^{ridge}\\) để tối thiểu hóa tổng của \\(RSS(\\beta)\\) với một hàm phạt\n\\[\\begin{align}\n\\hat{\\boldsymbol{\\beta}}^{ridge} = \\underset{\\boldsymbol{\\beta}}{\\operatorname{argmin}} \\sum\\limits_{=1}^n \\left( y_i - \\beta_0 - \\sum\\limits_{j = 1}^p \\beta_j \\cdot x_{,j} \\right)^2 + \\lambda \\cdot \\sum\\limits_{j=1}^p \\beta_j^2\n\\tag{8.45}\n\\end{align}\\]Hồi quy ridge sử dụng hàm phạt là \\(PF(\\boldsymbol{\\beta}) = \\sum\\limits_{j=1}^p \\beta_j^2\\). Lưu ý rằng hệ số chặn \\(\\beta_0\\) không có trong hàm phạt bởi vì hệ số này không bị tác động bởi các biến giải thích. Hệ số \\(\\lambda \\geq 0\\) điều khiển mức độ ảnh hưởng của hàm phạt lên giá trị tối ưu. Khi \\(\\lambda\\) nhỏ thì ảnh hưởng của hàm phạt lên giá trị tối ưu không đáng kể và nếu \\(\\lambda\\) lớn thì hàm phạt sẽ chiếm ưu thế trong bài toán tối ưu và làm cho tất cả các hệ số \\(\\hat{\\boldsymbol{\\beta}}^{ridge}\\) gần tới 0.Bài toán tối ưu trong phương trình (8.45) có thể phát biểu dưới dạng bài toán có ràng buộc như sau:\n\\[\\begin{align}\n& \\hat{\\boldsymbol{\\beta}}^{ridge} = \\underset{\\boldsymbol{\\beta}}{\\operatorname{argmin}} \\sum\\limits_{=1}^n \\left( y_i - \\beta_0 - \\sum\\limits_{j = 1}^p \\beta_j \\cdot x_{,j} \\right)^2, \\\\\n& \\text{ với ràng buộc: }  \\sum\\limits_{j=1}^p \\beta_j^2 \\leq c\n\\tag{8.46}\n\\end{align}\\]Trong trường hợp mô hình tuyến tính có các biến độc lập có tương quan cao với nhau ước lượng của các hệ số tuyến tính có độ biến động lớn khiến mô hình tuyến tính kém hiệu quả. Bằng cách sử dụng ràng buộc trên cho tổng bình phương các hệ số, hồi quy ridge kiểm soát được vấn đề các biến giải thích tương quan cao. Ngoài ra, khi thực hiện hồi quy ridge các hệ số tuyến tính sẽ phụ thuộc vào việc có hay không thực hiện biến đổi tuyến tính các biến giải thích, đó trước khi thực hiện hồi quy người xây dựng mô hình thường chuẩn hóa các biến giải thích. Sau khi các biến giải thích được chuẩn hóa, ước lượng cho hệ số chặn là giá trị trung bình của biến mục tiêu, và phương pháp bình phương nhỏ nhất được thực hiện để ước lượng \\(\\beta_1, \\beta_2, \\cdots, \\beta_p\\).Tổng bình phương sai số dưới trong hồi quy ridge được viết như sau\n\\[\\begin{align}\nRSS(\\boldsymbol{\\beta}) = (\\textbf{y} - \\textbf{x} \\boldsymbol{\\beta})^T \\ (\\textbf{y} - \\textbf{x} \\boldsymbol{\\beta}) + \\lambda \\boldsymbol{\\beta}^T \\boldsymbol{\\beta})\n\\end{align}\\]Lưu ý rằng ma trận biến giải thích \\(\\textbf{x}\\) sau khi chuẩn hóa có kích thước \\(n \\times p\\). Tương tự như mô hình tuyến tính, hệ số hồi quy được ước lượng bằng phương pháp bình phương nhỏ nhất\n\\[\\begin{align}\n\\hat{\\boldsymbol{\\beta}}^{ridge} = (\\textbf{x}^T \\ \\textbf{x} + \\lambda I_{p} )^{-1} \\textbf{x}^T \\ \\textbf{y}\n\\end{align}\\]\nvới \\(I_p\\) là ma trận đơn vị có kích thước \\(p \\times p\\). Như vậy, tương tự như hồi quy tuyến tính thông thường, véc-tơ hệ số \\(\\hat{\\boldsymbol{\\beta}}^{ridge}\\) vẫn là tổ hợp tuyến tính của véc-tơ biến mục tiêu \\(\\textbf{y}\\). Sự khác nhau của các hệ số đến ở chỗ hồi quy ridge thêm vào đường chéo chính của ma trận \\(\\textbf{x}^T \\ \\textbf{x}\\) giá trị \\(\\lambda\\) trước khi lấy nghịch đảo.Trong trường hợp các biến giải thích đôi một độc lập, có thể chứng minh được rằng\n\\[\\begin{align}\n\\hat{\\boldsymbol{\\beta}} = \\cfrac{\\hat{\\boldsymbol{\\beta}}^{ridge}}{1 + \\lambda}\n\\end{align}\\]\ntrong đó \\(\\hat{\\boldsymbol{\\beta}}\\) là ước lượng của các hệ số tuyến tính khi không sử dụng hàm phạt. Khi \\(\\lambda\\) đủ lớn sẽ làm cho các giá trị hệ số tuyến tính giảm dần về 0. Điều đó có nghĩa là khi \\(\\lambda\\) càng lớn, bậc tự của mô hình càng nhỏ. Trong mô hình tuyến tính thông thường, bậc tự của mô hình có thể hiểu một cách đơn giản là số lượng tham số và bằng \\((p+1)\\). Với \\(\\lambda > 0\\), vẫn có \\((p+1)\\) hệ số tuyến tính trong hồi quy ridge được ước lượng, tuy nhiên các hệ số bị ràng buộc với nhau làm cho bậc tự của mô hình giảm. Điều này cũng đồng nghĩa với việc mô hình ít bị phụ thuộc vào dữ liệu hơn (phương sai giảm) nhưng đánh đổi lại là sai lệch sẽ tăng.Khái niệm bậc tự trong các mô hình có ràng buộc về tham số thường được gọi là bậc tự hiệu quả thay vì bậc tự thông thường. Bậc tự hiệu quả được định nghĩa bằng tổng độ nhạy (đạo hàm) của các giá trị dự báo \\(\\hat{y}_i = \\hat{f}(x_i)\\) theo các giá trị quan sát của biến mục tiêu\n\\[\\begin{align}\n\\textit{Bậc tự hiệu quả} = \\sum\\limits_{=1}^n \\cfrac{\\partial \\hat{y}_i }{\\partial y_i}\n\\tag{8.47}\n\\end{align}\\]Bậc tự hiệu quả lớn có nghĩa là giá trị dự đoán \\(\\hat{y}_i\\) sẽ thay đổi lớn khi các giá trị quan sát \\(y_i\\) thay đổi, ngược lại, bậc tự hiệu quả nhỏ có nghĩa là các giá trị dự đoán \\(\\hat{y}_i\\) sẽ thay đổi nhỏ khi các giá trị quan sát được \\(y_i\\) thay đổi. Tương tự như khái niệm bậc tự trong mô hình tuyến tính thông thường, mô hình có bậc tự hiệu quả lớn nghĩa là mô hình có phương sai lớn.Trong hồi quy tuyến tính thông thường hoặc hồi quy tuyến tính ridge, giá trị dự báo của mô hình có dạng\n\\[\\begin{align}\n\\hat{\\textbf{y}} = \\textbf{y}\n\\end{align}\\]\nvới \\(= \\textbf{x} (\\textbf{x}^T \\ \\textbf{x} )^{-1} \\textbf{x}^T\\) trong mô hình tuyến tính thông thường, và \\(= \\textbf{x} (\\textbf{x}^T \\ \\textbf{x} + \\lambda I_{p} )^{-1} \\textbf{x}^T\\) trong hồi quy tuyến tính ridge.Bậc tự hiệu quả của các mô hình trên chính là vết (trace) của ma trận \\(\\)\n\\[\\begin{align}\n\\sum\\limits_{=1}^n \\cfrac{\\partial \\hat{y}_i }{\\partial y_i} = Trace() = \\sum\\limits_{=1}^n A_{,}\n\\tag{8.48}\n\\end{align}\\]\ntrong đó \\(A_{,}\\) là các phần tử thứ \\(\\) nằm trên đường chéo chính của ma trận \\(\\). Trong ước lượng mô hình tuyến tính thông thường ma trận \\(\\textbf{x}^T \\ \\textbf{x}\\) có hạng bằng \\((p+1)\\), chúng ta đã có \\(trace() = (p+1)\\), nghĩa là bậc tự hiệu quả của mô hình tuyến tính thông thường bằng số tham số trong mô hình.Để trả lời câu hỏi bậc tự hiệu quả của hồi quy ridge phụ thuộc vào \\(\\lambda\\) như thế nào, chúng ta giả sử ma trận \\(\\textbf{x}^T \\ \\textbf{x}\\) có hạng bằng \\(p\\). Khi đó, ma trận \\(\\textbf{x}^T \\ \\textbf{x}\\) có thể được viêt dưới dạng sau\n\\[\\begin{align}\n\\textbf{x}^T \\ \\textbf{x} = U D U^{T}\n\\end{align}\\]\nvới \\(U\\) là ma trận các véc-tơ riêng chuẩn hóa của \\(\\textbf{x}^T \\ \\textbf{x}\\) và \\(D\\) là ma trận đường chéo có các phần tử nằm trên đường chéo chính là các giá trị riêng của ma trận \\(\\textbf{x}^T \\ \\textbf{x}\\).\n\\[\\begin{align}\nD = Diag(\\lambda_1, \\lambda_2, \\cdots, \\lambda_p)\n\\end{align}\\]\nvới \\(\\lambda_i\\) là các giá trị riêng của ma trận \\(\\textbf{x}^T \\ \\textbf{x}\\). Lưu ý rằng ma trận \\(U\\) là ma trận unitary. Hơn thế nữa, ma trận \\(\\textbf{x}^T \\ \\textbf{x}\\) xác định dương nên tất cả các giá trị riêng \\(\\lambda_i\\) đều là các số dương. Chúng ta có bậc tự hiệu quả của hồi quy ridge được xác định như sau:\n\\[\\begin{align}\nTrace\\left(\\textbf{x} (\\textbf{x}^T \\ \\textbf{x} + \\lambda \\textbf{}_p )^{-1}\\right) & = Trace(\\textbf{x}^T \\textbf{x} (\\textbf{x}^T \\ \\textbf{x} + \\lambda \\textbf{}_p )^{-1}) \\\\\n& = Trace\\left[ Diag \\left( \\cfrac{\\lambda_1}{\\lambda_1 + \\lambda}, \\cfrac{\\lambda_2}{\\lambda_2 + \\lambda}, \\cdots, \\cfrac{\\lambda_{p}}{\\lambda_{p} + \\lambda} \\right) \\right] \\\\\n& = \\sum\\limits_{= 1}^{p} \\cfrac{\\lambda_i}{\\lambda_i + \\lambda}\n\\tag{8.49}\n\\end{align}\\]Có thể thấy rằng bậc tự hiệu quả của hồi quy ridge là hàm số giảm theo \\(\\lambda\\). Trong mô hình tuyến tính với \\(p\\) biến giải thích và không có hệ số chặn, bậc tự là \\(p\\). Khi sử dụng ràng buộc trên các hệ số tuyến tính, kể cả khi các hệ số khác 0 một cách có ý nghĩa, các hệ số vẫn bị ràng buộc bởi \\(\\lambda\\). Bậc tự hiệu quả nhận giá trị bằng \\(p\\) khi \\(\\lambda = 0\\), tương đương với bài toán tối ưu không có ràng buộc, trong khi bậc tự hiệu quả sẽ xấp xỉ 0 nếu chúng ta chọn \\(\\lambda\\) đủ lớn.\nFigure 8.12: Hồi quy ridge tương đương với bài toán tìm RSS nhỏ nhất với ràng buộc các hệ số tuyến tính.\nHình 8.12 mô tả quá trình ước lượng tham số của hồi quy ridge trên một dữ liệu được mô phỏng. Dữ liệu có 500 quan sát và các hệ số \\(\\beta_1\\) và \\(\\beta_2\\) được lựa chọn để sinh dữ liệu là \\(\\beta_1 = 2\\) và \\(\\beta_2 = -1\\). Bạn đọc có thể thấy rằng lời giải của bài toán tối ưu không có ràng buộc là điểm xấp xỉ \\((2,-1)\\). Giá trị \\(RSS(\\boldsymbol{\\beta})\\) tương ứng với điểm tối ưu này là 80. Khi chúng ta thêm ràng buộc với hệ số \\(\\beta_1\\) và \\(\\beta_2\\) như phương trình (8.46) với \\(c = 1\\) thì miền giá trị của các hệ số phải nằm trong hình tròn có tâm tại (0,0) và bán kính bằng 1 giống như trong hình vẽ. Để thỏa mãn được ràng buộc này, chúng ta phải chấp nhận giá trị của \\(RSS(\\boldsymbol{\\beta})\\) tăng lên. Tại mỗi giá trị của \\(RSS(\\boldsymbol{\\beta})\\) lớn hơn giá trị tối thiểu là 80, tập hợp các điểm (\\(\\beta_1\\), \\(\\beta_2\\)) sao cho giá trị của \\(RSS\\) không thay đổi là một hình ellipse khai triển công thức của RSS sẽ thu được phương trình của một ellipse trên các biến (\\(\\beta_1\\), \\(\\beta_2\\)). Hình 8.12 mô tả ba ellipse đồng tâm, có độ lớn tăng dần tương ứng với giá trị của RSS là 100, 230 và 480. Tại giá trị 480, ellipse tiếp xúc với hình tròn mô tả miền ràng buộc tham số tại điểm chính là lời giải của bài toán tối ưu có ràng buộc hay chính là giá trị ước lượng tham số của hồi quy ridge. Bạn đọc có thể thấy rằng hồi quy ridge luôn luôn đẩy các hệ số tuyến tính về gần 0 hơn và làm cho mô hình ít bị phụ thuộc hơn vào các tham số tuyến tính.","code":""},{"path":"mô-hình-hồi-quy-tuyến-tính.html","id":"phương-pháp-lasso","chapter":"Chương 8 Mô hình hồi quy tuyến tính","heading":"8.4.2 Phương pháp Lasso","text":"Lasso cũng là một phương pháp để tạo ràng buộc cho các hệ số của mô hình hồi quy tuyến tính. Lasso sử dụng hàm phạt dưới dạng tổng giá trị tuyệt đối của các hệ số thay vì tổng bình phương các hệ số tuyến tính. Lasso có thể được phát biểu dưới dạng bài toán tối ưu với ràng buộc như sau\n\\[\\begin{align}\n& \\hat{\\boldsymbol{\\beta}}^{ridge} = \\underset{\\boldsymbol{\\beta}}{\\operatorname{argmin}} \\sum\\limits_{=1}^n \\left( y_i - \\beta_0 - \\sum\\limits_{j = 1}^p \\beta_j \\cdot x_{,j} \\right)^2, \\\\\n& \\text{ với ràng buộc: }  \\sum\\limits_{j=1}^p |\\beta_j| \\leq c\n\\tag{8.50}\n\\end{align}\\]Tương tự như hồi quy ridge, các biến độc lập cũng sẽ được chuẩn hóa để có giá trị trung bình bằng 0. Khi đó, ước lượng cho \\(\\beta_0\\) là giá trị trung bình của biến phụ thuộc. Ước lượng tham số cho Lasso là quá trình tìm hệ số tuyến tính \\(\\hat{\\boldsymbol{\\beta}}^{Lasso}\\) để tối thiểu hóa tổng bình phương sai số cộng thêm một hàm phạt\\[\\begin{align}\n\\hat{\\boldsymbol{\\beta}}^{Lasso} = \\underset{\\boldsymbol{\\beta}}{\\operatorname{argmin}} \\sum\\limits_{=1}^n \\left( y_i - \\beta_0 - \\sum\\limits_{j = 1}^p \\beta_j \\cdot x_{,j} \\right)^2 + \\lambda \\cdot \\sum\\limits_{j=1}^p |\\beta_j|\n\\tag{8.51}\n\\end{align}\\]Không giống như hồi quy ridge, sử dụng hàm phạt là tổng giá trị tuyệt đối của các hệ số sẽ dẫn đến bài toán tìm \\(\\hat{\\boldsymbol{\\beta}}^{Lasso}\\) không có lời giải chính xác. Các phương pháp giải số thường được áp dụng để ước lượng tham số. Khi hằng số \\(c\\) trong ràng buộc của bài toán tối ưu (8.50) xấp xỉ 0, các hệ số tuyến tính cũng sẽ xấp xỉ 0. Ngược lại khi \\(c\\) đủ lớn, lời giải của bài toán tối ưu sẽ là hệ số của mô hình tuyến tính thông thường.\nFigure 8.13: Hồi quy ridge tương đương với bài toán tìm RSS nhỏ nhất với ràng buộc các hệ số tuyến tính.\nHình 8.13 mô tả quá trình ước lượng tham số của hồi quy lasso trên một dữ liệu mô phỏng mà chúng tôi đã sử dụng trong mô tả hồi quy ridge. Các hệ số \\(\\beta_1\\) và \\(\\beta_2\\) được lựa chọn để sinh dữ liệu là \\(\\beta_1 = 2\\) và \\(\\beta_2 = -1\\) và lời giải của bài toán tối ưu không có ràng buộc là điểm xấp xỉ \\((2,-1)\\). Giá trị \\(RSS(\\boldsymbol{\\beta})\\) tương ứng với điểm tối ưu này là 80. Khi chúng ta thêm ràng buộc với hệ số \\(\\beta_1\\) và \\(\\beta_2\\) như phương trình (8.50) với \\(c = 1\\) thì miền giá trị của các hệ số phải nằm trong hình kim cương 4 cạnh được mô tả như trong hình vẽ. Cũng giống như trong hồi quy ridge, để thỏa mãn được ràng buộc chúng ta phải chấp nhận giá trị của \\(RSS(\\boldsymbol{\\beta})\\) tăng lên. Hình 8.12 mô tả ba ellipse đồng tâm, có độ lớn tăng dần tương ứng với giá trị của RSS là 100, 240 và 550. Tại giá trị 550, ellipse tiếp xúc với miền ràng buộc tham số tại một điểm và điểm đó chính là lời giải của phương pháp lasso. Tương tự như hồi quy ridge, phương pháp lasso luôn kéo các hệ số tuyến tính về gần 0 và làm cho mô hình ít bị phụ thuộc hơn vào các tham số tuyến tính. đặc điểm của miền ràng buộc, phương pháp lasso nhiều khi còn hiệu quả hơn hồi quy ridge trong ràng buộc tham số bởi vì điểm tiếp xúc của ellipse với hình kim cương sẽ luôn khiến cho một trong hai hệ số nhận giá trị gần 0 hơn.","code":""},{"path":"mô-hình-hồi-quy-tuyến-tính.html","id":"thực-hành-xây-dựng-mô-hình-tuyến-tính-trên-dữ-liệu-boston","chapter":"Chương 8 Mô hình hồi quy tuyến tính","heading":"8.5 Thực hành xây dựng mô hình tuyến tính trên dữ liệu Boston","text":"Chúng ta sẽ thực hành xây dựng mô hình tuyến tính trên dữ liệu là dữ liệu về giá nhà tại các vùng ngoại ô của thành phố Boston. Biến mục tiêu có tên là \\(medv\\) là giá trị trung vị của giá nhà tại mỗi vùng. Có 13 biến giải thích bao gồm 12 biến giải thích là các biến liên tục và một biến giải thích định tính duy nhất là chas nhận hai giá trị là 1 nếu vùng ngoại ô nằm trên đường bờ sông và nhận giá trị bằng 0 nếu vùng đó không nằm trên đường bờ sông. Dữ liệu có 506 quan sát.Về nguyên tắc, dữ liệu trước khi sử dụng để xây dựng mô hình cần được làm sạch, xử lý giá trị không quan sát được, loại bỏ các giá trị ngoại lai, loại bỏ các biến không cần thiết,… Tuy nhiên, để không lặp lại các kiến thức đã trình bày trong các chương trước, chúng tôi sẽ bỏ qua phần này và trực tiếp đi vào phần xây dựng mô hình.Một lưu ý khác là đa số các hàm số dùng để xây dựng mô hình trên dữ liệu đều đã được phát triển dưới dạng các hàm số có sẵn trên R. Người xây dựng mô hình chỉ cần gọi đúng tên hàm số, khai báo chính xác tham số, và đọc được kết quả trả ra mà không cần phải hiểu chính xác cách viết các hàm số đó như thế nào. Đây là ưu điểm lớn nhất đồng thời cũng là nhược điểm lớn nhất khi sử dụng R để xây dựng mô hình. Là ưu điểm vì bạn đọc chỉ cần một dòng lệnh là đã có thể xây dựng được mô hình phức tạp trên dữ liệu mà không cần hiểu một cách chính xác về mô hình đó. Là nhược điểm bởi vì khi người xây dựng mô hình không hiểu rõ về bản chất có thể dẫn tới sử dụng mô hình không đúng mục đích và dẫn đến các nhận định sai lầm. Để hạn chế được nhược điểm này, trong một số trường hợp, chúng tôi sẽ yêu cầu bạn đọc tự viết các câu lệnh tính toán tham số trước khi gọi các hàm có sẵn.","code":""},{"path":"mô-hình-hồi-quy-tuyến-tính.html","id":"xây-dựng-mô-hình-hồi-quy-đa-biến","chapter":"Chương 8 Mô hình hồi quy tuyến tính","heading":"8.5.1 Xây dựng mô hình hồi quy đa biến","text":"Trước hết chúng ta sẽ xây dựng mô hình tuyến tính đơn biến mà giá nhà phụ thuộc vào một biến có tên là lstat. Các hệ số tuyến tính trong mô hình hồi quy đơn \\(Y \\sim \\beta_0 + \\beta_1 \\cdot X\\) được ước lượng bằng phương pháp bình phương nhỏ nhất được tính toán như sau:\n\\[\\begin{align}\n\\hat{\\beta}_1 = \\cfrac{cov(X,Y)}{var(X)} \\ \\ \\ ; \\ \\ \\ \\hat{\\beta}_0 = \\bar{Y} - \\hat{\\beta}_1 \\cdot \\bar{X}\n\\end{align}\\]Như vậy, chúng ta có mô hình tuyến tính đơn biến mà medv phụ thuộc vào lstat\n\\[\\begin{align}\n\\hat{medv} = 34.5538409 - 0.9500494 \\times lstat\n\\end{align}\\]Kết quả mô hình cho thấy khi lstat tăng thêm 1 (%) thì giá nhà trung bình sẽ giảm đi khoảng 0.950 (nghìn USD). Sai số (phần dư) của mô hình là hiệu số giữa giá trị của biến medv được ước lượng từ mô hình và giá trị của medv trong dữ liệu. Chúng ta có đồ thị phần dư như sauNhìn vào đồ thị phần dư, bạn đọc có thể dễ dàng nhận thấy rằng giả thiết phân phối chuẩn của phần dư và giả thiết các phần dư không tương quan với nhau là không đúng. Điều này có thể cải thiện bằng cách thêm các biến giải thích khác vào mô hình. Bỏ qua vấn đề này, chúng ta ước lượng được phương sai (RSE) của phần dư như sau\n\\[\\begin{align}\n\\hat{\\sigma} = RSE = \\sqrt{\\cfrac{RSS}{n-2}}\n\\end{align}\\]\ntrong đó RSS là tổng sai số bình phương và \\(n\\) là số quan sátHệ số \\(R^2\\) của mô hình được tính toán như sauPhương sai của các hệ số \\(\\beta_0\\), \\(\\beta_1\\), các giá trị của phân phối \\(student\\), và p-value được tính toán như sauCó thể thấy rằng các giá trị p-value đều nhỏ, cho thấy các hệ số tuyến tính đều khác không một cách có ý nghĩa.Tất cả các tính toán ở trên đều có thể được thực hiện thông qua hàm có sẵn là hàm lm(). Cách sử dụng hàm lm() xây dựng mô hình tuyến tính mà biến medv phụ thuộc vào biến lstat như sauBạn đọc có thể kiểm tra các tính toán ở trên với kết quả ước lượng từ hàm lm() là hoàn toàn tương tự nhau. Đối tượng lm1 là một list có 12 phần tử trong đó có các phần tử như coefficient chứa giá trị các hệ số ước lượng hay residuals là véc-tơ phần dư.Mô hình tuyến tính đa biến cũng có thể được ước lượng bằng hàm lm() giống như mô hình đơn biến. Chúng ta xây dựng mô hình tuyến tính đa biến mà trong đó biến medv phụ thuộc vào tất cả các biến còn lại như sauCó thể nhận thấy rằng hầu hết các biến trong mô hình đều có ý nghĩa ngoại trừ hai biến là indus và age.","code":"\ndat <- Boston\ny <- Boston$medv\nx <- Boston$lstat\nbeta1 <- cov(x,y)/var(x); beta0 <- mean(y) - beta1 * mean(x)\nprint(c(beta0,beta1))## [1] 34.5538409 -0.9500494\nphandu <- (y -  beta0 - beta1 * x)\ndata.frame(x = 1:length(phandu), phandu = phandu)%>%\n  ggplot(aes(x,phandu))+\n  geom_line(color = \"blue\",alpha = 0.3)+\n  geom_hline(yintercept = 0, col = \"orange\")+\n  theme_minimal()+\n  xlab(\"Số quan sát\")+ ylab(\"Phần dư\")\nRSS <- sum(phandu^2)\nRSE <- sqrt(RSS/(nrow(Boston)-2))\nprint(paste(\"RSE =\", RSE))## [1] \"RSE = 6.21576040539807\"\nTSS <- sum((y - mean(y))^2)\nR_squared <- 1 - RSS/TSS\nprint(paste(\"Hệ số R-squared: \", R_squared))## [1] \"Hệ số R-squared:  0.54414629758648\"\nn<-nrow(Boston)\nSE_beta0 <- RSE * sqrt(1/n+ mean(x)^2/sum(((x-mean(x))^2)))\nSE_beta1 <- RSE * sqrt(1/sum(((x-mean(x))^2)))\nt_beta0 <- beta0/SE_beta0\nt_beta1 <- beta1/SE_beta1\np_value_beta0 <- 2 * (1-pt(abs(t_beta0), df = n-2))\np_value_beta1 <- 2 * (1-pt(abs(t_beta1), df = n-2))\nprint(paste(\"Các giá trị phân phối student: \", t_beta0, \" - \", t_beta1))## [1] \"Các giá trị phân phối student:  61.4151455186417  -  -24.5278998511877\"\nprint(paste(\"Các giá trị p-value: \", p_value_beta0, \" - \",p_value_beta1))## [1] \"Các giá trị p-value:  0  -  0\"\nlm1<-lm(medv~lstat, data = Boston)\nsummary(lm1)## \n## Call:\n## lm(formula = medv ~ lstat, data = Boston)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -15.168  -3.990  -1.318   2.034  24.500 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(>|t|)    \n## (Intercept) 34.55384    0.56263   61.41   <2e-16 ***\n## lstat       -0.95005    0.03873  -24.53   <2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 6.216 on 504 degrees of freedom\n## Multiple R-squared:  0.5441, Adjusted R-squared:  0.5432 \n## F-statistic: 601.6 on 1 and 504 DF,  p-value: < 2.2e-16\nBoston$chas<-as.factor(Boston$chas)\nlm.all<-lm(medv~., data = Boston)\nsummary(lm.all)## \n## Call:\n## lm(formula = medv ~ ., data = Boston)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -15.595  -2.730  -0.518   1.777  26.199 \n## \n## Coefficients:\n##               Estimate Std. Error t value Pr(>|t|)    \n## (Intercept)  3.646e+01  5.103e+00   7.144 3.28e-12 ***\n## crim        -1.080e-01  3.286e-02  -3.287 0.001087 ** \n## zn           4.642e-02  1.373e-02   3.382 0.000778 ***\n## indus        2.056e-02  6.150e-02   0.334 0.738288    \n## chas1        2.687e+00  8.616e-01   3.118 0.001925 ** \n## nox         -1.777e+01  3.820e+00  -4.651 4.25e-06 ***\n## rm           3.810e+00  4.179e-01   9.116  < 2e-16 ***\n## age          6.922e-04  1.321e-02   0.052 0.958229    \n## dis         -1.476e+00  1.995e-01  -7.398 6.01e-13 ***\n## rad          3.060e-01  6.635e-02   4.613 5.07e-06 ***\n## tax         -1.233e-02  3.760e-03  -3.280 0.001112 ** \n## ptratio     -9.527e-01  1.308e-01  -7.283 1.31e-12 ***\n## black        9.312e-03  2.686e-03   3.467 0.000573 ***\n## lstat       -5.248e-01  5.072e-02 -10.347  < 2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 4.745 on 492 degrees of freedom\n## Multiple R-squared:  0.7406, Adjusted R-squared:  0.7338 \n## F-statistic: 108.1 on 13 and 492 DF,  p-value: < 2.2e-16"},{"path":"mô-hình-hồi-quy-tuyến-tính.html","id":"lựa-chọn-biến-trong-mô-hình-hồi-quy-tuyến-tính","chapter":"Chương 8 Mô hình hồi quy tuyến tính","heading":"8.5.2 Lựa chọn biến trong mô hình hồi quy tuyến tính","text":"Chúng ta sẽ sử dụng sai số xác thực chéo làm tiêu chí để lựa chọn biến trong mô hình hồi quy tuyến tính. Hàm số dùng để tạo chỉ số trong xác thực chéo là hàm createFolds() của thư viện \\(\\textbf{caret}\\). Dữ liệu sẽ được chia ngẫu nhiên thành \\(k = 5\\) phần theo biến mục tiêu để đảm bảo phân phối của biến mục tiêu trong từng tập xác thực tương tự nhau.Để tính sai số xác thực chéo, với mỗi \\(\\\\{1, 2, 3, 4, 5\\}\\) chúng ta xây dựng mô hình trên dữ liệu Boston loại trừ đi các quan sát thuộc tập xác thực \\(\\). Sau đó tính toán sai số của mô hình trên tập xác thực thứ \\(\\). Sai số trong bài toán hồi quy được tính bằng RMSE. Sai số xác thực chéo là giá trị trung bình của sai số trên các tập xác thực \\(\\), \\(1 \\leq \\leq 5\\). Ví dụ, chúng ta xác định sai số của mô hình đơn biến gồm một biến giải thích lstat và mô hình tuyến tính đa biến bao gồm tất cả các biến như sau:Dữ liệu Boston bao gồm 13 biến độc lập đó thực hiện lựa chọn biến bằng phương pháp best subset selection là có thể thực hiện được. Nguyên tắc lựa chọn mô hình là dựa trên sai số xác thực chéo, mô hình có sai số xác thực chéo nhỏ nhất sẽ được lựa chọn. Có \\((2^{13} - 1)\\) mô hình cần được xây dựng và tính toán.Như vậy mô hình tuyến tính có sai số xác thực chéo nhỏ nhất là mô hình có 11 biến liệt kê như trên. Lưu ý rằng, dữ liệu có kích thước không lớn nên kết quả của mô hình sẽ phụ thuộc vào việc chia dữ liệu thành các tập xác thực.Lựa chọn biến cho mô hình tuyến tính bằng phương pháp forward stepwise selection được thực hiện như sauBạn đọc có thể dễ dàng nhận thấy rằng thời gian để thực hiện lựa chọn biến bằng phương pháp forward stepwise selection là nhanh hơn rất nhiều với phương pháp lựa chọn tập hợp con tốt nhất.Tương tự, chúng ta có thể thực hiện lựa chọn biến bằng phương pháp backward stepwise selection được thực hiện như sau","code":"\n# Chia dữ liệu ngẫu nhiên thành 5 phần theo biến medv\nset.seed(10)\nfold_number = 5\nindex <- createFolds(Boston$medv, k = fold_number)\n# Sai số giữa hai véc-tơ số tính bằng RMSE\nRMSE <- function(y,y.hat) sqrt(mean((y - y.hat)^2))\n\n# Hàm số tính sai số xác thực chéo hồi quy tuyến tính\ncv.lm<-function(seed = 10, fold_number = 5, dat, target){\n  # seed: khởi tạo ngẫu nhiên\n  # fold_number: số lượng folds\n  # dat: dữ liệu xây dựng mô hình (X)\n  # target: biến mục tiêu (Y)\n  set.seed(seed)\n  n <- nrow(dat) ; p <- ncol(dat)\n  \n  # Tạo dữ liệu xác thực chéo\n  y <- target\n  x <- dat\n  index <- createFolds(y, k = fold_number)\n  \n  # Véc-tơ sai số xác thực chéo\n  error_reg <- rep(0, fold_number) \n  \n  for (i in 1:fold_number){\n    test.index <- index[[i]] # chỉ số tập xác thực\n    test.index <- (1:n) %in% test.index\n    \n    # Dữ liệu huấn luyện\n    x.train <- x %>% filter(!test.index)\n    y.train <- y[!test.index]\n    \n    # Dữ liệu xác thực\n    x.test <- x %>% filter(test.index)\n    y.test <- y[test.index]\n    \n    # Mô hình tuyến tính\n    lm.model <- lm(y.train~., data = x.train) # ước lượng\n    lm.pred <- predict(lm.model, x.test) # dự đoán\n    error_reg[i] <- RMSE(y.test,lm.pred)                \n  }\n  return(mean(error_reg))\n}\n\n# Sai số xác thực chéo hồi quy đơn\nmydat <- dplyr::select(Boston, lstat)\ny <- Boston$medv\ncv.lm(seed = 10, fold_number = 5, dat = mydat, target = y)## [1] 6.200729\n# Sai số xác thực chéo hồi quy bội\nmydat <- dplyr::select(Boston, -medv) # X\ncv.lm(seed = 10, fold_number = 5, dat = mydat, target = y)## [1] 4.857492\np <- ncol(Boston)-1\nnumber_model <- 2^p - 1\nvar_name <- select(Boston, - medv) %>% names() \ncv.error <- rep(0, number_model)\n\nfor (i in 1:1){\n#for (i in 1:number_model){\n  # Véc-tơ chứa tên các biến được lựa chọn\n  selected_variable <- var_name[as.logical(intToBits(i))[1:p]]\n  y <- Boston$medv\n  mydat <- select(Boston, selected_variable)\n  cv.error[i] <- cv.lm(seed = 10, fold_number = 5, dat = mydat, target = y)\n}\n\n# Mô hình có sai số xác thực chéo nhỏ nhất\nbest_model <- which.min(cv.error)\n# Danh sách các biến trong mô hình\nvar_name[as.logical(intToBits(best_model))[1:p]]## [1] \"zn\"\n# Thứ tự chạy mô hình từ ít đến nhiều biến\nM <- matrix(FALSE,number_model,p)\nfor (i in 1:number_model){ \n  M[i,] <- as.logical(intToBits(i))[1:p]\n}\nM <- M[order(apply(M,1,sum)),]\n\n# Kết quả của quá trình forward stepwise được lưu\nfws_result <- data.frame(Model_number = rep(0, p*(p-1)/2), \n                        Number_variable = rep(0, p*(p-1)/2), \n                        CV_error = rep(0, p*(p-1)/2))\ncurrent_best_model <- c()\nj <- 0\n# Lựa chọn mô hình\nfor (i in 1:number_model){\n  number_selected_variable <- sum(M[i,])\n  current_select <- var_name[M[i,]]\n  \n  if ( (i == 1) | all(current_best_model %in% current_select) ){\n    j <- j+1\n    mydat <- select(Boston, current_select)\n    cv.error <- cv.lm(seed = 10, fold_number = 5, dat = mydat, target = y)\n    fws_result[j,] <- c(i, number_selected_variable, cv.error)\n  }  \n  \n  # Lưu lại mô hình có các biến tốt nhất\n  if (i < number_model){\n    if (sum(M[i+1,])-sum(M[i,]) ==  1){\n      dat <- filter(fws_result, Number_variable == number_selected_variable)\n      ind_best_model <- dat$Model_number[which.min(dat$CV_error)]\n      current_best_model <- var_name[M[ind_best_model,]]\n    }\n  } \n}\n# Thứ tự chạy mô hình từ ít đến nhiều biến\nM <- matrix(FALSE,number_model,p)\nfor (i in 1:number_model){ \n  M[i,] <- as.logical(intToBits(i))[1:p]\n}\nM <- M[order(apply(M,1,sum),decreasing = TRUE),]\n\n# Kết quả của quá trình forward stepwise được lưu\nbws_result <- data.frame(Model_number = rep(0, p*(p-1)/2), \n                         Number_variable = rep(0, p*(p-1)/2), \n                         CV_error = rep(0, p*(p-1)/2))\ncurrent_best_model <- var_name\nj <- 0\n# Lựa chọn mô hình\nfor (i in 1:number_model){\n  number_selected_variable <- sum(M[i,])\n  current_select <- var_name[M[i,]]\n  \n  if ( (i == p) | all(current_select %in% current_best_model) ){\n    j <- j+1\n    mydat <- select(Boston, current_select)\n    cv.error <- cv.lm(seed = 10, fold_number = 5, dat = mydat, target = y)\n    bws_result[j,] <- c(i, number_selected_variable, cv.error)\n  }  \n  \n  # Lưu lại mô hình có các biến tốt nhất\n  if (number_selected_variable > 1){\n    if (sum(M[i,])-sum(M[i+1,]) ==  1){\n      dat <- filter(bws_result, Number_variable == number_selected_variable)\n      ind_best_model <- dat$Model_number[which.min(dat$CV_error)]\n      current_best_model <- var_name[M[ind_best_model,]]\n    }\n  } \n}"},{"path":"mô-hình-hồi-quy-tuyến-tính.html","id":"hồi-quy-ridge-và-lasso","chapter":"Chương 8 Mô hình hồi quy tuyến tính","heading":"8.5.3 Hồi quy ridge và lasso","text":"Hồi quy ridge và lasso được thực hiện bằng hàm số glmnet() của thư viện cùng tên . Tham số \\(\\lambda\\) của các phương pháp này được tìm kiếm bằng phương pháp xác thực chéo. Hàm số thực hiện tính toán sai số xác thực chéo của hồi quy ridge và lasso là hàm cv.glmnet().Sai số xác thực chéo tính bằng MSE được lưu trong véc-tơ con có tên là $cvm. Chúng ta có sai số xác thực chéo cho mỗi giá trị của \\(\\lambda\\) của hồi quy ridge và lasso như hình (??)\nFigure 8.14: Sai số xác thực chéo tương ứng với mỗi giá trị của siêu tham số lambda trong hồi quy ridge và lasso.\nCó thể thấy rằng trong hồi quy ridge và lasso, tham số \\(\\lambda\\) cho sai số xác thực chéo nhỏ nhất lần lượt là 0.14 và 0.027. Sai số xác thực chéo trong hồi quy ridge và lasso không tốt hơn với mô hình hồi quy tuyến tính bội.các phương pháp ràng buộc tham số sẽ tránh được hiện tượng khớp dữ liệu quá mức, chúng ta có thể tạo thêm các biến giải thích cho biến giá nhà bằng cách nhân chéo các biến giải thích hiện có. Tổng số biến mới được tạo thành là \\(p\\times(p+1)/2\\).Chúng ta có sai số xác thực chéo cho mỗi giá trị của \\(\\lambda\\) của hồi quy ridge và lasso với dữ liệu sau khi biến đổi như hình (??)\nFigure 8.15: Sai số xác thực chéo tương ứng với mỗi giá trị của siêu tham số lambda trong hồi quy ridge và lasso.\nCó thể thấy rằng việc áp dụng hồi quy ridge và lasso trên dữ liệu có số lượng lớn biến giải thích mới được tạo thành bằng cách nhân chéo các biến hiện có không chỉ tránh được hiện tương mô hình bị khớp quá mức mà còn cải thiện khả năng dự đoán của mô hình. Sai số xác thực chéo đã giảm đáng kể với mô hình chỉ sử dụng các biến ban đầu.","code":"\nlibrary(glmnet)\n\n# Chuẩn hóa dữ liệu về dạng số\ny <- Boston$medv\nx <- model.matrix(medv~.,Boston)\n\n# Các giá trị lambda\nr.lambda<-seq(0,0.3,length=100)\nl.lambda<-seq(0,0.1,length=100)\nset.seed(50)\n\n# Xác thực chéo với ridge (tham số alpha = 0)\ncv.ridge <- cv.glmnet(x, y, alpha = 0, \n                  nfolds = 5, lambda=r.lambda)\n\n\n# Xác thực chéo với lasso (tham số alpha = 1)\ncv.lasso <- cv.glmnet(x, y, alpha = 1, \n                  nfolds = 5, lambda=l.lambda)\ndat <- select(Boston, -medv)\np <- ncol(dat)\n\ndat <- sapply(dat,  \n             function(x){\n               x <- as.numeric(x)\n               return ((x - mean(x))/sd(x))\n             })\n\ndat<-as.data.frame(dat)\n\nfor (i in 1:p){\n  for (j in i:p){\n    dat <- mutate(dat, newcol = dat[,i]*dat[,j])\n    names(dat)[length(dat)] <- paste0(\"X\",i,\"_\",j)\n  }\n}\ndat<-mutate(dat, \"medv\" = Boston$medv)\n# Chuẩn hóa dữ liệu về dạng số\ny <- dat$medv\nx <- model.matrix(medv~.,dat)\n\n# Các giá trị lambda\nr.lambda<-seq(0,0.5,length=100)\nl.lambda<-seq(0,0.2,length=100)\nset.seed(50)\n\n# Xác thực chéo với ridge (tham số alpha = 0)\ncv.ridge <- cv.glmnet(x, y, alpha = 0, \n                  nfolds = 5, lambda=r.lambda)\n\n\n# Xác thực chéo với lasso (tham số alpha = 1)\ncv.lasso <- cv.glmnet(x, y, alpha = 1, \n                  nfolds = 5, lambda=l.lambda)"},{"path":"mô-hình-hồi-quy-tuyến-tính.html","id":"bài-tập-4","chapter":"Chương 8 Mô hình hồi quy tuyến tính","heading":"8.6 Bài tập","text":"","code":""},{"path":"mô-hình-hồi-quy-tuyến-tính.html","id":"bài-tập-lý-thuyết","chapter":"Chương 8 Mô hình hồi quy tuyến tính","heading":"8.6.1 Bài tập lý thuyết","text":"","code":""},{"path":"mô-hình-hồi-quy-tuyến-tính.html","id":"bài-tập-thực-hành","chapter":"Chương 8 Mô hình hồi quy tuyến tính","heading":"8.6.2 Bài tập thực hành","text":"","code":"## \n## Attaching package: 'dplyr'## The following objects are masked from 'package:stats':\n## \n##     filter, lag## The following objects are masked from 'package:base':\n## \n##     intersect, setdiff, setequal, union## \n## Attaching package: 'kableExtra'## The following object is masked from 'package:dplyr':\n## \n##     group_rows## \n## Attaching package: 'gridExtra'## The following object is masked from 'package:dplyr':\n## \n##     combine## \n## Attaching package: 'pryr'## The following object is masked from 'package:dplyr':\n## \n##     where## Classes and Methods for R developed in the\n## Political Science Computational Laboratory\n## Department of Political Science\n## Stanford University\n## Simon Jackman\n## hurdle and zeroinfl functions by Achim Zeileis"},{"path":"mô-hình-tuyến-tính-tổng-quát..html","id":"mô-hình-tuyến-tính-tổng-quát.","chapter":"Chương 9 Mô hình tuyến tính tổng quát.","heading":"Chương 9 Mô hình tuyến tính tổng quát.","text":"Mô hình tuyến tính tổng quát, Generalized Linear Model hay viết tắt là GLM, được sử dụng rộng rãi trong các doanh nghiệp, các cơ quan tổ chức hoạt động trong lĩnh vực tài chính, ngân hàng, và bảo hiểm. Các chuyên gia quản trị rủi ro trong các ngân hàng sử dụng GLM để chấm điểm tín dụng khách hàng và quyết định phê duyệt tín dụng. Các chuyên gia tính toán thường xuyên sử dụng mô hình GLM để xác định phí thuần của các sản phẩm bảo hiểm, để xác định dự phòng nghiệp vụ, hoặc để phân loại rủi ro mà công ty phải đối mặt. Khái niệm GLM lần đầu tiên được sử dụng trong nghiên cứu của Nelder và Wedderburn (1972) và từ đó đến nay đã có nhiều sách tham khảo tin cậy cho mô hình này như Alan Agresti (2015) hay Annette J. Dobson Adrian G. Barnett (2018). Đa số các tài liệu tham khảo giới thiệu GLM dưới góc độ toán học và mang nhiều tính lý thuyết. Chương sách này sẽ cố gắng giải thích và tiếp cận GLM từ một góc nhìn mang tính thực hành nhiều hơn. Chúng tôi sẽ không quá đi sâu vào các khía cạnh như giả thiết hay phương pháp ước lượng của mô hình GLM, mà sẽ tập trung vào hướng dẫn bạn đọc ứng dụng GLM trên nhiều kiểu dữ liệu nhất có thể.Mô hình tuyến tính tổng quát được phát biểu dưới dạng công thức như sau:\n\\[\\begin{align}\n& Y \\sim  \\mathcal{F}_{\\boldsymbol{\\theta}} \\\\\n& \\mathbb{E}(Y|\\textbf{X} = \\textbf{x}_i) = \\mu_i \\\\\n& g(\\mu_i) = \\beta_0 + \\beta_1 \\cdot x_{,1} + \\beta_2 \\cdot x_{,2} + \\cdots + \\beta_p \\cdot x_{,p} \\\\\n& \\mu_i = g^{-1}\\left(\\beta_0 + \\beta_1 \\cdot x_{,1} + \\beta_2 \\cdot x_{,2} + \\cdots + \\beta_p \\cdot x_{,p} \\right)\n\\tag{9.1}\n\\end{align}\\]\nvới \\(Y\\) là biến mục tiêu, \\(\\mathcal{F}_{\\boldsymbol{\\theta}}\\) là một phân phối xác suất có tham số là một véc-tơ \\(\\boldsymbol{\\theta}\\). Giá trị trung bình của biến mục tiêu \\(Y\\) phụ thuộc vào giá trị của (các) biến độc lập như sau: với điều kiện véc-tơ biến độc lập \\(\\textbf{X} = (X_1, X_2, \\cdots, X_p)\\) nhận giá trị \\(\\textbf{x_i} = (x_{,1}, x_{,2}, \\cdots, x_{,p})\\), giá trị trung bình của biến phụ thuộc với điều kiện \\(\\textbf{X} = x_i\\), ký hiệu \\(Y|\\textbf{X} = x_i\\) hoặc ngắn gọn hơn là \\(Y_i\\), được xác định bằng giá trị một hàm số ngược của một hàm số thực \\(g\\), ký hiệu là hàm \\(g^{-1}\\), tính tại một tổ hợp tuyến tính của các giá trị \\((x_{,1}, x_{,2}, \\cdots, x_{,p})\\).Thay vì cố gắng hiểu các khái niệm toán học ở trên, chúng ta hãy thử áp dụng mô hình kể trên trong một trường hợp cụ thể. Chúng ta sẽ xây dựng một mô hình tuyến tính tổng quát mà trong đó biến mục tiêu \\(Y\\) cho biết người mua bảo hiểm xe ô tô có hay không lựa chọn đầy đủ các quyền lợi bảo hiểm khi ký hợp đồng mua bảo hiểm trách nhiệm dân sự. Dữ liệu được sử dụng có tên là \\(MotoInsurance.csv\\). Dữ liệu có 6 biến độc lập là:Độ tuổi của người lái xe, biến \\(age\\), nhận giá trị là các số nguyên dương từ 15 đến 92 tuổi.\nĐộ tuổi của người lái xe, biến \\(age\\), nhận giá trị là các số nguyên dương từ 15 đến 92 tuổi.Kinh nghiệm lái xe, biến \\(seniority\\), cho biết số năm kinh nghiệm lái xe, giá trị là các số nguyên từ 2 đến 40 năm.\nKinh nghiệm lái xe, biến \\(seniority\\), cho biết số năm kinh nghiệm lái xe, giá trị là các số nguyên từ 2 đến 40 năm.Giới tính của người lái xe, biến \\(sex\\), nhận giá trị “M” nếu người lái xe là nam giới và “F” nếu người lái xe là nữ giới.\nGiới tính của người lái xe, biến \\(sex\\), nhận giá trị “M” nếu người lái xe là nam giới và “F” nếu người lái xe là nữ giới.Nơi xe được đăng ký, biến \\(urban\\), nhận giá trị là 1 nếu xe được đăng ký tại khu vực thành phố và nhận giá trị 0 trong các trường hợp còn lại.\nNơi xe được đăng ký, biến \\(urban\\), nhận giá trị là 1 nếu xe được đăng ký tại khu vực thành phố và nhận giá trị 0 trong các trường hợp còn lại.Loại hình đăng ký xe, biến \\(private\\), nhận giá trị là 1 nếu xe mua bảo hiểm là xe đăng ký theo cá nhân và nhận giá trị 0 trong các trường hợp còn lại.\nLoại hình đăng ký xe, biến \\(private\\), nhận giá trị là 1 nếu xe mua bảo hiểm là xe đăng ký theo cá nhân và nhận giá trị 0 trong các trường hợp còn lại.Tình trạng hôn nhân của người lái xe, biến \\(marital\\), nhận giá trị “C” nếu đã kết hôn, “S” tương ứng với chưa kết hôn, và “O” tương ứng với đã ly dị.\nTình trạng hôn nhân của người lái xe, biến \\(marital\\), nhận giá trị “C” nếu đã kết hôn, “S” tương ứng với chưa kết hôn, và “O” tương ứng với đã ly dị.Biến mục tiêu hay biến phụ thuộc là biến \\(Y\\) nhận một trong hai giá trị, “Yes” nếu người mua bảo hiểm trách nhiệm dân sự đồng ý lựa thêm quyền lợi bảo hiểm bổ sung và “” nếu người mua bảo hiểm trách nhiệm dân sự không lựa chọn mua quyền lợi bổ sung. Để mô hình ở dạng đơn giản nhất có thể, chúng tôi lựa chọn hai biến độc lập để xây dựng mô hình là biến \\(age\\) và biến \\(sex\\). Chúng ta sẽ sử dụng biến \\(age\\) như một biến kiểu số, trong khi biến \\(sex\\) là một biến kiểu phân loại/rời rạc nhận một trong hai giá trị là “M” tương ứng với nam giới và “F” tương ứng với nữ giới.Đoạn lệnh R dưới đây được sử dụng để lấy dữ liệu và phân tích nhanh ảnh hưởng của các biến \\(age\\) và \\(sex\\) lên quyết định mua bảo hiểm bổ sung của người sở hữu xe ô tô.Bạn đọc có thể thấy rằng: đồ thị bên trái cho thấy những người trẻ tuổi hơn có xu hướng đồng ý mua bảo hiểm bổ sung hơn những người nhiều tuổi; đồ thị bên phải cho thấy nữ giới có xu hướng mua bảo hiểm bổ sung cao hơn với nam giới.Chúng ta sẽ xây dựng một mô hình tuyến tính tổng quát để xác nhận lại các phân tích ở trên và lượng hóa được ảnh hưởng của các biến \\(age\\) và \\(sex\\) lên quyết định mua bảo hiểm bổ sung. Hàm số dùng để xây dựng và ước lượng mô hình tuyến tính tổng quát trong R là hàm glm() của thư viện \\(stat\\):Chúng ta có thể thấy rằng các hệ số của biến \\(age\\) và \\(sex\\) đều có ý nghĩa thống kê khi các giá trị \\(p-value\\) đều rất nhỏ. Đúng như chúng nhận định từ phần phân tích khai phá, hệ số tuyến tính của biến \\(age\\) là số âm, bằng -0.016, cho biết người trẻ tuổi hơn có khả năng đồng ý mua bảo hiểm bổ sung cao hơn. Hệ số ứng với biến giới tính nam là số âm, bằng -0.447, điều này cho biết khả năng nam giới đồng ý mua bảo hiểm bổ sung là thấp hơn với nữ giới.Chúng ta có thể viết các thành phần của mô hình tuyến tính tổng quát đã xây dựng ở trên như sau\\[\\begin{align}\n& Y \\sim  \\mathcal{B}(\\rho) \\\\\n& \\mathbb{E}(Y_i) = \\mathbb{E}\\left(Y|(age_i, sex_i)\\right) = \\rho_i \\\\\n& \\Phi^{-1}\\left(\\rho_i\\right) = 0.678 - 0.016 \\times age_i - 0.447 \\times sex_i \\\\\n& \\rho_i = \\Phi\\left(0.678 - 0.016 \\times age_i - 0.447 \\times sex_i\\right)\n\\tag{9.2}\n\\end{align}\\]\ntrong đó \\(\\rho_i\\) là xác suất hay khả năng người \\(\\) mua đầy đủ các quyền lợi của bảo hiểm sau khi đã mua bảo hiểm trách nhiệm dân sự. Hàm \\(\\Phi\\) là hàm phân phối xác suất của biến ngẫu nhiên phân phối chuẩn có trung bình 0 va phương sai bằng 1. Đây là hàm số mà chúng ta lựa chọn để liên kết giữa xác suất mua bảo hiểm bổ sung đến tổ hợp tuyến tính của các biến độc lập.Từ kết quả của mô hình, chúng ta có thể kết luận rằng khả năng mua bảo hiểm bổ sung phụ thuộc một cách có ý nghĩa thống kê biến độ tuổi (\\(age\\)) và giới tính (\\(sex\\)) của người tham gia bảo hiểm trách nhiệm dân sự bắt buộc. Sự phụ thuộc này cụ thể như sau:Xác suất mà một người mua đầy đủ các quyền lợi bảo hiểm sau khi mua bảo hiểm bắt buộc sẽ GIẢM nếu tuổi của người tham gia bảo hiểm TĂNG, điều này có nghĩa là những người trẻ tuổi hơn thường có nhu cầu mua đầy đủ các quyền lợi bảo hiểm hơn những người lớn tuổi.Xác suất mà một người mua đầy đủ các quyền lợi bảo hiểm sau khi mua bảo hiểm bắt buộc sẽ GIẢM nếu tuổi của người tham gia bảo hiểm TĂNG, điều này có nghĩa là những người trẻ tuổi hơn thường có nhu cầu mua đầy đủ các quyền lợi bảo hiểm hơn những người lớn tuổi.Nam giới ít có khả năng mua đầy đủ quyền lợi bảo hiểm như nữ giới.Nam giới ít có khả năng mua đầy đủ quyền lợi bảo hiểm như nữ giới.Mối liên hệ giữa xác suất mua bảo hiểm đầy đủ và các thuộc tính của người được quan sát được mô tả một cách định lượng thông qua phương trình\n\\[\\begin{align}\n\\rho_i = \\Phi\\left(0.678 - 0.016 \\times age_i - 0.447 \\times sex_i\\right)\n\\tag{9.3}\n\\end{align}\\]\ntrong đó\n\\(age_i\\) là tuổi của người tham gia bảo hiểm; \\(sex_i\\) nhận giá trị bằng 1 nếu người đó là nam giới và 0 nếu người đó là nữ giới; và \\(\\Phi\\) là hàm phân phối xác suất của biến ngẫu nhiên phân phối chuẩn \\(\\mathcal{N}(0,1)\\); miền giá trị của hàm số này đảm bảo cho giá trị xác suất \\(\\rho_i\\) được tính ra nằm trong khoảng (0,1).Bạn đọc có thể nhận thấy được sự khác biệt giữa mô hình tuyến tính tổng quát ở trên với mô hình tuyến tính thông thường ở hai điểm:Phân phối xác suất của biến mục tiêu \\(Y\\) là phân phối nhị thức chứ không phải là phân phối chuẩn.\nPhân phối xác suất của biến mục tiêu \\(Y\\) là phân phối nhị thức chứ không phải là phân phối chuẩn.Mối liên kết giữa giá trị trung bình của biến mục tiêu \\(Y\\) và tổ hợp tuyến tính của các biến độc lập được thể hiện thông qua một hàm số, trong trường hợp này là hàm \\(\\Phi\\). Trong mô hình tuyến tính thông thường, giá trị trunh bình của biến mục tiêu được mô tả trực tiếp bằng tổ hợp tuyến tính của các biến độc lập.\nMối liên kết giữa giá trị trung bình của biến mục tiêu \\(Y\\) và tổ hợp tuyến tính của các biến độc lập được thể hiện thông qua một hàm số, trong trường hợp này là hàm \\(\\Phi\\). Trong mô hình tuyến tính thông thường, giá trị trunh bình của biến mục tiêu được mô tả trực tiếp bằng tổ hợp tuyến tính của các biến độc lập.Hải điểm nêu trên cũng chính là hai cải tiến quan trọng của mô hình tuyến tính tổng với mô hình hồi quy tuyến tính thông thường. Việc tổng quát hóa phân phối của biến phụ thuộc và thiết lập một hàm liên kết giữa giá trị trung bình của biến phụ thuộc và với các biến độc lập giúp cho mô hình tuyến tính tổng quát linh hoạt hơn rất nhiều khi làm việc với các dữ liệu cụ thể và vẫn giữ được khả năng suy diễn giống như mô hình hồi quy tuyến tính thông thường. Trong phần tiếp theo của chương chúng ta sẽ thảo luận kỹ hơn về các vấn đề này.","code":"\ndat<-read.csv(\"../KHDL_KTKD Final/Dataset/MotoInsurance.csv\")\n\n# Đổi các biến Y và sex sang kiểu factor\ndat$Y<-as.factor(dat$Y)\ndat$sex<-as.factor(dat$sex)\n\n# Thực hiện các phân tích khai phá\np1<-dat%>%ggplot()+geom_boxplot(aes(x = Y, y = age))+\n  ggtitle(\"Mối liên hệ giữa biến age và biến Y\")+\n  theme_minimal()\n\np2<-dat%>%ggplot()+geom_bar(aes(x = sex, fill = Y),col = \"black\")+\n  ggtitle(\"Mối liên hệ giữa biến sex và biến Y\")+\n  theme_minimal()+\n  scale_fill_manual(values = c(\"white\",\"grey\"))\n\ngrid.arrange(p1,p2, ncol = 2)\n# Biến Y có phân phối nhị thức\n# Hàm g là hàm probit\nglm1<-glm(Y ~ age + sex, data=dat, \n          family = binomial(link = \"probit\")) # khai báo hàm g\nsummary(glm1)## \n## Call:\n## glm(formula = Y ~ age + sex, family = binomial(link = \"probit\"), \n##     data = dat)\n## \n## Deviance Residuals: \n##     Min       1Q   Median       3Q      Max  \n## -1.4495  -0.9278  -0.7382   1.2584   2.0391  \n## \n## Coefficients:\n##              Estimate Std. Error z value Pr(>|z|)    \n## (Intercept)  0.678644   0.079138   8.575   <2e-16 ***\n## age         -0.016257   0.001619 -10.041   <2e-16 ***\n## sexM        -0.446783   0.047421  -9.422   <2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## (Dispersion parameter for binomial family taken to be 1)\n## \n##     Null deviance: 5163.3  on 3999  degrees of freedom\n## Residual deviance: 4933.9  on 3997  degrees of freedom\n## AIC: 4939.9\n## \n## Number of Fisher Scoring iterations: 4"},{"path":"mô-hình-tuyến-tính-tổng-quát..html","id":"các-nhược-điểm-của-mô-hình-hồi-quy-tuyến-tính.","chapter":"Chương 9 Mô hình tuyến tính tổng quát.","heading":"9.1 Các nhược điểm của mô hình hồi quy tuyến tính.","text":"Mô hình hồi quy tuyến tính là nền tảng quan trọng cho hầu hết các mô hình học máy và các mô hình trí tuệ nhân tạo hiện tại. Trước khi những mô hình học máy được nghiên cứu và phát triển mạnh mẽ như hiện nay, những người xây dựng mô hình luôn gặp khó khăn khi sử dụng mô hình hồi quy tuyến tính trong nhiều hoàn cảnh. Nguyên nhân là giả thiết về phân phối xác suất của biến mục tiêu và miền giá trị trung bình của biến mục tiêu của mô hình hồi quy tuyến tính thông thường là không phù hợp với đa số dữ liệu thực tế.Thật vậy, mô hình hồi quy tuyến tính được thảo luận trong phần trước của cuốn sách có thể được tóm tắt như sau: người xây dựng mô hình cố gắng nghiên cứu mối quan hệ giữa một biến mục tiêu \\(Y\\) với véc-tơ biến độc lập \\(\\textbf{X} = (X_1, X_2, \\cdots, X_p)\\) bằng cách cho rằng mối liên hệ giữa \\(Y\\) và \\(\\textbf{X}\\) là một hàm tuyến tính. Mối liên hệ đó không đồng nhất nên sai số sẽ tồn tại và những người xây dựng mô hình cho rằng sai số có phân phối chuẩn với trung bình bằng 0 và độ lệch chuẩn là một hằng số \\(\\sigma > 0\\). Chúng ta biểu diễn mô hình tuyến tính thông thương như sau\\[\\begin{align}\n& Y = \\beta_0 + \\beta_1 \\cdot X_1 + \\beta_2 \\cdot X_2 + \\cdots + \\beta_p \\cdot X_p + \\epsilon \\\\\n& \\epsilon \\sim \\mathcal{N}(0, \\sigma^2)\n\\tag{9.4}\n\\end{align}\\]Bạn đọc có thể thấy rằng trong mô hình hồi quy tuyến tính, biến phụ thuộc \\(Y\\) là biến ngẫu nhiên có phân phối chuẩn có phương sai là \\(\\sigma^2\\) và giá trị trung bình phụ thuộc vào véc-tơ biến độc lập \\(\\textbf{X}_i\\). Với điều kiện biến độc lập nhận giá trị là \\(\\textbf{x}_i = (x_{,1}, x_{,2}, \\cdots, x_{,p})\\); chúng ta có mô hình hồi quy tuyến tính như sau:\\[\\begin{align}\n& Y_i \\sim  \\mathcal{N}(\\mu_i, \\sigma^2) \\\\\n& \\mu_i = \\beta_0 + \\beta_1 \\cdot x_{,1} + \\beta_2 \\cdot x_{,2} + \\cdots + \\beta_p \\cdot x_{,p}\n\\tag{9.5}\n\\end{align}\\]Ngoài giả thiết về phân phối chuẩn của \\(Y\\), mô hình hồi quy tuyến tính thông thường còn cho rằng giá trị trung bình của biến ngẫu nhiên \\(Y\\) với điều kiện các biến độc lập nhận giá trị \\(\\textbf{x}_i\\), ký hiệu \\(\\mu_i\\), là một tổ hợp tuyến tính của các biến độc lập. Khi các biến độc lập nhận các giá trị bất kỳ, miền giá trị của \\(\\mu_i\\) sẽ là (toàn bộ) tập các số thực \\(\\mathbb{R}\\).Câu hỏi đặt ra là: làm như thế nào để áp dụng mô hình hồi quy tuyến tính trong các trường hợp như sau?Biến mục tiêu \\(Y\\) chỉ nhận hai giá trị là 0 hoặc 1. Đây là trường hợp rất thường gặp phải trong nhiều lĩnh vực khi thực hiện phân tích dữ liệu. Có thể kể đến như: khi biến \\(Y\\) đại diện cho sự kiện một người có hay không tham gia bảo hiểm xã hội; một người có hay không thực hiện rút bảo hiểm xã hội một lần trong thời gian một khoảng thời gian; một khách hàng có hay không gửi yêu cầu thanh toán bảo hiểm; hay tương tự như ví dụ trong phần đầu của cuốn sách, một khách hàng đã mua bảo hiểm bắt buộc có hay không mua thêm các quyền lợi bảo hiểm bổ sung. Ngoài lĩnh vực bảo hiểm, biến mục tiêu \\(Y\\) chỉ nhận giá trị 0 hoặc 1 còn xuất hiện trong lĩnh vực ngân hàng tài chính: biến mục tiêu cho biết một giao dịch trực tuyến có phải là một giao dịch gian lận hay không; một khách hàng có hay không tiếp nhận sản phẩm dịch vụ tài chính với mới; một khách hàng có hay không hoàn trả khoản nợ thẻ tín dụng trong thời gian tới,… Trong tất cả các trường hợp kể trên không thể giả thiết phân phối xác suất của biến mục tiêu là phân phối chuẩn. Hơn thế nữa, giá trị trung bình của biến mục tiêu sẽ luôn nằm trong khoảng 0 đến 1, chứ không phải toàn bộ trục số thực. Nếu sử dụng một tổ hợp tuyến tính của các biến độc lập để tính toán giá trị trung bình của biến mục tiêu, chúng ta sẽ có thể gặp các giá trị nhỏ hơn 0 hoặc các giá trị lớn hơn 1.Biến mục tiêu \\(Y\\) chỉ nhận hai giá trị là 0 hoặc 1. Đây là trường hợp rất thường gặp phải trong nhiều lĩnh vực khi thực hiện phân tích dữ liệu. Có thể kể đến như: khi biến \\(Y\\) đại diện cho sự kiện một người có hay không tham gia bảo hiểm xã hội; một người có hay không thực hiện rút bảo hiểm xã hội một lần trong thời gian một khoảng thời gian; một khách hàng có hay không gửi yêu cầu thanh toán bảo hiểm; hay tương tự như ví dụ trong phần đầu của cuốn sách, một khách hàng đã mua bảo hiểm bắt buộc có hay không mua thêm các quyền lợi bảo hiểm bổ sung. Ngoài lĩnh vực bảo hiểm, biến mục tiêu \\(Y\\) chỉ nhận giá trị 0 hoặc 1 còn xuất hiện trong lĩnh vực ngân hàng tài chính: biến mục tiêu cho biết một giao dịch trực tuyến có phải là một giao dịch gian lận hay không; một khách hàng có hay không tiếp nhận sản phẩm dịch vụ tài chính với mới; một khách hàng có hay không hoàn trả khoản nợ thẻ tín dụng trong thời gian tới,… Trong tất cả các trường hợp kể trên không thể giả thiết phân phối xác suất của biến mục tiêu là phân phối chuẩn. Hơn thế nữa, giá trị trung bình của biến mục tiêu sẽ luôn nằm trong khoảng 0 đến 1, chứ không phải toàn bộ trục số thực. Nếu sử dụng một tổ hợp tuyến tính của các biến độc lập để tính toán giá trị trung bình của biến mục tiêu, chúng ta sẽ có thể gặp các giá trị nhỏ hơn 0 hoặc các giá trị lớn hơn 1.Biến mục tiêu \\(Y\\) là biến dạng đếm. Chẳng hạn như \\(Y\\) cho biết một người tham gia bảo hiểm xã hội gửi yêu cầu bồi thường bao nhiêu lần trong một năm; biến \\(Y\\) cho biết một khách hàng mua bảo hiểm xe ô tô gây ra bao nhiêu tai nạn trong thời gian được bảo hiểm,… Trong trường hợp này, \\(Y\\) sẽ nhận giá trị kiểu số đếm: \\(0, 1, 2, \\cdots\\) tương ứng với số lần khách hàng gửi yêu cầu bảo hiểm. Không thể sử dụng mô hình hồi quy tuyến tính thông thường biến \\(Y\\) là biến rời rạc đồng thời giá trị trung bình của \\(Y\\) là một số lớn hơn 0.Biến mục tiêu \\(Y\\) là biến dạng đếm. Chẳng hạn như \\(Y\\) cho biết một người tham gia bảo hiểm xã hội gửi yêu cầu bồi thường bao nhiêu lần trong một năm; biến \\(Y\\) cho biết một khách hàng mua bảo hiểm xe ô tô gây ra bao nhiêu tai nạn trong thời gian được bảo hiểm,… Trong trường hợp này, \\(Y\\) sẽ nhận giá trị kiểu số đếm: \\(0, 1, 2, \\cdots\\) tương ứng với số lần khách hàng gửi yêu cầu bảo hiểm. Không thể sử dụng mô hình hồi quy tuyến tính thông thường biến \\(Y\\) là biến rời rạc đồng thời giá trị trung bình của \\(Y\\) là một số lớn hơn 0.Ngay cả khi trong các trường hợp biến phụ thuộc \\(Y\\) là biến liên tục, sử dụng mô hình tuyến tính thông thường cũng sẽ gặp phải vấn đề. Chẳng hạn như khi \\(Y\\) là số tiền khách hàng yêu cầu bồi thường cho xe ô tô trong trường hợp xảy ra tai nạn. Biến \\(Y\\) chỉ nhận giá trị là số dương và thường có phân phối xác suất lệch phải với đuôi lớn. Sử dụng giả thiết phân phối chuẩn cho biến \\(Y\\) sẽ làm cho mô hình không đánh giá đúng khả mức độ nghiêm trọng của yêu cầu bồi thường phân phối chuẩn không có khả năng mô tả các rủi ro có đuôi lớn. Đồng thời, giá trị trung bình của số tiền yêu cầu bồi thường luôn là số dương, đó cũng không thể sử dụng tổ hợp tuyến tính của các biến độc lập để trực tiếp mô tả giá trị trung bình của \\(Y\\).Ngay cả khi trong các trường hợp biến phụ thuộc \\(Y\\) là biến liên tục, sử dụng mô hình tuyến tính thông thường cũng sẽ gặp phải vấn đề. Chẳng hạn như khi \\(Y\\) là số tiền khách hàng yêu cầu bồi thường cho xe ô tô trong trường hợp xảy ra tai nạn. Biến \\(Y\\) chỉ nhận giá trị là số dương và thường có phân phối xác suất lệch phải với đuôi lớn. Sử dụng giả thiết phân phối chuẩn cho biến \\(Y\\) sẽ làm cho mô hình không đánh giá đúng khả mức độ nghiêm trọng của yêu cầu bồi thường phân phối chuẩn không có khả năng mô tả các rủi ro có đuôi lớn. Đồng thời, giá trị trung bình của số tiền yêu cầu bồi thường luôn là số dương, đó cũng không thể sử dụng tổ hợp tuyến tính của các biến độc lập để trực tiếp mô tả giá trị trung bình của \\(Y\\).Có thể tổng kết rằng hai vấn đề thường gặp phải khi sử dụng mô hình hồi quy tuyến tính thông thường trên dữ liệu thực tế làThứ nhất: sự không phù hợp của giả thiết phân phối chuẩn đối với biến mục tiêu \\(Y\\); vàThứ nhất: sự không phù hợp của giả thiết phân phối chuẩn đối với biến mục tiêu \\(Y\\); vàThứ hai: miền giá trị trung bình của biến mục tiêu \\(Y\\) không phù hợp với miền giá trị của tổ hợp tuyến tính của biến độc lập. Giá trị \\(\\textbf{x}_i^{'} \\boldsymbol{\\beta} = \\beta_0 + \\beta_1 \\ x_{,1} + \\beta_2 \\ x_{,2} + \\cdots + \\beta_p \\ x_{,p}\\) có thể nhận bất kỳ giá trị nào trong \\(\\mathbb{R}\\), trong khi giá trị trung bình của biến mục tiêu \\(Y\\) trong các dữ liệu thực tế lại thường chỉ là một tập con của \\(\\mathbb{R}\\).Thứ hai: miền giá trị trung bình của biến mục tiêu \\(Y\\) không phù hợp với miền giá trị của tổ hợp tuyến tính của biến độc lập. Giá trị \\(\\textbf{x}_i^{'} \\boldsymbol{\\beta} = \\beta_0 + \\beta_1 \\ x_{,1} + \\beta_2 \\ x_{,2} + \\cdots + \\beta_p \\ x_{,p}\\) có thể nhận bất kỳ giá trị nào trong \\(\\mathbb{R}\\), trong khi giá trị trung bình của biến mục tiêu \\(Y\\) trong các dữ liệu thực tế lại thường chỉ là một tập con của \\(\\mathbb{R}\\).Mô hình tuyến tính tổng quát được xây dựng trên cơ sở của mô hình hồi quy tuyến tính thông thường với mục đích khắc phục hai nhược điểm kể trên.\n- Trước hết, mô hình tuyến tính tổng quát giả thiết một phân phối phù hợp cho biến phụ thuộc \\(Y\\), tùy vào dữ liệu sử dụng để phân tích, tạm gọi là phân phối \\(F\\) với tham số \\(\\boldsymbol{\\theta}\\), ký hiệu là \\(F_\\boldsymbol{\\theta}\\).\n- Tiếp theo, để đảm bảo miền giá trị của giá trị trung bình của \\(Y\\) với điều kiện các biến độc lập bằng \\(\\textbf{x}_i\\), \\(\\mu_i = E(Y|\\textbf{X} = \\textbf{x}_i)\\), phù hợp với miền giá trị của \\(\\textbf{x}_i^{'} \\boldsymbol{\\beta}\\), mô hình tuyển tính tổng quát sử dụng một hàm số đơn điệu \\(g\\), được gọi là hàm liên kết, để biến đổi miền giá trị của \\(\\mu_i\\). Chúng ta phát biểu mô hình tuyến tính tổng quát như sau\n\\[\\begin{align}\n& Y \\sim  F_\\theta \\\\\n& \\mathbb{E}(Y|\\textbf{X} = \\textbf{x}_i) = \\mu_i \\\\\n& g(\\mu_i) = \\beta_0 + \\beta_1 \\cdot x_{,1} + \\beta_2 \\cdot x_{,2} + \\cdots + \\beta_p \\cdot x_{,p}\n\\tag{9.6}\n\\end{align}\\]Trước hết, có thể thấy rằng mô hình hồi quy tuyến tính thông thường là trường hợp đặc biệt của mô hình tuyến tính tổng quát khi tham số \\(\\boldsymbol{\\theta}\\) là \\(\\sigma^2\\); phân phối \\(F\\) là phân phối chuẩn; và hàm \\(g\\) là hàm số đồng nhất \\(g(x) = x\\).Giả thiết đơn điệu của hàm liên kết \\(g\\) đảm bảo sự tồn tại của hàm số ngược \\(g^{-1}\\). Mối liên hệ của \\(\\mu_i\\) và \\(\\textbf{x}_i^{'} \\boldsymbol{\\beta}\\) có thể được viết lại dưới dạng hàm ngược của hàm liên kết như sau\n\\[\\begin{align}\n\\mu_i = g^{-1}(\\textbf{x}_i^{'} \\boldsymbol{\\beta})\n\\tag{9.7}\n\\end{align}\\]Quay trở lại ví dụ ở phần trước của chương sách, chúng ta phân tích về tác động của độ tuổi và giới tính lên quyết định có tham gia quyền lợi bảo hiểm bổ sung hay không. Mô hình tuyến tính tổng quát được xây dựng như sau:\\[\\begin{align}\n& Y \\sim  \\mathcal{B}(p) \\\\\n& \\mathbb{E}(Y|age_i, sex_i) = p_i \\\\\n& \\Phi^{-1}(p_i) = 0.678 - 0.016 \\times age_i - 0.446 \\times sex_i\n\\end{align}\\]Phân phối xác suất của biến \\(Y\\) là phân phối nhị thức \\(Y\\) chỉ có thể nhận là 0 hoặc 1. Đồng thời, giá trị trung bình của \\(Y\\) nằm trong khoảng \\((0,1)\\) chúng ta có thể chọn hàm liên kết \\(g\\) là hàm \\(\\Phi^{-1}\\). Đây là hàm ngược của hàm phân phối xác suất của biến ngẫu nhiên phân phối chuẩn \\(\\mathcal{N}(0,1)\\) nên thỏa mãn các điều kiện của một hàm liên kết: hàm đơn điệu tăng; có miền xác định là \\((0,1)\\); và miền giá trị là tập số thực \\(\\mathbb{R}\\). Nếu không lựa chọn \\(\\Phi^{-1}\\), mọi hàm số đơn điệu, có miền xác định là khoảng \\((0,1)\\), và miền giá trị là tập số thực \\(\\mathbb{R}\\) đều có thể được lựa chọn làm hàm số kết nối.","code":""},{"path":"mô-hình-tuyến-tính-tổng-quát..html","id":"xây-dựng-mô-hình-tuyến-tính-tổng-quát","chapter":"Chương 9 Mô hình tuyến tính tổng quát.","heading":"9.2 Xây dựng mô hình tuyến tính tổng quát","text":"Phần tiếp theo của cuốn sách sẽ thảo luận về cách xây dựng mô hình tuyến tổng quát với các kiểu giá trị khác nhau của biến phụ thuộc \\(Y\\). Xin được nhắc lại rằng đây là cuốn sách dành cho cả các bạn đọc không có nền tảng toán học nâng cao. đó, những thảo luận phức tạp liên quan đến các giả thiết của mô hình hay phương pháp ước lượng tham số sẽ được trình bày ở phần sau của chương sách. Chúng ta sẽ hiểu về mô hình tuyến tính tổng quát thông qua việc ứng dụng mô hình cho các các dữ liệu thực tế trước khi đi sâu vào bản chất toán học của mô hình.","code":""},{"path":"mô-hình-tuyến-tính-tổng-quát..html","id":"biến-phụ-thuộc-là-biến-dạng-nhị-phân.","chapter":"Chương 9 Mô hình tuyến tính tổng quát.","heading":"9.2.1 Biến phụ thuộc là biến dạng nhị phân.","text":"Đây là các trường hợp mà biến phụ thuộc chỉ nhận một trong hai giá trị. Ở phần trên chúng ta đã trình bày các ví dụ cho trường hợp này: khách hàng có hay không hoàn trả nợ thẻ tín dụng, khách hàng phản hồi tích cực hay tiêu cực về sản phầm, một yêu cầu bồi thường là trục lợi hay bình thường, một giao dịch rút tiền ngân hàng có hay không phải là giao dịch gian lận, … Mặc dù đây chỉ là một trường hợp đặc biệt của biến phụ thuộc nhận giá trị rời rạc nhưng qua các ví dụ thực tế lại thấy rằng phần lớn các dữ liệu gặp phải lại có biến phụ thuộc ở dạng nhị phân. Khi \\(Y\\) chỉ nhận hai giá trị, chúng ta sẽ luôn mã hóa giá trị của \\(Y\\) thành hai số là 0 và 1. Một vài cuốn sách khác, hoặc trong một vài lĩnh vực nghiên cứu khác, người xây dựng mô hình có thể mã hóa \\(Y\\) thành -1 và 1. Tuy nhiên hai cách mã hóa này chỉ khác nhau ở hình thức chứ không làm ảnh hưởng đến kết quả của mô hình.","code":""},{"path":"mô-hình-tuyến-tính-tổng-quát..html","id":"phân-phối-xác-suất-của-biến-phụ-thuộc.","chapter":"Chương 9 Mô hình tuyến tính tổng quát.","heading":"9.2.1.1 Phân phối xác suất của biến phụ thuộc.","text":"Khi biến phụ thuộc là biến dạng nhị phân, một cách tự nhiên, chúng ta sẽ sử dụng phân phối nhị thức, hay còn gọi là phân phối Bernoulli, để mô tả biến phụ thuộc. Biến ngẫu nhiên \\(B\\) có phân phối nhị thức với tham số \\(\\rho\\), \\(0 < \\rho < 1\\), ký hiệu \\(\\mathcal{B}(\\rho)\\), là biến ngẫu nhiên chỉ nhận hai giá trị là 0 và 1 với hàm khối lượng xác suất như sau\\[\\begin{align}\n\\mathbb{P}(B = x) = \\rho^x \\times (1-\\rho)^{(1-x)} \\text{ với } x \\\\{0;1\\}\n\\tag{9.8}\n\\end{align}\\]Chúng ta có giá trị trung bình và phương sai của \\(\\mathcal{B}(\\rho)\\).\n\\[\\begin{align}\n& \\mathbb{E}(B) = \\rho \\\\\n& \\mathbb{V}(B) = \\rho \\ (1-\\rho)\n\\tag{9.9}\n\\end{align}\\]Có thể thấy rằng phân phối nhị thức có duy nhất một tham số \\(\\rho\\) và tham số này cũng chính là giá trị trung bình của biến đó. Phương sai của \\(\\mathcal{B}(\\rho)\\) nhỏ hơn giá trị trung bình.","code":""},{"path":"mô-hình-tuyến-tính-tổng-quát..html","id":"lựa-chọn-hàm-liên-kết.","chapter":"Chương 9 Mô hình tuyến tính tổng quát.","heading":"9.2.1.2 Lựa chọn hàm liên kết.","text":"Khi biến phụ thuộc \\(Y\\) có phân phối nhị thức, giá trị trung bình của biến phụ thuộc sẽ nằm trong khoảng \\((0,1)\\). Từ công thức (9.6), nếu cho \\(\\rho_i\\) là giá trị trung bình của biến phụ thuộc với điều kiện các biến độc lập \\(\\textbf{X} = \\textbf{x}_i\\), ta có\\[\\begin{align}\n& \\mathbb{E}(Y|\\textbf{X} = \\textbf{x}_i) = \\rho_i \\\\\n& g(\\rho_i) = \\beta_0 + \\beta_1 \\cdot x_{,1} + \\beta_2 \\cdot x_{,2} + \\cdots + \\beta_p \\cdot x_{,3}\n\\tag{9.10}\n\\end{align}\\]Như vậy, mọi hàm số đơn điệu có miền xác định là khoảng \\((0,1)\\) và miền giá trị là toàn bộ tập số thực \\(\\mathbb{R}\\) đều có thể được lựa chọn để làm hàm ngược của hàm liên kết. Làm thế nào để có được những hàm số có tính chất như vậy? Chúng ta đều biết rằng các hàm phân phối xác suất của một biến ngẫu nhiên liên tục bất kỳ là các hàm số tăng, có miền xác định là \\(\\mathbb{R}\\) và miền giá trị là khoảng \\((0,1)\\). đó, hàm số ngược của các hàm phân phối xác suất, sẽ là các hàm số tăng, có miền xác định là khoảng \\((0,1)\\) và miền giá trị là \\(\\mathbb{R}\\). Nói một cách khác, hàm số ngược của các hàm phân phối xác suất bất kỳ thỏa mãn đầy đủ tính chất của hàm liên kết trong trường hợp \\(Y\\) có phân phối nhị thức.Trong thực tế, việc lựa chọn hàm liên kết còn có mục tiêu là để mô hình dễ giải thích và quá trình ước lượng mô hình đơn giản nhất có thể. Các hàm phân phối xác suất thường được lựa chọn làm \\(g^{-1}(.)\\) bao gồm có: thứ nhất là hàm phân phối của biến ngẫu nhiên logistic; thứ hai là hàm phân phối của biến ngẫu nhiên phân phối chuẩn; và thứ ba là hàm phân phối của biến ngẫu nhiên phân phối Cauchy.Hàm phân phối của biến ngẫu nhiên logistic và hàm ngược được cho bởi công thức sau\n\\[\\begin{align}\n& \\textit{Hàm phân phối xác suất: } \\ g^{-1}(x) = \\cfrac{1}{1 + e^{-x}} \\\\\n& \\textit{Hàm ngược: } \\ g(x) = ln(\\cfrac{x}{1- x})\n\\end{align}\\]Với biến phụ thuộc \\(Y\\) là có phân phối nhị thức và hàm \\(g\\) là hàm số ngược của hàm phân phối của biến ngẫu nhiên logistic, chúng ta có mô hình tuyến tính tổng quát như sau:\\[\\begin{align}\n& Y_i \\sim \\mathcal{B}(\\rho_i) \\\\\n& \\rho_i = \\cfrac{1}{1 + exp(-(\\beta_0 + \\beta_1 \\cdot x_{,1} + \\beta_2 \\cdot x_{,2} + \\cdots + \\beta_p \\cdot x_{,p}))} \\\\\n& ln(\\cfrac{\\rho_i}{1 - \\rho_i}) = \\beta_0 + \\beta_1 \\cdot x_{,1} + \\beta_2 \\cdot x_{,2} + \\cdots + \\beta_p \\cdot x_{,p}\n\\tag{9.11}\n\\end{align}\\]Đây là mô hình được biết đến rộng rãi với tên gọi là hồi quy logistic và cũng là mô hình thường được sử dụng nhất trong khi biến phụ thuộc là biến nhị phân. Mô hình có ưu điểm là sự dễ hiểu khi diễn giải kết quả:\\(\\rho_i\\) ngoài ý nghĩa là trung bình của biến ngẫu nhiên \\(Y_i\\) còn có ý nghĩa là xác suất xảy ra sự kiện \\(Y_i = 1\\).\\(\\rho_i\\) ngoài ý nghĩa là trung bình của biến ngẫu nhiên \\(Y_i\\) còn có ý nghĩa là xác suất xảy ra sự kiện \\(Y_i = 1\\).Giá trị \\(\\cfrac{\\rho_i}{1 - \\rho_i}\\) được gọi là odds của sự kiện \\(Y_i = 1\\). Mối liên hệ giữa quan sát thứ \\(\\) của biến độc lập \\(X_j\\) là \\(x_{,j}\\) và obbs của sự kiện \\(Y_i = 1\\) có thể được diễn giải thông qua hệ số \\(\\beta_j\\).Giá trị \\(\\cfrac{\\rho_i}{1 - \\rho_i}\\) được gọi là odds của sự kiện \\(Y_i = 1\\). Mối liên hệ giữa quan sát thứ \\(\\) của biến độc lập \\(X_j\\) là \\(x_{,j}\\) và obbs của sự kiện \\(Y_i = 1\\) có thể được diễn giải thông qua hệ số \\(\\beta_j\\).Hàm phân phối của biến ngẫu nhiên chuẩn \\(\\mathcal{N}(0,1)\\) và hàm ngược của hàm phân phối được cho bởi công thức sau\n\\[\\begin{align}\n& g^{-1}(x) = \\Phi(x) \\\\\n& g(x) = \\Phi^{-1}(x) \\\\\n& \\Phi(x) = \\cfrac{1}{\\sqrt{2 \\pi}} \\  \\int\\limits_{-\\infty}^x \\ exp(-t^2/2)  \\\\\n\\end{align}\\]Chúng ta có mô hình tuyến tính tổng quát như sau\\[\\begin{align}\n& Y_i \\sim \\mathcal{B}(\\rho_i) \\\\\n& \\rho_i = \\Phi(\\beta_0 + \\beta_1 \\cdot x_{,1} + \\beta_2 \\cdot x_{,2} + \\cdots + \\beta_p \\cdot x_{,p}) \\\\\n& \\Phi^{-1}(\\rho_i) = \\beta_0 + \\beta_1 \\cdot x_{,1} + \\beta_2 \\cdot x_{,2} + \\cdots + \\beta_p \\cdot x_{,p}\n\\tag{9.12}\n\\end{align}\\]Hàm số ngược của biến ngẫu nhiên phân phối chuẩn được gọi là hàm probit đó mô hình GLM trong trường hợp này còn được biết đến với tên gọi là mô hình probit.Hàm phân phối của biến ngẫu nhiên Cauchy và hàm ngược của hàm phân phối được cho bởi công thức sau\n\\[\\begin{align}\n& g^{-1}(x) = \\cfrac{1}{2} + \\cfrac{arctan(x)}{\\pi} \\\\\n& g(x) = tan\\left( \\pi \\left( x - \\cfrac{1}{2} \\right) \\right)\n\\tag{9.13}\n\\end{align}\\]Chúng ta có mô hình tuyến tính tổng quát như sau\\[\\begin{align}\n& Y_i \\sim \\mathcal{B}(\\rho_i) \\\\\n& p_i = \\cfrac{1}{2} + \\cfrac{arctan(\\beta_0 + \\beta_1 \\cdot x_{,1} + \\beta_2 \\cdot x_{,2} + \\cdots + \\beta_p \\cdot x_{,p})}{\\pi}  \\\\\n& tan\\left( \\pi \\left( \\rho_i - \\cfrac{1}{2} \\right) \\right) = \\beta_0 + \\beta_1 \\cdot x_{,1} + \\beta_2 \\cdot x_{,2} + \\cdots + \\beta_p \\cdot x_{,p}\n\\tag{9.14}\n\\end{align}\\]Hàm số ngược của phân phối cauchy còn được gọi là hàm cauchit, đó mô hình tuyến tính tổng quát trong trường hợp này còn được biết đến với tên là mô hình cauchit.Hình vẽ dưới đây mô tả hình dạng của ba hàm số kết nốiMô hình tuyến tính tổng quát trong trường hợp \\(Y\\) là biến nhị phân thường được ước lượng bằng phương pháp tối đa hóa hàm likelihood, hay gọi tắt là phương pháp MLE. Để tránh sự phức tạp không cần thiết chúng tôi sẽ trình bày phần ước lượng mô hình trong phần sau của chương sách.Trong R, hàm số glm() của thư viện \\(stat\\) được sử dụng để xây dựng mô hình tuyến tính tổng quát. Tham số \\(family\\) trong hàm glm() dùng để khai báo phân phối xác suất cho biến phụ thuộc \\(Y\\) và để lựa chọn hàm liên kết phù hợp. Trở lại với ví dụ khi \\(Y\\) là biến nhị phân mô tả khách hàng có hay không lựa chọn các quyền lợi đầy đủ khi tham gia bảo hiểm bổ sung, chúng ra thực hiện xây dựng mô hình như sauCả ba mô hình tuyến tính tổng quát ở trên đều cho \\(Y\\) là một biến ngẫu nhiên phân phối nhị thức, nhưng mối liên hệ giữa độ tuổi và giới tính đến giá trị trung bình của \\(Y\\) lại được mô tả bằng các công thức khác nhauTrong mô hình logit:\n\\[\\begin{align}\n\\mathbb{P}(Y_i = 1| age_i, sex_i) = \\cfrac{1}{1 + exp(-(1.109 - 0.026 \\times age_{} - 0.723 \\times sex_{}))}\n\\end{align}\\]Trong mô hình logit:\n\\[\\begin{align}\n\\mathbb{P}(Y_i = 1| age_i, sex_i) = \\cfrac{1}{1 + exp(-(1.109 - 0.026 \\times age_{} - 0.723 \\times sex_{}))}\n\\end{align}\\]Trong mô hình probit\n\\[\\begin{align}\n\\mathbb{P}(Y_i = 1| age_i, sex_i) = \\Phi\\left(0.678 - 0.016 \\times age_{} - 0.446 \\times sex_{}\\right)\n\\end{align}\\]Trong mô hình probit\n\\[\\begin{align}\n\\mathbb{P}(Y_i = 1| age_i, sex_i) = \\Phi\\left(0.678 - 0.016 \\times age_{} - 0.446 \\times sex_{}\\right)\n\\end{align}\\]Trong mô hình cauchit\n\\[\\begin{align}\n\\mathbb{P}(Y_i = 1| age_i, sex_i) = \\cfrac{1}{2} + \\cfrac{arctan\\left(0.997 - 0.024 \\times age_{} - 0.624 \\times sex_{}\\right)} {\\pi}\n\\end{align}\\]Trong mô hình cauchit\n\\[\\begin{align}\n\\mathbb{P}(Y_i = 1| age_i, sex_i) = \\cfrac{1}{2} + \\cfrac{arctan\\left(0.997 - 0.024 \\times age_{} - 0.624 \\times sex_{}\\right)} {\\pi}\n\\end{align}\\]Cả ba mô hình đều cho cùng một kết quả: người có độ tuổi càng cao thì càng ít có khả năng lựa chọn quyền lợi bảo hiểm bổ sung và xác suất nam giới lựa chọn quyền lợi bảo hiểm bổ sung là ít hơn với nữ giới. Bảng dưới đây đưa ra khả năng/xác suất chấp nhận của ba mô hình tính dựa trên hai biến độc lập là tuổi và giới tínhCó thể thấy rằng không có sự khác biệt lớn trong tính toán xác suất của \\(Y\\) khi sử dụng hàm liên kết khác nhau. Thực tế thì việc lựa chọn hàm kết nối sẽ không ảnh hưởng lớn đến kết quả của mô hình bằng việc lựa chọn phân phối cho biến phụ thuộc hay việc lựa chọn biến độc lập để giải thích mô hình.","code":"\n# Phân phối Y là nhị thức và hàm liên kết là hàm logit\nglm.binomial.logit<-glm(Y ~ age + sex, data=dat, \n          family = binomial(link = \"logit\"))\n\n# Phân phối Y là nhị thức và hàm liên kết là hàm probit\nglm.binomial.probit<-glm(Y ~ age + sex, data=dat, \n          family = binomial(link = \"probit\"))\n\n# Phân phối Y là nhị thức và hàm liên kết là hàm cauchit\nglm.binomial.cauchy<-glm(Y ~ age + sex, data=dat, \n          family = binomial(link = \"cauchit\"))"},{"path":"mô-hình-tuyến-tính-tổng-quát..html","id":"biến-phụ-thuộc-là-biến-rời-rạc-không-có-thứ-tự.","chapter":"Chương 9 Mô hình tuyến tính tổng quát.","heading":"9.2.2 Biến phụ thuộc là biến rời rạc không có thứ tự.","text":"Biến rời rạc không có thứ tự, hay còn gọi là biến rời rạc danh nghĩa (non-ordinal variable), là biến ngẫu nhiên mà các giá trị có thể nhận không có ý nghĩa sánh với nhau. Khi nói đến biến rời rạc không có thứ tự, chúng ta luôn hiểu rằng biến nhận từ ba giá trị trở lên bởi vì trường hợp có hai giá trị sẽ tương tự như biến có phân phối nhị phân. Một ví dụ đơn giản cho biến rời rạc không có thứ tự là màu sắc được chọn khi mua xe ô tô, hoặc loại hình bảo hiểm được lựa chọn bởi các khách hàng của một công ty bảo hiểm.Giả sử biến rời rạc danh nghĩa \\(Y\\) có thể nhận \\(J\\) giá trị khác nhau lần lượt là \\(1, 2, \\cdots, J\\). Với biến danh nghĩa, chúng ta không thể mô tả được mối liên hệ giữa xác suất của hai sự kiện \\((Y=)\\) và \\((Y=j)\\) với \\(1 \\leq < j \\leq J\\) dưới dạng tham số. Chính vì thế cấu trúc dạng tham số của mô hình tuyến tính tổng quát như phương trình (9.1) là không thể áp dụng được.Một phương pháp tiếp cận cho trường hợp biến mục tiêu \\(Y\\) là biến danh nghĩa là mở rộng mô hình tuyến tính tổng quát với biến nhị phân: với mỗi \\(j \\{1,2,\\cdots,J}\\)\\[\\begin{align}\n\\mathbb{P}\\left(Y = j\\right|\\textbf{x}_i) = \\cfrac{h(\\beta_{j,0} + \\beta_{j,1} \\cdot x_{,1} + \\cdots + \\beta_{j,p} \\cdot x_{,p} ) }{ \\sum\\limits_{j=1}^J h(\\beta_{j,0} + \\beta_{j,1} \\cdot x_{,1} + \\cdots + \\beta_{j,p} \\cdot x_{,p} ) }\n\\tag{9.15}\n\\end{align}\\]với hàm \\(h\\): \\(\\mathbb{R} \\rightarrow \\mathbb{R}^+\\). Hàm số \\(h\\) thường được lựa chọn là hàm lũy thừa cơ số tự nhiên exp(). Hàm số xác định xác suất xảy ra các sự kiện \\((Y=j)\\) trong phương trình (9.15) được gọi là hàm \\(softmax\\):\n\\[\\begin{align}\nsoftmax(z_1, z_2, \\cdots, z_p) = \\left(\\cfrac{e^{z_1}}{\\sum\\limits_{j=1}^p e^{z_j}}, \\cfrac{e^{z_2}}{\\sum\\limits_{j=1}^p e^{z_1}}, \\cdots, \\cfrac{e^{z_p}}{\\sum\\limits_{=1}^p e^{z_j}}  \\right)\n\\end{align}\\]Các tham số \\(\\beta_{j,k}\\) được ước lượng để tối thiểu hóa hàm tổn thất tính bằng cross-entropy:\n\\[\\begin{align}\n\\sum\\limits_{=1}^n \\sum\\limits_{j=1}^J y_{,j} \\cdot \\log\\left(\\mathbb{P}\\left(Y = j|\\textbf{x}_i\\right)\\right)\n\\end{align}\\]trong đó \\(\\mathbb{P}\\left(Y = j|\\textbf{x}_i\\right)\\) được tính toán từ công thức (9.15) và \\(y_{,j}\\) nhận một trong hai giá trị:bằng 1 nếu giá trị quan sát thứ \\(\\) của biến mục tiêu \\(Y\\) bằng \\(j\\)bằng 1 nếu giá trị quan sát thứ \\(\\) của biến mục tiêu \\(Y\\) bằng \\(j\\)bằng 0 nếu giá trị quan sát thứ \\(\\) của biến mục tiêu \\(Y\\) khác \\(j\\)bằng 0 nếu giá trị quan sát thứ \\(\\) của biến mục tiêu \\(Y\\) khác \\(j\\)Lưu ý rằng có \\(J\\) véc-tơ hệ số tuyến tính trong phương trình (9.15). Trong thực tế, khi biến mục tiêu \\(Y\\) có thể nhận \\(J\\) giá trị danh nghĩa, chúng ta xây dựng mô hình với \\((J-1)\\) véc-tơ hệ số tuyến tính tương ứng với các giá trị danh nghĩa \\(1, 2, \\cdots, (J-1)\\), đồng thời cố định giá trị của tất cả các hệ số tuyến tính bằng 0 với giá trị danh nghĩa \\(J\\). Khi tất cả các hệ số tuyến tính bằng 0, chúng ta có \\(h(.) = exp(0) = 1\\) với mọi \\(\\textbf{x}_i\\).\\[\\begin{align}\n& \\mathbb{P}\\left(Y = j\\right|x_i) = \\cfrac{h(\\beta_{j,0} + \\beta_{j,1} \\cdot x_{,1} + \\cdots + \\beta_{j,p} \\cdot x_{,p} ) }{1 + \\sum\\limits_{j=1}^{J-1} h(\\beta_{j,0} + \\beta_{j,1} \\cdot x_{,1} + \\cdots + \\beta_{j,p} \\cdot x_{,p} )} \\textit{ với } j < J \\\\\n& \\mathbb{P}\\left(Y = J\\right|x_i) = \\cfrac{1}{1 + \\sum\\limits_{j=1}^{J-1} h(\\beta_{j,0} + \\beta_{j,1} \\cdot x_{,1} + \\cdots + \\beta_{j,p} \\cdot x_{,p} )} \\\\\n\\tag{9.16}\n\\end{align}\\]Chúng ta sẽ xây dựng mô hình phân loại biến mục tiêu trên dữ liệu “travel insurance.csv”. Dữ liệu cung cấp thông tin về các sản phẩm bảo hiểm du lịch từ một đại lý du lịch, với biến mục tiêu là \\(product.name\\) cho biết khách hàng đã lựa chọn sản phẩm sản phẩm bảo hiểm nào. Biến mục tiêu có bốn giá trị tương ứng với bốn loại sản phẩm, mặc dù các sản phẩm này có mức giá khác nhau nhưng việc sánh các giá trị là không có ý nghĩa. Dữ liệu có 10 biến độc lập, tuy nhiên, để đơn giản hóa, chúng ta chỉ lấy hai biến là giới tính và độ tuổi của người tham gia bảo hiểm.Mối liên hệ giữa độ tuổi và giới tính của khách hàng đến sản phẩm khách hàng lựa chọn được mô tả qua hình vẽ dưới đâyCó thể thấy rằng có sự khác biệt về độ tuổi của những người lựa chọn các sản phẩm bảo hiểm: những người trẻ tuổi có xu hướng lựa chọn “Bronze Plan” và “Silver Plan” trong khi những người lớn tuổi có xu hướng lựa chọn “Basic Plan” và “Value Plan”. Có sự khác biệt về tỷ lệ nam và nữ khi lựa chọn sản phẩ bảo hiểm, tỷ lệ nam giới lựa chọn “Basic Plan” và “Value Plan” cao, trong khi nữ giới có xu hướng lựa chọn “Bronze Plan” và “Silver Plan”.Để xây dựng mô hình tuyến tính tổng quát cho biến phân loại danh nghĩa, chúng ta sử dụng hàm multinom() từ thư viện nnet:Dựa trên kết quả ước lượng, chúng ta có công thức tính xác suất chấp nhận các sản phẩm bảo hiểm du lịch của một người có giới tính là \\(g_i\\) và độ tuổi \\(a_i\\) như sau: với sản phẩm “Basic Plan”\n\\[\\begin{align}\n\\mathbb{P}(Y = Basic|g_i, a_i) = \\cfrac{1}{S_0}\n\\end{align}\\]\nvới\n\\[\\begin{align}\nS_0 = 1 + exp(1.512 - 0.558 \\cdot g_i - 0.038 \\cdot a_i) + exp(0.563 - 0.489 \\cdot g_i - 0.030 \\cdot a_i) +  exp(-3.028 - 0.246 \\cdot g_i - 0.039 \\cdot a_i)\n\\end{align}\\]\nVới các sản phẩm Bronze Plan, Silver Plan và Value Plan, ta có\n\\[\\begin{align}\n& \\mathbb{P}(Y = Bronze|g_i, a_i) = \\cfrac{exp(1.512 - 0.558 \\cdot g_i - 0.038 \\cdot a_i)}{S_0} \\\\\n& \\mathbb{P}(Y = Silver|g_i, a_i) = \\cfrac{exp(0.563 - 0.489 \\cdot g_i - 0.030 \\cdot a_i)}{S_0} \\\\\n& \\mathbb{P}(Y = Value|g_i, a_i) = \\cfrac{exp(-3.028 - 0.246 \\cdot g_i - 0.039 \\cdot a_i)}{S_0}\n\\end{align}\\]Kết quả từ mô hình cho thấy chỉ có “Basic Plan” và “Bronze Plan” được chọn nếu chúng ta chỉ sử dụng hai biến độc lập là giới tính và độ tuổi. Khả năng dự đoán của mô hình không cao mô hình còn quá đơn giản, chúng tôi muốn nhấn mạnh về cách xây dựng mô hình trước khi cố gắng xây dựng một mô hình có khả năng dự báo tốt. Chúng ta sẽ thảo luận về đánh giá hiệu quả của mô hình ở các phần sau của chương sách.","code":"## # weights:  16 (9 variable)\n## initial  value 19625.769270 \n## iter  10 value 18535.510505\n## final  value 18040.676017 \n## converged## Call:\n## multinom(formula = Product.Name ~ Gender + Age, data = dat1)\n## \n## Coefficients:\n##             (Intercept)    GenderM          Age\n## Bronze Plan   1.5631988 -0.5589519 -0.039513361\n## Silver Plan   0.6512458 -0.4871457 -0.031596816\n## Value Plan   -1.0970424  0.3380718  0.002018711\n## \n## Std. Errors:\n##             (Intercept)    GenderM         Age\n## Bronze Plan  0.07047991 0.04302279 0.001653049\n## Silver Plan  0.08280363 0.05117056 0.001938850\n## Value Plan   0.08264747 0.05092679 0.001676413\n## \n## Residual Deviance: 36081.35 \n## AIC: 36099.35"},{"path":"mô-hình-tuyến-tính-tổng-quát..html","id":"biến-phụ-thuộc-là-biến-rời-rạc-có-thứ-tự.","chapter":"Chương 9 Mô hình tuyến tính tổng quát.","heading":"9.2.3 Biến phụ thuộc là biến rời rạc có thứ tự.","text":"Biến rời rạc có thứ tự, còn gọi là ordinal categorical variable, là các biến nhận giá trị rời rạc mà các giá trị rời rạc có thể sánh được với nhau. Một ví dụ điển hình cho biến rời rạc có thứ tự là số lần mà một người đi khám chữa bệnh sử dụng bảo hiểm y tế hoặc một khách hàng gửi yêu cầu bồi thường đến công ty bảo hiểm. Đây là trường hợp mà chúng ta có thể sử dụng một phân phối xác suất rời rạc có tham số để mô tả biến phụ thuộc \\(Y\\) giống như mô hình (??).","code":""},{"path":"mô-hình-tuyến-tính-tổng-quát..html","id":"biến-phụ-thuộc-có-phân-phối-poisson.","chapter":"Chương 9 Mô hình tuyến tính tổng quát.","heading":"9.2.3.1 Biến phụ thuộc có phân phối Poisson.","text":"Phân phối rời rạc thường được lựa chọn cho biến phụ thuộc rời rạc có thứ tự là phân phối Poisson. Hàm phân phối xác suất của biến ngẫu nhiên \\(Y\\) có phân phối Poisson với tham số \\(\\lambda > 0\\), ký hiệu \\(Y \\sim \\mathcal{P}(\\lambda)\\) được cho bởi công thức sau\n\\[\\begin{align}\n\\mathbb{P}(Y = y) = e^{-\\lambda} \\cdot \\cfrac{\\lambda^y}{y!} \\text{ với } y = 0, 1, 2, \\cdots\n\\end{align}\\]Phân phối Poisson thường được sử dụng để mô tả số lần một hiện tượng xảy ra trong một khoảng thời gian nhất định. Nguyên nhân là phân phối Poisson này có mối liên hệ trực tiếp đến các mô hình có thời gian chờ có phân phối kiểu mũ. Thật vậy, nếu thời gian chờ giữa hai sự kiện liên tiếp xảy ra của một hiện tượng nào đó là một biến ngẫu nhiên liên tục có hàm phân phối xác suất kiểu mũ với tham số \\(\\gamma\\) thì số lần hiện tượng đó xảy ra trong một khoảng thời gian từ \\(t_1\\) đến \\(t_2\\) sẽ là một biến ngẫu nhiên phân phối Poisson với tham số \\(\\lambda = \\cfrac{(t_1 - t_2)}{\\gamma}\\). Thật vậy, với \\(T\\) là khoảng thời gian giữa 2 sự kiện liên tiếp xảy ra và \\(T\\) có phân phối mũ:\n\\[\\begin{align}\n\\mathbb{P}(T \\leq x) = 1 - exp(-\\gamma x)\n\\end{align}\\]\nvà nếu \\(N\\) là số lần xảy ra sự kiện (đi khám bệnh, lái xe gây ra tai nạn) giữa hai mốc thời gian \\(t_1 < t_2\\) thì \\(N\\) sẽ có phân phối Poisson với tham số \\(\\lambda\\):\n\\[\\begin{align}\n& \\mathbb{P}(N = k) = e^{-\\lambda} \\cdot \\cfrac{\\lambda^k}{k!}\\\\\n& \\lambda = \\cfrac{(t_1 - t_2)}{\\gamma}\n\\end{align}\\]Biến ngẫu nhiên có phân phối Poisson với tham số \\(\\lambda\\), ký hiệu \\(\\mathcal{P}(\\lambda)\\), có tính chất là giá trị trung bình và phương sai đều bằng tham số của biến đó là \\(\\lambda\\). Phân phối Poisson nằm trong họ các phân phối mũ nên sẽ rất thuận tiện trong xây dựng và ước lượng mô hình bằng phương pháp hợp lý tối đa. Ngoài ra, bằng cách cho tham số của phân phối Poisson một phân phối xác suất, chúng ta có thể thu được các phân phối rời rạc linh hoạt hơn trong mô tả các biến ngẫu nhiên dạng đếm được.Khi xây dựng mô hình tuyến tính với biến mục tiêu \\(Y\\) có phân phối Poisson, giá trị trung bình \\(Y\\) nhận giá trị dương nên chúng ta cần chọn các hàm liên kết \\(g\\) có miền xác định là tập các số thực dương \\(\\mathbb{R}^+\\) và miền giá trị là tập số thực \\(\\mathbb{R}\\). Hàm số thường được lựa chọn là hàm \\(log\\).Chúng ta có thể viết mô hình tuyến tính tổng quát khi \\(Y\\) có phân phối Poisson, thường được gọi tắt là hồi quy Poisson, như sau\\[\\begin{align}\n& Y_i \\sim \\mathcal{P}(\\lambda_i) \\\\\n& log(\\lambda_i) = \\left(\\beta_0 + \\beta_1 \\cdot x_{,1} + \\beta_2 \\cdot x_{,2} + \\cdots + \\beta_p \\cdot x_{,p}\\right) \\\\\n\\tag{9.17}\n\\end{align}\\]Dữ liệu được sử dụng để mô tả mô hình tuyến tính tổng quát với biến phụ thuộc phân phối Poisson là dữ liệu \\(SingaporeAuto.csv\\). Dữ liệu được tổng hợp bởi Hiệp hội bảo hiểm Singapore trong năm 1993 mô tả số vụ tai nạn ô tô xảy ra cùng với các đặc điểm của người lái và đặc điểm của xe gây tai nạn. Cũng giống như các phần trước, chúng ta sẽ xây dựng mô hình ở mức độ đơn giản nhất để bạn đọc dễ dàng hình dung. Biến phụ thuộc trong mô hình là biến \\(Clm\\_Count\\) cho biết số vụ tai nạn mà một lái xe gây ra trong vòng một năm, hai biến phụ thuộc bao gồm có:Biến \\(PC\\) là biến nhận hai giá trị là 0 tương ứng với xe được đăng ký theo công ty và nhận giá trị 1 tương ứng với xe được đăng ký theo cá nhân.Biến \\(PC\\) là biến nhận hai giá trị là 0 tương ứng với xe được đăng ký theo công ty và nhận giá trị 1 tương ứng với xe được đăng ký theo cá nhân.Biến \\(NCD\\), viết tắt của Claims Discount, cho biết lịch sử gây ra tai nạn của lái xe. Giá trị \\(NCD\\) càng cao nghĩa là lịch sử người lái xe càng gây ra ít tai nạn.Biến \\(NCD\\), viết tắt của Claims Discount, cho biết lịch sử gây ra tai nạn của lái xe. Giá trị \\(NCD\\) càng cao nghĩa là lịch sử người lái xe càng gây ra ít tai nạn.Chúng ta sử dụng hàm glm() để xây dựng và ước lượng mô hình tuyến tính tổng quát:Bạn đọc có thể thấy rằng cả hai biến \\(PC\\) và \\(NCD\\) đều có tác động đến giá trị trung bình của biến mục tiêu \\(Clm\\_Count\\) các giá trị p-value đều nhỏ. Mối liên hệ giữa số lượng tai nạn xảy ra và các biến \\(PC\\) và \\(NCD\\) được mô tả như sau:\\[\\begin{align}\n& Clm\\_Count_i \\sim \\mathcal{P}(\\lambda_i) \\\\\n& \\log(\\lambda_i) = -2.641 + 0.432 \\cdot PC_{} - 0.013 \\cdot NCD_{} \\\\\n& \\lambda_i = \\exp\\left(-2.641 + 0.432 \\cdot PC_{} - 0.013 \\cdot NCD_{} \\right)\n\\tag{9.18}\n\\end{align}\\]Hệ số của biến \\(PC\\) là số dương, điều này cho biết các xe đăng ký dưới dạng cá nhân có khả năng gây tai nạn cao hơn với xe đăng ký dưới hình thức doanh nghiệp. Đồng thời, hệ số của biến \\(NCD\\) âm cho biết lái xe có lịch sử lái xe tốt, tương ứng với \\(NCD\\) cao, ít có khả năng gây ra tai nạn hơn lái xe có lịch sử lái xe không tốt, tương ứng với \\(NCD\\) thấp.","code":"\ndat<-read.csv(\"../KHDL_KTKD Final/Dataset/SingaporeAuto.csv\")\nglm2<-glm(Clm_Count~PC+NCD, data=dat, family = poisson(link = \"log\"))\nsummary(glm2)## \n## Call:\n## glm(formula = Clm_Count ~ PC + NCD, family = poisson(link = \"log\"), \n##     data = dat)\n## \n## Deviance Residuals: \n##     Min       1Q   Median       3Q      Max  \n## -0.4685  -0.3801  -0.3520  -0.3283   4.1716  \n## \n## Coefficients:\n##              Estimate Std. Error z value Pr(>|z|)    \n## (Intercept) -2.641681   0.073836 -35.778  < 2e-16 ***\n## PC           0.431980   0.091840   4.704 2.56e-06 ***\n## NCD         -0.013943   0.002618  -5.327 9.99e-08 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## (Dispersion parameter for poisson family taken to be 1)\n## \n##     Null deviance: 2887.2  on 7482  degrees of freedom\n## Residual deviance: 2848.3  on 7480  degrees of freedom\n## AIC: 3849.4\n## \n## Number of Fisher Scoring iterations: 6"},{"path":"mô-hình-tuyến-tính-tổng-quát..html","id":"phân-phối-rời-rạc-có-lạm-phát-tại-giá-trị-0.","chapter":"Chương 9 Mô hình tuyến tính tổng quát.","heading":"9.2.3.2 Phân phối rời rạc có lạm phát tại giá trị 0.","text":"Biến phụ thuộc dạng đếm chứa một tỷ lệ lớn giá trị bằng 0 là các kiểu biến phụ thuộc thường gặp phải khi làm việc trên dữ liệu trong lĩnh vực bảo hiểm. Tỷ lệ lớn ở đây thường được hiểu là trên 90% giá trị quan sát được. Đa số các phân phối xác suất rời rạc thông thường, bao gồm phân phối Poisson, không mô tả tốt khi biến phụ thuộc \\(Y\\) trong trường hợp này.Một giải pháp phổ biến khi làm việc với kiểu dữ liệu như vậy là thay đổi phân phối dạng đếm thông thường để làm tăng tỷ lệ giá trị nhận được tại 0, để thu được các phân phối thường được gọi là các phân phối lạm phát tại 0, hay Zero-inflated variable. Phân phối lạm phát tại 0 là hỗn hợp của hai phân phối xác suất rời rạc, bao gồm một phân phối nhị thức chỉ báo cho trường hợp 0, và một phân phối xác suất dành cho biến đếm thông thường. Hàm phân phối của biến ngẫu nhiên dạng đếm có lạm phát tại 0, ký hiệu \\(ZI_Y\\), có thể được mô tả như sau\n\\[\\begin{align}\n\\mathbb{P}(ZI_Y = y) = \\begin{cases}\n& \\omega + (1-\\omega) \\cdot \\mathbb{P}(Y=0) \\text{ khi y = 0} \\\\\n& (1-\\omega) \\cdot \\mathbb{P}(Y=y) \\text{ khi y > 0}\n\\end{cases}\n\\tag{9.19}\n\\end{align}\\]\ntrong đó biến ngẫu nhiên \\(Y\\) tuân theo phân phối số đếm tiêu chuẩn được. Trong trường hợp tham số \\(\\omega\\) bằng 0, phân phối của biến ngẫu nhiên \\(ZI_Y\\) sẽ tương ứng tương ứng với phân phối của biến Y.Tất cả các phân phối kiểu đếm đều có thể được sử dụng để tạo ra các biến mới có lạm phát tại 0. Trong các mô hình tuyến tính tổng quát, biến dạng đếm thường được mô tả bằng phân phối cổ điển Poisson. Với việc sử dụng phương trình (9.19), hàm phân phối của biến ngẫu nhiên Poisson có lạm phát tại 0, ký hiệu là \\(ZI_\\mathcal{P}\\) được cho bởi công thức sau:\\[\\begin{align}\n\\mathbb{P}(ZI_\\mathcal{P} = y) = \\begin{cases}\n& \\omega + (1-\\omega) \\cdot e^{-\\lambda} \\text{ khi y = 0} \\\\\n& (1-\\omega) \\cdot e^{-\\omega} \\cdot \\cfrac{\\lambda^y}{y!} \\text{ khi y > 0}\n\\end{cases}\n\\tag{9.20}\n\\end{align}\\]Chúng ta có thể xác định giá trị trung bình và phương sai của phân phối \\(ZI_Y\\) dựa trên tham số \\(\\omega\\) và giá trị trung bình cũng như phương sai của biến \\(Y\\) như sau\n\\[\\begin{align}\n& \\mathbb{E}(ZI_Y) = (1-\\omega) \\cdot \\mathbb{E}(Y) \\\\\n& \\mathbb{V}(ZI_Y) = (1-\\omega) \\cdot \\mathbb{V}(Y) + \\omega(1-\\omega) \\cdot \\mathbb{E}(Y)^2\n\\tag{9.21}\n\\end{align}\\]Trong trường hợp \\(Y\\) có phân phối Poission, chúng ta có giá trị trung bình và phương sai của biến \\(ZI_\\mathcal{P}\\):\n\\[\\begin{align}\n& \\mathbb{E}(ZI_\\mathcal{P}) = (1-\\omega) \\cdot \\lambda \\\\\n& \\mathbb{V}(ZI_\\mathcal{P}) = \\mathbb{E}(ZI_\\mathcal{P}) \\cdot (1 + \\lambda - \\mathbb{E}(ZI_\\mathcal{P}))\n\\tag{9.22}\n\\end{align}\\]Với giá trị trung bình và phương sai của biến \\(ZI_\\mathcal{P}\\) như phương trình (9.19), chúng ta có thể xây dựng mô hình tuyến tính tổng quát với biến phụ thuộc là \\(ZI_\\mathcal{P}\\) như sau: giá trị trung bình \\(\\left((1-\\omega) \\cdot \\lambda\\right)\\) sẽ được giải thích thông qua các biến phụ thuộc trong khi tham số \\(\\lambda\\) sẽ được ước lượng bằng phương pháp hợp lý tối đa.Biến phụ thuộc phân phối \\(ZI_\\mathcal{P}\\) sẽ hữu ích cho mục đích lập mô hình trong trường hợp dữ liệu quan sát có sự tập trung quá mức tại giá trị 0. Ngoài phân phối Poisson, các phân phối mở rộng từ phân phối Poisson cũng có thể được sử dụng với để tạo ra các phân phối có lạm phát tại 0, chẳng hạn như biến ngẫu nhiên phân phối Poisson - Gamma, Poisson - Inverse gaussian.Trong các nghiên cứu thực nghiệm, nhiều tác giả đã chứng minh rằng việc áp dụng phân phối có lạm phát tại 0 để mô hình hóa số lượng yêu cầu bồi thường bảo hiểm là phù hợp để mô tả hành vi của người được bảo hiểm. Thật vậy, trong ngành bảo hiểm không phải tất cả các vụ tai nạn đều được báo cáo, công ty bảo hiểm chỉ có thông tin về các yêu cầu bồi thường được báo cáo. Có hai cách để giải thích cách phân phối có lạm phát tại 0 như sau: (1) Một số người được bảo hiểm không gửi yêu cầu bồi thường dù có xảy ra sự kiện bảo hiểm, họ không có nhận thức được về việc được bảo hiểm, hoặc không có nhu cầu gửi yêu cầu bảo hiểm. (2) Một cách giải thích khác của mô hình lạm phát tại 0 là xem xét xác suất của mỗi vụ tai nạn được báo cáo. Một hành vi thực tế của người được bảo hiểm là nếu họ đã báo cáo vụ tai nạn đầu tiên thì những vụ tai nạn tiếp theo cũng sẽ được báo cáo, còn nếu vụ tai nạn đầu tiên không được báo cáo thì các vụ tai nạn sau sẽ không được báo cáo. Cả hai cách giải thích dựa trên hành vi này đều dẫn đến việc số lượng biến mục tiêu nhận giá trị bằng 0 cao hơn với số lượng tai nạn thực tế xảy ra.\nTable 9.1: Khác nhau giữa GLM - Poisson và GLM - ZIP\nĐể xây dựng mô hình tuyến tính tổng quát trong đó biến phụ thuộc có phân phối \\(ZI_\\mathcal{P}\\) chúng ta sử dụng hàm số zeroinfl() của thư viện \\(pscl\\).Chúng ta có kết quả ước lượng các mô hình\nTable 9.2: Kết quả ước lượng GLM - Poisson và GLM - ZIP\nCó thể thấy rằng mô hình tuyến tính tổng quát với biến phụ thuộc phân phối \\(ZI_\\mathcal{P}\\) có giá trị hàm hợp lý tối đa lớn hơn, điều này cũng có nghĩa là phân phối \\(ZI_\\mathcal{P}\\) phù hợp hơn phân phối Poisson thông thường khi mô tả biến phụ thuộc trong dữ liệu. Dựa vào kết quả ước lượng từ hai mô hình, chúng ta có thể tính toán xác suất không có tai nạn và xác suất để xảy ra một tai nạn theo độ tuổi và giới tính của người tham gia bảo hiểm như sauCó thể nhận thấy rằng xác suất mà một người được bảo hiểm không để xảy ra tai nạn trong mô hình tuyến tính tổng quát với biến phụ thuộc có phân phối \\(ZI_\\mathcal{P}\\) là luôn cao hơn với mô hình tuyến tính tổng quát với biến phụ thuộc có phân phối Poisson thông thường. Đồng thời, mô hình tuyến tính tổng quát với biến phụ thuộc có phân phối \\(ZI_\\mathcal{P}\\) cho kết quả xác suất xảy ra đúng một tai nạn thấp hơn với mô hình có biến phụ thuộc có phân phối Poisson thông thường.","code":"\ndat<-read.csv(\"../KHDL_KTKD Final/Dataset/exposure.csv\")\n# Biến phụ thuộc có phân phối Poisson\nglm1<-glm(Claim_Count~Age+Gender,\n          family = poisson(link = \"log\"),\n          data=dat)\n\n# Biến phụ thuộc có phân phối Zero inflated poisson\nzip.glm<-zeroinfl(Claim_Count~Age+Gender|1,\n                  dist = 'poisson',\n                  link = \"log\",\n                  data = dat)\nsummary(glm1)\nsummary(zip.glm)"},{"path":"mô-hình-tuyến-tính-tổng-quát..html","id":"biến-phụ-thuộc-có-phân-phối-liên-tục.","chapter":"Chương 9 Mô hình tuyến tính tổng quát.","heading":"9.2.4 Biến phụ thuộc có phân phối liên tục.","text":"Một giả thiết quan trọng của mô hình tuyến tính tổng quát là biến phụ thuộc nằm trong họ các biến ngẫu nhiên có phân phối kiểu mũ, hay exponential family. Chúng ta sẽ thảo luận kỹ hơn về họ các biến ngẫu nhiên có phân phối kiểu mũ trong phần sau của cuốn sách. Lưu ý rằng họ các biến ngẫu nhiên có phân phối kiểu mũ không tương đồng với khái niệm biến ngẫu nhiên có phân phối mũ. Phân phối mũ chỉ là mộ trường hợp đặc biệt của phân phối kiển mũ. Có nhiều biến ngẫu nhiên liên tục khác có phân phối nằm trong họ các phân phối kiểu mũ. Có thể kể đến như phân phối gamma, phân phối chuẩn, phân phối chuẩn ngược…Các bước để xây dựng mô hình tuyến tính tổng quát trong trường hợp biến phụ thuộc \\(Y\\) là biến ngẫu nhiên liên tục hoàn toàn tương tự như cách xây dựng mô hình tuyến tính tổng quát ở trên, bao gồm bước chọn phân phối xác suất cho biến mục tiêu và lựa chọn hàm liên kết phù hợp. Chúng ta sẽ tiếp tục xây dựng mô hình với dữ liệu “exposure.csv” đã đề cập ở các phần trên. Biến mục tiêu không còn là số lần khách hàng gửi yêu cầu bồi thường, mà là số tiền trung bình mỗi lần khách hàng gửi yêu cầu (biến \\(Ave\\_Amount\\)). Các biến giải thích vẫn tiếp tục là giới tính (\\(Gender\\)) và độ tuổi (\\(Age\\)) của người được bảo hiểm.Mối liên hệ giữa \\(Ave\\_Amount\\) được thể hiện qua đồ thị dưới đâyĐồ thị bên trái cho thấy số tiền yêu cầu bồi thường trung bình của nữ là cao hơn nam giới. Phân phối xác suất của số tiền yêu cầu bồi thương trung bình là phân phối liên tục lệch phải, có đuôi bên phải lớn. Đồ thị bên phải cho thấy số tiền yêu cầu bồi thường trung bình có xu hướng tăng theo độ tuổi.Số tiền bảo hiểm trung bình là số dương nên chúng ta sẽ sử dụng phân phối \\(gamma\\) với hàm liên kết là hàm log(). Lưu ý rằng phân phối gamma() là phân phối có hai tham số, thường được ký hiệu là \\(\\alpha\\) và \\(\\gamma\\), với hàm mật độ xác suất, giá trị trung bình, và phương sai như sau\n\\[\\begin{align}\n& f_Y(y) = \\cfrac{\\beta^\\alpha}{\\Gamma(\\alpha)} \\ y^{\\alpha-1} \\ e^{-\\gamma y} \\\\\n& \\mathbb{E}(X) = \\cfrac{\\alpha}{\\gamma} \\\\\n& \\mathbb{V}(X) = \\cfrac{\\alpha}{\\gamma^2}\n\\end{align}\\]Khi phân phối \\(gamma\\) được sử dụng cho biến phụ thuộc, giá trị trung bình được tính bằng \\(\\alpha/\\gamma\\), sẽ được mô tả thông qua các biến độc lập trong khi tham số \\(\\alpha\\) sẽ được ước lượng dựa trên hàm hợp lý tối đa:Ngoài các hệ số của các biến độc lập trong mô hình tuyến tính tổng quát, hàm glm() còn cung cấp giá trị ước lượng được cho tham số \\(\\phi\\) (disperson paramter hay tham số phân tán) là \\(0.924\\). Tham số phân tán được định nghĩa trong họ các phân phối kiểu mũ sẽ được thảo luận trong phần tiếp theo. Đối với phân phối \\(gamma\\), tham số phân tán được tính bằng \\(\\phi = 1/\\alpha\\).Giá trị số tiền bồi thường trung bình được được mô tả bằng mô hình tuyến tính tổng quát như sau:\n\\[\\begin{align}\n& Y_i \\sim Gamma(\\mu_i = \\alpha_i/\\gamma_i , \\phi_i = 1/\\alpha_i) \\\\\n& \\phi_i = 0.924 \\\\\n& log(\\mu_i) = -0.0475 + 0.079 \\cdot Age_i + 0.534 \\cdot Gender_i\n\\end{align}\\]Trước khi đi vào chi tiết các thành phần của mô hình, chúng tôi muốn kết luận rằng mô hình tuyến tính tổng quát có thể được sử dụng để mô hình hóa dữ liệu trong nhiều hoàn cảnh khác nhau. Việc xây dựng mô hình tuyến tính tổng quát luôn được bắt đầu bằng việc lựa chọn phân phối cho biến phụ thuộc và lựa chọn hàm số liên kết. Có các quy tắc chung trong việc lựa chọn phân phối và hàm liên kết giống như chúng tôi đã trình bày ở phần trên. Tuy nhiên để đưa ra được mô hình tuyến tính tổng quát phù hợp cho từng dữ liệu cụ thể, hoặc để có thể mở rộng được mô hình tuyến tính tổng quát trong các kiểu dữ liệu phức tạp hơn, bạn đọc cần hiểu sâu hơn về các đặc điểm kỹ thuật của các mô hình này. Các kiến thức này sẽ được trình bày trong các phần tiếp theo của chương.","code":"\ndat<-read.csv(\"../KHDL_KTKD Final/Dataset/exposure.csv\")\ndat<-mutate(dat,Ave_Amount = ifelse(Claim_Count>0,Total_Claim/Claim_Count,0))\ndat1<-filter(dat,Claim_Count>0)\ndat1$Gender<-ifelse(dat1$Gender==0,\"F\",\"M\")\np1<-dat1%>%ggplot()+geom_boxplot(aes(Gender,y = Ave_Amount))+\n  ylim(0,200)+ggtitle(\"Số tiền yêu cầu bồi thường trung bình và giới tính\")\np2<-dat1%>%ggplot(aes(x=Age,y = Ave_Amount))+geom_point(alpha=0.2)+\n  geom_smooth(col=\"black\", size = 1, se = FALSE)+ylim(0,200)+\n  ggtitle(\"Số tiền yêu cầu bồi thường trung bình và độ tuổi\")\ngrid.arrange(p1,p2,ncol=2)\ndat<-read.csv(\"../KHDL_KTKD Final/Dataset/exposure.csv\")\ndat<-mutate(dat,Ave_Amount = ifelse(Claim_Count>0,Total_Claim/Claim_Count,0))\ndat1<-filter(dat,Claim_Count>0)\nglm3<-glm(Ave_Amount~Age+Gender, family = Gamma(link = \"log\"),data = dat1)\nsummary(glm3)## \n## Call:\n## glm(formula = Ave_Amount ~ Age + Gender, family = Gamma(link = \"log\"), \n##     data = dat1)\n## \n## Deviance Residuals: \n##     Min       1Q   Median       3Q      Max  \n## -4.3309  -0.9498  -0.3047   0.3360   2.9954  \n## \n## Coefficients:\n##              Estimate Std. Error t value Pr(>|t|)    \n## (Intercept) -0.047535   0.065517  -0.726    0.468    \n## Age          0.079089   0.001534  51.563   <2e-16 ***\n## Gender      -0.534010   0.041951 -12.730   <2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## (Dispersion parameter for Gamma family taken to be 0.9241118)\n## \n##     Null deviance: 4666.5  on 2110  degrees of freedom\n## Residual deviance: 2205.7  on 2108  degrees of freedom\n## AIC: 15710\n## \n## Number of Fisher Scoring iterations: 6"},{"path":"mô-hình-tuyến-tính-tổng-quát..html","id":"các-thành-phần-của-mô-hình-tuyến-tính-tổng-quát.","chapter":"Chương 9 Mô hình tuyến tính tổng quát.","heading":"9.3 Các thành phần của mô hình tuyến tính tổng quát.","text":"Như chúng ta đã thảo luận trong phần trước, mô hình tuyến tính tổng quát khắc phục hai giả thiết, cũng là hai nhược điểm của mô hình tuyến tính thông thường, đó là ) biến phụ thuộc có phân phối chuẩn, và ii) giá trị trung bình của biến phụ thuộc bằng một tổ hợp tuyến tính của các biến độc lập. Trong phần này của cuốn sách, chúng tôi sẽ thảo luận kỹ hơn vào cách tiếp cận để khắc phục các hạn chế kể trên.Thứ nhất, thay vì sử dụng phân phối chuẩn, mô hình tuyến tính tổng quát giả thiết rằng biến phụ thuộc có phân phối nằm trong họ các phân phối kiểu mũ (exponential family). Phân phối chuẩn chỉ là một trường hợp đặc biệt của họ các phân phối này.Thứ nhất, thay vì sử dụng phân phối chuẩn, mô hình tuyến tính tổng quát giả thiết rằng biến phụ thuộc có phân phối nằm trong họ các phân phối kiểu mũ (exponential family). Phân phối chuẩn chỉ là một trường hợp đặc biệt của họ các phân phối này.Thứ hai, mô hình tuyến tính tổng quát sử dụng một hàm \\(g\\), được gọi là hàm liên kết, để mô tả mối liên hệ giữa giá trị trung bình của biến phụ thuộc với tổ hợp tuyến tính của các biến độc lập. Lựa chọn hàm \\(g\\) có ý nghĩa quan trọng không chỉ trong cách luận giải kết quả của mô hình, mà còn ở việc dễ dàng ước lượng tham số của mô hình.Thứ hai, mô hình tuyến tính tổng quát sử dụng một hàm \\(g\\), được gọi là hàm liên kết, để mô tả mối liên hệ giữa giá trị trung bình của biến phụ thuộc với tổ hợp tuyến tính của các biến độc lập. Lựa chọn hàm \\(g\\) có ý nghĩa quan trọng không chỉ trong cách luận giải kết quả của mô hình, mà còn ở việc dễ dàng ước lượng tham số của mô hình.","code":""},{"path":"mô-hình-tuyến-tính-tổng-quát..html","id":"họ-các-biến-ngẫu-nhiên-có-phân-phối-kiểu-mũ.","chapter":"Chương 9 Mô hình tuyến tính tổng quát.","heading":"9.3.1 Họ các biến ngẫu nhiên có phân phối kiểu mũ.","text":"Thay vì giả thiết rằng \\(Y\\) có phân phối chuẩn trong mô hình tuyến tính thông thường, mô hình tuyến tính tổng quát giả thiết biến phụ thuộc \\(Y\\) nằm trong họ các biến ngẫu nhiên có phân phối kiểu mũ. Họ các biến ngẫu nhiên có phân phối kiểu mũ (exponential family) có hàm mật độ xác suất có thể viết dưới dạng như sau:\n\\[\\begin{align}\nf(y;\\theta,\\phi) = \\exp\\left[ \\cfrac{y \\theta - b(\\theta)}{(\\phi)} + c(y,\\phi) \\right]\n\\end{align}\\]\ntrong đóTham số \\(\\theta\\) được gọi là tham số \\(chính\\) \\(tắc\\) của phân phối kiểu mũ.Tham số \\(\\phi\\) được gọi là tham số \\(phân\\) \\(tán\\). Nguyên nhân là giá trị trung bình của biến ngẫu nhiên \\(Y\\) không phụ thuộc vào \\(\\phi\\). Tham số \\(\\phi\\), mà tổng quát hơn là hàm \\((\\phi)\\) sẽ xác định phương sai của biến phụ thuộc.Các hàm số \\(b(\\theta)\\), \\((\\phi)\\) và \\(c(y,\\phi)\\) sẽ quyết định kiểu phân phối của biến phụ thuộc.Giá trị trung bình và phương sai của biến phụ thuộc \\(Y\\) được cho bởi các công thức sau:\n\\[\\begin{align}\n& \\mathbb{E}(Y) = b^{'}(\\theta) \\\\\n& \\mathbb{V}(Y) = (\\phi) \\cdot b^{''}(\\theta)\n\\tag{9.23}\n\\end{align}\\]\nvới \\(b^{'}(\\theta)\\) và \\(b^{''}(\\theta)\\) lần lượt là đạo hàm bậc một và đạo hàm bậc hai của hàm số \\(b\\) theo biến \\(\\theta\\).Họ các biến ngẫu nhiên có phân phối kiểu mũ bao gồm đa số các biến ngẫu nhiên liên tục thông thường như phân phối chuẩn, phân phối mũ, phân phối Gamma, phân phối chuẩn ngược. Các biến ngẫu nhiên phân phối rời rạc như phân phối nhị thức, phân phối binomial, hoặc phân phối Poisson cũng nằm trong họ các biến ngẫu nhiên có phân phối kiểu mũ.Ví dụ 1: phân phối Poisson thường được sử dụng để mô tả phân phối của biến đếm trong mô hình tuyến tính tổng quát. Hàm phân phối của biến ngẫu nhiên Poisson với tham số \\(\\lambda\\), ký hiệu \\(\\mathcal{P}(\\lambda)\\), được cho bởi công thức\n\\[\\begin{align}\n\\mathbb{P}(Y = y; \\lambda) = exp(-\\lambda) \\ \\cfrac{\\lambda^y}{y!}\n\\end{align}\\]\nChúng ta có thể viết phân phối \\(\\mathcal{P}(\\lambda)\\) dưới dạng phân phối mũ như sau\n\\[\\begin{align}\n\\mathbb{P}(Y = y; \\theta) = exp\\left[ \\cfrac{\\theta y - exp(\\theta)}{1} - log(\\Gamma(y+1)) \\right] \\text{ với } \\lambda = exp(\\theta)\n\\end{align}\\]\nĐây là hàm phân phối của biến ngẫu nhiên nằm trong họ các phân phối kiểu mũ với \\((\\phi) = 1\\); \\(b(\\theta) = exp(\\theta)\\) và \\(c(y,\\phi) = log\\left(\\Gamma(y+1)\\right)\\). Bạn đọc có thể tính toán trung bình và phương sai của phân phối \\(\\lambda\\) dựa theo công thức (9.23)\n\\[\\begin{align}\n& \\mathbb{E}(Y) = b^{'}(\\theta) = exp(\\theta) = \\lambda \\\\\n& \\mathbb{V}(Y) = (\\phi) \\cdot b^{''}(\\theta) = 1 \\cdot exp(\\theta) = \\lambda\n\\end{align}\\]Ví dụ 1: phân phối Poisson thường được sử dụng để mô tả phân phối của biến đếm trong mô hình tuyến tính tổng quát. Hàm phân phối của biến ngẫu nhiên Poisson với tham số \\(\\lambda\\), ký hiệu \\(\\mathcal{P}(\\lambda)\\), được cho bởi công thức\n\\[\\begin{align}\n\\mathbb{P}(Y = y; \\lambda) = exp(-\\lambda) \\ \\cfrac{\\lambda^y}{y!}\n\\end{align}\\]\nChúng ta có thể viết phân phối \\(\\mathcal{P}(\\lambda)\\) dưới dạng phân phối mũ như sau\n\\[\\begin{align}\n\\mathbb{P}(Y = y; \\theta) = exp\\left[ \\cfrac{\\theta y - exp(\\theta)}{1} - log(\\Gamma(y+1)) \\right] \\text{ với } \\lambda = exp(\\theta)\n\\end{align}\\]\nĐây là hàm phân phối của biến ngẫu nhiên nằm trong họ các phân phối kiểu mũ với \\((\\phi) = 1\\); \\(b(\\theta) = exp(\\theta)\\) và \\(c(y,\\phi) = log\\left(\\Gamma(y+1)\\right)\\). Bạn đọc có thể tính toán trung bình và phương sai của phân phối \\(\\lambda\\) dựa theo công thức (9.23)\n\\[\\begin{align}\n& \\mathbb{E}(Y) = b^{'}(\\theta) = exp(\\theta) = \\lambda \\\\\n& \\mathbb{V}(Y) = (\\phi) \\cdot b^{''}(\\theta) = 1 \\cdot exp(\\theta) = \\lambda\n\\end{align}\\]Ví dụ 2: phân phối chuẩn là một biến ngẫu nhiên nằm trong họ các phân phối kiểu mũ. Thật vậy, chúng ta có hàm mật độ của biến ngẫu nhiên phân phối chuẩn với trung bình \\(\\mu\\) và độ lệch chuẩn \\(\\sigma\\), ký hiệu \\(\\mathcal{N}(\\mu,\\sigma)\\), như sau\n\\[\\begin{align}\nf(y,\\mu,\\sigma) = \\cfrac{1}{\\sqrt{2 \\pi \\sigma^2}} \\exp\\left[ \\cfrac{-(y - \\mu)^2}{2 \\ \\sigma^2} \\right]\n\\end{align}\\]\nHàm phân phối của \\(\\mathcal{N}(\\mu,\\sigma)\\) có thể được viết dưới dạng phân phối kiểu mũ\n\\[\\begin{align}\n\\cfrac{1}{\\sqrt{2 \\pi} \\sigma} \\exp\\left[ \\cfrac{-(y - \\mu)^2}{2 \\ \\sigma^2} \\right] &= \\exp\\left[ \\cfrac{-(y - \\mu)^2}{2 \\ \\sigma^2} - \\cfrac{1}{2} log(2 \\pi \\sigma^2) \\right] \\\\\n&= \\exp\\left[ \\cfrac{\\mu y - \\mu^2/2} {\\sigma^2} - \\cfrac{y^2}{2\\sigma^2} - \\cfrac{1}{2} log(2 \\pi \\sigma^2) \\right]\n\\end{align}\\]\nVới \\(\\theta = \\mu\\) và \\(\\phi = \\sigma^2\\) chúng ta có hàm mật độ của biến ngẫu nhiên \\(\\mathcal{N}(\\mu,\\sigma)\\) là hàm mật độ của biến ngẫu nhiên nằm trong họ các phân phối kiểu mũ, với \\((\\phi) = \\phi\\), \\(b(\\theta) = \\theta^2/2\\) và\n\\[\\begin{align}\nc(y,\\phi) = -\\cfrac{y^2}{2\\phi} - \\cfrac{1}{2} log(2 \\pi \\phi)\n\\end{align}\\]\nBạn đọc có thể kiểm tra giá trị trung bình và phương sai của phân phối \\(\\mathcal{N}(\\mu,\\sigma)\\) dựa theo công thức (9.23)\n\\[\\begin{align}\n& \\mathbb{E}(Y) = b^{'}(\\theta) = \\theta = \\mu \\\\\n& \\mathbb{V}(Y) = (\\phi) \\cdot b^{''}(\\theta) = \\phi \\cdot 1 = \\phi = \\sigma^2\n\\end{align}\\]Ví dụ 2: phân phối chuẩn là một biến ngẫu nhiên nằm trong họ các phân phối kiểu mũ. Thật vậy, chúng ta có hàm mật độ của biến ngẫu nhiên phân phối chuẩn với trung bình \\(\\mu\\) và độ lệch chuẩn \\(\\sigma\\), ký hiệu \\(\\mathcal{N}(\\mu,\\sigma)\\), như sau\n\\[\\begin{align}\nf(y,\\mu,\\sigma) = \\cfrac{1}{\\sqrt{2 \\pi \\sigma^2}} \\exp\\left[ \\cfrac{-(y - \\mu)^2}{2 \\ \\sigma^2} \\right]\n\\end{align}\\]\nHàm phân phối của \\(\\mathcal{N}(\\mu,\\sigma)\\) có thể được viết dưới dạng phân phối kiểu mũ\n\\[\\begin{align}\n\\cfrac{1}{\\sqrt{2 \\pi} \\sigma} \\exp\\left[ \\cfrac{-(y - \\mu)^2}{2 \\ \\sigma^2} \\right] &= \\exp\\left[ \\cfrac{-(y - \\mu)^2}{2 \\ \\sigma^2} - \\cfrac{1}{2} log(2 \\pi \\sigma^2) \\right] \\\\\n&= \\exp\\left[ \\cfrac{\\mu y - \\mu^2/2} {\\sigma^2} - \\cfrac{y^2}{2\\sigma^2} - \\cfrac{1}{2} log(2 \\pi \\sigma^2) \\right]\n\\end{align}\\]\nVới \\(\\theta = \\mu\\) và \\(\\phi = \\sigma^2\\) chúng ta có hàm mật độ của biến ngẫu nhiên \\(\\mathcal{N}(\\mu,\\sigma)\\) là hàm mật độ của biến ngẫu nhiên nằm trong họ các phân phối kiểu mũ, với \\((\\phi) = \\phi\\), \\(b(\\theta) = \\theta^2/2\\) và\n\\[\\begin{align}\nc(y,\\phi) = -\\cfrac{y^2}{2\\phi} - \\cfrac{1}{2} log(2 \\pi \\phi)\n\\end{align}\\]\nBạn đọc có thể kiểm tra giá trị trung bình và phương sai của phân phối \\(\\mathcal{N}(\\mu,\\sigma)\\) dựa theo công thức (9.23)\n\\[\\begin{align}\n& \\mathbb{E}(Y) = b^{'}(\\theta) = \\theta = \\mu \\\\\n& \\mathbb{V}(Y) = (\\phi) \\cdot b^{''}(\\theta) = \\phi \\cdot 1 = \\phi = \\sigma^2\n\\end{align}\\]Ví dụ 3: phân phối \\(Gamma(\\alpha,\\gamma)\\) nằm trong họ các phân phối kiểu mũ. Thật vậy\n\\[\\begin{align}\nf(y) &= \\cfrac{\\gamma^\\alpha}{\\Gamma(\\alpha)} \\ y^{\\alpha-1} \\ e^{-\\gamma \\cdot y} \\\\\n&= \\exp\\left[ -\\gamma \\cdot y + \\alpha \\log(\\gamma) - \\log(\\Gamma(\\alpha)) - (\\alpha-1) \\cdot \\log(y)    \\right]\n\\end{align}\\]\nVới \\(\\theta = -\\gamma/\\alpha\\) và \\(\\phi = 1/\\alpha\\) ta có\n\\[\\begin{align}\nf(y) & = \\exp\\left[ \\cfrac{\\theta y + \\log(-\\theta)}{1/\\alpha} - \\cfrac{\\log(\\alpha)}{1/\\alpha} - \\log(\\Gamma(\\alpha)) - (\\alpha-1) \\cdot \\log(y)    \\right] \\\\\n& = \\exp\\left[ \\cfrac{\\theta y + \\log(-\\theta)}{\\phi} + \\cfrac{\\log(\\phi)}{\\phi} - \\log(\\Gamma(1/\\phi)) - (1/\\phi-1) \\cdot \\log(y)    \\right]\n\\end{align}\\]\nChúng ta có phân phối kiểu mũ với \\((\\phi) = \\phi\\), \\(b(\\theta) = \\log(-\\theta)\\), và\n\\[\\begin{align}\nc(y,\\phi) = \\cfrac{\\log(\\phi)}{\\phi} - \\log(\\Gamma(1/\\phi)) - (1/\\phi-1) \\cdot \\log(y)\n\\end{align}\\]\nChúng ta kiểm tra giá trị trung bình và phương sai của phân phối \\(Gamma(\\alpha,\\beta)\\) theo công thức (9.23)\n\\[\\begin{align}\n& \\mathbb{E}(Y) = b^{'}(\\theta) = - \\cfrac{1}{\\theta} = \\cfrac{\\alpha}{\\gamma} \\\\\n& \\mathbb{V}(Y) = (\\phi) \\cdot b^{''}(\\theta) = \\cfrac{1}{\\alpha} \\cdot \\cfrac{\\alpha^2}{\\gamma^2} = \\cfrac{\\alpha}{\\gamma^2}\n\\end{align}\\]Ví dụ 3: phân phối \\(Gamma(\\alpha,\\gamma)\\) nằm trong họ các phân phối kiểu mũ. Thật vậy\n\\[\\begin{align}\nf(y) &= \\cfrac{\\gamma^\\alpha}{\\Gamma(\\alpha)} \\ y^{\\alpha-1} \\ e^{-\\gamma \\cdot y} \\\\\n&= \\exp\\left[ -\\gamma \\cdot y + \\alpha \\log(\\gamma) - \\log(\\Gamma(\\alpha)) - (\\alpha-1) \\cdot \\log(y)    \\right]\n\\end{align}\\]\nVới \\(\\theta = -\\gamma/\\alpha\\) và \\(\\phi = 1/\\alpha\\) ta có\n\\[\\begin{align}\nf(y) & = \\exp\\left[ \\cfrac{\\theta y + \\log(-\\theta)}{1/\\alpha} - \\cfrac{\\log(\\alpha)}{1/\\alpha} - \\log(\\Gamma(\\alpha)) - (\\alpha-1) \\cdot \\log(y)    \\right] \\\\\n& = \\exp\\left[ \\cfrac{\\theta y + \\log(-\\theta)}{\\phi} + \\cfrac{\\log(\\phi)}{\\phi} - \\log(\\Gamma(1/\\phi)) - (1/\\phi-1) \\cdot \\log(y)    \\right]\n\\end{align}\\]\nChúng ta có phân phối kiểu mũ với \\((\\phi) = \\phi\\), \\(b(\\theta) = \\log(-\\theta)\\), và\n\\[\\begin{align}\nc(y,\\phi) = \\cfrac{\\log(\\phi)}{\\phi} - \\log(\\Gamma(1/\\phi)) - (1/\\phi-1) \\cdot \\log(y)\n\\end{align}\\]\nChúng ta kiểm tra giá trị trung bình và phương sai của phân phối \\(Gamma(\\alpha,\\beta)\\) theo công thức (9.23)\n\\[\\begin{align}\n& \\mathbb{E}(Y) = b^{'}(\\theta) = - \\cfrac{1}{\\theta} = \\cfrac{\\alpha}{\\gamma} \\\\\n& \\mathbb{V}(Y) = (\\phi) \\cdot b^{''}(\\theta) = \\cfrac{1}{\\alpha} \\cdot \\cfrac{\\alpha^2}{\\gamma^2} = \\cfrac{\\alpha}{\\gamma^2}\n\\end{align}\\]Trong trường hợp hàm số \\((\\phi)\\) là hàm số tuyến tính theo \\(\\phi\\), nghĩa là tồn tại số \\(\\omega\\) sao cho \\((\\phi) = \\cfrac{\\phi}{\\omega}\\) chúng ta có hàm mật độ của biến ngẫu nhiên có phân phối kiểu mũ như sau\n\\[\\begin{align}\nf(y;\\theta,\\phi) = \\exp\\left[ \\cfrac{y \\theta - b(\\theta)}{\\phi/\\omega} + c(y,\\phi) \\right]\n\\end{align}\\]\nGiá trị trung bình và phương sai của biến ngẫu nhiên \\(Y\\) trong trường hợp này được xác định như sau:\n\\[\\begin{align}\n& \\mathbb{E}(Y) = b^{'}(\\theta) \\\\\n& \\mathbb{V}(Y) = \\cfrac{\\phi}{\\omega} \\cdot b^{''}(\\theta)\n\\end{align}\\]Giả sử hàm \\(b^{'}(.)\\) là hàm số đơn điệu và tồn tại hàm số ngược \\({b^{'}}^{-1}(.)\\) thì \\(\\theta = {b^{'}}^{-1}\\left(\\mathbb{E}(Y)\\right)\\). đó mối liên hệ giữa phương sai của biến ngẫu nhiên \\(Y\\) và giá trị trung bình của biến \\(Y\\) được thể hiện qua công thức sau\n\\[\\begin{align}\n\\mathbb{V}(Y) = \\cfrac{\\phi}{\\omega} \\cdot b^{''}({b^{'}}^{-1}(\\mathbb{E}(Y)))\n\\end{align}\\]Hàm số \\(V(\\cdot) = b^{''}({b^{'}}^{-1}(\\cdot))\\) được gọi là hàm phương sai của phân phối kiểu mũ. Tại sao chúng ta cần định nghĩa một hàm số phức tạp như vậy? Bởi vì hàm \\(V(\\cdot)\\) là cơ sở để người xây dựng mô hình kiểm soát phương sai của biến ngẫu nhiên kiểu mũ.Giả sử biến phụ thuộc quan sát được là \\(Y_1, Y_2, \\cdots, Y_n\\) có cùng phân phối kiểu mũ với cùng tham số \\(\\phi\\), cùng hàm \\(b(\\cdot)\\) và \\(c(\\cdot)\\), nhưng có giá trị \\(\\omega_i\\) khác nhau. Giá trị trung bình của biến \\(Y_i\\), ký hiệu là \\(\\mu_i\\) được giải thích thông qua các biến độc lập và không phụ thuộc vào \\(\\omega_i\\), trong khi phương sai của biến \\(Y_i\\) phụ thuộc vào \\(\\omega_i\\) và giá trị trung bình \\(\\mu_i\\):\n\\[\\begin{align}\n\\mathbb{V}(Y_i) = \\cfrac{\\phi}{\\omega_i} \\cdot V(\\mu_i)\n\\end{align}\\]Khi ước lượng mô hình tuyến tính tổng quát trên một dữ liệu cụ thể, giá trị hàm phương sai \\(V\\) phụ thuộc vào cách chúng ta lựa chọn hàm \\(b\\) và giá trị trung bình của \\(Y_i\\). Tham số \\(\\phi\\) không phụ thuộc vào quan sát \\(Y_i\\), đó hệ số \\(w_i\\) có ý nghĩa quyết định trong xác định phương sai của \\(Y_i\\). Chúng ta có thể lựa chọn đơn giản là cho \\(w_i\\) bằng 1 với mọi \\(\\), nếu chúng ta tin rằng tỷ lệ \\(\\cfrac{V(\\mu_i)}{\\mathbb{V}(Y_i)}\\) là hằng số. Trong một số trường hợp, \\(\\cfrac{V(\\mu_i)}{\\mathbb{V}(Y_i)}\\) thay đổi theo \\(\\), đó là lúc chúng ta cần lựa chọn \\(w_i\\) để cho kết quả tốt nhất. Chúng ta sẽ tiếp tục thảo luận về hàm phương sai của biến phụ thuộc trong phần hàm hợp lý tối đa của biến ngẫu nhiên nằm trong họ phân phối các phân phối kiểu mũ.","code":""},{"path":"mô-hình-tuyến-tính-tổng-quát..html","id":"hàm-liên-kết.","chapter":"Chương 9 Mô hình tuyến tính tổng quát.","heading":"9.3.2 Hàm liên kết.","text":"Hàm số liên kết \\(g(\\cdot)\\) liên kết giá trị trung bình của biến phụ thuộc với tổ hợp tuyến tính của các biến độc lập luôn được lựa chọn trong nhóm các hàm số đơn điệu và có đạo hàm trên miền xác định của hàm số đó. Các giả thiết này đảm bảo để hàm liên kết có hàm số ngược \\(g^{-1}(\\cdot)\\) cũng là hàm đơn điệu và có đạo hàm trên miền xác định. Một yếu tố quan trọng khác khi lựa chọn hàm liên kết đó là \\(g(\\cdot)\\) có miền xác định trùng với miền xác định của giá trị trung bình của biến phụ thuộc và miền giá trị của \\(g(\\cdot)\\) là tập số thực \\(\\mathbb{R}\\).Xin được nhắc lại rằng mối liên hệ giữa trung bình của biến phụ thuộc và tổ hợp tuyến tính của biến độc lập được mô tả thông qua hàm liên kết như sau:\n\\[\\begin{align}\n& g(\\mu_i) = \\beta_0 + \\beta_1 \\cdot x_{,1} +  \\beta_2 \\cdot x_{,2} + \\cdots + \\beta_p \\cdot x_{,d} \\\\\n& \\mu_i = g^{-1}\\left( beta_0 + \\beta_1 \\cdot x_{,1} +  \\beta_2 \\cdot x_{,2} + \\cdots + \\beta_p \\cdot x_{,d} \\right)\n\\end{align}\\]Danh sách các hàm số thường được sử dụng làm hàm liên kết được cho trong bảng dưới đâyVí dụ 1: khi \\(Y\\) mô tả số lần khách hàng gửi yêu cầu bảo hiểm, giá trị trung bình của \\(Y\\) sẽ là số thực dương. Các hàm số có miền xác định là tập các số thực dương sẽ phù hợp trong trường hợp này. Có thể thấy trong bảng trên các hàm \\(g(\\mu) = \\log(\\mu)\\) và hàm \\(g(\\mu) = 1/\\mu^2\\) là các hàm số có thể lựa chọn là hàm liên kết.Ví dụ 1: khi \\(Y\\) mô tả số lần khách hàng gửi yêu cầu bảo hiểm, giá trị trung bình của \\(Y\\) sẽ là số thực dương. Các hàm số có miền xác định là tập các số thực dương sẽ phù hợp trong trường hợp này. Có thể thấy trong bảng trên các hàm \\(g(\\mu) = \\log(\\mu)\\) và hàm \\(g(\\mu) = 1/\\mu^2\\) là các hàm số có thể lựa chọn là hàm liên kết.Ví dụ 2: khi \\(Y\\) mô tả khách hàng có gửi yêu cầu bảo hiểm hay không, hoặc mô tả sự kiện tai nạn có xảy ra hay không, giá trị trung bình của \\(Y\\) nằm trong khoảng \\((0,1)\\). đó các hàm \\(Logit\\), hàm \\(Probit\\), hàm \\(Log-Log\\), hoặc hàm \\(Cauchit\\) sẽ là lựa chọn phù hợp cho hàm liên kết.Ví dụ 2: khi \\(Y\\) mô tả khách hàng có gửi yêu cầu bảo hiểm hay không, hoặc mô tả sự kiện tai nạn có xảy ra hay không, giá trị trung bình của \\(Y\\) nằm trong khoảng \\((0,1)\\). đó các hàm \\(Logit\\), hàm \\(Probit\\), hàm \\(Log-Log\\), hoặc hàm \\(Cauchit\\) sẽ là lựa chọn phù hợp cho hàm liên kết.Khi lựa chọn hàm liên kết bạn đọc cần cân nhắc đến khả năng giải thích của mô hình và sự khó khăn có thể gặp phải khi ước lượng của tham số trong mô hình. Ví dụ như khi biến mục tiêu chỉ nhận giá trị 0 hoặc 1, hàm logit thường xuyên được sử dụng bởi vì khả năng giải thích tốt hơn hàm \\(Probit\\) hay \\(Log-Log\\). Hoặc khi cân nhắc lựa chọn giữa hàm \\(log\\) và hàm \\(inverse\\) \\(squared\\), hàm \\(log\\) thường được ưu tiên lựa chọn. Có thể sánh mối liên hệ giữa \\(\\mu_i\\) với các biến độc lập như dưới đây thông qua hàm \\(log\\) hoặc hàm \\(inverse\\) \\(squared\\) như sau\n\\[\\begin{align}\n&\\text{ Hàm log: } \\mu_i = exp(\\beta_0) \\cdot exp(\\beta_1 \\cdot x_{,1}) \\cdots exp(\\beta_p \\cdot x_{,p}) \\\\\n&\\text{ Inverse squared: } \\mu_i = \\cfrac{1}{\\sqrt{\\beta_0 + \\beta_1 \\cdot x_{,1} + \\cdots + \\beta_p \\cdot x_{,p}}}\n\\end{align}\\]\nKhi sử dụng hàm \\(log\\), mỗi biến độc lập tác động lên giá trị trung bình của biến phụ thuộc một cách độc lập với nhau theo quy tắc nhân, trong khi đó không dễ để đánh giá tác động của một biến độc lập lên biến phụ thuộc trong công thức của hàm \\(inverse\\) \\(squared\\). Hay nói cách khác, sử dụng hàm \\(log\\) thường sẽ dễ dàng giải thích kết quả hơn với khi sử dụng hàm \\(inverse\\) \\(squared\\).Với \\(Y\\) là biến ngẫu nhiên nằm trong họ các phân phối kiểu mũ ta có \\(\\mathbb{E}(Y_i) = b^{'}(\\theta_i)\\). Khi chúng ta lựa chọn hàm liên kết là \\(g(\\cdot) = (b^{'})^{-1}(\\cdot)\\) thì mối liên hệ giữa tham số chính tắc \\(\\theta_i\\) với các biến độc lập sẽ là tuyến tính. Thật vậy,\n\\[\\begin{align}\n& \\mu_i = g^{-1}(\\theta_i) \\\\\n& g(\\mu_i) = \\beta_0 + \\beta_1 \\cdot x_{,1} + \\cdots + \\beta_p \\cdot x_{,p}\\\\\n& \\rightarrow \\theta_i = \\beta_0 + \\beta_1 \\cdot x_{,1} + \\beta_2 \\cdot x_{,2} \\cdots + \\beta_p \\cdot x_{,p}\n\\end{align}\\]Như vậy, với lựa chọn hàm liên kết \\(g(\\cdot) = (b^{'})^{-1}(\\cdot)\\), tham số chính tắc \\(\\theta_i\\) của biến ngẫu nhiên phân phối mũ \\(Y_i\\) bằng tổ hợp tuyến tính của các biến độc lập. Trong trường hợp này, hàm liên kết \\((b^{'})^{-1}(\\cdot)\\) còn được gọi là hàm liên kết chính tắc của biến phụ thuộc \\(Y\\).Ví dụ 1: khi \\(Y\\) là biến ngẫu nhiên có phân phối chuẩn \\(\\mathcal{N}(\\mu, \\sigma)\\), chúng ta có \\(b(\\mu) = \\mu^2/2\\) và \\(b^{'}(\\mu) = \\mu\\) đó hàm liên kết chính tắc của biến ngẫu nhiên phân phối chuẩn \\(\\mathcal{N}(\\mu, \\sigma)\\) là \\(g(\\mu) = \\mu\\).Ví dụ 1: khi \\(Y\\) là biến ngẫu nhiên có phân phối chuẩn \\(\\mathcal{N}(\\mu, \\sigma)\\), chúng ta có \\(b(\\mu) = \\mu^2/2\\) và \\(b^{'}(\\mu) = \\mu\\) đó hàm liên kết chính tắc của biến ngẫu nhiên phân phối chuẩn \\(\\mathcal{N}(\\mu, \\sigma)\\) là \\(g(\\mu) = \\mu\\).Ví dụ 2: khi \\(Y\\) là biến ngẫu nhiên có phân phối \\(gamma(\\alpha,\\gamma)\\), chúng ta có \\(b(\\mu) = \\log(-\\mu)\\), và \\(b^{'}(\\mu) = -1/\\mu\\), đó hàm liên kết chính tắc của biến ngẫu nhiên phân phối gamma là \\(g(\\mu) = -1/\\mu\\). Trong trường hợp này hàm liên kết không có miền xác định là tập các số thực dương. Có thể thấy rằng lựa chọn hàm liên kết chính tắc không phải là một lựa chọn phù hợp.Ví dụ 2: khi \\(Y\\) là biến ngẫu nhiên có phân phối \\(gamma(\\alpha,\\gamma)\\), chúng ta có \\(b(\\mu) = \\log(-\\mu)\\), và \\(b^{'}(\\mu) = -1/\\mu\\), đó hàm liên kết chính tắc của biến ngẫu nhiên phân phối gamma là \\(g(\\mu) = -1/\\mu\\). Trong trường hợp này hàm liên kết không có miền xác định là tập các số thực dương. Có thể thấy rằng lựa chọn hàm liên kết chính tắc không phải là một lựa chọn phù hợp.","code":""},{"path":"mô-hình-tuyến-tính-tổng-quát..html","id":"hàm-hợp-lý-tối-đa-và-ước-lượng-mô-hình.","chapter":"Chương 9 Mô hình tuyến tính tổng quát.","heading":"9.4 Hàm hợp lý tối đa và ước lượng mô hình.","text":"Hệ số tuyến tính của các biến độc lập \\(\\boldsymbol{\\beta} = (\\beta_0, \\beta_1, \\cdots, \\beta_p)\\) được ước lượng từ dữ liệu \\(\\textbf{y} = (y_1, y_2, \\cdots y_n)\\) và \\(\\textbf{x} = (\\textbf{x}_1, \\textbf{x}_2, \\cdots \\textbf{x}_n)\\) bằng phương pháp tối đa hóa hàm hợp lý, hay còn gọi là hàm likelihood. Khi biến mục tiêu \\(Y_i\\) có phân phối kiểu mũ, hàm likelihood được viết như sau\n\\[\\begin{align}\nL(\\textbf{y},\\boldsymbol{\\beta}) = \\prod\\limits_{=1}^n \\ \\exp\\left[ \\cfrac{y_i \\theta - b(\\theta)}{a_i(\\phi)} + c_i(y_i,\\phi) \\right]\n\\end{align}\\]\nTrong hầu hết các trường hợp, chúng ta sẽ thực hiện tính toán trên hàm Log-likelihood thay vì hàm likelihood. Hàm Log-likelihood được ký hiệu \\(l(\\textbf{y},\\beta)\\) được xác định như sau\\[\\begin{align}\nl(\\textbf{y},\\beta) &= \\log \\left( \\prod\\limits_{=1}^n \\ \\exp\\left[ \\cfrac{y_i \\theta_i - b(\\theta_i)}{a_i(\\phi)} + c_i(y_i,\\phi) \\right] \\right) \\\\\n&= \\sum\\limits_{=1}^n  \\left(\\cfrac{y_i \\theta_i - b(\\theta_i)}{a_i(\\phi)} + c_i(y_i,\\phi)\\right)\n\\end{align}\\]\nVéc-tơ hệ số \\(\\boldsymbol{\\beta}\\) được ước lượng sao cho giá trị của hàm Log-likelihood đạt giá trị lớn nhất. Như vậy \\(\\boldsymbol{\\beta}\\) là nghiệm của hệ phương trình\n\\[\\begin{align}\n\\cfrac{\\partial l(\\textbf{y},\\boldsymbol{\\beta})}{\\partial \\beta_j} & = 0 \\ \\ \\forall j = 1,2, \\cdots, p\n\\end{align}\\]\nLưu ý rằng \\(b^{'}(\\theta_i) = \\mu_i\\) nên trong các thành phần của hàm mật độ xác suất của biến \\(Y_i\\) chỉ có hàm số \\(b(\\cdot)\\) và tham số \\(\\theta_i\\) phụ thuộc vào hệ số \\(\\boldsymbol{\\beta}\\). Đạo hàm của hàm Log-likelihood theo \\(\\beta\\) có thể viết được như sau\n\\[\\begin{align}\n\\cfrac{\\partial l(\\textbf{y},\\beta)}{\\partial \\beta_j} & = \\sum\\limits_{=1}^n  \\cfrac{1}{a_i(\\phi)} \\left(y_i \\cfrac{\\partial \\theta_i}{\\partial \\beta_j} - \\cfrac{\\partial b(\\theta_i)}{\\partial \\beta_j} \\right) \\\\\n& = \\sum\\limits_{=1}^n  \\cfrac{y_i - b^{'}(\\theta_i)}{a_i(\\phi)} \\cfrac{\\partial \\theta_i}{\\partial \\beta_j}\n\\end{align}\\]Lưu ý rằng \\(\\mu_i = b^{'}(\\theta_i)\\), đo đó\n\\[\\begin{align}\n\\theta_i = (b^{'})^{-1}(\\mu_i) = (b^{'})^{-1}\\left(g^{-1}(\\beta_0 + \\beta_1 \\cdot x_{,1} + \\cdots \\beta_p \\cdot x_{,p})\\right)\n\\end{align}\\]\nđồng thời, đạo hàm của hàm ngược được của các hàm \\(b^{'}(\\cdot)\\) và \\(g(\\cdot)\\) được xác định như sau\n\\[\\begin{align}\n& \\left((b^{'})^{-1}\\right)^{'}(\\mu_i) = \\cfrac{1}{(b^{''}\\left((b^{'})^{-1}(\\mu_i)\\right)} = \\cfrac{1}{b^{''}(\\theta_i)} \\\\\n& (g^{-1}(\\psi_i))^{'} = \\cfrac{1}{(g^{'}(g^{-1}(\\psi_i))} =  \\cfrac{1}{g^{'}(\\mu_i)}\n\\end{align}\\]\nvới \\(\\psi_i = \\beta_0 + \\beta_1 \\cdot x_{,1} + \\cdots \\beta_p \\cdot x_{,p}\\).Như vậy, đạo hàm của \\(\\theta_i\\) theo \\(\\beta_j\\) được tính như sau\n\\[\\begin{align}\n\\cfrac{\\partial \\theta_i}{\\partial \\beta_j} = \\cfrac{\\partial (b^{'})^{-1}(g^{-1}(\\psi_i(\\beta_j)))}{\\partial \\beta_j} = \\cfrac{x_{,j}}{b^{''}(\\theta_i) \\cdot g^{'}(\\mu_i)}\n\\end{align}\\]Ta có đạo hàm của hàm Log-likelihood theo các tham số \\(\\beta_j\\)\n\\[\\begin{align}\n\\cfrac{\\partial l(\\textbf{y},\\beta)}{\\partial \\beta_j} = \\sum\\limits_{=1}^n  \\cfrac{(y_i - \\mu_i) \\cdot x_{,j}}{a_i(\\phi) \\cdot b^{''}(\\theta_i) \\cdot g^{'}(\\mu_i)}\n\\end{align}\\]Với lựa chọn \\(a_i(\\phi) = \\cfrac{\\phi}{\\omega_i}\\), phương sai của biến phụ thuộc là \\(a_i(\\phi) \\cdot b^{''}(\\theta_i)\\) nên ta có thể viết hàm Log-likelihood theo các tham số \\(\\beta_j\\) theo giá trị trung bình và phương sai của biến phụ thuộc\n\\[\\begin{align}\n\\cfrac{\\partial l(\\textbf{y},\\boldsymbol{\\beta})}{\\partial \\beta_j} &= \\sum\\limits_{=1}^n  \\cfrac{(y_i - \\mu_i) \\cdot x_{,j}}{\\mathbb{V}(y_i) \\cdot g^{'}(\\mu_i)} \\\\\n& = \\sum\\limits_{=1}^n w_i \\cfrac{(y_i - \\mu_i) \\cdot x_{,j}}{\\phi V(\\mu_i) \\cdot g^{'}(\\mu_i)} \\\\\n& =\\cfrac{1}{\\phi} \\sum\\limits_{=1}^n w_i \\cfrac{x_{,j}}{V(\\mu_i) \\cdot g^{'}(\\mu_i)} (y_i - \\mu_i)\n\\end{align}\\]\nCó thể thầy rằng:Thứ nhất, \\(\\beta_j\\) là nghiệm của phương trình \\(\\cfrac{\\partial l(\\textbf{y},\\beta)}{\\partial \\beta_j} = 0\\) sẽ không phụ thuộc vào giá trị của tham số phân tán \\(\\phi\\).Thứ nhất, \\(\\beta_j\\) là nghiệm của phương trình \\(\\cfrac{\\partial l(\\textbf{y},\\beta)}{\\partial \\beta_j} = 0\\) sẽ không phụ thuộc vào giá trị của tham số phân tán \\(\\phi\\).Thứ hai, giá trị của \\(\\mu_i\\) phụ thuộc vào giá trị của các biến độc lập \\(\\textbf{x}_i\\) và phụ thuộc vào các hệ số \\(\\boldsymbol{\\beta}\\), đó giá trị \\(\\cfrac{x_{,j}}{V(\\mu_i) \\cdot g^{'}(\\mu_i)}\\) không phụ thuộc hoàn toàn vào cách chúng ta xây dựng mô hình.Thứ hai, giá trị của \\(\\mu_i\\) phụ thuộc vào giá trị của các biến độc lập \\(\\textbf{x}_i\\) và phụ thuộc vào các hệ số \\(\\boldsymbol{\\beta}\\), đó giá trị \\(\\cfrac{x_{,j}}{V(\\mu_i) \\cdot g^{'}(\\mu_i)}\\) không phụ thuộc hoàn toàn vào cách chúng ta xây dựng mô hình.Thứ ba, giá trị \\(w_i\\) không phụ thuộc vào dữ liệu đó có thể được sử dụng linh hoạt để ước lượng mô hình có kết quả tốt nhất.Thứ ba, giá trị \\(w_i\\) không phụ thuộc vào dữ liệu đó có thể được sử dụng linh hoạt để ước lượng mô hình có kết quả tốt nhất.Giải hệ phương trình với ẩn là véc-tơ tham số \\(\\boldsymbol{\\beta}\\) như trên thường phải sử dụng các phương pháp giải số. Phương pháp thường được sử dụng là thuật toán Newton Raphson.Hàm glm() sử dụng xuyên suốt trong chương sách cho phép bạn đọc ước lượng mô hình tuyến tính tổng quát cho đa số các phân phối thường gặp của \\(Y_i\\). Tham số \\(weight\\) trong hàm glm() là véc-tơ tham số \\(w_i\\) như chúng ta đã trình bày ở trên. Lựa chọn giá trị cho \\(w_i\\) hoàn toàn cách tiếp cận của người xây dựng mô hình.Ví dụ 1: khi lựa chọn \\(Y_i\\) có phân phối Poisson với tham số \\(\\lambda_i = exp(\\theta_i)\\) và hàm liên kết là hàm \\(g(\\cdot) = log(\\cdot)\\). Chúng ta có \\((\\phi) = 1\\) đó \\(w_i = 1 \\forall \\).\n\\[\\begin{align}\n& g(\\mu_i) = log(\\mu_i) \\rightarrow g^{'}(\\mu_i) = 1/\\mu_i \\\\\n& b(\\theta) = exp(\\theta) \\rightarrow b^{'}(\\theta) = b^{''}(\\theta) = exp(\\theta) \\rightarrow (b^{'})^{-1}(\\theta) = \\log(\\theta) \\rightarrow V(\\theta) = b^{''}(b^{'})^{-1})(\\theta) = \\theta\n\\end{align}\\]\nĐạo hàm của hàm Log-likelihood theo \\(\\beta_j\\) trở thành\n\\[\\begin{align}\n\\sum\\limits_{=1}^n w_i \\cdot \\cfrac{x_{,j}}{\\phi V(\\mu_i) \\cdot g^{'}(\\mu_i)} (y_i - \\mu_i) &= \\sum\\limits_{=1}^n x_{,j} (y_i - \\mu_i) \\\\\n&= \\sum\\limits_{=1}^n x_{,j} \\left[y_i - \\exp(\\beta_0 + \\beta_1 \\cdot x_{,1} + \\cdots + \\beta_p \\cdot x_{,p})\\right]\n\\end{align}\\]\nhay nói một cách khác, véc-tơ tham số \\(\\beta\\) để tối đa hóa giá trị của hàm Log-likelihood là nghiệm của hệ phương trình\n\\[\\begin{align}\n\\sum\\limits_{=1}^n x_{,j} \\left[y_i - \\exp \\left(\\beta_0 + \\beta_1 \\cdot x_{,1} + \\cdots + \\beta_p \\cdot x_{,p} \\right) \\right] = 0 \\ \\forall j\n\\end{align}\\]Ví dụ 1: khi lựa chọn \\(Y_i\\) có phân phối Poisson với tham số \\(\\lambda_i = exp(\\theta_i)\\) và hàm liên kết là hàm \\(g(\\cdot) = log(\\cdot)\\). Chúng ta có \\((\\phi) = 1\\) đó \\(w_i = 1 \\forall \\).\n\\[\\begin{align}\n& g(\\mu_i) = log(\\mu_i) \\rightarrow g^{'}(\\mu_i) = 1/\\mu_i \\\\\n& b(\\theta) = exp(\\theta) \\rightarrow b^{'}(\\theta) = b^{''}(\\theta) = exp(\\theta) \\rightarrow (b^{'})^{-1}(\\theta) = \\log(\\theta) \\rightarrow V(\\theta) = b^{''}(b^{'})^{-1})(\\theta) = \\theta\n\\end{align}\\]\nĐạo hàm của hàm Log-likelihood theo \\(\\beta_j\\) trở thành\n\\[\\begin{align}\n\\sum\\limits_{=1}^n w_i \\cdot \\cfrac{x_{,j}}{\\phi V(\\mu_i) \\cdot g^{'}(\\mu_i)} (y_i - \\mu_i) &= \\sum\\limits_{=1}^n x_{,j} (y_i - \\mu_i) \\\\\n&= \\sum\\limits_{=1}^n x_{,j} \\left[y_i - \\exp(\\beta_0 + \\beta_1 \\cdot x_{,1} + \\cdots + \\beta_p \\cdot x_{,p})\\right]\n\\end{align}\\]\nhay nói một cách khác, véc-tơ tham số \\(\\beta\\) để tối đa hóa giá trị của hàm Log-likelihood là nghiệm của hệ phương trình\n\\[\\begin{align}\n\\sum\\limits_{=1}^n x_{,j} \\left[y_i - \\exp \\left(\\beta_0 + \\beta_1 \\cdot x_{,1} + \\cdots + \\beta_p \\cdot x_{,p} \\right) \\right] = 0 \\ \\forall j\n\\end{align}\\]Ví dụ 2: khi \\(Y_i\\) có phân phối Gamma với tham số \\(\\alpha\\), \\(\\gamma_i\\) chúng ta có \\(b(\\theta_i) = log(-\\theta_i)\\) với \\(\\theta_i = - \\cfrac{\\gamma_i}{\\alpha}\\) đồng thời \\((\\phi) = \\phi\\) đó \\(w_i = 1\\) \\(\\forall \\). Giả sử hàm liên kết được lựa chọn là hàm \\(log\\): \\(g(\\cdot) = log(\\cdot)\\).\n\\[\\begin{align}\n& g(\\mu_i) = log(\\mu_i) \\rightarrow g^{'}(\\mu_i) = 1/\\mu_i \\\\\n& b(\\theta_i) = log(-\\theta_i) \\rightarrow b^{'}(\\theta_i) = -1/\\theta_i \\rightarrow b^{''}(\\theta) = 1/\\theta^2 \\rightarrow (b^{'})^{-1}(\\theta) = -1/\\theta \\rightarrow V(\\mu) = b^{''}(b^{'})^{-1})(\\mu) = \\mu^2\n\\end{align}\\]\nĐạo hàm của hàm Log-likelihood theo \\(\\beta_j\\) trở thành\n\\[\\begin{align}\n\\sum\\limits_{=1}^n w_i \\cdot \\cfrac{x_{,j}}{\\phi V(\\mu_i) \\cdot g^{'}(\\mu_i)} (y_i - \\mu_i) &= \\sum\\limits_{=1}^n \\cfrac{x_{,j}}{\\mu_i} (y_i - \\mu_i)\n\\end{align}\\]\nvéc-tơ tham số \\(\\boldsymbol{\\beta}\\) để tối đa hóa giá trị của hàm Log-likelihood là nghiệm của hệ phương trình\n\\[\\begin{align}\n\\sum\\limits_{=1}^n x_{,j} \\left\\{y_i \\cdot \\exp\\left[-(\\beta_0 + \\beta_1 \\cdot x_{,1} + \\cdots + \\beta_p \\cdot x_{,p}) \\right] - 1 \\right\\} = 0 \\ \\forall j\n\\end{align}\\]Ví dụ 2: khi \\(Y_i\\) có phân phối Gamma với tham số \\(\\alpha\\), \\(\\gamma_i\\) chúng ta có \\(b(\\theta_i) = log(-\\theta_i)\\) với \\(\\theta_i = - \\cfrac{\\gamma_i}{\\alpha}\\) đồng thời \\((\\phi) = \\phi\\) đó \\(w_i = 1\\) \\(\\forall \\). Giả sử hàm liên kết được lựa chọn là hàm \\(log\\): \\(g(\\cdot) = log(\\cdot)\\).\n\\[\\begin{align}\n& g(\\mu_i) = log(\\mu_i) \\rightarrow g^{'}(\\mu_i) = 1/\\mu_i \\\\\n& b(\\theta_i) = log(-\\theta_i) \\rightarrow b^{'}(\\theta_i) = -1/\\theta_i \\rightarrow b^{''}(\\theta) = 1/\\theta^2 \\rightarrow (b^{'})^{-1}(\\theta) = -1/\\theta \\rightarrow V(\\mu) = b^{''}(b^{'})^{-1})(\\mu) = \\mu^2\n\\end{align}\\]\nĐạo hàm của hàm Log-likelihood theo \\(\\beta_j\\) trở thành\n\\[\\begin{align}\n\\sum\\limits_{=1}^n w_i \\cdot \\cfrac{x_{,j}}{\\phi V(\\mu_i) \\cdot g^{'}(\\mu_i)} (y_i - \\mu_i) &= \\sum\\limits_{=1}^n \\cfrac{x_{,j}}{\\mu_i} (y_i - \\mu_i)\n\\end{align}\\]\nvéc-tơ tham số \\(\\boldsymbol{\\beta}\\) để tối đa hóa giá trị của hàm Log-likelihood là nghiệm của hệ phương trình\n\\[\\begin{align}\n\\sum\\limits_{=1}^n x_{,j} \\left\\{y_i \\cdot \\exp\\left[-(\\beta_0 + \\beta_1 \\cdot x_{,1} + \\cdots + \\beta_p \\cdot x_{,p}) \\right] - 1 \\right\\} = 0 \\ \\forall j\n\\end{align}\\]Ví dụ 3: khi \\(Y_i\\) là số tiền bồi thường trung bình cho 1 tai nạn trong vòng 1 năm của một khách hàng, biết rằng khách hàng được bồi thường \\(n_i\\) lần trong năm và số tiền bồi thường của một vụ tai nạn là biến ngẫu nhiên \\(Y^*_i\\) phân phối Gamma với tham số \\(\\alpha\\), \\(\\beta_i\\). \\(Y_i\\) là giá trị trung bình của \\(n_i\\) biến phân phối Gamma độc lập với cùng tham số \\(\\alpha\\), \\(\\beta_i\\) nên \\(Y_i\\) sẽ có phân phối Gamma với tham số \\((n_i \\alpha)\\) và \\((n_i \\beta_i)\\). Khi viết \\(Y_i\\) dưới dạng phân phối kiểu mũ, chúng ta có \\(b(\\theta_i) = log(-\\theta_i)\\) với \\(\\theta_i = - \\cfrac{\\beta_i}{\\alpha}\\) và \\((\\phi) = \\phi/n_i\\). Véc-tơ tham số \\(\\boldsymbol{\\beta}\\) để tối đa hóa giá trị của hàm Log-likelihood là nghiệm của hệ phương trình\n\\[\\begin{align}\n\\sum\\limits_{=1}^n n_i \\cdot \\ x_{,j} \\cdot  \\left[y_i \\cdot \\exp\\left(-(\\beta_0 + \\beta_1 \\cdot x_{,1} + \\cdots + \\beta_p \\cdot x_{,p}) \\right) - 1 \\right] = 0 \\ \\forall j\n\\end{align}\\]Ví dụ 3: khi \\(Y_i\\) là số tiền bồi thường trung bình cho 1 tai nạn trong vòng 1 năm của một khách hàng, biết rằng khách hàng được bồi thường \\(n_i\\) lần trong năm và số tiền bồi thường của một vụ tai nạn là biến ngẫu nhiên \\(Y^*_i\\) phân phối Gamma với tham số \\(\\alpha\\), \\(\\beta_i\\). \\(Y_i\\) là giá trị trung bình của \\(n_i\\) biến phân phối Gamma độc lập với cùng tham số \\(\\alpha\\), \\(\\beta_i\\) nên \\(Y_i\\) sẽ có phân phối Gamma với tham số \\((n_i \\alpha)\\) và \\((n_i \\beta_i)\\). Khi viết \\(Y_i\\) dưới dạng phân phối kiểu mũ, chúng ta có \\(b(\\theta_i) = log(-\\theta_i)\\) với \\(\\theta_i = - \\cfrac{\\beta_i}{\\alpha}\\) và \\((\\phi) = \\phi/n_i\\). Véc-tơ tham số \\(\\boldsymbol{\\beta}\\) để tối đa hóa giá trị của hàm Log-likelihood là nghiệm của hệ phương trình\n\\[\\begin{align}\n\\sum\\limits_{=1}^n n_i \\cdot \\ x_{,j} \\cdot  \\left[y_i \\cdot \\exp\\left(-(\\beta_0 + \\beta_1 \\cdot x_{,1} + \\cdots + \\beta_p \\cdot x_{,p}) \\right) - 1 \\right] = 0 \\ \\forall j\n\\end{align}\\]","code":""},{"path":"mô-hình-tuyến-tính-tổng-quát..html","id":"so-sánh-và-lựa-chọn-mô-hình-tuyến-tính-tổng-quát.","chapter":"Chương 9 Mô hình tuyến tính tổng quát.","heading":"9.5 So sánh và lựa chọn mô hình tuyến tính tổng quát.","text":"Nắm được các nguyên tắc chung bạn đọc có thể tự xây dựng nhiều mô hình tuyến tính tổng quát khác nhau cho một dữ liệu cụ thể. Thách thức đặt ra là một mô hình liệu có thực sự có tốt hơn các mô hình khác, hay trong số các mô hình bạn lựa chọn mô hình nào là phù hợp nhất? Phần này của chương sẽ thảo luận về vấn đề sánh mô hình và lựa chọn mô hình. Các chỉ tiêu thống kê được sủ dụng để sánh mô hình sẽ xoay quanh giá trị của hàm hợp lý tối đa, bao gồm có thước đo deviance, chỉ tiêu AIC, AICC, hay BIC.","code":""},{"path":"mô-hình-tuyến-tính-tổng-quát..html","id":"thước-đo-deviance","chapter":"Chương 9 Mô hình tuyến tính tổng quát.","heading":"9.5.1 Thước đo deviance","text":"Thước đo được gọi là deviance thường được sử dụng để đánh giá và sánh các mô hình tuyến tính tổng quát có cùng phân phối của biến độc lập. Chỉ tiêu này được tính toán dựa trên giá trị hàm Log-likelihood. Khi \\(Y\\) là biến ngẫu nhiên trong nhóm các phân phối mũ, hàm log-likelihood được viết như sau\n\\[\\begin{align}\nl(\\textbf{y},\\theta) & = \\sum\\limits_{=1}^n  \\left(\\cfrac{y_i \\theta_i - b(\\theta_i)}{a_i(\\phi)} + c_i(y_i,\\phi)\\right)\n\\end{align}\\]\nvới \\(\\boldsymbol{\\theta}\\) là véc-tơ các tham số chính tắc, \\(\\boldsymbol{\\theta} = (\\theta_1, \\theta_2, \\cdots, \\theta_n)\\). Sau khi phân phối của \\(Y\\) và hàm liên kết được lựa chọn, chúng ta ước lượng được véc-tơ tham số \\(\\boldsymbol{\\beta}\\) là hệ số của các biến độc lập bằng cách tối đa hóa hàm log-likelihood. Sau khi đã xác định được véc-tơ tham số \\(\\boldsymbol{\\beta}\\), với mỗi lựa chọn cho phân phối của \\(Y\\) và hàm liên kết, chúng ta sẽ tính toán được các tham số chính tắc của mô hình tuyến tính tổng quát. Nếu hàm liên kết được lựa chọn là hàm liên kết chính tắc, \\(g(\\cdot) = (b^{'})^{-1}(\\cdot)\\), chúng ta có các tham số chính tắc chính là tổ hợp tuyến tính của các biến độc lập \\(\\boldsymbol{\\theta}^M = (\\theta^M_1, \\theta^M_2, \\cdots, \\theta^M_n)\\) với\n\\[\\begin{align}\n\\theta^M_i = \\beta_0 + \\beta_1 \\cdot x_{,1} + \\cdots + \\beta_p \\cdot x_{,p}\n\\tag{9.24}\n\\end{align}\\]Với mỗi lựa chọn cho mô hình tuyến tính tổng quát, bao gồm lựa chọn cho phân phối của \\(Y\\) và hàm liên kết \\(g\\), tạm gọi là mô hình \\(M\\), chúng ta gọi \\(l(\\textbf{y},\\boldsymbol{\\theta}^M)\\) là giá trị của hàm Log-likelihood tại tham số \\(\\boldsymbol{\\theta}^M\\) được xác định qua phương trình (9.24). Lưu ý rằng véc-tơ \\(\\boldsymbol{\\theta}^M\\) có độ dài là \\(n\\), bằng với kích thước của dữ liệu, tuy nhiên các tham số này được tính toán từ \\((p+1)\\) giá trị \\(\\beta\\) ước lượng được và giá trị của biến độc lập.Điều gì xảy ra nếu tham số chính tắc \\(\\boldsymbol{\\theta}\\) hoàn toàn tự và không phụ thuộc vào biến độc lập? Hàm Log-likelihood sẽ đạt giá trị cực đại tại \\(\\theta^S = (\\theta^S_1, \\theta^S_2, \\cdots, \\theta^S_n)\\) với \\(\\theta^S_i\\) là giá trị sao cho đạo hàm của hàm Log-likelihood tại \\(\\theta^S_i\\) bằng 0. \\(\\theta\\) hoàn toàn tự nên chỉ có thành phần thứ \\(\\) của hàm Log-likelihood phụ thuộc vào \\(\\theta^S_i\\)\n\\[\\begin{align}\n\\cfrac{\\partial l(\\textbf{y},\\theta)}{\\partial \\theta_i} & =  \\cfrac{y_i - b^{'}(\\theta_i)}{a_i(\\phi)}\n\\end{align}\\]\nCho đạo hàm của \\(l(\\textbf{y},\\theta)\\) theo \\(\\theta_i\\) bằng 0 chúng ta có \\(y_i - b^{'}(\\theta^S_i) = 0\\) hay \\(\\theta^S_i = (b^{'})^{-1}(y_i)\\).Bạn đọc lưu ý rằng giá trị hàm Log-likelihood đạt cực đại tại \\(\\boldsymbol{\\theta}^S\\) không có nghĩa là tham số \\(\\boldsymbol{\\theta}^S\\) là lựa chọn tốt nhất cho mô hình tuyến tính tổng quát bởi tham số có đến \\(n\\) bậc tự . \\(\\boldsymbol{\\theta}^S\\) cho chúng ta thông tin về giá trị cận trên của \\(l(\\textbf{y},\\theta)\\) khi \\(\\theta\\) thay đổi. Thước đo deviance của mô hình \\(M\\), ký hiệu \\(D*(y,\\theta^M)\\) được định nghĩa là hai lần khoảng cách từ \\(l(\\textbf{y},\\boldsymbol{\\theta}^M)\\) đến giá trị tối đa \\(l(\\textbf{y},\\boldsymbol{\\theta}^S)\\). Deviance của một mô hình cho biết mô hình được lựa chọn gần với phân phối quan sát được như thế nào, và mô hình có deviance càng nhỏ thì càng giải thích tốt hơn biến phụ thuộc\n\\[\\begin{align}\nD^{*}(y, \\theta^M) &= 2 \\left( l(\\textbf{y},\\theta^S) - l(\\textbf{y},\\theta^M) \\right) \\\\\n& = 2 \\sum\\limits_{=1}^n  \\cfrac{y_i (\\theta^S_i - \\theta^M_i) - (b(\\theta^S_i) - b(\\theta^M_i))}{a_i(\\phi)}\n\\end{align}\\]Trong trường hợp hàm \\(a_i(\\phi)\\) là tuyến tính theo \\(\\phi\\); \\(a_i(\\phi) = \\cfrac{\\phi}{w_i}\\), ta có\n\\[\\begin{align}\nD^{*}(y, \\theta^M) & = \\cfrac{1}{\\phi} \\sum\\limits_{=1}^n 2w_i \\cdot \\left[y_i (\\theta^S_i - \\theta^M_i) - (b(\\theta^S_i) - b(\\theta^M_i)) \\right]  = \\cfrac{D(y, \\theta^M)}{\\phi}\n\\end{align}\\]\nvới\n\\[\\begin{align}\nD(y, \\theta^M) & = \\sum\\limits_{=1}^n 2w_i \\cdot \\left[y_i (\\theta^S_i - \\theta^M_i) - (b(\\theta^S_i) - b(\\theta^M_i)) \\right]\n\\end{align}\\]\ntrong đó \\(D(y, \\theta^M)\\) là thước đo deviance bỏ qua ảnh hưởng của tham số dispersion \\(\\phi\\).Ví dụ 1: biến mục tiêu \\(Y_i\\) có phân phối chuẩn: \\(Y_i \\sim \\mathcal{N}(\\mu_i, \\sigma)\\) và hàm liên kết \\(g(x) = x\\). Có thể viết hàm mật độ của \\(Y_i\\) dưới dạng phân phối mũ như sau\n\\[\\begin{align}\nf(y, \\theta_i, \\phi) & = \\exp\\left[ \\cfrac{\\theta_i y - \\theta_i^2/2} {\\phi} - \\cfrac{y^2}{2\\phi} - \\cfrac{1}{2} log(2 \\pi \\phi) \\right]\n\\end{align}\\]\nvới \\(\\theta_i = \\mu_i\\) và \\(\\phi = \\sigma^2\\). Ta có \\(\\theta^S_i = (b^{'})^{-1}(y_i) = y_i\\), đó deviance tính trên quan sát \\(\\textbf{y}\\) được xác định như sau\n\\[\\begin{align}\nD(y, \\theta^M) & = \\sum\\limits_{=1}^n 2 \\cdot \\left[y_i (\\theta^S_i - \\theta^M_i) - (b(\\theta^S_i) - b(\\theta^M_i)) \\right] \\\\\n& = \\sum\\limits_{=1}^n (y_i - \\theta^M_i)^2 \\\\\n& = \\sum\\limits_{=1}^n \\left[y_i -  (\\beta_0 + \\beta_1 \\cdot x_{,1} + \\cdots + \\beta_p \\cdot x_{p,1}) \\right]^2\n\\end{align}\\]\nTrong trường hợp hồi quy tuyến tính thông thường, deviance chính là tổng bình phương sai số.Ví dụ 1: biến mục tiêu \\(Y_i\\) có phân phối chuẩn: \\(Y_i \\sim \\mathcal{N}(\\mu_i, \\sigma)\\) và hàm liên kết \\(g(x) = x\\). Có thể viết hàm mật độ của \\(Y_i\\) dưới dạng phân phối mũ như sau\n\\[\\begin{align}\nf(y, \\theta_i, \\phi) & = \\exp\\left[ \\cfrac{\\theta_i y - \\theta_i^2/2} {\\phi} - \\cfrac{y^2}{2\\phi} - \\cfrac{1}{2} log(2 \\pi \\phi) \\right]\n\\end{align}\\]\nvới \\(\\theta_i = \\mu_i\\) và \\(\\phi = \\sigma^2\\). Ta có \\(\\theta^S_i = (b^{'})^{-1}(y_i) = y_i\\), đó deviance tính trên quan sát \\(\\textbf{y}\\) được xác định như sau\n\\[\\begin{align}\nD(y, \\theta^M) & = \\sum\\limits_{=1}^n 2 \\cdot \\left[y_i (\\theta^S_i - \\theta^M_i) - (b(\\theta^S_i) - b(\\theta^M_i)) \\right] \\\\\n& = \\sum\\limits_{=1}^n (y_i - \\theta^M_i)^2 \\\\\n& = \\sum\\limits_{=1}^n \\left[y_i -  (\\beta_0 + \\beta_1 \\cdot x_{,1} + \\cdots + \\beta_p \\cdot x_{p,1}) \\right]^2\n\\end{align}\\]\nTrong trường hợp hồi quy tuyến tính thông thường, deviance chính là tổng bình phương sai số.Ví dụ 2: biến mục tiêu \\(Y_i\\) có phân phối Poisson \\(Y_i \\sim \\mathcal{P}(\\lambda_i)\\) và hàm liên kết \\(g(\\cdot) = log(\\cdot)\\).\n\\[\\begin{align}\nf(y; \\theta_i) = exp\\left[ \\cfrac{\\theta_i y - exp(\\theta_i)}{1} - log(\\Gamma(y+1)) \\right] \\text{ với } \\lambda_i = exp(\\theta_i)\n\\end{align}\\]\nTa có \\(\\theta^S_i = (b^{'})^{-1}(y_i) = log(y_i)\\), đó deviance tính trên quan sát \\(\\textbf{y}\\) được xác định như sau\n\\[\\begin{align}\nD(y, \\theta^M) & = \\sum\\limits_{=1}^n 2 \\cdot \\left[y_i (\\theta^S_i - \\theta^M_i) - (b(\\theta^S_i) - b(\\theta^M_i)) \\right] \\\\\n& = \\sum\\limits_{=1}^n 2 \\cdot \\left[y_i (log(y_i) - \\theta^M_i) - (y_i - exp(\\theta^M_i)) \\right] \\\\\n& = \\sum\\limits_{=1}^n 2 \\cdot \\left[y_i \\left(log(y_i) - log(\\mu^M_i)\\right) - (y_i - \\mu^M_i) \\right]\n\\end{align}\\]\nvới \\(\\mu^M_i = \\exp(\\beta_0 + \\beta_1 \\cdot x_{,1} + \\cdots + \\beta_p \\cdot x_{p,1})\\).Ví dụ 2: biến mục tiêu \\(Y_i\\) có phân phối Poisson \\(Y_i \\sim \\mathcal{P}(\\lambda_i)\\) và hàm liên kết \\(g(\\cdot) = log(\\cdot)\\).\n\\[\\begin{align}\nf(y; \\theta_i) = exp\\left[ \\cfrac{\\theta_i y - exp(\\theta_i)}{1} - log(\\Gamma(y+1)) \\right] \\text{ với } \\lambda_i = exp(\\theta_i)\n\\end{align}\\]\nTa có \\(\\theta^S_i = (b^{'})^{-1}(y_i) = log(y_i)\\), đó deviance tính trên quan sát \\(\\textbf{y}\\) được xác định như sau\n\\[\\begin{align}\nD(y, \\theta^M) & = \\sum\\limits_{=1}^n 2 \\cdot \\left[y_i (\\theta^S_i - \\theta^M_i) - (b(\\theta^S_i) - b(\\theta^M_i)) \\right] \\\\\n& = \\sum\\limits_{=1}^n 2 \\cdot \\left[y_i (log(y_i) - \\theta^M_i) - (y_i - exp(\\theta^M_i)) \\right] \\\\\n& = \\sum\\limits_{=1}^n 2 \\cdot \\left[y_i \\left(log(y_i) - log(\\mu^M_i)\\right) - (y_i - \\mu^M_i) \\right]\n\\end{align}\\]\nvới \\(\\mu^M_i = \\exp(\\beta_0 + \\beta_1 \\cdot x_{,1} + \\cdots + \\beta_p \\cdot x_{p,1})\\).Kết quả ước lượng khi sử dụng hàm glm() trong R hiển thị hai giá trị là Null deviance và Residual deviance. Null deviance được tính toán với giả thiết là chỉ có hệ số chặn \\(\\beta_0\\), còn Residual deviance được tính toán với véc-tơ \\(\\boldsymbol{\\beta}\\) đầy đủ.Giá trị Residual deviance ngoài sử dụng để sánh hai mô hình có cùng phân phối của biến phụ thuộc còn được sử dụng để lựa chọn biến trong mô hình. Giả sử hai mô hình \\(M1\\) và \\(M2\\) có cùng phân phối cho biến phụ thuộc và có cùng hàm liên kết \\(g(.)\\), mô hình \\(M1\\) có \\(m_1\\) biến độc lập trong khi mô hình \\(M_2\\) có \\(m_2\\) biến độc lập, bao gồm tất cả các biến độc lập của mô hình \\(M1\\). Nói một cách khác, \\(M_2\\) có nhiều hơn \\(M_1\\) là \\((m_2 - m_1)\\) biến độc lập. Mô hình \\(M_2\\) sẽ có deviance lớn hơn mô hình \\(M_1\\) có nhiều biến độc lập hơn, tuy nhiên việc thêm \\((m_2 - m_1)\\) biến độc lập vào mô hình có ý nghĩa thống kê nếu hiệu số giữa \\(D^*(y, \\theta^{M_2})\\) và \\(D^*(y, \\theta^{M_1})\\) là đủ lớn.Có thể chứng minh được rằng khi kích thước dữ liệu đủ lớn, hiệu số giữa \\(D^*(y, \\theta^{M_2})\\) và \\(D^*(y, \\theta^{M_1})\\) là một biến ngẫu nhiên phân phối xấp xỉ phân phối \\(\\chi^2\\) với bậc tự \\((m_2 - m_1)\\)\n\\[\\begin{align}\nD^{*}(y, \\theta^{M_1}) - D^{*}(y, \\theta^{M_2}) = 2 \\cdot \\log\\left( \\cfrac{L(y,\\theta^{M_2})}{L(y,\\theta^{M_1})} \\right) \\sim \\chi^2(m_2 - m_1)\n\\end{align}\\]Ví dụ 3: trong dữ liệu \\(exposure.csv\\), khi biến \\(Claim\\_Count\\) được giả thiết có phân phối Poisson và giá trị trung bình được giải thích bằng độ tuổi, hoặc giới tính của người được bảo hiểm, hoặc cả hai biến. Chúng ta gọi mô hình \\(M_1\\) là mô hình mà giá trị trung bình của biến phụ thuộc được giải thích bằng biến độ tuổi, mô hình \\(M_2\\) là mô hình mà giá trị trung bình của biến phụ thuộc được giải thích bằng cả biến giới tính, và mô hình \\(M_3\\) là mô hình mà biến phụ thuộc phụ thuộc vào cả độ tuổi và giới tính của khách hàng.Mô hình \\(M_1\\) và \\(M_2\\) đều chỉ có một biến độc lập và \\(M_1\\) có deviance nhỏ hơn \\(M_2\\) đó mô hình \\(M_1\\) tốt hơn \\(M_2\\). sánh mô hình \\(M_1\\) với 1 biến độc lập là \\(Age\\) với mô hình \\(M_3\\) có 2 biến độc lập là \\(Age\\) và \\(Gender\\) cần dựa trên kiểm định \\(\\chi^2\\). Ta có \\(D^{*}(y, \\theta^{M_1}) - D^{*}(y, \\theta^{M_2}) = 8324.2 - 8316.8 = 7.4\\). Giá trị 7.4 tương ứng với mức xác suất \\(0.994\\) của phân phối \\(\\chi^2(1)\\). Điều này có ý nghĩa là ở mức độ tin cậy 99%, có thể kết luận rằng thêm biến \\(Gender\\) vào mô hình \\(M_1\\) là có ý nghĩa thống kê, tuy nhiên việc thêm biến \\(Gender\\) lại không có ý nghĩa thống kê ở mức độ tin cậy \\(99.5\\%\\).","code":"\ndat<-read.csv(\"../KHDL_KTKD Final/Dataset/exposure.csv\")\n\nglm_M1<-glm(Claim_Count~Age, family = poisson(link=\"log\"), data = dat)\nglm_M2<-glm(Claim_Count~Gender, family = poisson(link=\"log\"), data = dat)\nglm_M3<-glm(Claim_Count~Age+Gender, family = poisson(link=\"log\"), data = dat)\n\nsummary(glm_M1)\nsummary(glm_M2)\nsummary(glm_M3)"},{"path":"mô-hình-tuyến-tính-tổng-quát..html","id":"giá-trị-hàm-log-likelihood-aic-và-bic","chapter":"Chương 9 Mô hình tuyến tính tổng quát.","heading":"9.5.2 Giá trị hàm log-likelihood, AIC và BIC","text":"Deviance là một thước đo hữu ích trong sánh các mô hình có cùng phân phối của biến phụ thuộc và hàm liên kết. Khi các mô hình cần sánh không có chung phân phối của biến phụ thuộc thì hiệu giữa hai hàm log-likelihood sẽ không có phân phối \\(\\chi^2\\) và khi đó việc sánh các mô hình là không thế thực hiện được.Trong phần trước, chúng ta đã định nghĩa \\(l(y,\\theta^M)\\) là giá trị tối đa của hàm log-likelihood cho mô hình M với tập hợp các biến độc lập đã lựa chọn. Nhìn chung, giá trị của \\(l(y,\\theta^M)\\) không cho biết nhiều thông tin về độ phù hợp của mô hình, nhưng người xây dựng mô hình thường mong muốn giá trị này càng lớn thì càng tốt. Tuy nhiên, có một vấn đề khi sánh trực tiếp giá trị \\(l(y,\\theta^M)\\) giữa các mô hình là: một mô hình bao gồm nhiều biến độc lập hơn thì rất có thể có giá trị \\(l(y,\\theta^M)\\) lớn hơn. Để giải quyết vấn đề này, có ba thước đo được điều chỉnh từ \\(l(y,\\theta^M)\\) thường được sử dụng cho mô hình tuyến tính tổng quát với mục đích đánh giá và sánh giữa các mô hình là AIC, AICC và BIC.Chỉ tiêu AIC, viết tắt của Akaike information criterion, là chỉ tiêu đơn giản nhất được tính toán từ công thức như sau\n\\[\\begin{align}\nAIC = 2 \\left(-l(y,\\theta^M)+r\\right)\n\\end{align}\\]\ntrong đó \\(r\\) là số lượng tham số cần ước lượng trong mô hình. Giá trị \\(AIC\\) càng nhỏ thì mô hình càng khớp hơn với dữ liệu.Chỉ tiêu AICC (hoặc AICs) được điều chỉnh từ AIC, được sử dụng khi kích thước dữ liệu không quá lớn để thay thế cho AIC, và được tính bởi công thức như sau\n\\[\\begin{align}\nAICC = AIC + \\cfrac{2r(r+1)}{n-r-1}\n\\end{align}\\]\nKhi kích thước dữ liệu \\(n\\) lớn, AIC và AICC sẽ tương đương nhau \\(\\cfrac{2r(r+1)}{n-r-1}\\) nhỏ.Chỉ tiêu thứ ba là BIC, là viết tắt của Bayesian information criterion. Trong một vài tài liệu BIC còn được gọi là SBC. Cũng giống như AIC và AICC, BIC là chỉ tiêu được tính toán từ giá trị cực đại của hàm log-likelihood điểu chỉnh để phản ánh số lượng tham số sử dụng trong mô hình là nhiều hay ít\n\\[\\begin{align}\nBIC = 2 \\left(-l(y,\\theta^M)+r log(n)\\right)\n\\end{align}\\]Cơ sở lý thuyết cho các chỉ tiêu AIC, AICC, và BIC bạn đọc có thể tìm thấy trong bất kỳ tài liệu thống kê toán nào, đó chúng tôi sẽ không trình bày trong cuốn sách này. Chúng tôi muốn nhấn mạnh vào góc độ ứng dụng của các chỉ tiêu này khi sử dụng để sánh các mô hình.Ví dụ 1: chúng ta quay trở lại dữ liệu “MotoInsurance.csv” khi biến mục tiêu \\(Y\\) chỉ nhận hai giá trị là “” hoặc “Yes”. Biến \\(Y\\) có thể là biến dạng nhị phân hoặc vừa có thể là biến dạng đếm nếu chúng ta cho tương đương giá trị “” tương đương với 0 và giá trị “Yes” tương ứng với 1. Khi xây dựng mô hình tuyến tính tổng quát, chúng ta có thể sử dụng phân phối Poisson cho biến mục tiêu với hàm liên kết \\(g(\\cdot) = log(\\cdot)\\).\\(Y\\) là biến nhị phân nên một cách tự nhiên, bạn đọc sẽ cân nhắc sử dụng phân phối nhị phân cho \\(Y\\). Chúng ta xây dựng mô hình tuyến tính tổng quát với phân phối nhị thức cho \\(Y\\) và hàm liên kết là hàm \\(logit\\). Thay vì sử dụng cả 4 biến độc lập, chúng ta chỉ sử dụng hai biến độc lập là \\(age\\) và \\(sex\\),Tương tự như mô hình \\(glm.pois\\), mô hình \\(glm.binom.logit\\) cũng có tất cả các hệ số của biến độc lập khác 0. Làm thế nào để biết rằng mô hình \\(glm.pois\\) sử dụng phân phối Poisson với 4 biến độc lập hay mô hình \\(glm.binom.logit\\) với phân phối nhị thức cho \\(Y\\) và 2 biến độc lập là tốt hơn? Nếu chúng ta sử dụng thước đo deviance thì kết quả sẽ không chính xác vì các mô hình có phân phối của biến phụ thuộc khác nhau. Chỉ tiêu \\(AIC\\) được tính toán sẵn từ hàm \\(glm\\) có thể được sử dụng để sánh hai mô hình trong trường hợp này. Chỉ tiêu AIC của \\(glm.pois\\) là 5202.7 trong khi chỉ tiêu AIC của \\(glm.binom.logit\\) là 4941.3. Mô hình \\(glm.binom.logit\\) có AIC nhỏ hơn nên sẽ tốt hơn để mô hình hóa dữ liệu trong trường hợp này.Ví dụ 2: chúng ta sẽ tiếp tục với dữ liệu “MotoInsurance.csv”. Một cách tự nhiên, khi sử dụng phân phối nhị thức cho biến phụ thuộc, bạn đọc sử dụng hàm liên kết là hàm \\(logit\\). Liệu lựa chọn này có là tốt nhất để mô hình hóa dữ liệu mà chúng ta đang nghiên cứu? Bạn đọc có thể sử dụng chỉ tiêu AIC để lựa chọn hàm liên kết phù hợp. Chúng ta sẽ sử dụng 4 biến độc lập là \\(age\\), \\(sex\\), \\(urban\\), và \\(seniority\\) để giải thích giá trị trung bình của biến phụ thuộc khi xây dựng mô hìnhMô hình \\(glm.binom.probit\\) có deviance và AIC nhỏ hơn \\(glm.binom.logit\\), điều này cho biết sử dụng hàm liên kết \\(probit\\) sẽ cho kết quả tốt hơn với hàm liên kết \\(logit\\).","code":"\ndat<-read.csv(\"../KHDL_KTKD Final/Dataset/MotoInsurance.csv\")\ndat$Y<-ifelse(dat$Y==\"Yes\",1,0)\ndat$sex<-as.factor(dat$sex)\ndat$urban<-as.factor(dat$urban)\nglm.pois<-glm(Y~age+sex+urban+seniority,data=dat, family = poisson(link = \"log\"))\nsummary(glm.pois)## \n## Call:\n## glm(formula = Y ~ age + sex + urban + seniority, family = poisson(link = \"log\"), \n##     data = dat)\n## \n## Deviance Residuals: \n##     Min       1Q   Median       3Q      Max  \n## -2.0824  -0.7362  -0.5465   0.5711   1.9556  \n## \n## Coefficients:\n##              Estimate Std. Error z value Pr(>|z|)    \n## (Intercept) -0.286511   0.105615  -2.713  0.00667 ** \n## age         -0.034131   0.002546 -13.406  < 2e-16 ***\n## sexM        -0.491418   0.057331  -8.572  < 2e-16 ***\n## urban1       0.625032   0.053999  11.575  < 2e-16 ***\n## seniority    0.070888   0.004257  16.652  < 2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## (Dispersion parameter for poisson family taken to be 1)\n## \n##     Null deviance: 2938.1  on 3999  degrees of freedom\n## Residual deviance: 2418.7  on 3995  degrees of freedom\n## AIC: 5202.7\n## \n## Number of Fisher Scoring iterations: 5\ndat$Y<-as.factor(dat$Y) # đổi biến Y thành factor\nglm.binom.logit<-glm(Y~age+sex,data=dat, family = binomial(link = \"logit\"))\nsummary(glm.binom.logit)## \n## Call:\n## glm(formula = Y ~ age + sex, family = binomial(link = \"logit\"), \n##     data = dat)\n## \n## Deviance Residuals: \n##     Min       1Q   Median       3Q      Max  \n## -1.4544  -0.9261  -0.7396   1.2607   2.0088  \n## \n## Coefficients:\n##              Estimate Std. Error z value Pr(>|z|)    \n## (Intercept)  1.109828   0.130876   8.480   <2e-16 ***\n## age         -0.026603   0.002709  -9.822   <2e-16 ***\n## sexM        -0.723479   0.076585  -9.447   <2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## (Dispersion parameter for binomial family taken to be 1)\n## \n##     Null deviance: 5163.3  on 3999  degrees of freedom\n## Residual deviance: 4935.3  on 3997  degrees of freedom\n## AIC: 4941.3\n## \n## Number of Fisher Scoring iterations: 4\nglm.binom.logit<-glm(Y~age+sex+seniority+urban,data=dat, family = binomial(link = \"logit\"))\nglm.binom.probit<-glm(Y~age+sex+seniority+urban,data=dat, family = binomial(link = \"probit\"))\nsummary(glm.binom.logit)## \n## Call:\n## glm(formula = Y ~ age + sex + seniority + urban, family = binomial(link = \"logit\"), \n##     data = dat)\n## \n## Deviance Residuals: \n##     Min       1Q   Median       3Q      Max  \n## -2.4424  -0.8115  -0.5046   1.0440   2.7439  \n## \n## Coefficients:\n##              Estimate Std. Error z value Pr(>|z|)    \n## (Intercept)  0.793757   0.144541   5.492 3.98e-08 ***\n## age         -0.058754   0.003525 -16.668  < 2e-16 ***\n## sexM        -0.983040   0.085377 -11.514  < 2e-16 ***\n## seniority    0.133569   0.006808  19.618  < 2e-16 ***\n## urban1       1.174515   0.078060  15.046  < 2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## (Dispersion parameter for binomial family taken to be 1)\n## \n##     Null deviance: 5163.3  on 3999  degrees of freedom\n## Residual deviance: 4295.8  on 3995  degrees of freedom\n## AIC: 4305.8\n## \n## Number of Fisher Scoring iterations: 4\nsummary(glm.binom.probit)## \n## Call:\n## glm(formula = Y ~ age + sex + seniority + urban, family = binomial(link = \"probit\"), \n##     data = dat)\n## \n## Deviance Residuals: \n##     Min       1Q   Median       3Q      Max  \n## -2.4931  -0.8168  -0.4987   1.0639   2.9359  \n## \n## Coefficients:\n##              Estimate Std. Error z value Pr(>|z|)    \n## (Intercept)  0.449579   0.085724   5.244 1.57e-07 ***\n## age         -0.034482   0.002009 -17.163  < 2e-16 ***\n## sexM        -0.577303   0.050821 -11.360  < 2e-16 ***\n## seniority    0.077888   0.003890  20.021  < 2e-16 ***\n## urban1       0.697759   0.046129  15.126  < 2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## (Dispersion parameter for binomial family taken to be 1)\n## \n##     Null deviance: 5163.3  on 3999  degrees of freedom\n## Residual deviance: 4292.4  on 3995  degrees of freedom\n## AIC: 4302.4\n## \n## Number of Fisher Scoring iterations: 5"},{"path":"reference.html","id":"reference","chapter":"Chương 10 REFERENCE","heading":"Chương 10 REFERENCE","text":"1. Annette J. Dobson Adrian G. Barnett (2018). Introduction Generalized Linear Models.2. Alan Agresti. (2015). Foundations Linear Generalized Linear Models.","code":"## \n## Attaching package: 'dplyr'## The following objects are masked from 'package:stats':\n## \n##     filter, lag## The following objects are masked from 'package:base':\n## \n##     intersect, setdiff, setequal, union## \n## Attaching package: 'kableExtra'## The following object is masked from 'package:dplyr':\n## \n##     group_rows## \n## Attaching package: 'gridExtra'## The following object is masked from 'package:dplyr':\n## \n##     combine## \n## Attaching package: 'pryr'## The following object is masked from 'package:dplyr':\n## \n##     where## Loading required package: lattice## Loading required package: splines## Loading required package: foreach## Loaded gam 1.22-3## randomForest 4.7-1.1## Type rfNews() to see new features/changes/bug fixes.## \n## Attaching package: 'randomForest'## The following object is masked from 'package:gridExtra':\n## \n##     combine## The following object is masked from 'package:ggplot2':\n## \n##     margin## The following object is masked from 'package:dplyr':\n## \n##     combine"},{"path":"mô-hình-cây-quyết-định.html","id":"mô-hình-cây-quyết-định","chapter":"Chương 11 Mô hình cây quyết định","heading":"Chương 11 Mô hình cây quyết định","text":"Trong chương này, chúng tôi trình bày các phương pháp hồi quy và phân loại dựa trên cây quyết định. Mô hình dạng cây quyết định liên quan đến kỹ thuật phân chia miền giá trị của các biến giải thích hành các miền nhỏ có tính chất tương tự nhau. Để đưa ra dự đoán cho một quan sát, mô hình cây quyết định hồi quy sử dụng giá trị trung bình của các quan sát nằm trong miền tương ứng. Đối với bài toán phân loại, cây quyết định thường sử dụng giá trị mode của các quan sát nằm trong miền này. Các quy tắc dùng để phân tách miền xác định của các biến giải thích được sử dụng có thể được mô tả theo kiểu một cây quyết định nên các kiểu xây dựng mô hình như vậy thường được gọi là mô hình dạng cây quyết định.Các phương pháp hồi quy và phân loại dựa trên cây quyết định khá đơn giản và dễ dàng trong việc diễn giải. Tuy nhiên, các phương pháp này thường không sánh được về khả năng dự đoán chính xác với các phương pháp học máy có giám sát đã được trình bày trong các chương trước, chẳng hạn như hồi quy Splines, Smoothing Splines, hoặc mô hình cộng tính tổng quát khi có nhiều biến giải thích. Tuy nhiên, mô hình dạng cây quyết định lại thích hợp khi sử dụng kết hợp vơi các kỹ thuật thống kê hiện đại như rừng ngẫu nhiên hoặc học tăng cường để cho kết quả dự đoán chính xác vượt trội. Một lợi thế khác của mô hình dạng cây quyết định đó là mô hình này có thể sử dụng trực tiếp với cả bài toán hồi quy và phân loại mà không cần một sự biến đổi đáng kể nào về cách tiếp cận.Trong chương này chúng tôi sẽ trình bày về mô hình dạng cây quyết định sử dụng trong bài toán hồi quy và phân loại và sau đó giới thiệu đến bạn đọc thuật toán kỹ thuật kết hợp nhiều cây quyết định để cải thiện khả năng dự đoán của mô hình còn được biến đến với tên gọi là thuật toán \\(rừng\\) \\(ngẫu\\) \\(nhiên\\). Kỹ thuật học tăng cường (boosting) sẽ được giới thiệu đến bạn đọc trong chương tiếp theo.","code":""},{"path":"mô-hình-cây-quyết-định.html","id":"cơ-bản-về-mô-hình-cây-quyết-định","chapter":"Chương 11 Mô hình cây quyết định","heading":"11.1 Cơ bản về mô hình cây quyết định","text":"Như chúng tôi đã đề cập, cây quyết định có thể được áp dụng cho cả bài toán hồi quy và bài toán phân loại. Chúng ta xem xét bài toán hồi quy trước sau đó chuyển sang cây quyết định phân loại. Bạn đọc sẽ thấy rằng cách xây dựng mô hình cây quyết định và cây quyết định hồi quy là hoàn toàn tương tự nhau, chỉ khác nhau về mục tiêu tối thiểu hóa.","code":""},{"path":"mô-hình-cây-quyết-định.html","id":"mô-hình-cây-quyết-định-hồi-quy","chapter":"Chương 11 Mô hình cây quyết định","heading":"11.1.1 Mô hình cây quyết định hồi quy","text":"","code":""},{"path":"mô-hình-cây-quyết-định.html","id":"xây-dựng-cây-quyết-định-hồi-quy-trên-dữ-liệu-boston","chapter":"Chương 11 Mô hình cây quyết định","heading":"11.1.1.1 Xây dựng cây quyết định hồi quy trên dữ liệu Boston","text":"Để bắt đầu với cây quyết định hồi quy, chúng ta bắt đầu bằng một ví dụ đơn giản trên một cây hồi quy đơn biến. Chúng tôi sử dụng dữ liệu Boston để dự đoán biến mục tiêu là giá nhà tại các vùng (biến \\(medv\\), đơn vị là nghìn USD) dựa trên tỷ lệ số người có mức sống thấp trong vùng đó (biến \\(lstat\\), đơn vị là %). Hình 11.1 mô tả một cây hồi quy phù hợp với dữ liệu này.\nFigure 11.1: Cây quyết định hồi quy kích thước bằng 3 mô tả giá nhà phụ thuộc vào tỷ lệ người sống dưới mức trung bình trên dữ liệu Boston. Tại mỗi điểm phân nhánh, ký hiệu lstat < c chỉ định phân nhánh sang bên trái, nghĩa là các điểm dữ liệu thuộc có tính chất lstat < c nằm trong nhánh bên trái trong khi các điểm dữ liệu có lstat lớn hơn hoặc bằng c sẽ nằm sang nhánh bên phải. Cây quyết định ở trên có hai nút và ba lá tương ứng với ba tập hợp con của dữ liệu.\nBạn đọc có thể thấy rằng một cây quyết định bao gồm một chuỗi các quy tắc phân chia, bắt đầu từ ngọn cây xuống dưới. Phần phân chia trên cùng chỉ định các quan sát có \\(lstat < 9.725 (\\%)\\) cho nhánh cây bên trái và các quan sát có \\(lstat \\geq 9.725(\\%)\\) cho nhánh bên phải. Giá nhà cho các vùng có tỷ lệ người có thu nhập thấp lớn hơn hoặc bằng 9.725% được dự đoán bằng giá trị trung bình của các quan sát trong dữ liệu huấn luyện và bằng 17.34 nghìn USD.\nNhững vùng có \\(lstat \\geq 9.725(\\%)\\) được chỉ định vào nhánh bên trái và sau đó lại được chia nhỏ hơn thành hai nhánh, nhánh bên phải là các quan sát có \\(lstat < 4.65(\\%)\\) và nhánh bên phải là các quan sát có \\(lstat \\geq 4.65(\\%)\\). Giá nhà được dự đoán tại các vùng có \\(lstat < 4.65(\\%)\\) là giá trị trung bình của các ngôi nhà trong dữ liệu huấn luyện mô hình có tính chất tương ứng và bằng 39.72 nghìn USD. Tương tự, tại các vùng có \\(lstat\\) nhận giá trị từ 4.65% đến 9.725% giá nhà được dự đoán là 26.65 nghìn USD. Ba tập con riêng biệt của dữ liệu về giá nhà ở Boston được phân tách theo cây quyết định dựa trên biến \\(lstat\\) và giá trị ước lượng tương ứng cho giá nhà được tổng kết lại như sau:\n\\[\\begin{align}\nR_1 &= \\{X \\ | lstat < 4.65\\} \\rightarrow \\hat{y} = 39.72 \\\\\nR_2 &= \\{X \\ | lstat \\geq 4,65 \\ \\ \\& \\ \\ lstat < 9.725 \\} \\rightarrow \\hat{y} = 26.65  \\\\\nR_3 &= \\{X \\ | lstat \\geq 9.725 \\} \\rightarrow \\hat{y} = 17.34\n\\tag{11.1}\n\\end{align}\\]\nFigure 11.2: Hàm f được ước lượng từ mô hình cây quyết định có dạng hàm bậc thang nhận giá trị bằng hằng số trên các miền giá trị được phân tách của biến giải thích.\nHình 11.2 mô tả hàm số được ước lượng theo cây quyết định khi có một biến giải thích duy nhất. Bạn đọc có thể thấy rằng trong trường hợp chỉ có một biến, cây quyết định cũng giống như các hồi quy theo đa thức theo từng đoạn, với bậc của các đa thức là bằng 0. Nói cách khác, hàm \\(f\\) là hằng số trên các khoảng giá trị khác nhau của biến giải thích. Các nút chia dữ liệu ra thành các miền con là 9.725 và 4.65 được tính toán sao cho sai số (RSS) trên dữ liệu huấn luyện mô hình là nhỏ nhất. Chúng ta sẽ thảo luận chi tiết về ước lượng mô hình cây quyết định ở phần tiếp theo của chương.\nĐiều gì xảy ra nếu chúng ta sử dụng đồng thời hai biến là biến \\(lstat\\) và biến \\(rm\\). Biến \\(rm\\) cho biết trung bình trong một ngôi nhà ở vùng quan sát có bao nhiêu phòng. Hình 11.3 mô tả một cây quyết định hồi quy khi sử dụng đồng thời hai biến \\(lstat\\) biến \\(rm\\)\nFigure 11.3: Cây quyết định hồi quy kích thước bằng ba mô tả giá nhà phụ thuộc vào tỷ lệ người sống dưới mức trung bình và số lượng phòng trung bình của mỗi ngôi nhà trên dữ liệu Boston. Điểm phân nhánh (nút) đầu tiên phụ thuộc vào biến rm. Nút thứ hai phụ thuộc vào biến lstat.\nBạn đọc có thể nhận thấy sự khác biệt giữa mô hình cây quyết định có một biến giải thích trong Hình 11.1 và mô hình cây quyết định có hai biến giải thích 11.3. Khi có hai biến giải thích, nút đầu tiên được sử dụng để phân tách dữ liệu là \\(rm < 6.941\\) và nút thứ hai sử dụng để phân tách dữ liệu là \\(lstat < 14.4\\). Tuy nhiên, cần lưu ý rằng nút phân tách thứ hai không được thực hiện trên toàn bộ dữ liệu, mà chỉ được thực hiện trên miền bên trái của nút \\(rm < 6.941\\). Như vậy, ba miền dữ liệu được định nghĩa theo cây quyết định này và giá trị ước lượng tương ứng cho giá nhà có thể được tóm tắt như sau\n\\[\\begin{align}\nR_1 &= \\{X \\ | rm < 6.941 \\ \\ \\& \\ \\  lstat < 14.4 \\} \\rightarrow \\hat{y} = 23.35 \\\\\nR_2 &= \\{X \\ | rm < 6.941 \\ \\ \\& \\ \\ lstat \\geq 14.4  \\} \\rightarrow \\hat{y} = 14.96  \\\\\nR_3 &= \\{X \\ | rm \\geq 6.941 \\} \\rightarrow \\hat{y} = 37.24\n\\tag{11.2}\n\\end{align}\\]\nFigure 11.4: Mô hình cây quyết định hồi quy chia hình chữ nhật thành ba phần R1, R2, và R3 và giá nhà trong mỗi miền là hằng số\nMô hình cây quyết định hồi quy được mô tả trong các hình 11.3, 11.4, hoặc phương trình (11.1) có thể được giải thích rất dễ dàng như sau: ở những vùng có số phòng trung bình lớn hơn 6.941 (miền R3) giá nhà được dự đoán là 37.24 nghìn USD, ở những vùng số phòng trung bình nhỏ hơn 6.941 và tỷ lệ số người thu nhập thấp dưới 14.4(%) (miền R1) thì giá nhà được dự đoán là 23.35 nghìn USD, và cuối cung ở những vùng số phòng trung bình nhỏ hơn 6.941 và tỷ lệ số người thu nhập thấp lớn hơn 14.4(%) (miền R2) thì giá nhà được dự đoán là 14.96 nghìn USD. Bạn đọc có thể thấy rằng các mô hình dạng cây quyết định là rất dễ dàng để giải thích, thậm chí còn dễ dàng hơn với các mô hình hồi quy tuyến tính.Một cách tổng quát, có thể tóm tắt lại quá trình xây dựng một cây quyết định hồi quy để mô tả mối liên hệ giữa biến mục tiêu \\(Y\\) và các \\(p\\) biến giải thích \\(X_1\\), \\(X_2\\), \\(\\cdots\\), \\(X_p\\) như sau:Thứ nhất: Chúng ta chia không gian các giá trị có thể có của các biến giải thích thành \\(J\\) các vùng riêng biệt và không chồng lấn lên nhau, tạm gọi là \\(R_1\\), \\(R_2\\), \\(\\cdots\\) , \\(R_K\\).Thứ hai: Đối với tất cả quan sát rơi vào vùng \\(R_k\\), với \\(1 \\leq k \\leq K\\), chúng ta đưa ra dự đoán giống nhau là giá trị trung bình của các giá trị của biến mục tiêu \\(Y\\) của các quan sát của dữ liệu huấn luyện nằm trong vùng \\(R_k\\). Ví dụ: giả sử ở bước thứ nhất, chúng ta có hai vùng, \\(R_1\\) và \\(R_2\\), và trung bình của biến mục tiêu của các quan sát trong dữ liệu huấn luyện ở vùng \\(R_1\\) là 5, trong khi trung bình của biến mục tiêu của các quan sát trong dữ liệu huấn luyện ở vùng \\(R_2\\) là 10. Khi đó, đối với một quan sát bất kỳ \\(X = x\\), nếu \\(x \\R_1\\) chúng ta sẽ dự đoán biến mục tiêu là 5 và nếu \\(x \\R_2\\) chúng ta sẽ dự đoán biến mục tiêu là 10.","code":""},{"path":"mô-hình-cây-quyết-định.html","id":"ước-lượng-tham-số-mô-hình-cây-quyết-định","chapter":"Chương 11 Mô hình cây quyết định","heading":"11.1.1.2 Ước lượng tham số mô hình cây quyết định","text":"Về lý thuyết, ước lượng tham số cho mô hình cây quyết định là quá trình tìm cách xây dựng các vùng \\(R_1\\), \\(R_2\\), \\(\\cdots\\), \\(R_K\\)? Các vùng này có thể có hình dạng bất kỳ miễn là các vùng không có chồng lấn và hợp của các vùng là miền giá trị của các biến giải thích. Tuy nhiên, nếu lựa chọn chia không gian các biến giải thích thành vùng có dạng hình chữ nhật nhiều chiều thì mô hình sẽ dễ diễn giải và ước lượng hơn rất nhiều. Một vùng hình chữ nhật nhiều chiều có thể được hiểu là một vùng mà mỗi biến giải thích \\(X_j\\) chỉ bị giới hạn bởi hai giá trị đầu mút là \\(a_j\\) và \\(b_j\\) và không chịu tác động từ các biến giải thích khác:\n\\[\\begin{align}\n\\cup_{j=1}^p \\{X_j \\[a_j, b_j] \\}\n\\end{align}\\]Với \\(R_k\\) là các hình chữ nhật nhiều chiều, chúng ta cần tìm cách phân chia miền giá trị của các biến giải thích ra thành \\(K\\) hình chữ nhật như vậy với mục tiêu là tối thiểu hóa sai số RSS. Lưu ý rằng RSS được tính dựa trên dự báo cho mỗi miền \\(R_k\\) bằng \\(\\hat{y}_{R_k}\\) là giá trị trung bình của biến mục tiêu trong miền này. Tuy nhiên khó khăn thực tế là không thể tính toán được hết mọi phân vùng có thể có của không gian các biến giải thích. Các tiếp cận khả thi để tìm kiếm phân vùng theo cách phân tách nhị phân từ trên xuống:Tại bước thứ nhất: chúng ta tìm cách chia toàn bộ không gian biến giải thích thành hai hình chữ nhật sao cho sai số RSS trên dữ liệu huấn luyện mô hình là nhỏ nhất.Tại bước thứ nhất: chúng ta tìm cách chia toàn bộ không gian biến giải thích thành hai hình chữ nhật sao cho sai số RSS trên dữ liệu huấn luyện mô hình là nhỏ nhất.Tại bước thứ hai: chúng ta tìm cách chia một trong hai hình chữ nhật thu được trong bước thứ nhất sao cho sai số RSS trên dữ liệu huấn luyện mô hình là nhỏ nhất.Tại bước thứ hai: chúng ta tìm cách chia một trong hai hình chữ nhật thu được trong bước thứ nhất sao cho sai số RSS trên dữ liệu huấn luyện mô hình là nhỏ nhất.……Tại bước thứ \\(K\\): chúng ta tìm cách chia một trong \\(K-1\\) hình chữ nhật thu được trong bước thứ \\(K-1\\) thành hai hình chữ nhật con sao cho sai số RSS tính từ \\(K\\) hình chữ nhật là nhỏ nhất.Tại bước thứ \\(K\\): chúng ta tìm cách chia một trong \\(K-1\\) hình chữ nhật thu được trong bước thứ \\(K-1\\) thành hai hình chữ nhật con sao cho sai số RSS tính từ \\(K\\) hình chữ nhật là nhỏ nhất.Đây là quá trình tìm kiếm \\(tham\\) \\(lam\\) bởi vì tại mỗi bước của quá trình xây dựng cây quyết định, chúng ta luôn tìm kiếm sự phân tách tốt nhất có thể dựa trên kết quả của bước tốt nhất trước đó. Cách tìm kiếm này là khả thi nhưng không chắc chắn rằng chúng ta có thể tìm được phân vùng tối ưu. Trong một hình hộp bất kỳ, để thực hiện phân tách thành hai phần, chúng ta cần lựa chọn biến giải thích \\(X_j\\) và điểm cắt \\(s\\) nằm trong miền giá trị của \\(X_j\\) sao cho khi chia hình chữ nhật thành hai vùng: vùng các biến giải thích có \\(X_j < s\\) và vùng các biến giải thích \\(X_j \\leq s\\)\n\\[\\begin{align}\nR_1(X_j, s) = \\{\\textbf{X} | X_j < s \\} \\\\\nR_2(X_j, s) = \\{\\textbf{X} | X_j \\leq s \\}\n\\tag{11.3}\n\\end{align}\\]\nsau đó chúng ta cần tìm \\(j\\) và \\(s\\) sao cho tổng bình phương sai số là nhỏ nhất:\n\\[\\begin{align}\n\\sum\\limits_{x_i \\R_1} (y_i - \\hat{y}_{R_1})^2 + \\sum\\limits_{x_i \\R_2} (y_i - \\hat{y}_{R_2})^2\n\\tag{11.4}\n\\end{align}\\]\ntrong đó \\(\\hat{y}_{R_1}\\) là giá trị trung bình của biến mục tiêu trong miền \\(R_1\\) và \\(\\hat{y}_{R_2}\\) là giá trị trung bình của biến mục tiêu trong miền \\(R_2\\). Quá trình tìm các giá trị của \\(j\\) và \\(s\\) để tối thiểu hóa (11.4) có thể được thực hiện khá nhanh, đặc biệt khi số lượng biến giải thích \\(p\\) không quá lớn. Chúng ta sẽ lặp lại quy trình tìm kiếm biến giải thích tốt nhất và điểm cắt tốt nhất trên biến giải thích đó để tiếp tục phân tách dữ liệu và để giảm RSS. Quá trình tiếp tục cho đến khi cây quyết định đạt đến một tiêu chí dừng nào đó. Chúng ta cần đặt ra các tiêu chí dừng bởi vì nếu tiếp tục quá trình phân tách dữ liệu cho đến \\(n\\) bước với \\(n\\) là số quan sát trong dữ liệu huấn luyện mô hình thì mô hình cây quyết định sẽ chia dữ liệu ra thành \\(n\\) vùng và mỗi vùng tương ứng với một quan sát. Một cây quyết định như vậy có sai số trên huấn luyện mô hình bằng 0 nhưng không có ý nghĩa trong diễn giải hoặc dự đoán biến mục tiêu. Các tiêu chí dừng thường được sử dụng thường là số lượng dữ liệu nằm trong một vùng không được phép ít hơn một số nào đó. Nếu mọi sự phân tách đều dẫn đến việc số lượng điểm dữ liệu trong một lá nhỏ hơn một ngưỡng, thường là 5 hoặc 10 điểm dữ liệu, thì chúng ta nên dừng quá trình phân tách.\nFigure 11.5: Cây quyết định hồi quy kích thước bằng 4, mô tả giá nhà phụ thuộc vào tỷ lệ người sống dưới mức trung bình và số lượng phòng trung bình của mỗi ngôi nhà trên dữ liệu Boston. với cây quyết định có 3 lá ở trên, vùng R3 đã được chia thành hai phần là R3-1 và R3-2. Các vùng R1 và R2 không thay đổi\n\nFigure 11.6: Cây quyết định hồi quy kích thước bằng 5, mô tả giá nhà phụ thuộc vào tỷ lệ người sống dưới mức trung bình và số lượng phòng trung bình của mỗi ngôi nhà trên dữ liệu Boston. với cây quyết định có 4 lá ở trên, vùng R1 đã được chia thành hai phần là R1-1 và R1-2. Các vùng R2, R3-1, và R3-2 không thay đổi\nCác Hình 11.5 và 11.6 mô tả các bước thứ tư và bước thứ năm trong xây dựng mô hình cây quyết định mô tả biến giá nhà (\\(medv\\)) phụ thuộc vào hai biến \\(lstat\\) và \\(rm\\). Để xây dựng mô hình cây quyết định có 4 lá, từ ba vùng \\(R_1\\), \\(R_2\\), và \\(R_3\\) thu được trong Hình 11.3, có sáu lựa chọn để chia không gian các biến giải thích thành hai vùng (có 3 vùng và mỗi vùng có hai lựa chọn để phân chia theo một trong hai biến là \\(rm\\) hoặc \\(lstat\\)). Trong số các lựa chọn đó, phân chia vùng \\(R3\\) theo biến \\(rm\\) tại điểm cắt \\(rm = 7.437\\) là cách phân chia làm cho giá trị RSS là nhỏ nhất. Kết quả thu được là một cây quyết định có bốn lá tương ứng với bốn vùng là \\(R_1\\), \\(R_2\\), \\(R_3-1\\), và \\(R_3-2\\).Quá trình ước lượng cây quyết định có năm lá cũng diễn ra tương tự. Từ bốn vùng thu được từ bước trước, chúng ta có 8 lựa chọn để tiếp tục phân chia dữ liệu thành năm vùng (4 vùng và 2 biến giải thích). Trong số các lựa chọn đó, phân chia vùng \\(R_1\\) thành hai vùng theo biến \\(lstat\\) tại điểm 4.91 là phân chia làm cho RSS giảm đi nhiều nhất. Kết quả thu được là một cây quyết định có năm lá tương ứng với năm vùng \\(R_1-1\\), \\(R_1-2\\), \\(R_2\\), \\(R_3-1\\), và \\(R_3-2\\).Như chúng tôi đã đề cập ở trên, nếu chúng ta tiếp tục quá trình phân chia dữ liệu sẽ thu được một cây quyết định đủ lớn để có thể nội suy lại chính xác biến mục tiêu. Kể cả khi chúng ta đặt ra các tiêu chí dừng, các cây quyết định có kích thước lớn thường gặp phải hiện thượng overfitting. đó, tham số kích thước của cây quyết định \\(K\\) thường được lựa chọn dựa trên xác thực chéo. Các cây quyết định có ưu điểm là đơn giản và không tốn nguồn lực tính toán nhiều, đó xác thực chéo hoàn toàn có thể thực hiện được trong đa số các trường hợp.\nFigure 11.7: Mô hình cây quyết định biến medv phụ thuộc vào lstat và rm trên dữ liệu Boston. Kích thước cây quyết định được lựa chọn dựa trên xác thực chéo. Số lượng lá cho sai số xác thực chéo nhỏ nhất là 9\nTrước khi chuyển sang phần cây quyết định phân loại, chúng tôi sẽ thảo luận về lựa chọn điểm cắt phù hợp để tách dữ liệu thành hai phần. Chúng ta quay trở lại với mô hình cây quyết định với một biến giải thích duy nhất là \\(lstat\\) trong Hình 11.1, khi mà điểm cắt đầu tiên được ước lượng là 9.725%. Nguyên tắc ước lượng ra điểm cắt này như sau: với biến giải thích \\(X_j\\) là biến liên tục, chúng ta loại bỏ các quan sát của biến \\(X_j\\) bị trùng lặp và sắp xếp lại các quan sát này theo thứ tự tăng dần, chẳng hạn như \\(x_{1j}\\) < \\(x_{2j}\\) < \\(\\cdots\\) < \\(x_{nj}\\). Khi đó, có \\(n-1\\) điểm cắt cần được tính toán tổng sai số bình phương (RSS) là các điểm\n\\[\\begin{align}\n\\cfrac{x_{1j} + x_{2j}}{2}, \\cfrac{x_{2j} + x_{3j}}{2}, \\cdots, \\cfrac{x_{(n-1)j} + x_{nj}}{2}\n\\end{align}\\]\nvà điểm cắt tối ưu là điểm cắt có RSS nhỏ nhất. Bạn đọc có thể thấy rằng điểm cắt 9.725 là giá trị trung bình của hai giá trị quan sát được của biến \\(medv\\) trong dữ liệu là điểm 9.71 và điểm 9.74.Nếu biến giải thích \\(X_j\\) là biến dạng rời rạc hoặc biến định tính với có thể nhận \\(k\\) giá trị khác nhau. Khi đó chúng ta xắp xếp \\(k\\) giá trị đó theo thứ tự mà giá trị trung bình của biến mục tiêu tính trên nhóm đó tăng dần. Chẳng hạn như \\(k\\) giá trị của \\(X_j\\) được xắp xếp theo thứ tự là \\(c_{1} \\rightarrow c_{2} \\rightarrow \\cdots \\rightarrow c_k\\), nghĩa là giá trị trung bình của biến mục tiêu trong miền \\(X_j = c_i\\) nhỏ hơn giá trị trung bình của biến mục tiêu trong miền \\(X_j = c_{+1}\\). Khi đó, có \\(k-1\\) cách phân chia dữ liệu cần được cân nhắc\n\\[\\begin{align}\n&\\text{Cách 1. Vùng 1: } X_j = c_1 \\text{ và vùng 2: } X_j \\\\{c_2, c_3, \\cdots, c_k\\} \\\\\n&\\text{Cách 2. Vùng 1: } X_j \\\\{c_1, c_2\\} \\text{ và vùng 2: } X_j \\\\{c_3, c_4, \\cdots, c_k\\} \\\\\n&\\cdots \\\\\n&\\text{Cách (k-1). Vùng 1: } X_j \\\\{c_1, c_2, \\cdots, c_{k-1}\\} \\text{ và vùng 2: } X_j = c_k\n\\end{align}\\]\nvà lựa chọn phân vùng tối ưu là lựa chọn phân vùng có RSS nhỏ nhất. Ví dụ, chúng ta xây dựng mô hình cây quyết định trong đó biến mục tiêu \\(medv\\) phụ thuộc vào biến \\(rad\\) là biến rời rạc mô tả khả năng kết nối với đường cao tốc của vùng. Biến \\(rad\\) trong dữ liệu Boston có 9 giá trị riêng biệt là các số tự nhiên từ 1 đến 8 và số 24. Lưu ý rằng đây là biến rời rạc danh nghĩa không có ý nghĩa sánh giữa các giá trị với nhau. Giá trị trung bình của biến mục tiêu theo các giá trị riêng biệt của \\(rad\\) được cho trong bảng 11.1\nTable 11.1: Giá nhà trung bình tính theo các giá trị riêng biệt của biến rời rạc rad\nDựa theo tính toán từ bảng 11.1, có 8 cách phân vùng có thể khi sử dụng biến \\(rad\\) để phân vùng được liệt kê như sau\n\\[\\begin{align}\n&\\text{Cách 1. Vùng 1: } rad = 24 \\text{ và vùng 2: } rad \\\\{6, 4, 1, 5, 2, 7, 3, 8 \\} \\\\\n&\\text{Cách 2. Vùng 1: } rad \\\\{24, 6 \\} \\text{ và vùng 2: } rad \\\\{4, 1, 5, 2, 7, 3, 8\\} \\\\\n&\\text{Cách 3. Vùng 1: } rad \\\\{24, 6, 4 \\} \\text{ và vùng 2: } rad \\\\{1, 5, 2, 7, 3, 8\\} \\\\\n&\\text{Cách 4. Vùng 1: } rad \\\\{24, 6, 4, 1 \\} \\text{ và vùng 2: } rad \\\\{5, 2, 7, 3, 8\\} \\\\\n&\\text{Cách 5. Vùng 1: } rad \\\\{24, 6, 4, 1, 5 \\} \\text{ và vùng 2: } rad \\\\{2, 7, 3, 8\\} \\\\\n&\\text{Cách 6. Vùng 1: } rad \\\\{24, 6, 4, 1, 5, 2 \\} \\text{ và vùng 2: } rad \\\\{7, 3, 8\\} \\\\\n&\\text{Cách 7. Vùng 1: } rad \\\\{24, 6, 4, 1, 5, 2, 7 \\} \\text{ và vùng 2: } rad \\\\{3, 8\\} \\\\\n&\\text{Cách 8. Vùng 1: } rad \\\\{24, 6, 4, 6, 4, 1, 5, 2, 7 \\} \\text{ và vùng 2: } rad = 8\n\\end{align}\\]\nvà cách phân vùng cho RSS nhỏ nhất sẽ là phân vùng được lựa chọn. Hình 11.8 mô tả cây quyết định có hai lá trong đó biến mục tiêu là \\(medv\\) và biến giải thích là biến \\(rad\\).\nFigure 11.8: Cây quyết định hồi quy kích thước bằng hai mô tả giá nhà phụ thuộc vào biến rời rạc là khả năng kết nối của ngôi nhà đến đường cao tốc trên dữ liệu Boston.\nNhư vậy phân vùng có RSS nhỏ nhất là cách phân vùng thứ 3. Các giá trị rời rạc 24, 4, 6 của biến \\(rad\\) được cho vào nhánh bên trái cây quyết định với dự đoán cho giá nhà là 18.89 nghìn USD trong khi các giá trị rời rạc còn lại được cho vào nhánh bên phải của cây quyết định với dự đoán cho giá nhà là 26.63 nghìn USD","code":""},{"path":"mô-hình-cây-quyết-định.html","id":"cây-quyết-định-phân-loại","chapter":"Chương 11 Mô hình cây quyết định","heading":"11.1.2 Cây quyết định phân loại","text":"Cây quyết định có lợi thế là có thể sử dụng cho cả bài toán hồi quy và bài toán phân loại mà không cần có sự biến đổi đáng kể nào về mặt mô hình. Thay đổi duy nhất để cây quyết định phù hợp với bài toán phân loại đó là thay đổi hàm tổn thất từ RSS sang các hàm tổn thất phù hợp với bài toán phân loại. RSS không phù hợp trong bài toán phân loại vì giá trị dự đoán chúng ta đưa ra tại mỗi lá của cây quyết định là giá trị xuất hiện với tần suất lớn nhất chứ không phải giá trị trung bình của các biến nằm trong lá đó.Quá trình xây dựng cây quyết định phân loại hoàn toàn tương tự như quá trình xây dựng cây quyết định hồi quy. Chúng ta sử dụng chuỗi các phân tách nhị phân để phân vùng không gian giá trị các biến giải thích thành \\(k\\) vùng \\(R_1, R_2, \\cdots, R_K\\) không chồng lấn lên nhau và ở mỗi vùng \\(R_k\\) chúng ta đưa ra một dự đoán \\(\\hat{y}_{R_k}\\) cho biến mục tiêu. Trong mô hình hồi quy giá trị dự đoán là giá trị trung bình của biến mục tiêu tương ứng với các quan sát nằm trong vùng \\(R_k\\) còn trong bài toán phân loại giá trị dự đoán \\(\\hat{y}_{R_k}\\) là giá trị của biến mục tiêu xuất hiện với tần suất lớn nhất tương ứng với các quan sát trong vùng \\(R_k\\).Như đã trình bày trong cây phân loại hồi quy, để thực hiện phân tách một vùng thành hai phần, chúng ta cần lựa chọn biến giải thích \\(X_j\\) và điểm cắt \\(s\\) nằm trong miền giá trị của \\(X_j\\) sao cho khi chia hình chữ nhật thành hai vùng: vùng các biến giải thích có \\(X_j < s\\) và vùng các biến giải thích \\(X_j \\leq s\\)\n\\[\\begin{align}\nR_1(X_j, s) = \\{\\textbf{X} | X_j < s \\} \\\\\nR_2(X_j, s) = \\{\\textbf{X} | X_j \\leq s \\}\n\\end{align}\\]\nvới các dự đoán cho biến mục tiêu tương ứng với hai vùng là \\(\\hat{y}_{R_1}\\) và \\(\\hat{y}_{R_2}\\), chúng ta cần tìm \\(j\\) và \\(s\\) để tối thiểu hóa sai số của bài toán phân loại. Để lượng hóa sai số của bài toán phân loại, có nhiều cách tiếp cận. Cách tiếp cận tự nhiên và đơn giản nhất là sử dụng \\(tỷ\\) \\(lệ\\) \\(dự\\) \\(đoán\\) \\(sai\\) (còn được gọi là classification error rate hay viết tắt là CER). Cho một véc-tơ \\(\\textbf{y}\\) có độ dài \\(n\\) chỉ nhận các giá trị rời rạc là \\(1, 2, \\cdots, m\\) và tần suất xuất hiện của các giá trị rời rạc này lần lượt là \\(n_1\\), \\(n_2\\), \\(\\cdots\\), \\(n_m\\). Nếu \\(\\textbf{y}\\) là các giá trị của biến mục tiêu quan sát được trong một vùng của biến giải thích thì giá trị dự đoán cho biến mục tiêu sẽ là \\(h \\\\{1, 2, \\cdots, m \\}\\) sao cho \\(n_h = max(n_1, n_2, \\cdots, n_m)\\). Tỷ lệ dự đoán sai trên véc-tơ \\(\\textbf{y}\\) được tính như sau\n\\[\\begin{align}\nCER(\\textbf{y}) = 1 - \\cfrac{n_h}{n}\n\\tag{11.5}\n\\end{align}\\]\nCó thể thấy rằng CER là tỷ lệ số dự đoán sai trên véc-tơ \\(\\textbf{y}\\) khi sử dụng đoán là \\(h = mode(\\textbf{y})\\). CER có ưu điểm là dễ hiểu và đơn giản, tuy nhiên nhược điểm lớn nhất của CER là chỉ tính đến giá trị xuất hiện với tần suất nhiều nhất mà không tính đến các giá trị khác, đó chỉ tiêu này không phù hợp khi sử dụng để phân vùng các cây phân loại, đặc biệt là trong trường hợp biến giải thích nhận nhiều hơn 2 giá trị.Hai chỉ số khác thường xuyên được sử dụng thay thế cho nhau để tìm ra phân vùng tối ưu trong cây quyết định phân loại là chỉ số Gini và chỉ số Entropy. Hai chỉ số này có ưu điểm là tính toán đến sự xuất hiện của tất cả các giá trị có xuất hiện trong véc-tơ \\(\\textbf{y}\\)\n\\[\\begin{align}\nGini(\\textbf{y}) &= \\sum\\limits_{l = 1}^m \\ \\cfrac{n_l}{n} \\ \\left( 1 - \\cfrac{n_l}{n}\\right) \\\\\nEntropy(\\textbf{y}) &= - \\sum\\limits_{l = 1}^m \\ \\cfrac{n_l}{n} \\ \\log\\left(\\cfrac{n_l}{n}\\right)\n\\tag{11.6}\n\\end{align}\\]\nCác chỉ số Gini và Entropy còn được gọi là các thước đo độ thuần (purity) của véc-tơ \\(\\textbf{y}\\). \\(n_l/n\\) nằm trong đoạn \\([0,1]\\) và tổng các tần suất \\(n_l/n\\) bằng 1 nên các chỉ số Gini và Entropy sẽ có giá trị nhỏ khi tồn tại một giá trị \\(n_l/n\\) xấp xỉ 1 và các giá trị còn lại xấp xỉ 0. Nếu cố định \\(n_h/n\\) với \\(h\\) là giá trị xuất hiện nhiều nhất trong \\(y\\) và thay đổi các \\(n_l\\) khác thì CER sẽ không thay đổi trong khi Gini và Entropy sẽ thay đổi. Đây là lý tại sao chỉ số Gini và Entropy sẽ phù hợp hơn khi đo lường độ thuần của một véc-tơ.Để thấy được sự phù hợp của chỉ số Gini và chỉ số Entropy trong lựa chọn phân vùng giá trị của biến giải thích, hãy quan sát ví dụ trong Hình 11.9\nFigure 11.9: Ví dụ về hai lựa chọn phân vùng khác nhau cho véc-tơ biến mục tiêu bao gồm ba giá trị riêng biệt là 1, 2, và 3 với tần suất xuất hiện là 6 lần, 3 lần và 3 lần. Nếu sử dụng chỉ số tỷ lệ dự đoán sai thì hai cách phân vùng là không có sự khác biệt. Nếu sử dụng chỉ số Entropy hoặc Gini thì cách phân vùng thứ 2 tốt hơn\nGiả sử chúng ta phải đưa ra lựa chọn đâu là phân vùng tốt hơn giữa hai cách phân vùng được mô tả trong hình 11.9. Một cách trực giác, bạn đọc có thể nhận thấy rằng cách phân vùng thứ hai sẽ dẫn đến một cây phân loại tốt hơn vì đã tách biệt được hoàn toàn giá trị 2 và giá trị 3 vào hai vùng. Với cách phân vùng như vậy, nếu chúng ta tiếp tục thực hiện phân vùng thì nhiều khả năng trong bước tiếp theo chúng ta sẽ phân loại được hoàn toàn giá trị 1 và giá trị 2 trong vùng \\(R_1\\) và phân loại được hoàn toàn giá trị 1 và giá trị 3 trong vùng \\(R_2\\). Trái lại, trong cách phân vùng thứ nhất, không thể phân tách hai giá trị 2 và 3 ra thành các vùng riêng biệt. Nếu chúng ta tiếp tục phát triển thêm 1 lá cho cây quyết định như trong cách thứ nhất, không thể thu được phân loại chính xác cho cả ba giá trị của biến mục tiêu.Bạn đọc có thể sử dụng các chỉ số sai số phân loại, Gini và Entropy để sánh hai cách phân vùng như sau:Cho cách phân vùng thứ nhất:\n\\[\\begin{align}\nCER & = \\cfrac{6}{12} \\times \\cfrac{3}{6} + \\cfrac{6}{12} \\times \\cfrac{3}{6} = 0.5 \\\\\nGini & = \\cfrac{6}{12} \\times \\left[\\cfrac{3}{6}\\left(1 - \\cfrac{3}{6}\\right) + \\cfrac{2}{6}\\left(1 - \\cfrac{2}{6}\\right) + \\cfrac{1}{6}\\left(1 - \\cfrac{1}{6}\\right) \\right] + \\\\\n& \\cfrac{6}{12} \\times \\left[\\cfrac{3}{6}\\left(1 - \\cfrac{3}{6}\\right) + \\cfrac{1}{6}\\left(1 - \\cfrac{1}{6}\\right) + \\cfrac{2}{6}\\left(1 - \\cfrac{2}{6}\\right) \\right] = 0.61\\\\\nEntropy & = - \\cfrac{6}{12} \\times \\left[\\cfrac{3}{6} \\log\\left(\\cfrac{3}{6}\\right) + \\cfrac{2}{6}\\log\\left(\\cfrac{2}{6}\\right) + \\cfrac{1}{6}\\log\\left(1 - \\cfrac{1}{6}\\right) \\right] + \\\\\n& \\cfrac{6}{12} \\times \\left[\\cfrac{3}{6}\\log\\left(\\cfrac{3}{6}\\right) + \\cfrac{1}{6}\\log\\left(\\cfrac{1}{6}\\right) + \\cfrac{2}{6}\\log\\left( \\cfrac{2}{6}\\right) \\right] = 1.01\n\\end{align}\\]Cho cách phân vùng thứ nhất:\n\\[\\begin{align}\nCER & = \\cfrac{6}{12} \\times \\cfrac{3}{6} + \\cfrac{6}{12} \\times \\cfrac{3}{6} = 0.5 \\\\\nGini & = \\cfrac{6}{12} \\times \\left[\\cfrac{3}{6}\\left(1 - \\cfrac{3}{6}\\right) + \\cfrac{2}{6}\\left(1 - \\cfrac{2}{6}\\right) + \\cfrac{1}{6}\\left(1 - \\cfrac{1}{6}\\right) \\right] + \\\\\n& \\cfrac{6}{12} \\times \\left[\\cfrac{3}{6}\\left(1 - \\cfrac{3}{6}\\right) + \\cfrac{1}{6}\\left(1 - \\cfrac{1}{6}\\right) + \\cfrac{2}{6}\\left(1 - \\cfrac{2}{6}\\right) \\right] = 0.61\\\\\nEntropy & = - \\cfrac{6}{12} \\times \\left[\\cfrac{3}{6} \\log\\left(\\cfrac{3}{6}\\right) + \\cfrac{2}{6}\\log\\left(\\cfrac{2}{6}\\right) + \\cfrac{1}{6}\\log\\left(1 - \\cfrac{1}{6}\\right) \\right] + \\\\\n& \\cfrac{6}{12} \\times \\left[\\cfrac{3}{6}\\log\\left(\\cfrac{3}{6}\\right) + \\cfrac{1}{6}\\log\\left(\\cfrac{1}{6}\\right) + \\cfrac{2}{6}\\log\\left( \\cfrac{2}{6}\\right) \\right] = 1.01\n\\end{align}\\]Cho cách phân vùng thứ hai\n\\[\\begin{align}\nCER & = \\cfrac{6}{12} \\times \\cfrac{3}{6} + \\cfrac{6}{12} \\times \\cfrac{3}{6} = 0.5 \\\\\nGini & = \\cfrac{6}{12} \\times \\left[\\cfrac{3}{6}\\left(1 - \\cfrac{3}{6}\\right) + \\cfrac{3}{6}\\left(1 - \\cfrac{3}{6}\\right)\\right] + \\\\\n& \\cfrac{6}{12} \\times \\left[\\cfrac{3}{6}\\left(1 - \\cfrac{3}{6}\\right) + \\cfrac{3}{6}\\left(1 - \\cfrac{3}{6}\\right) \\right] = 0.5\\\\\nEntropy & = - \\cfrac{6}{12} \\times \\left[\\cfrac{3}{6} \\log\\left(\\cfrac{3}{6}\\right) + \\cfrac{3}{6} \\log\\left(\\cfrac{3}{6}\\right) \\right] + \\\\\n& \\cfrac{6}{12} \\times \\left[\\cfrac{3}{6} \\log\\left(\\cfrac{3}{6}\\right) + \\cfrac{3}{6} \\log\\left(\\cfrac{3}{6}\\right) \\right] = 0.69\n\\end{align}\\]Cho cách phân vùng thứ hai\n\\[\\begin{align}\nCER & = \\cfrac{6}{12} \\times \\cfrac{3}{6} + \\cfrac{6}{12} \\times \\cfrac{3}{6} = 0.5 \\\\\nGini & = \\cfrac{6}{12} \\times \\left[\\cfrac{3}{6}\\left(1 - \\cfrac{3}{6}\\right) + \\cfrac{3}{6}\\left(1 - \\cfrac{3}{6}\\right)\\right] + \\\\\n& \\cfrac{6}{12} \\times \\left[\\cfrac{3}{6}\\left(1 - \\cfrac{3}{6}\\right) + \\cfrac{3}{6}\\left(1 - \\cfrac{3}{6}\\right) \\right] = 0.5\\\\\nEntropy & = - \\cfrac{6}{12} \\times \\left[\\cfrac{3}{6} \\log\\left(\\cfrac{3}{6}\\right) + \\cfrac{3}{6} \\log\\left(\\cfrac{3}{6}\\right) \\right] + \\\\\n& \\cfrac{6}{12} \\times \\left[\\cfrac{3}{6} \\log\\left(\\cfrac{3}{6}\\right) + \\cfrac{3}{6} \\log\\left(\\cfrac{3}{6}\\right) \\right] = 0.69\n\\end{align}\\]Bạn đọc có thể thấy rằng chỉ số CER trong hai cách phân vùng bằng nhau và bằng 0.5, nghĩa là CER không có khả năng phân biệt giữa cách phân vùng thứ nhất và cách phân vùng thứ hai. Chỉ số Gini và Entropy của phân vùng thứ hai đều nhỏ hơn cho với cách phân vùng thứ nhất, điều này cho thấy hai chỉ số này có khả năng phân biệt cách phân vùng tốt hơn với CER.Hình 11.10 mô tả cây quyết định được xây dựng trên dữ liệu OJ, dữ liệu chứa thông tin về 1070 lần khách hàng mua một trong hai loại sản phẩm nước cam Citrus Hill hoặc Minute Maid, với biến mục tiêu là \\(Purchase\\) chứa một trong hai giá trị là \\(CH\\) hoặc \\(MM\\) cho biết sản phẩm được mua tương ứng là Citrus Hill hay Minute Maid. Dữ liệu có 17 biến giải thích bao gồm các biến có nhiều khả năng cho ý nghĩa quan trọng trong xây dựng mô hình như sự khác biệt về giá của hai loại sản phẩm (biến \\(PriceDiff\\)) cho biết giá của \\(MM\\) cao hơn giá của \\(CH\\) như thế nào, hoặc biến \\(LoyalCH\\) là biến đo sự trung thành của khách hàng với nhãn hiệu Citrus Hill. Sự trung thành của khách hàng là thước đo sự mua hàng lặp lại trên một nhãn hiệu dựa trên đánh giá về chất lượng hoặc dịch vụ tốt hơn các đối thủ cạnh tranh và không phụ thuộc vào giá cả của nhãn hiệu đó.\nFigure 11.10: Cây quyết định phân loại cho biết quyết định mua sản phẩm nước cam của khách hàng dựa vào các biến giải thích khác sử dụng dữ liệu OJ.\nCó thể thấy rằng biến \\(LoyalCH\\) xuất hiện ở hầu hết các nút của cây quyết định, điều này cho thấy sự trung thành của khách hàng với nhãn hiệu sản phẩm quyết định rất lớn đến quyết định mua hàng của khách hàng. Chúng ta có thể giải thích cây quyết định phân loại trên như sau:Khi lòng trung thành của khách hàng với sản phẩm Citrus Hill thấp (nhỏ hơn 0.5), tương ứng với nhánh cây quyết định bên trái của nút trên cùng, thì đa số các khách hàng sẽ lựa chọn sản phẩm Minute Maid. Trong nhánh bên trái này, những khách hàng có chỉ số lòng trung thành với Citrus Hill dưới 0.27 sẽ chọn sản phẩm Minute Maid bất kể sự khác biệt về giá cả. Còn những khách hàng có chỉ số trung thành với sản phẩm Citrus Hill từ 0.27 trở lên sẽ có thay đổi quyết định để mua sản phẩm Citrus Hill nếu giá của Minute Maid không lớn hơn giá của Citrus Hill cộng thêm 0.05.Khi lòng trung thành của khách hàng với sản phẩm Citrus Hill thấp (nhỏ hơn 0.5), tương ứng với nhánh cây quyết định bên trái của nút trên cùng, thì đa số các khách hàng sẽ lựa chọn sản phẩm Minute Maid. Trong nhánh bên trái này, những khách hàng có chỉ số lòng trung thành với Citrus Hill dưới 0.27 sẽ chọn sản phẩm Minute Maid bất kể sự khác biệt về giá cả. Còn những khách hàng có chỉ số trung thành với sản phẩm Citrus Hill từ 0.27 trở lên sẽ có thay đổi quyết định để mua sản phẩm Citrus Hill nếu giá của Minute Maid không lớn hơn giá của Citrus Hill cộng thêm 0.05.Khi lòng trung thành của khách hàng với sản phẩm Citrus Hill lớn (lớn hơn 0.5), tương ứng với nhánh cây quyết định bên phải của nút trên cùng, thì đa số các khách hàng sẽ lựa chọn sản phẩm Citrus Hill. Trong nhánh này, những khách hàng có chỉ số lòng trung thành với Citrus Hill trên 0.76 sẽ chọn sản phẩm Citrus Hill bất kể sự khác biệt về giá cả. Còn những khách hàng có chỉ số trung thành với sản phẩm Citrus Hill dưới 0.76 sẽ có thay đổi quyết định và mua sản phẩm Minute Maid nếu giá của Minute Maid thấp hơn giá của Citrus Hill 0.165Khi lòng trung thành của khách hàng với sản phẩm Citrus Hill lớn (lớn hơn 0.5), tương ứng với nhánh cây quyết định bên phải của nút trên cùng, thì đa số các khách hàng sẽ lựa chọn sản phẩm Citrus Hill. Trong nhánh này, những khách hàng có chỉ số lòng trung thành với Citrus Hill trên 0.76 sẽ chọn sản phẩm Citrus Hill bất kể sự khác biệt về giá cả. Còn những khách hàng có chỉ số trung thành với sản phẩm Citrus Hill dưới 0.76 sẽ có thay đổi quyết định và mua sản phẩm Minute Maid nếu giá của Minute Maid thấp hơn giá của Citrus Hill 0.165Một sự khác biệt trong kết quả của cây quyết định phân loại và cây phân loại hồi quy đó là hai lá tách ra từ một nút vẫn có thể nhận kết quả giống nhau. Chẳng hạn như trong hình 11.10 bạn đọc có thể thấy rằng tại nút \\(LoyalCH < 0.0356\\), cả hai lá đều cho kết quả là \\(MM\\). Nguyên nhân là chúng ta sử dụng chỉ số Gini để phân vùng dữ liệu. Sai số phân loại của cây quyết định sẽ không giảm nếu hai lá từ một nút cho cùng một kết quả là \\(MM\\) nhưng độ thuần (purity) của các nút sẽ giảm.","code":""},{"path":"mô-hình-cây-quyết-định.html","id":"cây-quyết-định-và-mô-hình-tuyến-tính","chapter":"Chương 11 Mô hình cây quyết định","heading":"11.1.3 Cây quyết định và mô hình tuyến tính","text":"Cây quyết định hồi quy và cây quyết định phân loại có cách tiếp cận hoàn toàn khác với các mô hình tuyến tính mà chúng tôi thảo luận trong cá chương trước. Nếu như mô hình tuyến tính cho rằng hàm \\(f\\) mô tả mối liên hệ giữa biến mục tiêu \\(Y\\) và các biến giải thích \\(X_1\\), \\(X_2\\), \\(\\cdots\\), \\(X_p\\) có dạng tuyến tính\n\\[\\begin{align}\nf(\\textbf{X}) = \\beta_0 + \\sum\\limits_{j=1}^p \\beta_j \\cdot X_j\n\\end{align}\\]\nthì mô hình cây quyết định sử dụng dạng của hàm \\(f\\) như sau\n\\[\\begin{align}\nf(\\textbf{X}) = \\sum\\limits_{m=1}^M c_m \\cdot \\mathbb{}_{\\textbf{X} \\R_m}\n\\end{align}\\]\ntrong đó \\(R_1, R_2, \\cdots, R_M\\) là một phân vùng của miền xác định của véc-tơ biến giải thích \\(\\textbf{X}\\). Không có câu trả lời cho câu hỏi là mô hình nào tốt hơn. Câu trả lời hoàn toàn tùy thuộc vào dữ liệu mà chúng ta xây dựng mô hình. Nếu mối liên hệ của biến mục tiêu và biến giải thích là mối liên hệ tuyến tính, mô hình tuyến tính sẽ cho kết quả tốt hơn, còn nếu mối liên hệ đó là phi tuyến, mô hình cây quyết định sẽ cho kết quả tốt hơn. Tất nhiên, với một dữ liệu cụ thể, không thể biết được chính xác mối liên hệ giữa biến mục tiêu với các biến khác là tuyến tính hay phi tuyến, đó cách tốt nhất để biết mô hình nào tốt hơn là xây dựng cả hai dạng mô hình và sau đó sử dụng sai số xác thực chéo để đưa ra kết luận.Việc lựa chọn mô hình cũng phụ thuộc vào mục tiêu của người xây dựng mô hình. Nếu mục tiêu của xây dựng mô hình là để suy diễn, đánh giá tác động của các biến giải thích lên biến mục tiêu thì mô hình cây quyết định là mô hình dễ diễn giải và mô tả dữ liệu hơn cả. Mô hình tuyến tính thông thường cũng có khả năng diễn giải kết quả tốt trong khi các mở rộng của mô hình tuyến tính như hồi quy Splines, hay mô hình cộng tính tổng quát dẫn đến các kết quả ít có ý nghĩa diễn giải. Ngược lại, nếu mục tiêu của người xây dựng mô hình là dự đoán biến mục tiêu, nghĩa là để giảm thiểu sai số dự đoán trên dữ liệu kiểm thử mô hình, các mở rộng của mô hình tuyến tính sẽ cho sai số dự báo tốt hơn nhiều với mô hình cây quyết định.Trước khi chuyển sang phần tiếp theo, chúng ta có thể tổng kết lại những điểm mạnh và điểm yếu của mô hình cây quyết định như sau:Lợi thế thứ nhất của mô hình cây quyết định trước tiên là khả năng diễn giải mô hình. Bạn đọc có thể thấy rằng mô hình cây quyết định thậm chí còn dễ giải thích hơn cả mô hình hồi quy tuyến tính thông thường. Một trong những nguyên nhân khiến cho cây quyết định dễ diễn giải là cách cây quyết định được xây dựng có mối liên hệ chặt chẽ với việc ra quyết định của não bộ của con người.Lợi thế thứ nhất của mô hình cây quyết định trước tiên là khả năng diễn giải mô hình. Bạn đọc có thể thấy rằng mô hình cây quyết định thậm chí còn dễ giải thích hơn cả mô hình hồi quy tuyến tính thông thường. Một trong những nguyên nhân khiến cho cây quyết định dễ diễn giải là cách cây quyết định được xây dựng có mối liên hệ chặt chẽ với việc ra quyết định của não bộ của con người.Thứ hai, mô hình cây quyết định có thể được trực quan hóa bằng đồ họa và có thể dễ dàng để hiểu được ngay cả với người không có nền tảng về toán học.Thứ hai, mô hình cây quyết định có thể được trực quan hóa bằng đồ họa và có thể dễ dàng để hiểu được ngay cả với người không có nền tảng về toán học.Thứ ba, mô hình cây quyết định có thể sử dụng cho dữ liệu có biến giải thích và biến mục tiêu định tính mà không cần có thay đổi đáng kể nào. Bạn đọc có thể thấy rằng gần như không có sự khác biệt trong cách xây dựng cây quyết định hồi quy và cây quyết định phân loại. Đồng thời cách xây dựng cây quyết định dựa trên biến giải thích định lượng và định tính gần như không có sự khác biệt.Thứ ba, mô hình cây quyết định có thể sử dụng cho dữ liệu có biến giải thích và biến mục tiêu định tính mà không cần có thay đổi đáng kể nào. Bạn đọc có thể thấy rằng gần như không có sự khác biệt trong cách xây dựng cây quyết định hồi quy và cây quyết định phân loại. Đồng thời cách xây dựng cây quyết định dựa trên biến giải thích định lượng và định tính gần như không có sự khác biệt.Để nói về những hạn chế khi sử dụng mô hình dạng cây, trước hết phải khẳng định rằng mô hình cây quyết định trình bày trong các phần trước không có khả năng dự báo chính xác như các mô hình khác. Đó cũng là sự đánh đổi mà bạn đọc thường xuyên gặp phải khi xây dựng mô hình trên dữ liệu, luôn có sự đánh đổi giữa khả năng dự báo và khả năng diễn giải của mô hình.Để nói về những hạn chế khi sử dụng mô hình dạng cây, trước hết phải khẳng định rằng mô hình cây quyết định trình bày trong các phần trước không có khả năng dự báo chính xác như các mô hình khác. Đó cũng là sự đánh đổi mà bạn đọc thường xuyên gặp phải khi xây dựng mô hình trên dữ liệu, luôn có sự đánh đổi giữa khả năng dự báo và khả năng diễn giải của mô hình.Nhược điểm thứ hai, cũng là nguyên nhân giải thích nhược điểm thứ nhất, đó là mô hình cây quyết định là các mô hình có phương sai lớn, nghĩa là chỉ cần có một thay đổi nhỏ trong dữ liệu xây dựng mô hình, kết quả của mô hình sẽ có sự thay đổi đáng kể.Nhược điểm thứ hai, cũng là nguyên nhân giải thích nhược điểm thứ nhất, đó là mô hình cây quyết định là các mô hình có phương sai lớn, nghĩa là chỉ cần có một thay đổi nhỏ trong dữ liệu xây dựng mô hình, kết quả của mô hình sẽ có sự thay đổi đáng kể.Trong phần cuối của chương sách này và trong chương sau, chúng tôi sẽ giới thiệu đến bạn đọc các kỹ thuật thống kê hiện đại có thể sử dụng kết hợp với mô hình cây quyết định để khắc phục được nhược điểm của mô hình này. Các kỹ thuật này bao gồm có mô hình rừng ngẫu nhiên (còn gọi là random forest) và kỹ thuật học tăng cường (còn được gọi là boosting).","code":""},{"path":"mô-hình-cây-quyết-định.html","id":"thực-hành-xây-dựng-mô-hình-cây-quyết-định-sử-dụng-r","chapter":"Chương 11 Mô hình cây quyết định","heading":"11.2 Thực hành: xây dựng mô hình cây quyết định sử dụng R","text":"","code":""},{"path":"mô-hình-cây-quyết-định.html","id":"cây-quyết-định-hồi-quy-trên-dữ-liệu-boston","chapter":"Chương 11 Mô hình cây quyết định","heading":"11.2.1 Cây quyết định hồi quy trên dữ liệu Boston","text":"","code":""},{"path":"mô-hình-cây-quyết-định.html","id":"cây-quyết-định-phân-loại-trên-dữ-liệu-titanic","chapter":"Chương 11 Mô hình cây quyết định","heading":"11.2.2 Cây quyết định phân loại trên dữ liệu Titanic","text":"","code":""},{"path":"mô-hình-cây-quyết-định.html","id":"thuật-toán-rừng-ngẫu-nhiên","chapter":"Chương 11 Mô hình cây quyết định","heading":"11.3 Thuật toán rừng ngẫu nhiên","text":"","code":""},{"path":"mô-hình-cây-quyết-định.html","id":"phương-pháp-lấy-mẫu-lặp-lại","chapter":"Chương 11 Mô hình cây quyết định","heading":"11.3.1 Phương pháp lấy mẫu lặp lại","text":"Phương pháp lấy mẫu lặp lại (thường gọi là boostrap) là một công cụ vô cùng quan trọng trong thống kê hiện đại. Phương pháp này liên quan đến việc lấy mẫu lặp lại nhiều lần từ dữ liệu huấn luyện ban đầu và ước lượng mô hình trên mỗi mẫu để có thông tin đầy đủ về mô hình hay tham số của mô hình mà chúng ta đang nghiên cứu. Ví dụ: để đánh giá mô hình hồi quy tuyến tính đơn biến trong đó biến mục tiêu \\(Y\\) phụ thuộc vào biến giải thích \\(X\\) dựa trên dữ liệu quan sát được: \\((x_i, y_i)\\) với \\(1 \\leq \\leq n\\), nếu chúng ta sử dụng toàn bộ \\(n\\) quan sát để ước tính hệ số chặn và hệ số góc, chúng ta chỉ có một ước lượng điểm duy nhất. Nếu không có giả thiết quan trọng của mô hình hồi quy tuyến tính là \\(Y\\) có phân phối chuẩn thì chúng ta sẽ không đưa ra được phân phối xác suất hay các khoảng tin cậy cho hệ số của mô hình tuyến tính. Kỹ thuật lấy mẫu lặp tiếp cận theo hướng hoàn toàn khác, thay vì xuất phát từ phân phối xác suất của biến mục tiêu, chúng ta có thể liên tục lấy các mẫu khác nhau từ dữ liệu huấn luyện ban đầu, với mỗi mẫu lấy được chúng ta ước lượng được một hệ số chặn và một hệ số góc, từ đó thu được một phân phối xác suất của các hệ số.Để mô tả phương pháp lấy mẫu lặp lại, chúng ta sử dụng ví dụ về dữ liệu Quảng cáo mà trong đó doanh thu bán sản phẩm phụ thuộc vào các biến giải thích là chi phí quảng cáo trên truyền hình (\\(TV\\)), chi phí quảng cáo trên mạng xã hội (\\(Social\\_Media\\)) và chi phí quảng cáo qua tờ rơi (\\(Flyer\\)). biến quảng cáo qua tờ rơi không có ý nghĩa trong mô hình hồi quy đa biến nên mô hình được lựa chọn chỉ bao gồm hai biến giải thích là \\(TV\\) và \\(Social\\_Media\\).\n\\[\\begin{align}\nSales = \\beta_0 + \\beta_1 \\cdot TV + \\beta_2 \\cdot Social\\_Media + \\epsilon\n\n\\end{align}\\]Hệ số ước lượng của mô hình tuyến tính đa biến như sau\nTable 11.2: Các hệ số ước lượng trong mô hình hồi quy đa biến trên dữ liệu Quảng cáo.\nTừ kết quả ước lượng, chúng ta có các hệ số = \\(\\beta_0\\), \\(\\beta_1\\), \\(\\beta_2\\) là các biến ngẫu nhiên phân phối chuẩn với giá trị trung bình lần lượt là 4.0642, 0.0192, 0.0699 và độ lệch chuẩn lần lượt là 0.7349, 0.0049, 0.0128. Các tính toán này được dựa trên giả thiết quan trọng là phần dư \\(\\epsilon\\) có phân phối chuẩn \\(\\mathcal{N}(0,\\sigma^2)\\). Phương pháp lấy mẫu lặp không cần tính đến giả thiết phân phối của phần dư, mà sử dụng phép lấy mẫu lặp lại từ dữ liệu quảng cáo. Mỗi lần lấy mẫu lặp, chúng ta tạo ra một dữ liệu có số quan sát đúng bằng số quan sát của dữ liệu ban đầu, tuy nhiên một quan sát có thể bị lặp lại nhiều lần. Với mỗi mẫu lặp như vậy, chúng ta thực hiện một ước lượng bằng phương pháp bình phương nhỏ nhất để thu được các hệ số. Phân phối xác suất của các hệ số bằng phương pháp hồi quy truyền thống và phương pháp lấy mẫu lặp được trình bày trong hình 11.11\nFigure 11.11: Phân phối xác suất của các hệ số của mô hình hồi quy tuyến tính đa biến. Cột bên trái: phân phối xác suất được tính toán từ phương pháp bình phương nhỏ nhất truyền thống. Cột bên phải: phân phối xác suất được tạo thành từ 1000 lần lấy mẫu lặp. Hàng thứ nhất: phân phối xác suất của hệ số chặn. Hàng thứ hai: phân phối xác suất của hệ số tuyến tính của biến TV. Hàng thứ ba: phân phối xác suất của hệ số tuyến tính của biến Social_Media. Các đường màu đỏ ở giữa cho biết giá trị trung bình của hệ số.\nBạn đọc có thể thấy rằng phân phối xác suất của các hệ số tính toán từ phương pháp lấy mẫu lặp không khác đáng kể với phân phối xác suất được ước lượng với giả thiết phân phối chuẩn. Như vậy với phương pháp lấy mẫu lặp, bạn đọc có thể tính toán được giá trị trung bình của các hệ số và sai số của ước lượng mà không cần thêm bất kỳ giả thiết nào về hình dạng của phân phối. Đánh đổi lại, để thực hiện ước lượng tham số bằng lấy mẫu lặp, nguồn lực tính toán tăng lên theo số lần chúng ta lấy mẫu.","code":""},{"path":"mô-hình-cây-quyết-định.html","id":"thuật-toán-rừng-ngẫu-nhiên-1","chapter":"Chương 11 Mô hình cây quyết định","heading":"11.3.2 Thuật toán rừng ngẫu nhiên","text":"Phương pháp lấy mẫu lặp lại, hay bootstrap, được giới thiệu trước bởi vì đó là ý tưởng chủ đạo trong mô hình rừng ngẫu nhiên. Các mô hình cây quyết định được biết đến là các mô hình có phương sai lớn, nghĩa là chỉ cần có một thay đổi nhỏ trong dữ liệu huấn luyện, mô hình có thể sẽ thay đổi đáng kể. Như bạn đọc đã biết, một phương pháp đơn giản để không làm thay đổi giá trị trung bình và giảm phương sai của dự đoán đó là dự đoán nhiều lần và sử dụng kết quả trung bình của các dự đoán. Nói một cách khác, giả sử chúng ta sử dụng một biến ngẫu nhiên \\(Z_1\\) có giá trị trung bình \\(\\mu\\) (không biết) để làm dự đoán cho \\(\\mu\\). Bằng một cách nào đó, thay vì sử dụng một dự đoán duy nhất, nếu chúng ta có thể tạo ra một dãy các biến ngẫu nhiên \\(Z_1\\), \\(Z_2\\), \\(\\cdots\\), \\(Z_n\\) và sử dụng \\(\\bar{Z}\\) là giá trị trung bình của các biến ngẫu nhiên kể trên, thì phương sai trong dự đoán sẽ nhỏ hơn phương sai khi sử dụng một dự đoán duy nhất. Trong trường hợp đặc biệt, nếu các \\(Z_i\\) độc lập với nhau, phương sai của \\(\\bar{Z}\\) bằng \\(\\frac{1}{n}\\) phương sai của một dự đoán duy nhất, với \\(n\\) là số lần dự đoán.Với ý tưởng tương tự như vậy, để giảm phương sai của các mô hình có phương sai lớn, chúng ta cần xây dựng nhiều mô hình dự đoán riêng bằng cách sử dụng nhiều dữ liệu huấn luyện và sử dụng giá trị trung bình để dự đoán kết quả. Nói cách khác, nếu chúng ta có \\(T\\) dữ liệu huấn luyện mô hình, chúng ta có thể xây dựng \\(T\\) mô hình cây quyết định (phân loại hoặc hồi quy) \\(\\hat{f}_1\\), \\(\\hat{f}_2\\), \\(\\cdots\\) ,\\(\\hat{f}_T\\) và sau đó lấy trung bình của các mô hình này để thu được một mô hình có phương sai thấp\n\\[\\begin{align}\n\\hat{f}_{avg} = \\cfrac{1}{T} \\ \\sum\\limits_{t = 1}^T \\ \\hat{f}_t\n\\tag{11.7}\n\\end{align}\\]Tất nhiên, chúng ta không thể có được \\(T\\) tập dữ liệu huấn luyện mô hình để xây dựng \\(T\\) mô hình dự đoán. Thay vào đó, chúng ta có thể thực hiện phương pháp lấy mẫu lặp lại \\(T\\) lần trên dữ liệu huấn luyện mô hình ban đầu. Với mỗi dữ liệu thu được, chúng ta xây dựng một cây quyết định và mô hình thu được sau cùng là giá trị trung bình của \\(T\\) mô hình thu được giống như phương trình (11.7). Với cây quyết định hồi quy giá trị trung bình được hiểu đúng với ý nghĩa, nghĩa là nếu một biến mục tiêu \\(y_i\\) được dự đoán từ \\(T\\) cây quyết định hồi quy lần lượt là \\(\\hat{y}_{,1}\\), \\(\\hat{y}_{,2}\\), \\(\\cdots\\), \\(\\hat{y}_{,T}\\) thì giá trị dự đoán của mô hình trung bình là\n\\[\\begin{align}\n\\hat{y}_i = \\cfrac{1}{T} \\sum\\limits_{t = 1}^T \\ \\hat{y}_{,t}\n\\end{align}\\]\nĐối với cây quyết định phân loại, giá trị dự đoán trong mô hình trung bình là mode của véc-tơ các giá trị dự đoán, nghĩa là nếu biến mục tiêu \\(y_i\\) được dự đoán từ \\(T\\) cây quyết định phân loại lần lượt là \\(\\hat{y}_{,1}\\), \\(\\hat{y}_{,2}\\), \\(\\cdots\\), \\(\\hat{y}_{,T}\\) thì giá trị dự đoán của mô hình trung bình là\n\\[\\begin{align}\n\\hat{y}_i =  mode\\left(\\hat{y}_{,1}, \\hat{y}_{,2}, \\cdots, \\hat{y}_{,T} \\right)\n\\end{align}\\]Sử dụng giá trị trung bình của nhiều mô hình để dự đoán cho phép cải thiện đáng kể khả năng dự đoán và có thể sử dụng trong nhiều kiểu mô hình khác nhau. Cách tiếp cận này đặc biệt hữu ích trên mô hình cây quyết định và thuật ngữ “forest” có thể hiểu đơn giản là sử dụng nhiều cây quyết định kết hợp với nhau thành một mô hình duy nhất. Mỗi cây quyết định riêng lẻ có phương sai cao nhưng độ lệch thấp, và trung bình của \\(T\\) cây như vậy có thể làm giảm phương sai. Thực tế chỉ ra rằng kỹ thuật này mang lại những cải tiến vượt bậc về độ chính xác trong dự đoán khi kết hợp hàng trăm hoặc thậm chí hàng nghìn cây vào một mô hình duy nhất.Chúng tôi đã giải thích thuật ngữ “forest”, tiếp theo sẽ là thuật ngữ \\(\"random\"\\). Quay trở lại ý tưởng khi sử dụng nhiều biến ngẫu nhiên \\(Z_i\\) để dự đoán giá trị trung bình \\(\\mathbb{E}(Z_i) = \\mu\\). Nếu các biến \\(Z_i\\) có tương quan dương rất cao với nhau, thì phương sai của \\(\\bar{Z}\\) sẽ không được giảm đi một cách đáng kể với phương sai của một biến duy nhất. Các cây quyết định được xây dựng để đưa ra dự đoán duy nhất trong phương trình (11.7) rất có khả năng là có tương quan cao với nhau, bởi vì các cây được xây dựng từ các dữ liệu được lấy mẫu lặp lại từ một dữ liệu huấn luyện mô hình, và các cây có cùng một tập hợp các biến giải thích cho cùng một biến mục tiêu. Hiện tượng tương quan cao với nhau giữa các cây thường xảy ra khi trong tập hợp các biến giải thích có một biến có ý nghĩa giải thích mạnh vượt trội với các biến giải thích khác. Khi chúng ta xây dựng các cây quyết định trên các dữ liệu được lấy mẫu từ dữ liệu ban đầu, biến giải thích này luôn luôn chiếm ưu thế và xuất hiện trong các nút phân vùng đầu tiên. Việc này làm cho kết quả dự đoán từ các cây quyết định có tương quan rất cao với nhau và dẫn đến phương sai của mô hình trung bình không được giảm đi đáng kể với một cây quyết định riêng lẻ.Để tránh gặp phải hiện tượng này, khi xây dựng cây quyết định từ một dữ liệu được boostrap từ dữ liệu huấn luyện mô hình, chúng ta chỉ sử dụng một tập hợp con bao gồm \\(m\\) biến giải thích, \\(m < p\\), được lựa chọn một cách ngẫu nhiên từ \\(p\\) biến giải thích từ mô hình ban đầu. Trong trường hợp dữ liệu có một hoặc một vài biến giải thích mạnh, khả năng mà một biến này được sử dụng trong mô hình là \\(\\cfrac{m}{p}\\). Nói một cách khác, bằng cách chỉ sử dụng ngẫu nhiên \\(m\\) biến trong số \\(p\\) biến giải thích tại mỗi lần xây dựng cây quyết định, chúng ta có thể xây dựng được các cây quyết định ít có tương quan cao với nhau hơn, và đó có thể thu được một mô hình trung bình có phương sai nhỏ hơn.Khi xây dựng mô hình rừng ngẫu nhiên bao gồm \\(T\\) cây quyết định, tại bước thứ \\(t\\) chúng ta có dữ liệu \\(Data_t\\) được boostrap từ dữ liệu huấn luyện mô hình, người xây dựng mô hình lựa chọn ngẫu nhiên \\(m_t\\) biến giải thích từ \\(p\\) biến giải thích ban đầu, sau đó xây dựng một cây quyết định \\(\\hat{f}_t\\) với kích thước \\(L_t\\) để mô tả mối quan hệ giữa biến giải thích và các biến mục tiêu. Các tham số \\(m_t\\) và \\(L_t\\) tại mỗi bước \\(t\\) và số lượng cây quyết định \\(T\\) là các tham số của mô hình rừng ngẫu nhiên. Thông thường thì tại mỗi bước \\(t\\), kích thước cây \\(L_t\\) sẽ được xác định bằng một tiêu chí dừng nào đó, giống như cách xây dựng một cây quyết định thông thường. Số lượng biến giải thích \\(m_t\\) nếu nhận giá trị khác nhau tại các bước thì rất khó để điều khiển mô hình, đó người xây dựng mô hình rừng ngẫu nhiên thường sử dụng \\(m_t\\) cố định . Nói một cách khác, mô hình rừng ngẫu nhiên có hai tham số cần phải ước lượng là: 1. Số lượng cây quyết định (tham số \\(T\\)) và 2. Số lượng biến giải thích để ước lượng cây quyết định \\(m\\). Các tham số này thường được tính toán sao cho sai số từ xác thực chéo là nhỏ nhất.\nFigure 11.12: Sai số xác thực chéo theo số lượng cây khi sử dụng thuật toán rừng ngẫu nhiên. Mỗi đường thể hiện cho một lựa chọn khác nhau của tham số m là số lượng biến giải thích được lựa chọn trong mỗi cây.\nHình 11.12 mô tả quá trình sử dụng xác thực chéo để tìm tham số \\(T\\) và tham số \\(m\\) khi xây dựng mô hình rừng ngẫu nhiên trên dữ liệu Boston với biến mục tiêu là biến giá nhà (\\(medv\\)). Cặp tham số cho sai số xác thực chéo nhỏ nhất là \\(T = 120\\) cây và \\(m = 6\\) biến. giới hạn của khả năng tính toán nên chúng ta chỉ có thể thử trên các lựa chọn là \\(m = 2, 4\\) hoặc \\(6\\) và \\(T \\leq 500\\). Trên thực tế, \\(m\\) thương được lựa chọn xung quang giá trị của \\(p/2\\) nếu \\(p\\) nhỏ và \\(\\sqrt{p}\\) nếu \\(p\\) lớn trong khi \\(T\\) chỉ có thể được lựa chọn thông qua xác thực chéo. Đồng thời, khi \\(m\\) nhỏ thì tương quan giữa các cây quyết định sẽ thấp hơn nhưng khả năng dự đoán của các cây quyết định riêng lẻ sẽ kém đi nên thường cần nhiều cây quyết định hơn để đưa một kết quả có độ chính xác tương đương như khi \\(m\\) lớn. Bạn đọc có thể thấy rằng khi \\(m = 2\\) hoặc \\(m = 4\\) thì sai số xác thực chéo có xu hướng tiếp tục giảm kể cả khi chúng ta tăng \\(T\\) hơn 500, trong khi khi \\(m = 6\\) thì điểm tối ưu đạt được ngay khi \\(T = 120\\).Bạn đọc có thể nhận thấy ngay sự khác biệt trong khả năng dự đoán của mô hình rừng ngẫu nhiên trong Hình 11.12 và mô hình cây quyết định trong Hình 11.7. Sai số xác thực chéo của mô hình rừng ngẫu nhiên với \\(m = 6\\) và \\(T = 120\\) là khoảng 3100 nghìn USD trong khi sai số xác thực chéo của cây quyết định tốt nhất là 4600 USD. Tuy nhiên, giải thích hay suy diễn kết quả của một cây quyết định là khá đơn giản trong khi việc diễn giải kết quả cho mô hình rừng ngẫu nhiên là vô cùng khó khăn. Đây chính là sự đánh đổi thường phải chấp nhận đối với người xây dựng mô hình.Khi cố gắng giải thích một mô hình rừng ngẫu nhiên người xây dựng mô hình thường sử dụng thước đo sự quan trọng của từng biến giải thích. Có hai cách để định nghĩa sự quan trọng của biến giải thích: thứ nhất là tính toán giá trị trung bình của sự suy giảm độ chính xác của các mô hình cây quyết định khi biến đó không được tính vào trong mô hình, và thứ hai là tính toán tổng của sự suy giảm trong độ thuần (purity) của các nút có chứa biến đó trên tất cả các cây quyết định. Các thước đo này tính trên một biến giải thích lớn tương đối với các biến giải thích khác nghĩa là biến đó quan trọng hơn và có nhiều ý nghĩa hơn trong mô hình rừng ngẫu nhiên. Bạn đọc có thể tham khảo thêm về cách tính toán mức độ quan trọng của các biến của mô hình rừng ngẫu nhiên trong phần ??.\nFigure 11.13: Mức độ quan trọng của các biến trong mô hình rừng ngẫu nhiên trên dữ liệu Boston. Hình bên trái: sự quan trọng tính bằng sự suy giảm trung bình trong khả năng dự báo của các cây quyết định khi bỏ biến giải thích ra khỏi mô hình. Hình bên phải: sự quan trọng tính bằng tổng sự suy giảm trong độ thuần của các nút khi sử dụng biến giải thích.\nHình 11.13 cho thấy hai biến quan trọng nhất khi sử dụng mô hình rừng ngẫu nhiên để dự đoán giá nhà là \\(rm\\) và \\(lstat\\), nhóm các biến quan trọng thứ hai bao gồm có \\(dis\\), \\(nox\\), \\(ptratio\\), và \\(crim\\). Nhóm các biến ít có ý nghĩa trong dự đoán giá nhà là \\(chas\\), \\(zn\\) và \\(rad\\).","code":""},{"path":"mô-hình-cây-quyết-định.html","id":"thực-hành-mô-hình-rừng-ngẫu-nhiên","chapter":"Chương 11 Mô hình cây quyết định","heading":"11.4 Thực hành: mô hình rừng ngẫu nhiên","text":"","code":""},{"path":"mô-hình-cây-quyết-định.html","id":"xây-dựng-mô-hình-rừng-ngẫu-nhiên-trên-dữ-liệu-boston","chapter":"Chương 11 Mô hình cây quyết định","heading":"11.4.1 Xây dựng mô hình rừng ngẫu nhiên trên dữ liệu Boston","text":"","code":""},{"path":"mô-hình-cây-quyết-định.html","id":"xây-dựng-mô-hình-rừng-ngẫu-nhiên-trên-dữ-liệu-oj","chapter":"Chương 11 Mô hình cây quyết định","heading":"11.4.2 Xây dựng mô hình rừng ngẫu nhiên trên dữ liệu OJ","text":"","code":""},{"path":"mô-hình-cây-quyết-định.html","id":"bài-tập-5","chapter":"Chương 11 Mô hình cây quyết định","heading":"11.5 Bài tập","text":"","code":""},{"path":"mô-hình-cây-quyết-định.html","id":"bài-tập-lý-thuyết-1","chapter":"Chương 11 Mô hình cây quyết định","heading":"11.5.1 Bài tập lý thuyết","text":"","code":""},{"path":"mô-hình-cây-quyết-định.html","id":"bài-tập-thực-hành-1","chapter":"Chương 11 Mô hình cây quyết định","heading":"11.5.2 Bài tập thực hành","text":"","code":""},{"path":"mô-hình-cây-quyết-định.html","id":"phụ-lục-4","chapter":"Chương 11 Mô hình cây quyết định","heading":"11.6 Phụ lục","text":"","code":""},{"path":"mô-hình-cây-quyết-định.html","id":"tính-toán-sự-quan-trọng-của-biến-giải-thích-trong-mô-hình-rừng-ngẫu-nhiên","chapter":"Chương 11 Mô hình cây quyết định","heading":"11.6.1 Tính toán sự quan trọng của biến giải thích trong mô hình rừng ngẫu nhiên","text":"","code":"## \n## Attaching package: 'dplyr'## The following objects are masked from 'package:stats':\n## \n##     filter, lag## The following objects are masked from 'package:base':\n## \n##     intersect, setdiff, setequal, union## \n## Attaching package: 'kableExtra'## The following object is masked from 'package:dplyr':\n## \n##     group_rows## \n## Attaching package: 'gridExtra'## The following object is masked from 'package:dplyr':\n## \n##     combine## \n## Attaching package: 'pryr'## The following object is masked from 'package:dplyr':\n## \n##     where## Loading required package: shape"},{"path":"neuralnetwork.html","id":"neuralnetwork","chapter":"Chương 12 Mô hình mạng nơ-ron","heading":"Chương 12 Mô hình mạng nơ-ron","text":"Chương sách này thảo luận một chủ đề quan trọng có ứng dụng rộng rãi nhất trong lĩnh vực trí tuệ nhân tạo là mô hình mạng học sâu (deep learning). Tại thời điểm nhóm tác giả viết cuốn sách (2023), học sâu là một lĩnh vực nghiên cứu tích cực nhất không chỉ trong khoa học máy tính, công nghệ thông tin mà còn cả trong các lĩnh vực khác như kinh tế, tài chính, y tế, xây dựng,… Nền tảng của mô hình mạng học sâu là mô hình mạng nơ-ron (hay neural network). Mô hình mạng nơ-ron đã được biết đến đến rộng rãi vào cuối những năm 1980 bởi cách vận hành của mô hình mô tả lại cách thức mà hệ thống thần kinh của con người xử lý thông tin. Mặc dù các đặc tính của mô hình mạng nơ-ron được phân tích bởi những nhà toán học và nhà thống kê nhiều thuật toán liên quan đến mô hình này đã được cải thiện với sự ra đời của các phương pháp học máy khác như SVM, rừng ngẫu nhiên, học tăng cường,…, mà mô hình mạng nơ-ron phần nào không được ưa chuộng.Từ những năm 2010, với nhu cầu xử lý các dữ liệu ngày càng phức tạp và sự ra đời của các kiến trúc máy tính lớn, mô hình mạng nơ-ron đã quay trở lại với tên mới là mạng học sâu (deep learning). Mạng học sâu vượt trội hoàn toàn các mô hình học máy thông thường trong phân loại hình ảnh/video và mô hình hóa ngôn ngữ tự nhiên bao gồm dữ liệu kiểu văn bản và giọng nói (natural langugue processing hay NLP). Các nhà khoa học trong lĩnh vực này tin rằng lý chính cho những thành công của mô hình mạng nơ-ron là càng ngày những người xây dựng mô hình càng chú trọng vào xây dựng các bộ dữ liệu khổng lồ để huấn luyện môn hình và cấu trúc của mô hình cho phép nó đáp ứng được với bất kỳ tập kích thước dữ liệu nào.","code":""},{"path":"neuralnetwork.html","id":"nnonelayer","chapter":"Chương 12 Mô hình mạng nơ-ron","heading":"12.1 Mạng nơ-rơn có một lớp ẩn","text":"Mô hình mạng nơ-ron lấy một véc-tơ đầu vào gồm \\(p\\) biến \\(\\textbf{X} = (X_1, X_2, \\cdots , X_p)\\) và xây dựng một hàm phi tuyến \\(\\hat{f}\\) để dự đoán biến mục tiêu \\(Y\\) . Chúng ta đã xây dựng các mô hình dự đoán phi tuyến trong các chương trước, ví dụ như mô hình cộng tính tổng quát, mô hình cây quyết định, mô hình rừng ngẫu nhiên, mô hình tăng cường. Điều làm nên sự khác biệt của mô hình mạng nơ-ron là cấu trúc xây dựng của mô hình. Hình 12.1 mô tả một mạng nơ-ron chuyển tiếp để mô hình biến mục tiêu \\(Y\\) định lượng từ \\(p = 3\\) biến giải thích là \\(X_1\\), \\(X_2\\), và \\(X_3\\).\nFigure 12.1: Mô hình mạng nơ-ron có p = 3 đơn vị trong lớp đầu vào, một lớp ẩn có năm đơn vị, và một đơn vị đầu ra.\nTheo thuật ngữ chuyên môn, ba biến giải thích \\(X_1\\), \\(X_2\\), và \\(X_3\\) là các đơn vị (unit) của lớp đầu vào (input layer). Các mũi tên được dùng để mô tả rằng mỗi đơn vị đầu vào sẽ chuyển tiếp thông tin vào \\(k = 5\\) đơn vị của lớp ẩn (hidden layer) được ký hiệu là \\(H_i\\) với \\(= 1, 2, \\cdots, k\\). Dạng của hàm \\(f\\) trong mô hình mạng nơ-ron nhân tạo sẽ được viết như sau:\n\\[\\begin{align}\nf(\\textbf{X}) & = \\beta_0 + \\sum\\limits_{= 1}^k \\beta_i \\cdot h_i\\left(\\textbf{X}\\right) \\\\\n& = \\beta_0 + \\sum\\limits_{= 1}^k \\beta_i \\cdot g\\left(w_{,0} + \\sum\\limits_{j=1}^p w_{,j} \\cdot X_j\\right)\n\\tag{12.1}\n\\end{align}\\]\ntrong đó \\(g\\) là một hàm số phi tuyến được xác định trước, được gọi theo thuật ngữ chuyên môn là các hàm kích hoạt (activation function). Các \\(\\beta_i\\) và \\(w_{,j}\\) là các hằng số và cũng là các tham số cần được ước lượng của mô hình. Hàm \\(f\\) trong phương trình (??) được xây dựng theo hai bước:Bước thứ nhất, các đơn vị \\(H_i\\) trong lớp ẩn được tính bằng hàm kích hoạt tính trên tổ hợp tuyến tính của các biến đầu vào:\\[\\begin{align}\nH_i = g\\left(w_{,0} + \\sum\\limits_{j=1}^p w_{,j} \\cdot X_j\\right)\n\\tag{12.2}\n\\end{align}\\]Bước thứ hai, \\(k\\) đơn vị của lớp ẩn là yếu tố đầu vào để tính toán giá trị biến đầu ra định lượng:\n\\[\\begin{align}\nY = \\beta_0 + \\sum\\limits_{= 1}^k \\beta_i \\cdot H_i\n\\tag{12.3}\n\\end{align}\\]Quá trình tính toán đi từ lớp đầu vào qua các lớp ẩn và kết thúc ở mạng đầu ra được gọi là quá trình chuyển tiếp, và hàm \\(f\\) trong phương trình (12.1) được gọi là một mạng nơ-ron chuyển tiếp. Tất cả các tham số \\(\\beta_0\\), \\(\\beta_1\\), \\(\\cdots\\), \\(\\cdots\\) , \\(\\beta_k\\) và \\(w_{1,0}\\) , \\(\\cdots\\) , \\(w_{k,p}\\) được ước lượng từ dữ liệu. Các hàm kích hoạt thường được sử dụng là hàm Sigmoid và hàm ReLU. Hàm sigmoid là hàm được thường xuyên sử dụng trong hồi quy logistic để chuyển hàm tuyến tính thành xác suất giữa 0 và 1 trong khi hàm ReLU là hàm phi tuyến được xây dựng một cách đơn giản nhất nhằm mục đích dễ dàng tuyến tính trong các mạng phức tạp.\n\\[\\begin{align}\n\\text{Sigmoid: } & g(x) = \\cfrac{e^x}{1 + e^{x}} = \\cfrac{1}{1 + e^{-x}} \\\\\n\\text{ReLU: } & g(x) = max(x , 0) = (x)^+\n\\end{align}\\]\nFigure 12.2: Hàm kích hoạt sigmoid và hàm kích hoạt ReLU. Hàm Sigmoid có đạo hàm tại mọi điểm trong khi hàm ReLU không có đạo hàm tại 0\nHình 12.2 mô tả giá trị của hàm Sigmoid và hàm ReLU trên đoạn từ -4 đến 4. Trong thời kỳ đầu của mô hình mạng nơ-ron, hàm Sigmoid được ưa chuộng vì hàm số này có đạo hàm liên tục tại mọi giát trị của \\(x\\). Sau đó, hàm ReLU lại là lựa chọn ưa thích trong các mô hình mạng nơ-ron hiện đại vì hàm số này đơn giản, dễ tính toán và cho hiệu quả tốt hơn với hàm Sigmoid.Có thể tóm tắt lại mô hình được mô tả trong Hình 12.1 như sau: từ 3 biến giải thích ban đầu là \\(X_1\\), \\(X_2\\), và \\(X_3\\) chúng ta tạo ra năm biến giải thích mới là \\(H_1\\), \\(H_2\\), \\(H_3\\), \\(H_4\\) và \\(H_5\\) được tính toán bằng giá trị của hàm kích hoạt \\(g(.)\\) trên các tổ hợp tuyến tính của các biến giải thích ban đầu. Sau đó chúng ta sử dụng năm biến giải thích \\(H_i\\), \\(= 1, 2, 3, 4, 5\\) để xây dựng một mô hình hồi quy tuyến tính mà trong đó biến phụ thuộc là \\(Y\\). Tham số của các mô hình bao gồm các hệ số \\(w_{,j}\\) để tính các biến \\(H_i\\), và các hệ số \\(\\beta_j\\) trong mô hình hồi quy tuyến tính \\(Y\\) theo các biến \\(H\\).Mô hình có tên là mạng nơ-ron bởi vì cấu trúc của mô hình bao gồm các đơn vị \\(H_i\\) hoạt động giống như các tế bào thần kinh trong não bộ của con người. Các đơn vị \\(H_i\\) xấp xỉ hoặc bằng 0 giống như các tế bào thần kinh im lặng (slient neuron), những tế bào ít bị kích hoạt trong quá trình lan truyền thông tin, trong khi các đơn vị \\(H_i\\) lớn (khi sử dụng hàm ReLU), hoặc xấp xỉ 1 (khi sử dụng hàm Sigmoid) giống như những tế bào bị kích hoạt mạng trong quá trình lan truyền thông tin.Sử dụng các hàm kích hoạt \\(g(.)\\) phi tuyến là đặc biệt quan trọng tong vì nếu không hàm \\(f\\) sẽ suy biến thành mô hình tuyến tính thông thường với \\(p = 3\\) biến giải thích trong \\(X_1\\), \\(X_2\\), và \\(X_3\\). Ngoài ra, hàm kích hoạt phi tuyến cho phép mô hình mạng nơ-ron mô tả được những mối liên hệ phi tuyến và phức tạp giữa các biến giải thích \\(\\textbf{X}\\) và biến mục tiêu \\(Y\\).Giả sử trong mô hình mạng nơ-ron được mô tả trong Hình 12.1, chúng ta có hàm kích hoạt \\(g(x) = x^2\\) và giá trị của các hệ số \\(\\boldsymbol{\\beta} = (\\beta_0, \\beta_1, \\beta_2, \\beta_3, \\beta_4, \\beta_5)\\) và \\(w_{,j}\\), với \\(1 \\leq \\leq 5\\) và \\(0 \\leq j \\leq 3\\), được cho như sau\n\\[\\begin{align}\n& \\text{hệ số chặn: } \\beta_0 = w_{1,0} = w_{2,0} = w_{3,0} = w_{4,0} = w_{5,0} 0 \\\\\n& \\\\\n& \\begin{pmatrix}\n\\beta_1 \\\\\n\\beta_2 \\\\\n\\beta_3 \\\\\n\\beta_4 \\\\\n\\beta_5\n\\end{pmatrix} = \\begin{pmatrix}\n0.5 \\\\\n0.5 \\\\\n0.5 \\\\\n1 \\\\\n2\n\\end{pmatrix} \\ \\text{ và } \\ \\begin{pmatrix}\nw_{1,1} & w_{1,2} & w_{1,3} \\\\\nw_{2,1} & w_{2,2} & w_{2,3} \\\\\nw_{3,1} & w_{3,2} & w_{3,3} \\\\\nw_{4,1} & w_{4,2} & w_{4,3} \\\\\nw_{5,1} & w_{5,2} & w_{5,3}\n\\end{pmatrix} = \\begin{pmatrix}\n1 & -1 & 1 \\\\\n1 & 2 & -1 \\\\\n0 & 0 & 0 \\\\\n0 & 0 & 0 \\\\\n0 & 0 & 0\n\\end{pmatrix}\n\\tag{12.4}\n\\end{align}\\]Trước hết, để tránh sự phức tạp chúng tôi cho giá trị các hàng 3, 4, và 5 của ma trận \\(\\boldsymbol{w}\\) đều bằng 0, điều này dẫn đến giá trị tại các đơn vị \\(H_3\\), \\(H_4\\) và \\(H_5\\) của lớp ẩn sẽ bằng 0. Các đơn vị này hoạt động như các tế bào im lặng trong mạng nơ-rơn và không có ảnh hưởng đến biến mục tiêu \\(Y\\). Chúng ta có giá trị tại \\(H_1\\) và \\(H_2\\) được tính theo các biến giải thích và hàm kích hoạt:\n- Giá trị tại \\(H_1\\)\n\\[\\begin{align}\nH_1\\left(X_1, X_2, X_3\\right) & = g\\left(w_{1,1} \\cdot X_1 + w_{1,2} \\cdot X_2 + w_{1,3} \\cdot X_3\\right) \\\\\n& = \\left(w_{1,1} \\cdot X_1 + w_{1,2} \\cdot X_2 + w_{1,3} \\cdot X_3\\right)^2 \\\\\n& = \\left(X_1 - X_2 + X_3\\right)^2\n\\tag{12.5}\n\\end{align}\\]Giá trị tại \\(H_2\\)\n\\[\\begin{align}\nH_2 \\left(X_1, X_2, X_3\\right) & = g\\left(w_{2,1} \\cdot X_1 + w_{2,2} \\cdot X_2 + w_{2,3} \\cdot X_3\\right) \\\\\n& = \\left(w_{2,1} \\cdot X_1 + w_{2,2} \\cdot X_2 + w_{2,3} \\cdot X_3\\right)^2 \\\\\n& = \\left(X_1 + 2 \\cdot X_2 - X_3\\right)^2\n\\tag{12.6}\n\\end{align}\\]Giá trị của hàm số \\(f(\\text{X})\\) là đầu ra của mạng nơ-ron được xác định như sau:\n\\[\\begin{align}\nf(X_1, X_2, X_3) & = 0.5 \\cdot H_1\\left(X_1, X_2, X_3\\right) + 0.5 \\cdot H_2 \\left(X_1, X_2, X_3\\right) \\\\\n& = X_1^2 + 2.5 \\cdot X_2^2 + X_3^2 + X_1 \\cdot X_2 - 3 \\cdot X_2 \\cdot X_3\n\\tag{12.7}\n\\end{align}\\]Bạn đọc có thể thấy rằng, việc sử dụng hàm kích hoạt phi tuyến cho phép chúng ta có hàm đầu ra bao gồm hàm phi tuyến trên các giá trị biến đầu vào, mà còn tính đến cả biến tương tác giữa các biến ban đầu. Trong ví dụ ở phương trình (12.7) là các giá trị \\(X_1 \\cdot X_2\\) và \\(3 \\cdot X_2 \\cdot X_3\\). Trong thực tế, chúng ta sẽ không sử dụng hàm bậc hai hay hàm đa thức cho hàm kích hoạt \\(g(.)\\) hàm kích hoạt đa thức sẽ dẫn tới kết quả cũng chỉ là dạng hàm đa thức. Các hàm kích hoạt Sigmoid hoặc ReLU không bị giới hạn như vậy.Trong ví dụ trên chúng ta đã cho trước các tham số bao gồm các hệ số \\(\\boldsymbol{\\beta}\\) và ma trận \\(\\boldsymbol{w}\\). Tuy nhiên trong thực tế, các tham số này được lựa chọn để giảm thiểu sai số giữa giá trị dự đoán và giá trị quan sát của biến mục tiêu:\n\\[\\begin{align}\n(\\hat{\\boldsymbol{\\beta}},\\hat{\\boldsymbol{w}}) = \\underset{\\boldsymbol{\\beta},\\boldsymbol{w}}{\\operatorname{argmin}} \\sum\\limits_{=1}^n \\left( y_i - f(\\textbf{x}_i) \\right)^2\n\\tag{12.8}\n\\end{align}\\]Chúng ta sẽ thảo luận về ước lượng tham số cho mô hình mạng nơ-ron trong phần 12.3.","code":""},{"path":"neuralnetwork.html","id":"nnmultilayer","chapter":"Chương 12 Mô hình mạng nơ-ron","heading":"12.2 Mạng nơ-ron có nhiều lớp ẩn","text":"Mô hình mạng nơ-ron được sử dụng hiện tại thường có nhiều hơn một lớp ẩn và có nhiều đơn vị trên mỗi lớp. Về lý thuyết, một lớp ẩn duy nhất với số lượng lớn các đơn vị có khả năng xấp xỉ hầu hết các dạng hàm \\(f\\). Tuy nhiên, thực tế chỉ ra rằng xây dựng những cấu trúc nhiều lớp và mỗi lớp có kích thước hợp lý là giải pháp tốt hơn với cấu trúc chỉ có một lớp ẩn và có rất nhiều đơn vị trên cùng một lớp.Cấu trúc có thể mở rộng của mô hình mạng nơ-ron nhân tạo cho phép nó có khả năng mô hình hóa tốt những bộ dữ liệu phức tạp, mà điển hình là dữ liệu dạng ảnh, dạng văn bản, hay dạng tín hiệu. Để minh họa cho khả năng phù hợp của mô hình với những dữ liệu phức tạp, chúng ta sẽ xây dựng một cấu trúc mạng nơ-ron có nhiều lớp ẩn để dự đoán dữ liệu là ảnh chứa các chữ số viết tay. Dữ liệu được sử dụng để huấn luyện mô hình là tập dữ liệu chữ số viết tay nổi tiếng \\(\\textbf{MNIST}\\). Hình 12.3 minh họa một số quan sát trong dữ liệu về các chữ số viết tay được lưu trữ trong dữ liệu \\(\\textbf{MNIST}\\). Mỗi quan sát của dữ liệu sử dụng để xây dựng mô hình là một hình ảnh có kích thước \\(p = 28 \\times 28 = 784\\) pixel và biến mục tiêu là giá trị số của hình ảnh xuất hiện trong biến giải thích. Có tất cả 10 giá trị khác nhau cho biến mục tiêu là các chữ số viết tay tương ứng từ 0 đến 9.\nFigure 12.3: Năm mươi giá trị đầu tiên trong dữ liệu số viết tay MNIST. Mỗi số viết tay là một quan sát trong dữ liệu. Một bức ảnh được lưu dưới dạng một véc-tơ có độ dài p = 784. Mỗi giá trị trong véc-tơ là một số tự nhiên nhận giá trị từ 0 đến 255 cho biết độ tối của điểm ảnh.\nÝ tưởng là xây dựng một mô hình để phân loại các hình ảnh thành chữ số từ 0 đến 9. Chúng ta sẽ sử dụng cấu trúc mạng nơ-ron với hai lớp ẩn được minh họa trong Hình 12.4 để xây dựng mô hình phân loại hình ảnh số viết tay.\nFigure 12.4: Mạng nơ-ron xây dựng trên dữ liệu MNIST có hai lớp ẩn, mỗi lớp ẩn có nhiều đơn vị và lớp đầu ra có 10 đơn vị tương ứng với m = 10 giá trị có thể của các số viết tay từ 0 đến 9. Lớp đầu vào có p = 784 đơn vị tương ứng với 784 điểm ảnh.\nGiá trị đầu ra trong cấu trúc mạng nơ-ron trong Hình 12.4 là biến kiểu factor, được biểu thị bằng véc-tơ \\(\\textbf{Y} = (Y_1, Y_2, \\cdots , Y_{m})\\) với \\(m = 10\\). Dữ liệu có 60 nghìn bức ảnh được sử dụng để huấn luyện mô hình và 10 nghìn bức ảnh được sử dụng để kiểm tra mô hình. Cấu trúc mạng trong Hình 12.4 có hai lớp ẩn thay vì một lớp ẩn giống như trước.Lớp ẩn thứ nhất có \\(k_1\\) đơn vị được tính toán từ \\(p\\) đầu vào ban đầu với hàm kích hoạt \\(g_1(.)\\). Giả sử các nút trong lớp ẩn đầu tiên lần lượt là \\(H^{(1)}_1\\), \\(H^{(1)}_2\\), \\(\\cdots\\), \\(H^{(1)}_{k_1}\\). Ta có \\(H^{(1)}_j\\) được tính toán từ các đầu vào với \\((p+1)\\) tham số \\(w^{(1)}_{j,}\\) với \\(0 \\leq \\leq p\\) như sau:\n\\[\\begin{align}\nH^{(1)}_j = g_1\\left( w^{(1)}_{j,0} + w^{(1)}_{j,1} X_1 + w^{(1)}_{j,2} X_2 + \\cdots + w^{(1)}_{j,p} X_p \\right)\n\\tag{12.9}\n\\end{align}\\]\nCó thể thấy rằng, để tính toán tất cả \\(k_1\\) đơn vị trong lớp ẩn thứ nhất, chúng ta cần sử dụng \\(k_1 \\times (p+1)\\) tham số.Lớp ẩn thứ nhất có \\(k_1\\) đơn vị được tính toán từ \\(p\\) đầu vào ban đầu với hàm kích hoạt \\(g_1(.)\\). Giả sử các nút trong lớp ẩn đầu tiên lần lượt là \\(H^{(1)}_1\\), \\(H^{(1)}_2\\), \\(\\cdots\\), \\(H^{(1)}_{k_1}\\). Ta có \\(H^{(1)}_j\\) được tính toán từ các đầu vào với \\((p+1)\\) tham số \\(w^{(1)}_{j,}\\) với \\(0 \\leq \\leq p\\) như sau:\n\\[\\begin{align}\nH^{(1)}_j = g_1\\left( w^{(1)}_{j,0} + w^{(1)}_{j,1} X_1 + w^{(1)}_{j,2} X_2 + \\cdots + w^{(1)}_{j,p} X_p \\right)\n\\tag{12.9}\n\\end{align}\\]\nCó thể thấy rằng, để tính toán tất cả \\(k_1\\) đơn vị trong lớp ẩn thứ nhất, chúng ta cần sử dụng \\(k_1 \\times (p+1)\\) tham số.Lớp ẩn thứ hai có \\(k_2\\) đơn vị được tính toán từ \\(k_1\\) đơn vị của lớp ẩn thứ nhất và hàm kích hoạt \\(g_2(.)\\). Gọi các nút trong lớp ẩn thứ hai lần lượt là \\(H^{(2)}_1\\), \\(H^{(2)}_2\\), \\(\\cdots\\), \\(H^{(2)}_{k_2}\\). Ta có \\(H^{(2)}_j\\) được tính toán từ lớp ẩn thứ nhất vào với \\((k_1+1)\\) tham số \\(w^{(2)}_{j,}\\) với \\(0 \\leq \\leq k_1\\) như sau:\n\\[\\begin{align}\nH^{(2)}_j = g_2\\left( w^{(2)}_{j,0} + w^{(2)}_{j,1} H^{(2)}_1 + w^{(2)}_{j,2} H^{(2)}_1 + \\cdots + w^{(2)}_{j,k_1} H^{(2)}_{k_1} \\right)\n\\tag{12.10}\n\\end{align}\\]\nĐể tính toán tất cả \\(k_2\\) đơn vị trong lớp ẩn thứ hai, chúng ta cần sử dụng \\(k_2 \\times (k_1+1)\\) tham số.Lớp ẩn thứ hai có \\(k_2\\) đơn vị được tính toán từ \\(k_1\\) đơn vị của lớp ẩn thứ nhất và hàm kích hoạt \\(g_2(.)\\). Gọi các nút trong lớp ẩn thứ hai lần lượt là \\(H^{(2)}_1\\), \\(H^{(2)}_2\\), \\(\\cdots\\), \\(H^{(2)}_{k_2}\\). Ta có \\(H^{(2)}_j\\) được tính toán từ lớp ẩn thứ nhất vào với \\((k_1+1)\\) tham số \\(w^{(2)}_{j,}\\) với \\(0 \\leq \\leq k_1\\) như sau:\n\\[\\begin{align}\nH^{(2)}_j = g_2\\left( w^{(2)}_{j,0} + w^{(2)}_{j,1} H^{(2)}_1 + w^{(2)}_{j,2} H^{(2)}_1 + \\cdots + w^{(2)}_{j,k_1} H^{(2)}_{k_1} \\right)\n\\tag{12.10}\n\\end{align}\\]\nĐể tính toán tất cả \\(k_2\\) đơn vị trong lớp ẩn thứ hai, chúng ta cần sử dụng \\(k_2 \\times (k_1+1)\\) tham số.Trong lớp đầu ra, đây là bài toán phân loại, nên chúng ta sử dụng \\(m = 10\\) đơn vị tương ứng với 10 chữ số viết tay từ 0 đến 9. Trong bài toán phân loại, hàm kích hoạt để tính toán các đơn vị trong lớp đầu ra thường là hàm softmax. Giá trị tại đơn vị \\(Y_j\\) với \\(1 \\leq m\\) trong lớp đầu ra được xác định như sau:\n\\[\\begin{align}\nZ_j &= \\beta_{j,0} + \\beta_{j,1} H^{(2)}_1 + \\beta_{j,2} H^{(2)}_2 + \\cdots +\\beta_{j,k_2} H^{(2)}_{k_2} \\\\\n\\textbf{Y} &= softmax(\\textbf{Z}) \\rightarrow Y_j = \\cfrac{exp(Z_j)}{exp(Z_1) + exp(Z_2) + \\cdots + exp(Z_m)}\n\\tag{12.11}\n\\end{align}\\]\nĐể tính toán \\(m\\) đơn vị trong lớp đầu ra, chúng ta cần \\(m \\times (k_2 + 1)\\) tham số. Lưu ý rằng khi sử dụng hàm softmax thì tổng giá trị của các đơn vị trong lớp đầu ra luôn bằng 1.Trong lớp đầu ra, đây là bài toán phân loại, nên chúng ta sử dụng \\(m = 10\\) đơn vị tương ứng với 10 chữ số viết tay từ 0 đến 9. Trong bài toán phân loại, hàm kích hoạt để tính toán các đơn vị trong lớp đầu ra thường là hàm softmax. Giá trị tại đơn vị \\(Y_j\\) với \\(1 \\leq m\\) trong lớp đầu ra được xác định như sau:\n\\[\\begin{align}\nZ_j &= \\beta_{j,0} + \\beta_{j,1} H^{(2)}_1 + \\beta_{j,2} H^{(2)}_2 + \\cdots +\\beta_{j,k_2} H^{(2)}_{k_2} \\\\\n\\textbf{Y} &= softmax(\\textbf{Z}) \\rightarrow Y_j = \\cfrac{exp(Z_j)}{exp(Z_1) + exp(Z_2) + \\cdots + exp(Z_m)}\n\\tag{12.11}\n\\end{align}\\]\nĐể tính toán \\(m\\) đơn vị trong lớp đầu ra, chúng ta cần \\(m \\times (k_2 + 1)\\) tham số. Lưu ý rằng khi sử dụng hàm softmax thì tổng giá trị của các đơn vị trong lớp đầu ra luôn bằng 1.Như vậy, để tính toán được \\(m = 10\\) giá trị đầu ra cho cấu trúc mạng nơ-ron được trình bày trong Hình 12.4, số lượng tham số cần sử dụng là\n\\[\\begin{align}\nk_1 \\cdot (p + 1) + k_2 \\cdot (k_1 + 1) + m \\cdot (k_2+1)\n\\end{align}\\]Ví dụ, nếu chúng ta sử dụng 512 đơn vị trong lớp ẩn thứ nhất và 256 đơn vị trong lớp ẩn thứ hai, số lượng tham số của mạng nơ-ron với 784 đơn vị đầu vào và 10 đơn vị đầu ra là\n\\[\\begin{align}\n512 \\cdot (784 + 1) + 256 \\cdot (512 + 1) + 10 \\cdot (256 + 1) = 535.818\n\\end{align}\\]\nNói cách khác, mô hình sử dụng hơn 500 nghìn tham số để tính toán 10 đơn vị đầu ra từ 784 đơn vị đầu vào. Các tham số này được tính toán sao cho sai số giữa véc-tơ đầu ra tính toán từ mô hình và giá trị đầu ra quan sát được trên dữ liệu là nhỏ nhất. Trong bài toán phân loại, sai số thường được sử dụng là hàm cross-entropy. Với quan sát thứ \\(\\) của biến giải thích \\(\\textbf{x}_i\\) thì quan sát \\(y_i\\) tương ứng của biến mục tiêu (nhận một trong các giá trị từ \\(1\\) đến \\(m\\)) sẽ được viết dưới dạng véc-tơ đầu ra \\(\\textbf{y}_i = (y^{}_{1},y^i_2,\\cdots,y^i_m)\\) sao cho\n\\[\\begin{align}\ny^i_j &= 1 \\text{ nếu } y_i = j \\\\\ny^i_j &= 0 \\text{ nếu } y_i \\neq j\n\\tag{12.12}\n\\end{align}\\]\nCách biến đổi biến này thường được gọi là one-hot encoding. Với véc-tơ đầu ra được tính toán từ véc-tơ đầu vào \\(\\textbf{x}_i\\) theo cấu trúc mạng nơ-ron bằng các phương trình (12.9), (12.10), và (12.11) là \\(\\hat{\\textbf{y}}_i = (\\hat{y}^{}_{1},\\hat{y}^i_2,\\cdots,\\hat{y}^i_m)\\) thì sai số tính bằng cross-entropy tại quan sát thứ \\(\\) là\n\\[\\begin{align}\n\\sum\\limits_{j=1}^m \\ y^{}_{j} \\cdot \\log(\\hat{y}^{}_{j}) = y^{}_{1} \\cdot \\log(\\hat{y}^{}_{1}) + y^{}_{2} \\cdot \\log(\\hat{y}^{}_{2}) + \\cdots + y^{}_{m} \\cdot \\log(\\hat{y}^{}_{m})\n\\tag{12.13}\n\\end{align}\\]\nvà sai số tính bằng cross-entropy trên \\(n\\) dữ liệu huấn luyện mô hình là\n\\[\\begin{align}\nCE\\_loss = \\sum\\limits_{=1}^n \\sum\\limits_{j=1}^m \\ y^{}_{j} \\cdot \\log(\\hat{y}^{}_{j})\n\\tag{12.14}\n\\end{align}\\]Trong mô hình mạng nơ-ron, để đơn giản hóa ký hiệu, chúng ta sẽ sử dụng ký hiệu dạng ma trận. Cấu trúc mạng nơ-ron có hai lớp ẩn được mô tả trong hình 12.4 có ba ma trận tham số \\(\\boldsymbol{w}_1\\), \\(\\boldsymbol{w}_2\\), và \\(\\boldsymbol{\\beta}\\) được định nghĩa như sau\n\\[\\begin{align}\n\\boldsymbol{\\beta} &= \\begin{pmatrix}\n\\beta_{1,0} & \\beta_{1,1} & \\cdots & \\beta_{1,k_2} \\\\\n\\beta_{2,0} & \\beta_{2,1} & \\cdots & \\beta_{2,k_2} \\\\\n\\cdots & \\cdots & \\cdots & \\cdots \\\\\n\\beta_{m,0} & \\beta_{m,1} & \\cdots & \\beta_{m,k_2}\n\\end{pmatrix} \\\\\n& \\\\\n\\boldsymbol{w}_1 &= \\begin{pmatrix}\nw^{(1)}_{1,0} & w^{(1)}_{1,1} & \\cdots & w^{(1)}_{1,p} \\\\\nw^{(1)}_{2,0} & w^{(1)}_{2,1} & \\cdots & w^{(1)}_{2,p} \\\\\n\\cdots & \\cdots & \\cdots & \\cdots \\\\\nw^{(1)}_{k_1,0} & w^{(1)}_{k_1,1} & \\cdots & w^{(1)}_{k_1,p}\n\\end{pmatrix};\n\\boldsymbol{w}_2 = \\begin{pmatrix}\nw^{(2)}_{1,0} & w^{(2)}_{1,1} & \\cdots & w^{(2)}_{1,k_1} \\\\\nw^{(2)}_{2,0} & w^{(2)}_{2,1} & \\cdots & w^{(2)}_{2,p} \\\\\n\\cdots & \\cdots & \\cdots & \\cdots \\\\\nw^{(2)}_{k_2,0} & w^{(2)}_{k_2,1} & \\cdots & w^{(2)}_{k_2,k_1}\n\\end{pmatrix}\n\\tag{12.15}\n\\end{align}\\]Quá trình ước lượng tham số cho mạng nơ-ron là quá trình tìm các ma trận tham số \\(\\boldsymbol{w}_1\\), \\(\\boldsymbol{w}_2\\), và \\(\\boldsymbol{\\beta}\\) để tối thiểu hóa tổn thất tính bằng cross-entropy\\[\\begin{align}\n(\\hat{\\boldsymbol{w}}_1, \\hat{\\boldsymbol{w}}_2, \\hat{\\boldsymbol{\\beta}}) &= \\underset{\\boldsymbol{w}_1, \\boldsymbol{w}_2, \\boldsymbol{\\beta}}{\\operatorname{argmin}} \\sum_{=1}^n \\sum_{j=1}^m y^{}_{j} \\cdot \\log(\\hat{y}^{}_{j})\n\\label{#eq:nn016}\n\\end{align}\\]Như chúng tôi đã đề cập ở phía trước, các mô hình mạng nơ-ron có nhiều lớp ẩn với số lượng đơn vị trong các lớp ẩn không quá nhiều thường cho kết quả tốt hơn với các mô hình có ít lớp ẩn và sử dụng nhiều đơn vị trong một lớp. Tuy nhiên khi tăng số lớp ẩn lên sẽ làm cho số lượng tham số cần được ước lượng tăng lên rất nhanh, khiến cho mô hình mạng nơ-ron rất dễ rơi vào tình trạng overfitting, nghĩa là sai số trên tập dữ liệu huấn luyện mô hình nhỏ nhưng sai số trên dữ liệu kiểm tra mô hình lại rất lớn. Chính vì thế, trong quá trình ước lượng tham số, người xây dựng mô hình thường sử dụng thêm các ràng buộc tham số, chẳng hạn như ràng buộc tham số kiểu hồi quy ridge. Nghĩa là tổn thất của mô hình tính bằng cross-entropy sẽ được điều chỉnh để cân bằng giữa phương sai và độ lệch của mô hình. Chúng ta sẽ thảo luận về vấn đề này trong phần ước lượng tham số cho mạng nơ-ron.","code":""},{"path":"neuralnetwork.html","id":"nnestimation","chapter":"Chương 12 Mô hình mạng nơ-ron","heading":"12.3 Ước lượng tham số của mạng nơ-ron","text":"Tham số của mạng nơ-ron được chia thành hai nhóm:Nhóm thứ nhất bao gồm các siêu tham số như số lượng lớp ẩn trong cấu trúc mạng và trong mỗi lớp ẩn có bao nhiêu đơn vị. Chẳng hạn như cấu trúc được mô tả trong hình 12.4 có hai lớp ẩn, lớp ẩn thứ nhất có 512 đơn vị, lớp ẩn thứ hai có 256 đơn vị. Trong trường hợp chúng ta có sử dụng ràng buộc tham số, chúng ta có thêm một tham số điều chỉnh sự đánh đổi giữa sai lệch và phương sai giống như tham số \\(\\lambda\\) trong hồi quy ridge.Nhóm thứ nhất bao gồm các siêu tham số như số lượng lớp ẩn trong cấu trúc mạng và trong mỗi lớp ẩn có bao nhiêu đơn vị. Chẳng hạn như cấu trúc được mô tả trong hình 12.4 có hai lớp ẩn, lớp ẩn thứ nhất có 512 đơn vị, lớp ẩn thứ hai có 256 đơn vị. Trong trường hợp chúng ta có sử dụng ràng buộc tham số, chúng ta có thêm một tham số điều chỉnh sự đánh đổi giữa sai lệch và phương sai giống như tham số \\(\\lambda\\) trong hồi quy ridge.Với mỗi lựa chọn cho các tham số trong nhóm thứ nhất, chúng ta có các tham số để tính toán cấu trúc mạng bao gồm các ma trận \\(\\boldsymbol{w}\\) và ma trận \\(\\boldsymbol{\\beta}\\) được định nghĩa trong phương trình (12.15). Quá trình ước lượng các tham số này là quá trình giải bài toán tối thiểu hóa hàm tổn thất dạng tổng sai số bình phương trong bài toán hồi quy hoặc hàm cross-entropy trong bài toán phân loại. Nhìn chung, không thể tính toán được lời giải chính xác cho bài toán tối ưu mà chúng ta sẽ phải ước lượng tham số bằng các phương pháp giải số, mà cụ thể là phương pháp stochastic gradient descent. đó, chúng ta thường phải xác định định thêm các tham số như tốc độ học, số lượng dữ liệu được sử dụng trong mỗi bước tính toán, hay số vòng lặp của thuật toán gradient descent.Với mỗi lựa chọn cho các tham số trong nhóm thứ nhất, chúng ta có các tham số để tính toán cấu trúc mạng bao gồm các ma trận \\(\\boldsymbol{w}\\) và ma trận \\(\\boldsymbol{\\beta}\\) được định nghĩa trong phương trình (12.15). Quá trình ước lượng các tham số này là quá trình giải bài toán tối thiểu hóa hàm tổn thất dạng tổng sai số bình phương trong bài toán hồi quy hoặc hàm cross-entropy trong bài toán phân loại. Nhìn chung, không thể tính toán được lời giải chính xác cho bài toán tối ưu mà chúng ta sẽ phải ước lượng tham số bằng các phương pháp giải số, mà cụ thể là phương pháp stochastic gradient descent. đó, chúng ta thường phải xác định định thêm các tham số như tốc độ học, số lượng dữ liệu được sử dụng trong mỗi bước tính toán, hay số vòng lặp của thuật toán gradient descent.Lựa chọn tham số trong nhóm thứ nhất có thể ảnh hưởng lớn đến kết quả của mô hình mạng nơ-ron, nhưng lại không có phương pháp chính xác nào để xác định các tham số này. Nếu nguồn lực tính toán cho phép, các tham số này sẽ được xác định bằng cách thử nghiệm và lựa chọn. Nếu nguồn lực tính toán không cho phép, người xây dựng mô hình thường lựa chọn các tham số này dựa trên kinh nghiệm và cấu trúc mạng đã có sẵn trên các bộ dữ liệu tương tự.Trong phần này, chúng tôi sẽ tập trung vào các tham số cần được ước lượng trong nhóm thứ hai, nghĩa là tập trung vào ước lượng các ma trận \\(\\boldsymbol{w}\\) và ma trận \\(\\boldsymbol{\\beta}\\) khi chúng ta đã có một cấu trúc mạng cụ thể.","code":""},{"path":"neuralnetwork.html","id":"ước-lượng-tham-số-cho-mạng-nơ-ron-hồi-quy-có-một-lớp-ẩn","chapter":"Chương 12 Mô hình mạng nơ-ron","heading":"12.3.1 Ước lượng tham số cho mạng nơ-ron hồi quy có một lớp ẩn","text":"Quá trình ước lượng mạng nơ-ron đòi hỏi kiến thức và các kỹ thuật toán học khá phức tạp và chúng tôi sẽ cố gắng chỉ trình bày tổng quan và ngắn gọn. Bạn đọc cảm thấy khó khăn về phần này này có thể yên tâm bỏ qua và chuyển sang phần tiếp theo bởi chúng ta có thể sử dụng các thư viện như hay để ước lượng mô hình mà không cần hiểu quá sâu về các chi tiết kỹ thuật trong quy trình xây dựng mô hình.Chúng ta sẽ bắt đầu bằng mô hình mạng nơ-ron hồi quy có một lớp ẩn duy nhất được trình bày trong hình 12.1 và phương trình (12.1). Để mô hình giữ nguyên tính tổng quát, chúng tôi sử dụng \\(p\\) là số tham số đầu vào, \\(k\\) là số lượng đơn vị trong lớp ẩn. Chúng ta cần tìm các tham số là véc-tơ \\(\\boldsymbol{\\beta}\\) và ma trận \\(\\boldsymbol{w}\\) để tối thiểu hóa tổng sai số bình phương:\\[\\begin{align}\nRSS\\left( \\boldsymbol{\\beta}, \\boldsymbol{w} \\right) & = \\cfrac{1}{2} \\ \\sum\\limits_{=1}^n \\ \\left(y_i - f_{\\boldsymbol{\\beta}, \\boldsymbol{w}}\\left(\\textbf{x}_i\\right) \\right)^2 \\\\\n& = \\cfrac{1}{2} \\sum\\limits_{=1}^n \\ \\left(y_i - \\beta_0 - \\sum\\limits_{t = 1}^k \\beta_t \\cdot g\\left(z_{t,}\\right) \\right)^2\n\\tag{12.16}\n\\end{align}\\]\nvới\n\\[\\begin{align}\nz_{,t} = w_{t,0} + \\sum\\limits_{j=1}^p w_{t,j} \\cdot x_{,j}\n\\tag{12.17}\n\\end{align}\\]\nMặc dù hàm \\(RSS\\left( \\boldsymbol{\\beta}, \\boldsymbol{w} \\right)\\) trong phương trình (12.16) không quá phức tạp, nhưng để giải bài toán tối thiểu hóa hàm số này trên dữ liệu \\((\\textbf{x}_i,y_i)\\) với \\(= 1, 2, \\cdots, n\\) và hàm số \\(g\\) cho trước không phải là một nhiệm vụ dễ dàng. Trước hết, mặc dù chúng ta có dạng hàm tường minh cho các tham số, nhưng đây không phải là hàm số lồi theo các tham số, đó quá trình giải bài toán tối ưu thường chỉ cho đáp số là một điểm cực tiểu địa phương chứ không chắc chắn là điểm cực tiểu toàn cục. Thứ hai, không thể có lời giải chính xác cho bài toán tối ưu nên chúng ta sẽ cần tìm lời giải số, trong trường hợp này là phương pháp gradient descent. Việc lựa chọn các tham số cho thuật toán này cũng sẽ là câu hỏi cần được giải đáp cho quá trình xây dựng mô hình. Thứ ba, mô hình mạng nơ-ron có rất nhiều tham số, đó rất dễ dẫn đến hiện tượng mô hình quá khớp với dữ liệu huấn luyện mô hình. Hàm mục tiêu thường sẽ bằng RSS cộng thêm một hàm phạt, khiến cho quá trình giải số trở nên khó khăn hơn.Trước tiên, giả sử bài toán tối ưu trong (12.16) không có ràng buộc tham số, chúng ta cần xác định gradient của RSS theo \\(\\boldsymbol{\\beta}\\) và \\(\\boldsymbol{w}\\). Để đơn giản hóa, chúng ta giả sử bình phương của sai số thứ \\(\\) là \\(RSS_i\\)\n\\[\\begin{align}\nRSS_i = \\cfrac{1}{2} \\ \\left[y_i - \\beta_0 - \\sum\\limits_{t = 1}^k \\beta_t \\cdot g\\left(z_{,t}\\right) \\right]^2\n\\end{align}\\]Đạo hàm của bình phương sai số thứ \\(\\) theo \\(\\beta_l\\), với \\(0 \\leq l \\leq k\\), được xác định như sau\n\\[\\begin{align}\n\\cfrac{\\partial RSS_i}{\\partial \\beta_l} & = \\begin{cases}\n-\\left(y_i - f\\left(\\textbf{x}_i\\right) \\right) & \\text{ nếu } l = 0 \\\\\n-g\\left(z_{,l}\\right)  \\left(y_i - f\\left(\\textbf{x}_i\\right) \\right) & \\text{ nếu } l > 0\n\\end{cases}\n\\tag{12.18}\n\\end{align}\\]Đạo hàm của bình phương sai số thứ \\(\\) theo \\(\\beta_l\\), với \\(0 \\leq l \\leq k\\), được xác định như sau\n\\[\\begin{align}\n\\cfrac{\\partial RSS_i}{\\partial \\beta_l} & = \\begin{cases}\n-\\left(y_i - f\\left(\\textbf{x}_i\\right) \\right) & \\text{ nếu } l = 0 \\\\\n-g\\left(z_{,l}\\right)  \\left(y_i - f\\left(\\textbf{x}_i\\right) \\right) & \\text{ nếu } l > 0\n\\end{cases}\n\\tag{12.18}\n\\end{align}\\]Đạo hàm của bình phương sai số thứ \\(\\) theo \\(w_{l,j}\\), với \\(1 \\leq l \\leq k\\) và \\(0 \\leq j \\leq p\\), được xác định như sauĐạo hàm của bình phương sai số thứ \\(\\) theo \\(w_{l,j}\\), với \\(1 \\leq l \\leq k\\) và \\(0 \\leq j \\leq p\\), được xác định như sau\\[\\begin{align}\n\\cfrac{\\partial RSS_i}{\\partial w_{l,j}} & = - \\cfrac{\\partial  \\beta_l \\cdot g\\left(z_{,l}\\right) }{\\partial w_{l,j}} \\left(y_i - f\\left(\\textbf{x}_i\\right) \\right) \\\\\n& =\n\\begin{cases}\n- \\beta_l \\cdot g^{'}\\left(z_{,l}\\right) \\cdot \\left(y_i - f\\left(\\textbf{x}_i\\right) \\right) & \\text{ nếu } j = 0 \\\\\n- \\beta_l \\cdot x_{,j} \\cdot g^{'}\\left(z_{,l}\\right) \\cdot \\left(y_i - f\\left(\\textbf{x}_i\\right) \\right) & \\text{ nếu } j > 0\n\\end{cases}\n\\tag{12.19}\n\\end{align}\\]Trước tiên, có thể thấy rằng cả hai biểu thức đạo hàm của sai số bình phương này đều chứa phần dư \\(\\left(y_i − f(\\textbf{x})\\right)\\). Trong công thức (12.18) chúng ta thấy rằng giá trị tuyệt đối của gradient theo các \\(\\beta_l\\) bằng phần dư nhân với giá trị \\(g\\left(z_{,l}\\right)\\), chính là giá trị tại nút \\(H_l\\) tính theo đầu vào \\(\\textbf{x}_i\\). Tiếp theo, trong công thức (12.19) chúng ta thấy sự thay đổi của RSS theo tham số \\(w_{l,j}\\), tương ứng với hệ số của đầu vào \\(X_j\\) khi tính toán đơn vị \\(H_l\\) của mạng nơ-ron một lớp ẩn, cũng phụ thuộc vào phần dư. Sự ảnh hưởng của phần dư lên đạo hàm theo từng tham số của mô hình được gọi là quá trình lan truyền ngược trong mô hình mạng nơ-ron.Các công thức đạo hàm ở trên luôn yêu cầu tính toán giá trị của hàm kích hoạt và đạo hàm tại các điểm \\(z_{,l}\\). Về lý thuyết, mọi hàm đơn điệu tăng, có đạo hàm, và không tuyến tính đều có thể được sử dụng làm hàm kích hoạt. Tuy nhiên, nếu dạng hàm quá phức tạp, việc tính toán sẽ trở nên phưc tạp, nhất là khi sử dụng nhiều lớp ẩn và trong mỗi lớp ẩn có nhiều đơn vị. Điều này giải thích tại sao hàm ReLU thường xuyên được sử dụng làm hàm kích hoạt để tính toán gradient là đơn giản nhất có thể. Giả sử hàm \\(g\\) trong các công thức (12.18) và (12.19) là hàm ReLU, chúng ta có thể đơn giản hóa các đạo hàm như sau:Đạo hàm theo hệ số \\(\\beta_l\\):\\[\\begin{align}\n\\cfrac{\\partial RSS_i}{\\partial \\beta_l} & = \\begin{cases}\n-\\left(y_i - f\\left(\\textbf{x}_i\\right) \\right) & \\text{ nếu } l = 0 \\\\\n-\\mathbb{}(z_{,l} > 0) \\cdot z_{,l} \\cdot \\left(y_i - f\\left(\\textbf{x}_i\\right) \\right) & \\text{ nếu } l > 0\n\\end{cases}\n\\tag{12.20}\n\\end{align}\\]Đạo hàm theo \\(w_{l,0}\\),\\[\\begin{align}\n  \\cfrac{\\partial RSS_i}{\\partial w_{l,0}} & = - \\beta_l \\cdot \\mathbb{}(z_{,l} > 0) \\cdot \\left(y_i - f\\left(\\textbf{x}_i\\right) \\right)\n  \\tag{12.21}\n\\end{align}\\]Đạo hàm theo \\(w_{l,j}\\), với \\(j > 0\\) thì\\[\\begin{align}\n\\cfrac{\\partial RSS_i}{\\partial w_{l,j}} & =\n- \\beta_l \\cdot \\mathbb{}(z_{,l} > 0) \\cdot x_{,j} \\cdot \\left(y_i - f\\left(\\textbf{x}_i\\right) \\right)\n\\tag{12.22}\n\\end{align}\\]Gradient của tổng các \\(RSS_i\\) được xác định như sau:Theo \\(\\beta_0\\)\\[\\begin{align}\n\\cfrac{\\partial RSS}{\\partial \\beta_0} &= \\sum\\limits_{=1}^n \\cfrac{\\partial RSS_i}{\\partial \\beta_0} \\\\\n& = - \\sum\\limits_{=1}^n \\left(y_i - f\\left(\\textbf{x}_i\\right) \\right)\n\\tag{12.23}\n\\end{align}\\]Theo \\(\\beta_l\\) với \\(l > 0\\)\\[\\begin{align}\n\\cfrac{\\partial RSS}{\\partial \\beta_l} &= \\sum\\limits_{=1}^n \\cfrac{\\partial RSS_i}{\\partial \\beta_l} \\\\\n& = - \\sum\\limits_{=1}^n \\mathbb{}(z_{,l} > 0) \\cdot z_{,l} \\cdot \\left(y_i - f\\left(\\textbf{x}_i\\right) \\right)\n\\tag{12.24}\n\\end{align}\\]Theo \\(w_{l,0}\\),\\[\\begin{align}\n\\cfrac{\\partial RSS_i}{\\partial w_{l,0}} & =  - \\beta_l \\sum\\limits_{=1}^n \\mathbb{}(z_{,l} > 0) \\cdot \\left(y_i - f\\left(\\textbf{x}_i\\right) \\right)\n\\tag{12.25}\n\\end{align}\\]Theo \\(w_{l,j}\\) với j > 0\n\\[\\begin{align}\n\\cfrac{\\partial RSS_i}{\\partial w_{l,j}} & = - \\beta_l \\sum\\limits_{=1}^n \\mathbb{}(z_{,l} > 0) \\cdot x_{,j} \\cdot \\left(y_i - f\\left(\\textbf{x}_i\\right) \\right)\n\\tag{12.26}\n\\end{align}\\]Sau khi tính toán gradient của RSS theo các tham số, quá trình ước lượng sẽ được thực hiện thông qua thuật toán Stochastic gradient descent. Bạn đọc tham khảo thuật toán này trong Phụ lục ?? của chương Kiến thức R nâng cao. Kết quả của thuật toán Stochastic gradient descent phụ thuộc rất lớn vào giá trị khởi tạo ban đầu của các tham số, và nhất là khi số lượng tham số là rất lớn.Để mô tả quá trình ước lượng tham số của mạng nơ-ron, chúng ta sẽ sử dụng dữ liệu mô phỏng. Ma trận biến giải thích \\(\\textbf{X}\\) có kích thước \\(10^4 \\times 3\\) là các số ngẫu nhiên độc lập có phân phối chuẩn \\(\\mathcal{N}(0,1)\\). Hàm \\(f\\) được tạo bởi mô hình mạng nơ-ron với 3 đơn vị của lớp đầu vào, một lớp ẩn với 5 đơn vị, và một đơn vị trong lớp đầu đầu ra. Hàm kích hoạt được sử dụng là hàm ReLU. Biến giải thích \\(Y\\) được tính toán từ hàm \\(f(\\textbf{X})\\) công thêm một sai số độc lập với \\(\\textbf{X}\\) là một biến ngẫu nhiên phân phối chuẩn với trung bình bằng 0 và độ lệch chuẩn là 0.5. Các tham số dùng để tính toán \\(f\\) được đơn giản hóa: \\(\\beta_l = 2 \\forall l = 0, 1, \\cdots, 5\\) và $w_{l,j} = 1 l = 1, 2, , $ và $j = , $. Quá trình tối thiểu hóa tổng sai số bình phương được mô tả thông qua hình 12.5\nFigure 12.5: Sai số của mô hình mạng nơ-ron giảm dần trong quá trình ước lượng tham số sử dụng thuật toán stochastic gradient descent. Hình bên trái: điểm bắt đầu của các tham số là biến ngẫu nhiên phân phối chuẩn độc lập có trung bình bằng 0 và phương sai bằng 3. Hình bên phải: điểm bắt đầu của các tham số expression{beta} là biến ngẫu nhiên có trung bình bằng 2 và phương sai bằng 1. Điểm bắt đầu của các tham số w là biến ngẫu nhiên có trung bình bằng 1 và phương sai bằng 1\nHình 12.5 mô tả quá trình tối thiểu hóa sai số tính bằng RSS trên dữ liệu mô phỏng bằng phương pháp stochastic gradient descent. Tổng số tham số cần được ước lượng của mô hình là 26, bao gồm 6 giá trị của véc-tơ \\(\\boldsymbol{\\beta}\\) và 20 giá trị của ma trận \\(\\boldsymbol{w}\\). Chúng tôi sử dụng 5 nghìn lần lặp và trong mỗi lần lặp sử dụng 5% dữ liệu để tính các gradient.Hình bên trái mô tả 10 quá trình ước lượng tham số mà các điểm bắt đầu của \\(\\boldsymbol{\\beta}\\) và \\(\\boldsymbol{w}\\) là hoàn toàn ngẫu nhiên. Chúng tôi cho các giá trị ban đầu là các biến ngẫu nhiên phân phối chuẩn với trung bình bằng 0 và phương sai bằng 3. Hình bên phải mô tả 10 quá trình ước lượng tham số với các điểm bắt đầu của \\(\\boldsymbol{\\beta}\\) có giá trị trung bình là 2 bằng với giá trị dùng để mô phỏng dữ liệu và \\(\\boldsymbol{w}\\) có giá trị trung bình là 1 cũng bằng với giá trị dùng để mô phỏng dữ liệu. Phương sai của 26 tham số khởi đầu đều bằng 1. Có thể thấy rằng khi các tham số khởi đầu là hoàn toàn ngẫu nhiên thì về trung bình các quá trình hội tụ về giá ngưỡng nho nhất của RSS chậm hơn khi chúng ta có các giá trị khởi đầu tốt hơn. Có hai trên tám quá trình sau 5000 bước lặp vẫn chưa cho sai số tiệm cận đến giá trị RSS nhỏ nhất. Trong hình bên phải thì các quá trình đều cho kết quả gần với giá trị RSS nhỏ nhất.","code":""},{"path":"neuralnetwork.html","id":"ước-lượng-tham-số-cho-mạng-nơ-ron-phân-loại-có-hai-lớp-ẩn","chapter":"Chương 12 Mô hình mạng nơ-ron","heading":"12.3.2 Ước lượng tham số cho mạng nơ-ron phân loại có hai lớp ẩn","text":"Giả sử cấu trúc mạng nơ-ron có hai lớp ẩn như hình 12.4. Các tham số cần ước lượng của mô hình bao gồm các ma trận \\(\\boldsymbol{w}_1\\), \\(\\boldsymbol{w}_2\\), và \\(\\boldsymbol{\\beta}\\) được cho bởi phương trình (12.15). đây là bài toán phân loại nên hàm mục tiêu được sử dụng là hàm cross-entropy\n\\[\\begin{align}\n(\\hat{\\boldsymbol{w}}_1, \\hat{\\boldsymbol{w}}_2, \\hat{\\boldsymbol{\\beta}}) &= \\underset{\\boldsymbol{w}_1, \\boldsymbol{w}_2, \\boldsymbol{\\beta}}{\\operatorname{argmin}} \\sum\\limits_{=1}^n \\sum\\limits_{j=1}^m \\ y^{}_{j} \\cdot \\log(\\hat{y}^{}_{j}) \\\\\n\\tag{12.27}\n\\end{align}\\]Giá trị của hàm cross-entropy tính trên quan sát thứ \\(\\) có thể được rút gọn như sau\n\\[\\begin{align}\n\\sum\\limits_{j=1}^m \\ y^{}_{j} \\cdot \\log(\\hat{y}^{}_{j}) &= \\sum\\limits_{j=1}^m \\ y^{}_{j} \\cdot \\log\\left(\\cfrac{exp\\left(z_{,j}\\right)}{exp\\left(z_{,1}\\right)+exp\\left(z_{,2}\\right)+\\cdots+exp\\left(z_{,m}\\right)}\\right) \\\\\n& = \\sum\\limits_{j=1}^m \\ y^{}_{j} \\cdot \\left(z_{,j} -  \\log\\left(exp\\left(z_{,1}\\right)+exp\\left(z_{,2}\\right)+\\cdots+exp\\left(z_{,m}\\right)\\right) \\right)\\\\\n& =  \\sum\\limits_{j=1}^m \\ y^{}_{j} \\cdot z_{,j} - \\log\\left(\\sum\\limits_{j=1}^m exp\\left(z_{,j}\\right)\\right)\n\\tag{12.28}\n\\end{align}\\]\nvới \\(z_{,j}\\) là tổ hợp tuyến tính của giá trị các nút ẩn thứ hai tính theo đầu vào \\(\\textbf{x}_i\\)\n\\[\\begin{align}\nz_{,j} = \\beta_{j,0} + \\beta_{j,1} h^{(2)}_{,1} + \\beta_{j,2} h^{(2)}_{,2} + \\cdots + \\beta_{j,k_2} h^{(2)}_{,k_2}\n\\tag{12.29}\n\\end{align}\\]Với mọi tham số \\(\\theta\\) được sử dụng để tính toán giá trị tại các nút đầu ra, đạo hàm của hàm cross-entropy sẽ được tính toán thông qua các \\(z_{,j}\\)\n\\[\\begin{align}\n\\cfrac{\\partial CE\\_Loss_i}{\\partial \\theta} &= \\sum\\limits_{j=1}^m \\ y^{}_{j} \\ \\cfrac{\\partial z_{,j}}{\\partial \\theta} - \\sum\\limits_{j=1}^m \\cfrac{\\partial z_{,j}}{\\partial \\theta} \\cfrac{exp(z_{,j})}{\\sum exp\\left(z_{,j}\\right)} \\\\\n& = \\sum\\limits_{j=1}^m  \\cfrac{\\partial z_{,j}}{\\partial \\theta} \\left(y^{}_{j} - p_{,j}\\right)\n\\tag{12.30}\n\\end{align}\\]\ntrong đó\n\\[\\begin{align}\np_{,j} = \\cfrac{exp(z_{,j})}{\\sum\\limits_{j=1}^m exp\\left(z_{,j}\\right)}\n\\tag{12.31}\n\\end{align}\\]Từ công thức (12.29) có thể thấy rằng: véc-tơ dữ liệu đầu ra \\(\\textbf{y}_i = (y^{}_{1}, y^{}_{2}, \\cdots, y^{}_{m})\\) nhận giá trị bằng 1 tại một vị trí và nhận giá trị bằng 0 tại \\(m-1\\) vị trí còn lại, trong khi đó \\(p_{,j}\\) có thể được hiểu là xác suất mà đầu ra thứ \\(\\) nhận giá trị bằng \\(j\\) được xác định bởi mô hình mạng nơ-ron. Đạo hàm của hàm tổn thất tính bằng cross-entropy theo tham số \\(\\theta\\) bất kỳ, là một phần tử của ma trận \\(\\boldsymbol{\\beta}\\) hoặc các \\(\\boldsymbol{w}\\), phụ thuộc vào sai số giữa véc-tơ xác suất từ dữ liệu quan sát được \\(\\textbf{y}^\\) và véc-tơ xác suất được xác định bởi mô hình mạng nơ-ron \\(\\textbf{p}_{} = (p_{,1}, p_{,2}, \\cdot, p_{,m})\\). Như vậy, gradient của hàm tổn thất theo từng tham số phụ thuộc vào sai số của mô hình hiện tại. Nói một cách khác, sai số của mô hình trong bước hiện tại sẽ tác động đến việc cập nhật tham số trong bước tiếp theo khi chúng ta sử dụng phương pháp gradient descent. Đây là quá trình lan truyền ngược trong mô hình mạng nơ-ron mà chúng tôi đã đề cập đến trong phần mô hình mạng nơ-ron hồi quy có một lớp ẩn.Quá trình tính toán đạo hàm của \\(z_{,j}\\) theo các tham số của ma trận \\(\\boldsymbol{\\beta}\\) là khá hiển nhiên \\(z_{,j}\\) là hàm tuyến tính theo các tham số này. Để tính toán đạo hàm của \\(z_{,j}\\) theo các tham số của các ma trận \\(\\boldsymbol{w}\\), chúng ta sử dụng nguyên tắc chain-rule đã trình bày trong phụ lục của phần Kiến thức R nâng cao. chỉ số của các tham số trong ma trận là quá phức tạp nên chúng tôi sẽ chỉ trình bày cách tính đạo hàm mang tính tổng quát. Ta có mỗi \\(z\\) được tính từ phương trình (12.29) được xác định từ dữ liệu đầu vào \\(x\\) thông qua các hàm kích hoạt \\(g_1\\) và \\(g_2\\) như sau\n\\[\\begin{align}\nz & = \\beta h^{(2)} \\\\\nh^{(2)} &= g_2(w^{(2)}\\cdot h^{(1)}) \\\\\nh^{(1)} &= g_1(w^{(1)}\\cdot x)\n\\tag{12.32}\n\\end{align}\\]Trong đó \\(\\beta\\), \\(w^{(2)}\\), và \\(w^{(1)}\\) là các tham số của mô hình. Giá trị đạo hàm của \\(z\\) theo các tham số được xác định như sauTheo \\(\\beta\\)\n\\[\\begin{align}\n\\cfrac{\\partial z}{\\partial \\beta} = h^{(2)}\n\\tag{12.33}\n\\end{align}\\]Theo \\(\\beta\\)\n\\[\\begin{align}\n\\cfrac{\\partial z}{\\partial \\beta} = h^{(2)}\n\\tag{12.33}\n\\end{align}\\]Theo \\(w^{(2)}\\), chúng ta áp dụng nguyên tắc chain-rule\n\\[\\begin{align}\n\\cfrac{\\partial z}{\\partial w^{(2)}} & = \\cfrac{\\partial z}{\\partial h^{(2)}} \\times \\cfrac{\\partial h^{(2)}}{\\partial w^{(2)}} \\\\\n& =  \\beta \\cdot h^{(1)} \\cdot  g^{'}_2(w^{(2)}\\cdot h^{(1)})\n\\tag{12.34}\n\\end{align}\\]Theo \\(w^{(2)}\\), chúng ta áp dụng nguyên tắc chain-rule\n\\[\\begin{align}\n\\cfrac{\\partial z}{\\partial w^{(2)}} & = \\cfrac{\\partial z}{\\partial h^{(2)}} \\times \\cfrac{\\partial h^{(2)}}{\\partial w^{(2)}} \\\\\n& =  \\beta \\cdot h^{(1)} \\cdot  g^{'}_2(w^{(2)}\\cdot h^{(1)})\n\\tag{12.34}\n\\end{align}\\]Theo \\(w^{(1)}\\), áp dụng nguyên tắc chain-rule\n\\[\\begin{align}\n\\cfrac{\\partial z}{\\partial w^{(1)}} & = \\cfrac{\\partial z}{\\partial h^{(2)}} \\times \\cfrac{\\partial h^{(2)}}{\\partial h^{(1)}} \\times \\cfrac{\\partial h^{(1)}}{\\partial w^{(1)}} \\\\\n& =  \\beta \\cdot w^{(2)} \\cdot g^{'}_2(w^{(2)}\\cdot h^{(1)}) \\cdot x \\cdot g^{'}_1(w^{(1)}\\cdot x)\n\\tag{12.35}\n\\end{align}\\]Theo \\(w^{(1)}\\), áp dụng nguyên tắc chain-rule\n\\[\\begin{align}\n\\cfrac{\\partial z}{\\partial w^{(1)}} & = \\cfrac{\\partial z}{\\partial h^{(2)}} \\times \\cfrac{\\partial h^{(2)}}{\\partial h^{(1)}} \\times \\cfrac{\\partial h^{(1)}}{\\partial w^{(1)}} \\\\\n& =  \\beta \\cdot w^{(2)} \\cdot g^{'}_2(w^{(2)}\\cdot h^{(1)}) \\cdot x \\cdot g^{'}_1(w^{(1)}\\cdot x)\n\\tag{12.35}\n\\end{align}\\]Số lượng tham số cần được tính toán trong mô hình mạng nơ-ron phân loại với \\(p\\) đầu vào, \\(m\\) đầu ra, hai lớp ẩn với số lượng đơn vị lần lượt là \\(k_1\\) và \\(k_2\\) là\\(m \\cdot (k_2+1)\\) tham số trong ma trận \\(\\boldsymbol{\\beta}\\),\\(m \\cdot (k_2+1)\\) tham số trong ma trận \\(\\boldsymbol{\\beta}\\),\\(k_2 \\cdot (k_1 + 1)\\) tham số trong ma trận \\(\\boldsymbol{w}_2\\)\\(k_2 \\cdot (k_1 + 1)\\) tham số trong ma trận \\(\\boldsymbol{w}_2\\)\\(k_1 \\cdot (p + 1)\\) tham số trong ma trận \\(\\boldsymbol{w}_1\\)\\(k_1 \\cdot (p + 1)\\) tham số trong ma trận \\(\\boldsymbol{w}_1\\)Số lượng tham số quá lớn sẽ dẫn đến các vấn đề bao gồm sự phức tạp khi tiến hành tính toán, khối lượng tính toán lớn, và rất dễ dẫn đến overfitting. Để khắc phục vấn đề này, mô hình mạng nơ-ron thường phải sử dụng thêm các kỹ thuật để thêm ràng buộc tham số hoặc loại bỏ đơn vị trong các lớp. Chúng ta sẽ thảo luận các kỹ thuật này trong phần tiếp theo.","code":""},{"path":"neuralnetwork.html","id":"khắc-phục-hiện-tượng-khớp-quá-mức-của-mạng-nơ-ron","chapter":"Chương 12 Mô hình mạng nơ-ron","heading":"12.3.3 Khắc phục hiện tượng khớp quá mức của mạng nơ-ron","text":"Phương pháp trước hết để hạn chế mô hình khớp quá mức là sử dụng ràng buộc tham số. Nguyên tắc ràng buộc tham số trong mô hình mạng nơ-ron cũng tương tự trong hồi quy ridge. Ví dụ, với mô hình mạng nơ-ron hồi quy được trình bày trong phần 12.1 hàm mục tiêu cần được tối thiểu hóa là tổng bình phương sai số cộng thêm một hàm phạt có dạng tổng bình phương các tham số sử dụng trong mô hình.\\[\\begin{align}\n(\\hat{\\boldsymbol{\\beta}},\\hat{\\boldsymbol{w}}) &= \\underset{\\boldsymbol{\\beta},\\boldsymbol{w}}{\\operatorname{argmin}} &= \\sum\\limits_{=1}^n \\ \\left(y_i - f_{\\boldsymbol{\\beta}, \\boldsymbol{w}}\\left(\\textbf{x}_i\\right) \\right)^2 + \\lambda_1 \\cdot \\sum\\limits_{j} \\beta_j^2 + \\lambda_2 \\sum\\limits_{l,j} w_{l,j}^2\n\\tag{12.36}\n\\end{align}\\]Trong mô hình mạng nơ-ron phân loại có hai lớp ẩn trong phần 12.2, hàm mục tiêu tính bằng cross-entropy cũng được biến đổi bằng cách thêm vào một hàm phạt dạng tổng bình phương của các tham số:Người xây dựng mô hình có thể sử dụng các hàm phạt khác như hàm tổng giá trị tuyệt đối trong hồi quy Lasso, hoặc kết hợp giữa Lasso và ridge. Một lưu ý khác khi sử dụng ràng buộc tham số đó là người xây dựng mô hình thường không sử dụng ràng buộc tham số trực tiếp tính lớp đầu ra, nghĩa là thường cho \\(\\lambda_1\\) trong các phương trình (12.36) và (??) bằng 0. Với mỗi giá trị của các tham số \\(\\lambda\\), chúng ta tiến hành ước lượng tham số của mạng nơ-ron giống như đã trình bày trong phần 12.3.Một phương pháp khác để giảm bớt hiện tượng khớp quá mức của mạng nơ-ron là phương pháp loại bỏ đơn vị (unit dropout). Các đơn vị của lớp đầu vào và các lớp ẩn có thể tham gia vào mô hình mạng nơ-ron với xác suất là \\(p\\) và không tham gia vào mô hình với xác suất \\((1-p)\\) trong mỗi bước của quá trình huấn luyện mô hình, trong đó \\(p\\) là tham số của kỹ thuật dropout. Ý tưởng của phương pháp này hoàn toàn tương tự như ý tưởng của thuật toán rừng ngẫu nhiên áp dụng trên cây quyết định. Quá trình ước lượng tham số của mô hình mạng nơ-ron bao gồm hai quá trình: quá trình chuyển tiếp từ đầu vào, qua các lớp ẩn, và kết thúc tại lớp đầu ra, và quá trình lan truyền ngược, khi sai số tính tại lớp đầu ra được sử dụng để tính toán sự thay đổi cho tất cả các tham số của mô hình hiện tại. Nếu trong tất cả các lần chuyển tiếp và lan truyền ngược, chúng ta giữ nguyên cấu trúc của mạng, trong mỗi lớp đầu vào hoặc lớp ẩn có thể có một hoặc một số đơn vị chiếm ưu thế với các đơn vị khác, làm cho kết quả tại đầu ra phụ thuộc rất lớn vào các đơn vị này. Cũng giống như ý tưởng của thuật toán rừng ngẫu nhiên, trong mỗi lần chuyển tiếp và lan truyền ngược tương ứng, tại lớp đầu vào và các lớp ẩn, người xây dựng mô hình chỉ lựa chọn một số ngẫu nhiên các đơn vị vào trong quá trình tính toán tham số. Nói cách khác, mỗi đơn vị được lựa chọn vào quá trình tính toán với xác suất là \\(p\\). Chúng tôi sẽ mô tả cách thực hiện kỹ thuật dropout trong phần thực hành.Hàng thứ hai trong Bảng 10.1 được dán nhãn dropout. Đây là một dạng chính quy hóa tương đối mới bị loại bỏ và hiệu quả, tương tự ở một số khía cạnh với chính quy hóa đường gờ. Lấy cảm hứng từ các khu rừng ngẫu nhiên (Phần 8.2), ý tưởng là loại bỏ ngẫu nhiên một phần φ của các đơn vị trong một lớp khi tạo mô hình. Hình 10.19 minh họa điều này. Việc này được thực hiện riêng biệt mỗi khi xử lý quan sát đào tạo. Các đơn vị còn sống thay thế cho những đơn vị bị thiếu và trọng số của chúng được tăng lên theo hệ số 1/(1 − φ) để bù đắp. Điều này ngăn các nút trở nên chuyên biệt hóa quá mức và có thể được coi là một hình thức chính quy hóa. Trong thực tế, việc bỏ học đạt được bằng cách đặt ngẫu nhiên các kích hoạt cho các đơn vị “bị bỏ” về 0, trong khi vẫn giữ nguyên kiến trúc.","code":""},{"path":"neuralnetwork.html","id":"pickands-dependent-function","chapter":"Chương 12 Mô hình mạng nơ-ron","heading":"12.4 Pickands dependent function","text":"\\[\\begin{align}\nC(u,v) = exp\\left( log(uv) \\cdot \\left(\\cfrac{log(v)}{log(u)+log(v)} \\right) \\right)\n\\end{align}\\]Đạo hàm của \\(C(u,v)\\) tính theo \\(\\)\n\\[\\begin{align}\n\\cfrac{\\partial C(u,v)}{\\partial u} &= C(u,v) \\times \\cfrac{1}{u} \\cdot \\left((t) - ^{'}(t) \\cdot t \\right) \\\\\n\\cfrac{\\partial C(u,v)}{\\partial v} &= C(u,v) \\times \\cfrac{1}{v} \\cdot \\left((t) + ^{'}(t) \\cdot (1-t) \\right) \\\\\n\\end{align}\\]\n\\(t = \\cfrac{log(v)}{log(u)+log(v)}\\)Density của \\(C(u,v)\\)\n\\[\\begin{align}\n\\cfrac{\\partial^2 C(u,v)}{\\partial u \\partial v}  &= C(u,v) \\times \\cfrac{1}{uv} \\cdot \\left[ \\left((t) - ^{'}(t) \\cdot t \\right) \\cdot \\left((t) + ^{'}(t) \\cdot (1-t) \\right) - \\cfrac{^{''}(t) t(1-t)}{log(uv)} \\right]\\\\\n\\end{align}\\]","code":""},{"path":"neuralnetwork.html","id":"nhóm-các-hàm-at-để-cuv-là-một-copula","chapter":"Chương 12 Mô hình mạng nơ-ron","heading":"12.5 Nhóm các hàm A(t) để C(u,v) là một copula","text":"Các điều kiện biên: \\(C(0,v) = 0\\), \\(C(u,v) = 0\\), \\(C(1,v) = v\\), và \\(C(u,1) = u\\)\nKhi \\(u \\rightarrow 0\\) thì \\(t \\rightarrow 0\\) và khi \\(v \\rightarrow 0\\) thì \\(t \\rightarrow 1\\)\nCác điều kiện biên: \\(C(0,v) = 0\\), \\(C(u,v) = 0\\), \\(C(1,v) = v\\), và \\(C(u,1) = u\\)Khi \\(u \\rightarrow 0\\) thì \\(t \\rightarrow 0\\) và khi \\(v \\rightarrow 0\\) thì \\(t \\rightarrow 1\\)\\[\\begin{align}\nC(0,v) = exp\\left( log(0 \\cdot v) \\cdot \\left(0 \\right) \\right) = 0^1 = 0\n\\end{align}\\]\\[\\begin{align}\nC(u,0) = exp\\left( log(u \\cdot 0) \\cdot \\left(0 \\right) \\right) = 0^1 = 0\n\\end{align}\\]Khi \\(u \\rightarrow 1\\) thì \\(t \\rightarrow 1\\) và khi \\(v \\rightarrow 1\\) thì \\(t \\rightarrow 0\\)\\[\\begin{align}\nC(1,v) = exp\\left( log(1 \\cdot v) \\cdot \\left(1 \\right) \\right) = v^1 = v\n\\end{align}\\]\\[\\begin{align}\nC(u,1) = exp\\left( log(u \\cdot 1) \\cdot \\left(0 \\right) \\right) = u^1 = u\n\\end{align}\\]Các hàm \\(C_u(u,v)\\) và \\(C_v(u,v)\\) là hàm phân phối xác suất: \\(C_u(u,v)\\) là hàm phân phối xác suất của biến \\(V\\) với mọi \\(u\\): Khi \\(v \\rightarrow 0\\),\\[\\begin{align}\nC(u,v) \\times \\cfrac{1}{u} \\cdot \\left((t) - ^{'}(t) \\cdot t \\right) &\\rightarrow C(u,0) \\times \\cfrac{1}{u} \\cdot \\left((1) - ^{'}(1) \\cdot 1 \\right)\\\\\n& = 0 \\times \\cfrac{1}{u} \\cdot \\left((1) - ^{'}(1) \\right) \\\\\n& = 0\n\\end{align}\\]Khi \\(v \\rightarrow 1\\),\\[\\begin{align}\nC(u,v) \\times \\cfrac{1}{u} \\cdot \\left((t) - ^{'}(t) \\cdot t \\right) &\\rightarrow C(u,1) \\times \\cfrac{1}{u} \\cdot \\left((0) - ^{'}(0) \\cdot 0 \\right)\\\\\n& = 1 \\times 1 \\\\\n& = 1\n\\end{align}\\]Đạo hàm của hàm \\(C_u(u,v)\\) theo \\(v\\) là hàm tăng theo \\(v\\): Nếu \\(^{''}(t) \\geq 0\\) \\(\\forall t\\) thì\\[\\begin{align}\nC(u,v) \\times \\cfrac{1}{uv} \\cdot \\left[ \\left((t) - ^{'}(t) \\cdot t \\right) \\cdot \\left((t) + ^{'}(t) \\cdot (1-t) \\right) - \\cfrac{^{''}(t) t(1-t)}{log(uv)} \\right] \\geq 0\n\\end{align}\\]\nvới mọi \\(u,v\\)","code":""},{"path":"neuralnetwork.html","id":"cách-thứ-nhất-để-tham-số-hóa-đa-thức-từng-phần","chapter":"Chương 12 Mô hình mạng nơ-ron","heading":"12.5.1 Cách thứ nhất để tham số hóa đa thức từng phần","text":"Hàm \\(f\\) là đa thức từng phần thỏa mãn điều kiện thành pickand dependent function\n\\[\\begin{align}\n\\theta \\[0.5,1] \\\\\n\\lambda_1 \\geq \\theta \\\\\n\\lambda_2 = \\cfrac{ \\left(\\cfrac{\\theta^3}{6}+\\lambda_1 \\cdot \\cfrac{\\theta^2}{2}\\right) \\cdot \\theta - \\left(\\theta+\\lambda_1\\right) \\cdot \\left(\\cfrac{\\theta^3}{6}-\\cfrac{\\theta}{2}+\\cfrac{1}{3}\\right)  }{\\cfrac{(1-\\theta)^2\\cdot(\\theta+\\lambda_1)}{2} - \\left(\\cfrac{\\theta^3}{6}+\\lambda_1 \\cdot \\cfrac{\\theta^2}{2}\\right)}\n\\end{align}\\]","code":""},{"path":"neuralnetwork.html","id":"cách-thứ-hai","chapter":"Chương 12 Mô hình mạng nơ-ron","heading":"12.5.2 Cách thứ hai","text":"Chọn điểm cắt \\(\\theta\\), lựa chọn hàm \\((t)\\) như sau\n\\[\\begin{align}\n(t) = \\mathbb{}_{(t \\leq \\theta)} \\left(a_0 + a_1 \\cdot x + a_2 \\cdot x^2 + a_3 \\cdot x^3\\right) +\n\\mathbb{}_{(t > \\theta)} \\left(b_0 + b_1 \\cdot (1-x) + b_2 \\cdot (1-x)^2 + b_3 \\cdot (1-x)^3\\right)\n\\end{align}\\]\nvới các ràng buộc:Hàm \\((t)\\) có đạo hàm cấp 0,1,2 liên tục tại \\(\\theta\\)\\[\\begin{align}\n& \\begin{pmatrix}\n(1-\\theta) & (1-\\theta)^2 & (1-\\theta)^3 \\\\\n- 1 & - 2\\cdot(1 - \\theta) & -3(1-\\theta)^2 \\\\\n0 & 1 & 3(1-\\theta)\\\\\n\\end{pmatrix} \\times\n\\begin{pmatrix}\nb_1\\\\\nb_2\\\\\nb_3\\\\\n\\end{pmatrix} =\n\\begin{pmatrix}\n\\theta & \\theta^2 & \\theta^3 \\\\\n1 & 2\\theta & 3\\theta^2 \\\\\n0 & 1 & 3\\theta\\\\\n\\end{pmatrix} \\times\n\\begin{pmatrix}\na_1\\\\\na_2\\\\\na_3\\\\\n\\end{pmatrix} \\\\\n& \\\\\n\\rightarrow &\n\\begin{pmatrix}\n(1-\\theta) & (1-\\theta)^2 & (1-\\theta)^3 \\\\\n- 1 & - 2\\cdot(1 - \\theta) & -3(1-\\theta)^2 \\\\\n0 & 1 & 3(1-\\theta)\\\\\n\\end{pmatrix}^{-1}\n\\times \\begin{pmatrix}\n\\theta & \\theta^2 & \\theta^3 \\\\\n1 & 2\\theta & 3\\theta^2 \\\\\n0 & 1 & 3\\theta\\\\\n\\end{pmatrix} \\times\n\\begin{pmatrix}\na_1\\\\\na_2\\\\\na_3\\\\\n\\end{pmatrix} = \\begin{pmatrix}\nb_1\\\\\nb_2\\\\\nb_3\\\\\n\\end{pmatrix}\n& \\\\\n\\rightarrow &\n\\begin{pmatrix}\n\\cfrac{3}{1-\\theta} & 2 & 1 - \\theta \\\\\n- \\cfrac{3}{(1-\\theta)^2} & - \\cfrac{3}{1-\\theta} & -2 \\\\\n\\cfrac{1}{(1-\\theta)^3} & \\cfrac{1}{(1-\\theta)^2} & \\cfrac{1}{(1-\\theta)}\\\\\n\\end{pmatrix}\n\\times \\begin{pmatrix}\n\\theta & \\theta^2 & \\theta^3 \\\\\n1 & 2\\theta & 3\\theta^2 \\\\\n0 & 1 & 3\\theta\\\\\n\\end{pmatrix} \\times\n\\begin{pmatrix}\na_1\\\\\na_2\\\\\na_3\\\\\n\\end{pmatrix} = \\begin{pmatrix}\nb_1\\\\\nb_2\\\\\nb_3\\\\\n\\end{pmatrix}\n& \\\\\n\\rightarrow &\n\\begin{pmatrix}\n\\cfrac{2+\\theta}{1-\\theta} & \\cfrac{2\\theta+1}{1-\\theta} & \\cfrac{3\\theta}{1 - \\theta} \\\\\n- \\cfrac{3}{(1-\\theta)^2} & \\cfrac{\\theta^2-2\\theta-2}{(1-\\theta)^2} & \\cfrac{3\\theta^2-6\\theta}{(1-\\theta)^2} \\\\\n\\cfrac{1}{(1-\\theta)^3} & \\cfrac{1}{(1-\\theta)^3} & \\cfrac{1 - (1-\\theta)^3}{(1-\\theta)^3}\\\\\n\\end{pmatrix}\n\\times\n\\begin{pmatrix}\na_1\\\\\na_2\\\\\na_3\\\\\n\\end{pmatrix} = \\begin{pmatrix}\nb_1\\\\\nb_2\\\\\nb_3\\\\\n\\end{pmatrix}\n\n\n\\end{align}\\]","code":""},{"path":"neuralnetwork.html","id":"thực-hành","chapter":"Chương 12 Mô hình mạng nơ-ron","heading":"12.6 Thực hành:","text":"","code":""},{"path":"neuralnetwork.html","id":"mô-hình-mạng-nơ-ron-trên-dữ-liệu-boston","chapter":"Chương 12 Mô hình mạng nơ-ron","heading":"12.6.1 Mô hình mạng nơ-ron trên dữ liệu Boston","text":"","code":""},{"path":"neuralnetwork.html","id":"mô-hình-mạng-nơ-ron-để-phân-loại-khách-hàng","chapter":"Chương 12 Mô hình mạng nơ-ron","heading":"12.6.2 Mô hình mạng nơ-ron để phân loại khách hàng","text":"","code":""},{"path":"neuralnetwork.html","id":"phụ-lục-5","chapter":"Chương 12 Mô hình mạng nơ-ron","heading":"12.7 Phụ lục","text":"","code":""},{"path":"neuralnetwork.html","id":"bài-tập-6","chapter":"Chương 12 Mô hình mạng nơ-ron","heading":"12.8 Bài tập","text":"","code":""}]
